Script started on 2022-04-24 12:35:20-04:00 [TERM="xterm-256color" TTY="/dev/pts/3" COLUMNS="203" LINES="53"]
bash: devel/setup.bash: No such file or directory
]0;hjkwon@HJK-AI-Robotics: ~/scripts/stable_baselines/pandaAndrew/2022-04-24[01;32mhjkwon@HJK-AI-Robotics[00m:[01;34m~/scripts/stable_baselines/pandaAndrew/2022-04-24[00m$ cd ..
]0;hjkwon@HJK-AI-Robotics: ~/scripts/stable_baselines/pandaAndrew[01;32mhjkwon@HJK-AI-Robotics[00m:[01;34m~/scripts/stable_baselines/pandaAndrew[00m$ ls
[0m[01;34m11April2022[0m  [01;34m20April2022[0m        [01;34mlogs[0m                            mainOptVectSparse_V00.py   [01;34moptunaCustVect[0m               pandaWrapperBW.py                      register.py         [01;34mtmpTestMulti[0m
[01;34m13April2022[0m  [01;34m21April2022[0m        mainFullMultiVectSparse_V00.py  modelLoader.py             optunaCustVectDense_V00.py   pandaWrapper.py                        render.py           [01;34mvideos[0m
[01;34m14April2022[0m  [01;34m6April2022[0m         mainFull_V00.py                 multiCompare.py            optunaCustVectSparse_V00.py  pandaWrapperVect_BW_D_0.py             robottaskenv.py
[01;34m16April2022[0m  check              mainFull_V01.py                 optimize_V01.py            pandaCust.py                 pandaWrapperVect_BW_D_0_Supervised.py  [01;34mruns[0m
[01;34m17April2022[0m  customCnn_V01.py   mainFull_V02.py                 optimize_V02.py            pandagraspdepth.py           pickPlaceVectOptV01.py                 [01;34msupervisedLearning[0m
[01;34m18April2022[0m  customCnn_V02.py   mainMulti_V00.py                optimizeVectDense_V00.py   pandaGrasp.py                PPANDPUSH_VECT.py                      testScript.py
[01;34m19April2022[0m  GPU_monitor.log    mainOpt_V01.py                  optimizeVectSparse_V00.py  pandaGraspTestPID.ipynb      pybulletCust.py                        timer.py
[01;34m2022-04-22[0m   [01;34mimages[0m             mainOpt_V02.py                  optunaCust_V01.py          pandareachdepth.py           [01;34m__pycache__[0m                            [01;34mtmp[0m
[01;34m2022-04-24[0m   linearSchedule.py  mainOptVectDense_V00.py         optunaCust_V02.py          pandaWrapperBW_D_0.py        RAM_monitor.log                        [01;34mtmpMulti[0m
]0;hjkwon@HJK-AI-Robotics: ~/scripts/stable_baselines/pandaAndrew[01;32mhjkwon@HJK-AI-Robotics[00m:[01;34m~/scripts/stable_baselines/pandaAndrew[00m$ python3 mainFullV[KMultiVectSparse_V00.py [K
pybullet build time: Dec  1 2021 18:34:28
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
_XSERVTransSocketUNIXCreateListener: ...SocketCreateListener() failed
_XSERVTransMakeAllCOTSServerListeners: server already running
(EE) 
Fatal server error:
(EE) Cannot establish any listening sockets - Make sure an X server isn't already running(EE) 
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
Using cuda device
^CTraceback (most recent call last):
  File "/home/hjkwon/scripts/stable_baselines/pandaAndrew/mainFullMultiVectSparse_V00.py", line 294, in <module>
    model = PPO(env=vec_env, **kwargs)
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py", line 152, in __init__
    self._setup_model()
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py", line 155, in _setup_model
    super(PPO, self)._setup_model()
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 127, in _setup_model
    self.policy = self.policy.to(self.device)
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 908, in to
    return self._apply(convert)
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 579, in _apply
    module._apply(fn)
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 579, in _apply
    module._apply(fn)
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 579, in _apply
    module._apply(fn)
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 602, in _apply
    param_applied = fn(param)
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 906, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
KeyboardInterrupt
Exception ignored in: <function BulletClient.__del__ at 0x7ff66f42bee0>
Traceback (most recent call last):
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/pybullet_utils/bullet_client.py", line 39, in __del__
AttributeError: 'NoneType' object has no attribute 'getpid'
Exception ignored in: <function BulletClient.__del__ at 0x7ff66f42bee0>
Traceback (most recent call last):
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/pybullet_utils/bullet_client.py", line 39, in __del__
AttributeError: 'NoneType' object has no attribute 'getpid'
Exception ignored in: <function BulletClient.__del__ at 0x7ff66f42bee0>
Traceback (most recent call last):
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/pybullet_utils/bullet_client.py", line 39, in __del__
AttributeError: 'NoneType' object has no attribute 'getpid'

]0;hjkwon@HJK-AI-Robotics: ~/scripts/stable_baselines/pandaAndrew[01;32mhjkwon@HJK-AI-Robotics[00m:[01;34m~/scripts/stable_baselines/pandaAndrew[00m$ python3 mainFullMultiVectSparse_V00.py[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[Kconda activate rl_)e[K[Kenv
(rl_env) ]0;hjkwon@HJK-AI-Robotics: ~/scripts/stable_baselines/pandaAndrew[01;32mhjkwon@HJK-AI-Robotics[00m:[01;34m~/scripts/stable_baselines/pandaAndrew[00m$ conda activate rl_envpython3 mainFullMultiVectSparse_V00.py
pybullet build time: Dec  1 2021 18:34:28
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
_XSERVTransSocketUNIXCreateListener: ...SocketCreateListener() failed
_XSERVTransMakeAllCOTSServerListeners: server already running
(EE) 
Fatal server error:
(EE) Cannot establish any listening sockets - Make sure an X server isn't already running(EE) 
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
Using cuda device

 
 
 

-----------------------------------------------------------------------------------
TRAINING OPTIMIZED STUDY

 
 
 

Num timesteps: 3000
Best mean reward: -inf - Last mean reward per episode: -47.60
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 6000
Best mean reward: -47.60 - Last mean reward per episode: -47.52
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 9000
Best mean reward: -47.52 - Last mean reward per episode: -47.95
Num timesteps: 12000
Best mean reward: -47.52 - Last mean reward per episode: -47.48
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 15000
Best mean reward: -47.48 - Last mean reward per episode: -47.54
Num timesteps: 18000
Best mean reward: -47.48 - Last mean reward per episode: -47.88
Num timesteps: 21000
Best mean reward: -47.48 - Last mean reward per episode: -47.38
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 24000
Best mean reward: -47.38 - Last mean reward per episode: -47.99
Num timesteps: 27000
Best mean reward: -47.38 - Last mean reward per episode: -48.96
Num timesteps: 30000
Best mean reward: -47.38 - Last mean reward per episode: -48.70
Num timesteps: 33000
Best mean reward: -47.38 - Last mean reward per episode: -48.61
Num timesteps: 36000
Best mean reward: -47.38 - Last mean reward per episode: -47.17
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 39000
Best mean reward: -47.17 - Last mean reward per episode: -46.23
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 42000
Best mean reward: -46.23 - Last mean reward per episode: -46.96
Num timesteps: 45000
Best mean reward: -46.23 - Last mean reward per episode: -48.89
Num timesteps: 48000
Best mean reward: -46.23 - Last mean reward per episode: -49.12
Num timesteps: 51000
Best mean reward: -46.23 - Last mean reward per episode: -47.60
Num timesteps: 54000
Best mean reward: -46.23 - Last mean reward per episode: -48.48
Num timesteps: 57000
Best mean reward: -46.23 - Last mean reward per episode: -47.30
Num timesteps: 60000
Best mean reward: -46.23 - Last mean reward per episode: -48.15
Num timesteps: 63000
Best mean reward: -46.23 - Last mean reward per episode: -48.42
Num timesteps: 66000
Best mean reward: -46.23 - Last mean reward per episode: -47.00
Num timesteps: 69000
Best mean reward: -46.23 - Last mean reward per episode: -46.94
Num timesteps: 72000
Best mean reward: -46.23 - Last mean reward per episode: -46.26
Num timesteps: 75000
Best mean reward: -46.23 - Last mean reward per episode: -47.28
Num timesteps: 78000
Best mean reward: -46.23 - Last mean reward per episode: -47.34
Num timesteps: 81000
Best mean reward: -46.23 - Last mean reward per episode: -48.21
Num timesteps: 84000
Best mean reward: -46.23 - Last mean reward per episode: -49.33
Num timesteps: 87000
Best mean reward: -46.23 - Last mean reward per episode: -49.06
Num timesteps: 90000
Best mean reward: -46.23 - Last mean reward per episode: -48.16
Num timesteps: 93000
Best mean reward: -46.23 - Last mean reward per episode: -48.93
Num timesteps: 96000
Best mean reward: -46.23 - Last mean reward per episode: -48.73
Num timesteps: 99000
Best mean reward: -46.23 - Last mean reward per episode: -48.34
Num timesteps: 102000
Best mean reward: -46.23 - Last mean reward per episode: -49.04
Num timesteps: 105000
Best mean reward: -46.23 - Last mean reward per episode: -49.09
Num timesteps: 108000
Best mean reward: -46.23 - Last mean reward per episode: -47.90
Num timesteps: 111000
Best mean reward: -46.23 - Last mean reward per episode: -47.74
Num timesteps: 114000
Best mean reward: -46.23 - Last mean reward per episode: -47.79
Num timesteps: 117000
Best mean reward: -46.23 - Last mean reward per episode: -48.86
Num timesteps: 120000
Best mean reward: -46.23 - Last mean reward per episode: -47.62
Num timesteps: 123000
Best mean reward: -46.23 - Last mean reward per episode: -46.90
Num timesteps: 126000
Best mean reward: -46.23 - Last mean reward per episode: -47.73
Num timesteps: 129000
Best mean reward: -46.23 - Last mean reward per episode: -47.95
Num timesteps: 132000
Best mean reward: -46.23 - Last mean reward per episode: -48.13
Num timesteps: 135000
Best mean reward: -46.23 - Last mean reward per episode: -48.28
Num timesteps: 138000
Best mean reward: -46.23 - Last mean reward per episode: -47.86
Num timesteps: 141000
Best mean reward: -46.23 - Last mean reward per episode: -46.61
Num timesteps: 144000
Best mean reward: -46.23 - Last mean reward per episode: -47.18
Num timesteps: 147000
Best mean reward: -46.23 - Last mean reward per episode: -45.86
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 150000
Best mean reward: -45.86 - Last mean reward per episode: -46.04
Num timesteps: 153000
Best mean reward: -45.86 - Last mean reward per episode: -48.00
Num timesteps: 156000
Best mean reward: -45.86 - Last mean reward per episode: -47.39
Num timesteps: 159000
Best mean reward: -45.86 - Last mean reward per episode: -46.09
Num timesteps: 162000
Best mean reward: -45.86 - Last mean reward per episode: -46.94
Num timesteps: 165000
Best mean reward: -45.86 - Last mean reward per episode: -48.48
Num timesteps: 168000
Best mean reward: -45.86 - Last mean reward per episode: -47.20
Num timesteps: 171000
Best mean reward: -45.86 - Last mean reward per episode: -47.10
Num timesteps: 174000
Best mean reward: -45.86 - Last mean reward per episode: -47.91
Num timesteps: 177000
Best mean reward: -45.86 - Last mean reward per episode: -47.59
Num timesteps: 180000
Best mean reward: -45.86 - Last mean reward per episode: -46.92
Num timesteps: 183000
Best mean reward: -45.86 - Last mean reward per episode: -47.53
Num timesteps: 186000
Best mean reward: -45.86 - Last mean reward per episode: -48.26
Num timesteps: 189000
Best mean reward: -45.86 - Last mean reward per episode: -48.19
Num timesteps: 192000
Best mean reward: -45.86 - Last mean reward per episode: -47.21
Num timesteps: 195000
Best mean reward: -45.86 - Last mean reward per episode: -47.57
Num timesteps: 198000
Best mean reward: -45.86 - Last mean reward per episode: -46.92
Num timesteps: 201000
Best mean reward: -45.86 - Last mean reward per episode: -46.29
Num timesteps: 204000
Best mean reward: -45.86 - Last mean reward per episode: -46.75
Num timesteps: 207000
Best mean reward: -45.86 - Last mean reward per episode: -46.58
Num timesteps: 210000
Best mean reward: -45.86 - Last mean reward per episode: -46.30
Num timesteps: 213000
Best mean reward: -45.86 - Last mean reward per episode: -48.28
Num timesteps: 216000
Best mean reward: -45.86 - Last mean reward per episode: -48.40
Num timesteps: 219000
Best mean reward: -45.86 - Last mean reward per episode: -47.65
Num timesteps: 222000
Best mean reward: -45.86 - Last mean reward per episode: -47.71
Num timesteps: 225000
Best mean reward: -45.86 - Last mean reward per episode: -47.42
Num timesteps: 228000
Best mean reward: -45.86 - Last mean reward per episode: -47.77
Num timesteps: 231000
Best mean reward: -45.86 - Last mean reward per episode: -48.32
Num timesteps: 234000
Best mean reward: -45.86 - Last mean reward per episode: -47.94
Num timesteps: 237000
Best mean reward: -45.86 - Last mean reward per episode: -46.94
Num timesteps: 240000
Best mean reward: -45.86 - Last mean reward per episode: -47.41
Num timesteps: 243000
Best mean reward: -45.86 - Last mean reward per episode: -46.96
Num timesteps: 246000
Best mean reward: -45.86 - Last mean reward per episode: -48.18
Num timesteps: 249000
Best mean reward: -45.86 - Last mean reward per episode: -47.37
Num timesteps: 252000
Best mean reward: -45.86 - Last mean reward per episode: -46.74
Num timesteps: 255000
Best mean reward: -45.86 - Last mean reward per episode: -47.20
Num timesteps: 258000
Best mean reward: -45.86 - Last mean reward per episode: -48.49
Num timesteps: 261000
Best mean reward: -45.86 - Last mean reward per episode: -48.85
Num timesteps: 264000
Best mean reward: -45.86 - Last mean reward per episode: -47.84
Num timesteps: 267000
Best mean reward: -45.86 - Last mean reward per episode: -48.44
Num timesteps: 270000
Best mean reward: -45.86 - Last mean reward per episode: -48.75
Num timesteps: 273000
Best mean reward: -45.86 - Last mean reward per episode: -47.21
Num timesteps: 276000
Best mean reward: -45.86 - Last mean reward per episode: -46.60
Num timesteps: 279000
Best mean reward: -45.86 - Last mean reward per episode: -48.88
Num timesteps: 282000
Best mean reward: -45.86 - Last mean reward per episode: -48.25
Num timesteps: 285000
Best mean reward: -45.86 - Last mean reward per episode: -47.52
Num timesteps: 288000
Best mean reward: -45.86 - Last mean reward per episode: -47.46
Num timesteps: 291000
Best mean reward: -45.86 - Last mean reward per episode: -47.93
Num timesteps: 294000
Best mean reward: -45.86 - Last mean reward per episode: -47.89
Num timesteps: 297000
Best mean reward: -45.86 - Last mean reward per episode: -46.21
Num timesteps: 300000
Best mean reward: -45.86 - Last mean reward per episode: -45.69
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 303000
Best mean reward: -45.69 - Last mean reward per episode: -46.18
Num timesteps: 306000
Best mean reward: -45.69 - Last mean reward per episode: -48.42
Num timesteps: 309000
Best mean reward: -45.69 - Last mean reward per episode: -47.56
Num timesteps: 312000
Best mean reward: -45.69 - Last mean reward per episode: -46.70
Num timesteps: 315000
Best mean reward: -45.69 - Last mean reward per episode: -47.58
Num timesteps: 318000
Best mean reward: -45.69 - Last mean reward per episode: -48.35
Num timesteps: 321000
Best mean reward: -45.69 - Last mean reward per episode: -46.28
Num timesteps: 324000
Best mean reward: -45.69 - Last mean reward per episode: -47.22
Num timesteps: 327000
Best mean reward: -45.69 - Last mean reward per episode: -49.15
Num timesteps: 330000
Best mean reward: -45.69 - Last mean reward per episode: -48.85
Num timesteps: 333000
Best mean reward: -45.69 - Last mean reward per episode: -47.77
Num timesteps: 336000
Best mean reward: -45.69 - Last mean reward per episode: -48.47
Num timesteps: 339000
Best mean reward: -45.69 - Last mean reward per episode: -47.03
Num timesteps: 342000
Best mean reward: -45.69 - Last mean reward per episode: -46.81
Num timesteps: 345000
Best mean reward: -45.69 - Last mean reward per episode: -46.76
Num timesteps: 348000
Best mean reward: -45.69 - Last mean reward per episode: -47.22
Num timesteps: 351000
Best mean reward: -45.69 - Last mean reward per episode: -47.53
Num timesteps: 354000
Best mean reward: -45.69 - Last mean reward per episode: -48.19
Num timesteps: 357000
Best mean reward: -45.69 - Last mean reward per episode: -47.88
Num timesteps: 360000
Best mean reward: -45.69 - Last mean reward per episode: -47.55
Num timesteps: 363000
Best mean reward: -45.69 - Last mean reward per episode: -47.18
Num timesteps: 366000
Best mean reward: -45.69 - Last mean reward per episode: -47.06
Num timesteps: 369000
Best mean reward: -45.69 - Last mean reward per episode: -47.13
Num timesteps: 372000
Best mean reward: -45.69 - Last mean reward per episode: -47.95
Num timesteps: 375000
Best mean reward: -45.69 - Last mean reward per episode: -48.31
Num timesteps: 378000
Best mean reward: -45.69 - Last mean reward per episode: -48.09
Num timesteps: 381000
Best mean reward: -45.69 - Last mean reward per episode: -45.40
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 384000
Best mean reward: -45.40 - Last mean reward per episode: -46.20
Num timesteps: 387000
Best mean reward: -45.40 - Last mean reward per episode: -47.75
Num timesteps: 390000
Best mean reward: -45.40 - Last mean reward per episode: -47.46
Num timesteps: 393000
Best mean reward: -45.40 - Last mean reward per episode: -47.36
Num timesteps: 396000
Best mean reward: -45.40 - Last mean reward per episode: -48.20
Num timesteps: 399000
Best mean reward: -45.40 - Last mean reward per episode: -47.34
Num timesteps: 402000
Best mean reward: -45.40 - Last mean reward per episode: -47.06
Num timesteps: 405000
Best mean reward: -45.40 - Last mean reward per episode: -46.58
Num timesteps: 408000
Best mean reward: -45.40 - Last mean reward per episode: -47.02
Num timesteps: 411000
Best mean reward: -45.40 - Last mean reward per episode: -45.58
Num timesteps: 414000
Best mean reward: -45.40 - Last mean reward per episode: -47.86
Num timesteps: 417000
Best mean reward: -45.40 - Last mean reward per episode: -45.71
Num timesteps: 420000
Best mean reward: -45.40 - Last mean reward per episode: -46.95
Num timesteps: 423000
Best mean reward: -45.40 - Last mean reward per episode: -45.07
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 426000
Best mean reward: -45.07 - Last mean reward per episode: -45.99
Num timesteps: 429000
Best mean reward: -45.07 - Last mean reward per episode: -47.06
Num timesteps: 432000
Best mean reward: -45.07 - Last mean reward per episode: -47.81
Num timesteps: 435000
Best mean reward: -45.07 - Last mean reward per episode: -47.79
Num timesteps: 438000
Best mean reward: -45.07 - Last mean reward per episode: -45.91
Num timesteps: 441000
Best mean reward: -45.07 - Last mean reward per episode: -47.18
Num timesteps: 444000
Best mean reward: -45.07 - Last mean reward per episode: -45.05
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 447000
Best mean reward: -45.05 - Last mean reward per episode: -46.81
Num timesteps: 450000
Best mean reward: -45.05 - Last mean reward per episode: -46.91
Num timesteps: 453000
Best mean reward: -45.05 - Last mean reward per episode: -47.07
Num timesteps: 456000
Best mean reward: -45.05 - Last mean reward per episode: -47.80
Num timesteps: 459000
Best mean reward: -45.05 - Last mean reward per episode: -45.88
Num timesteps: 462000
Best mean reward: -45.05 - Last mean reward per episode: -46.02
Num timesteps: 465000
Best mean reward: -45.05 - Last mean reward per episode: -47.02
Num timesteps: 468000
Best mean reward: -45.05 - Last mean reward per episode: -46.86
Num timesteps: 471000
Best mean reward: -45.05 - Last mean reward per episode: -47.56
Num timesteps: 474000
Best mean reward: -45.05 - Last mean reward per episode: -49.15
Num timesteps: 477000
Best mean reward: -45.05 - Last mean reward per episode: -47.30
Num timesteps: 480000
Best mean reward: -45.05 - Last mean reward per episode: -46.86
Num timesteps: 483000
Best mean reward: -45.05 - Last mean reward per episode: -46.66
Num timesteps: 486000
Best mean reward: -45.05 - Last mean reward per episode: -47.42
Num timesteps: 489000
Best mean reward: -45.05 - Last mean reward per episode: -48.59
Num timesteps: 492000
Best mean reward: -45.05 - Last mean reward per episode: -47.45
Num timesteps: 495000
Best mean reward: -45.05 - Last mean reward per episode: -45.40
Num timesteps: 498000
Best mean reward: -45.05 - Last mean reward per episode: -47.86
Num timesteps: 501000
Best mean reward: -45.05 - Last mean reward per episode: -45.17
Num timesteps: 504000
Best mean reward: -45.05 - Last mean reward per episode: -44.02
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 507000
Best mean reward: -44.02 - Last mean reward per episode: -44.47
Num timesteps: 510000
Best mean reward: -44.02 - Last mean reward per episode: -44.29
Num timesteps: 513000
Best mean reward: -44.02 - Last mean reward per episode: -44.87
Num timesteps: 516000
Best mean reward: -44.02 - Last mean reward per episode: -46.70
Num timesteps: 519000
Best mean reward: -44.02 - Last mean reward per episode: -47.49
Num timesteps: 522000
Best mean reward: -44.02 - Last mean reward per episode: -45.47
Num timesteps: 525000
Best mean reward: -44.02 - Last mean reward per episode: -47.18
Num timesteps: 528000
Best mean reward: -44.02 - Last mean reward per episode: -46.39
Num timesteps: 531000
Best mean reward: -44.02 - Last mean reward per episode: -46.22
Num timesteps: 534000
Best mean reward: -44.02 - Last mean reward per episode: -47.86
Num timesteps: 537000
Best mean reward: -44.02 - Last mean reward per episode: -46.76
Num timesteps: 540000
Best mean reward: -44.02 - Last mean reward per episode: -45.82
Num timesteps: 543000
Best mean reward: -44.02 - Last mean reward per episode: -45.08
Num timesteps: 546000
Best mean reward: -44.02 - Last mean reward per episode: -46.33
Num timesteps: 549000
Best mean reward: -44.02 - Last mean reward per episode: -46.92
Num timesteps: 552000
Best mean reward: -44.02 - Last mean reward per episode: -47.07
Num timesteps: 555000
Best mean reward: -44.02 - Last mean reward per episode: -47.61
Num timesteps: 558000
Best mean reward: -44.02 - Last mean reward per episode: -47.87
Num timesteps: 561000
Best mean reward: -44.02 - Last mean reward per episode: -47.25
Num timesteps: 564000
Best mean reward: -44.02 - Last mean reward per episode: -46.13
Num timesteps: 567000
Best mean reward: -44.02 - Last mean reward per episode: -45.82
Num timesteps: 570000
Best mean reward: -44.02 - Last mean reward per episode: -44.14
Num timesteps: 573000
Best mean reward: -44.02 - Last mean reward per episode: -42.61
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 576000
Best mean reward: -42.61 - Last mean reward per episode: -44.28
Num timesteps: 579000
Best mean reward: -42.61 - Last mean reward per episode: -46.14
Num timesteps: 582000
Best mean reward: -42.61 - Last mean reward per episode: -47.43
Num timesteps: 585000
Best mean reward: -42.61 - Last mean reward per episode: -46.01
Num timesteps: 588000
Best mean reward: -42.61 - Last mean reward per episode: -46.18
Num timesteps: 591000
Best mean reward: -42.61 - Last mean reward per episode: -45.23
Num timesteps: 594000
Best mean reward: -42.61 - Last mean reward per episode: -43.78
Num timesteps: 597000
Best mean reward: -42.61 - Last mean reward per episode: -44.81
Num timesteps: 600000
Best mean reward: -42.61 - Last mean reward per episode: -44.24
Num timesteps: 603000
Best mean reward: -42.61 - Last mean reward per episode: -45.15
Num timesteps: 606000
Best mean reward: -42.61 - Last mean reward per episode: -44.61
Num timesteps: 609000
Best mean reward: -42.61 - Last mean reward per episode: -41.39
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 612000
Best mean reward: -41.39 - Last mean reward per episode: -42.83
Num timesteps: 615000
Best mean reward: -41.39 - Last mean reward per episode: -44.12
Num timesteps: 618000
Best mean reward: -41.39 - Last mean reward per episode: -43.96
Num timesteps: 621000
Best mean reward: -41.39 - Last mean reward per episode: -44.60
Num timesteps: 624000
Best mean reward: -41.39 - Last mean reward per episode: -42.85
Num timesteps: 627000
Best mean reward: -41.39 - Last mean reward per episode: -44.55
Num timesteps: 630000
Best mean reward: -41.39 - Last mean reward per episode: -44.39
Num timesteps: 633000
Best mean reward: -41.39 - Last mean reward per episode: -43.08
Num timesteps: 636000
Best mean reward: -41.39 - Last mean reward per episode: -43.65
Num timesteps: 639000
Best mean reward: -41.39 - Last mean reward per episode: -43.81
Num timesteps: 642000
Best mean reward: -41.39 - Last mean reward per episode: -44.68
Num timesteps: 645000
Best mean reward: -41.39 - Last mean reward per episode: -43.83
Num timesteps: 648000
Best mean reward: -41.39 - Last mean reward per episode: -44.17
Num timesteps: 651000
Best mean reward: -41.39 - Last mean reward per episode: -44.84
Num timesteps: 654000
Best mean reward: -41.39 - Last mean reward per episode: -45.10
Num timesteps: 657000
Best mean reward: -41.39 - Last mean reward per episode: -44.08
Num timesteps: 660000
Best mean reward: -41.39 - Last mean reward per episode: -43.55
Num timesteps: 663000
Best mean reward: -41.39 - Last mean reward per episode: -41.93
Num timesteps: 666000
Best mean reward: -41.39 - Last mean reward per episode: -42.37
Num timesteps: 669000
Best mean reward: -41.39 - Last mean reward per episode: -43.11
Num timesteps: 672000
Best mean reward: -41.39 - Last mean reward per episode: -42.06
Num timesteps: 675000
Best mean reward: -41.39 - Last mean reward per episode: -44.68
Num timesteps: 678000
Best mean reward: -41.39 - Last mean reward per episode: -43.65
Num timesteps: 681000
Best mean reward: -41.39 - Last mean reward per episode: -43.48
Num timesteps: 684000
Best mean reward: -41.39 - Last mean reward per episode: -44.48
Num timesteps: 687000
Best mean reward: -41.39 - Last mean reward per episode: -43.55
Num timesteps: 690000
Best mean reward: -41.39 - Last mean reward per episode: -43.87
Num timesteps: 693000
Best mean reward: -41.39 - Last mean reward per episode: -41.58
Num timesteps: 696000
Best mean reward: -41.39 - Last mean reward per episode: -39.15
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 699000
Best mean reward: -39.15 - Last mean reward per episode: -42.88
Num timesteps: 702000
Best mean reward: -39.15 - Last mean reward per episode: -42.25
Num timesteps: 705000
Best mean reward: -39.15 - Last mean reward per episode: -42.09
Num timesteps: 708000
Best mean reward: -39.15 - Last mean reward per episode: -42.12
Num timesteps: 711000
Best mean reward: -39.15 - Last mean reward per episode: -41.93
Num timesteps: 714000
Best mean reward: -39.15 - Last mean reward per episode: -41.35
Num timesteps: 717000
Best mean reward: -39.15 - Last mean reward per episode: -41.45
Num timesteps: 720000
Best mean reward: -39.15 - Last mean reward per episode: -40.65
Num timesteps: 723000
Best mean reward: -39.15 - Last mean reward per episode: -41.86
Num timesteps: 726000
Best mean reward: -39.15 - Last mean reward per episode: -43.05
Num timesteps: 729000
Best mean reward: -39.15 - Last mean reward per episode: -41.28
Num timesteps: 732000
Best mean reward: -39.15 - Last mean reward per episode: -41.77
Num timesteps: 735000
Best mean reward: -39.15 - Last mean reward per episode: -41.08
Num timesteps: 738000
Best mean reward: -39.15 - Last mean reward per episode: -43.49
Num timesteps: 741000
Best mean reward: -39.15 - Last mean reward per episode: -43.72
Num timesteps: 744000
Best mean reward: -39.15 - Last mean reward per episode: -43.81
Num timesteps: 747000
Best mean reward: -39.15 - Last mean reward per episode: -38.98
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 750000
Best mean reward: -38.98 - Last mean reward per episode: -40.17
Num timesteps: 753000
Best mean reward: -38.98 - Last mean reward per episode: -40.95
Num timesteps: 756000
Best mean reward: -38.98 - Last mean reward per episode: -40.94
Num timesteps: 759000
Best mean reward: -38.98 - Last mean reward per episode: -41.66
Num timesteps: 762000
Best mean reward: -38.98 - Last mean reward per episode: -43.04
Num timesteps: 765000
Best mean reward: -38.98 - Last mean reward per episode: -43.02
Num timesteps: 768000
Best mean reward: -38.98 - Last mean reward per episode: -43.10
Num timesteps: 771000
Best mean reward: -38.98 - Last mean reward per episode: -41.23
Num timesteps: 774000
Best mean reward: -38.98 - Last mean reward per episode: -40.21
Num timesteps: 777000
Best mean reward: -38.98 - Last mean reward per episode: -40.64
Num timesteps: 780000
Best mean reward: -38.98 - Last mean reward per episode: -42.63
Num timesteps: 783000
Best mean reward: -38.98 - Last mean reward per episode: -41.86
Num timesteps: 786000
Best mean reward: -38.98 - Last mean reward per episode: -41.98
Num timesteps: 789000
Best mean reward: -38.98 - Last mean reward per episode: -40.83
Num timesteps: 792000
Best mean reward: -38.98 - Last mean reward per episode: -39.00
Num timesteps: 795000
Best mean reward: -38.98 - Last mean reward per episode: -41.59
Num timesteps: 798000
Best mean reward: -38.98 - Last mean reward per episode: -41.21
Num timesteps: 801000
Best mean reward: -38.98 - Last mean reward per episode: -37.69
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 804000
Best mean reward: -37.69 - Last mean reward per episode: -39.11
Num timesteps: 807000
Best mean reward: -37.69 - Last mean reward per episode: -35.52
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 810000
Best mean reward: -35.52 - Last mean reward per episode: -37.15
Num timesteps: 813000
Best mean reward: -35.52 - Last mean reward per episode: -38.86
Num timesteps: 816000
Best mean reward: -35.52 - Last mean reward per episode: -40.04
Num timesteps: 819000
Best mean reward: -35.52 - Last mean reward per episode: -40.08
Num timesteps: 822000
Best mean reward: -35.52 - Last mean reward per episode: -40.68
Num timesteps: 825000
Best mean reward: -35.52 - Last mean reward per episode: -40.56
Num timesteps: 828000
Best mean reward: -35.52 - Last mean reward per episode: -43.22
Num timesteps: 831000
Best mean reward: -35.52 - Last mean reward per episode: -39.73
Num timesteps: 834000
Best mean reward: -35.52 - Last mean reward per episode: -41.07
Num timesteps: 837000
Best mean reward: -35.52 - Last mean reward per episode: -40.44
Num timesteps: 840000
Best mean reward: -35.52 - Last mean reward per episode: -36.23
Num timesteps: 843000
Best mean reward: -35.52 - Last mean reward per episode: -37.83
Num timesteps: 846000
Best mean reward: -35.52 - Last mean reward per episode: -37.20
Num timesteps: 849000
Best mean reward: -35.52 - Last mean reward per episode: -39.18
Num timesteps: 852000
Best mean reward: -35.52 - Last mean reward per episode: -41.51
Num timesteps: 855000
Best mean reward: -35.52 - Last mean reward per episode: -39.95
Num timesteps: 858000
Best mean reward: -35.52 - Last mean reward per episode: -41.10
Num timesteps: 861000
Best mean reward: -35.52 - Last mean reward per episode: -36.69
Num timesteps: 864000
Best mean reward: -35.52 - Last mean reward per episode: -37.18
Num timesteps: 867000
Best mean reward: -35.52 - Last mean reward per episode: -37.56
Num timesteps: 870000
Best mean reward: -35.52 - Last mean reward per episode: -40.12
Num timesteps: 873000
Best mean reward: -35.52 - Last mean reward per episode: -40.34
Num timesteps: 876000
Best mean reward: -35.52 - Last mean reward per episode: -42.68
Num timesteps: 879000
Best mean reward: -35.52 - Last mean reward per episode: -40.06
Num timesteps: 882000
Best mean reward: -35.52 - Last mean reward per episode: -39.37
Num timesteps: 885000
Best mean reward: -35.52 - Last mean reward per episode: -40.06
Num timesteps: 888000
Best mean reward: -35.52 - Last mean reward per episode: -37.68
Num timesteps: 891000
Best mean reward: -35.52 - Last mean reward per episode: -34.70
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 894000
Best mean reward: -34.70 - Last mean reward per episode: -34.56
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 897000
Best mean reward: -34.56 - Last mean reward per episode: -34.88
Num timesteps: 900000
Best mean reward: -34.56 - Last mean reward per episode: -33.91
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 903000
Best mean reward: -33.91 - Last mean reward per episode: -34.79
Num timesteps: 906000
Best mean reward: -33.91 - Last mean reward per episode: -34.02
Num timesteps: 909000
Best mean reward: -33.91 - Last mean reward per episode: -38.23
Num timesteps: 912000
Best mean reward: -33.91 - Last mean reward per episode: -34.13
Num timesteps: 915000
Best mean reward: -33.91 - Last mean reward per episode: -34.35
Num timesteps: 918000
Best mean reward: -33.91 - Last mean reward per episode: -37.04
Num timesteps: 921000
Best mean reward: -33.91 - Last mean reward per episode: -40.01
Num timesteps: 924000
Best mean reward: -33.91 - Last mean reward per episode: -34.29
Num timesteps: 927000
Best mean reward: -33.91 - Last mean reward per episode: -35.21
Num timesteps: 930000
Best mean reward: -33.91 - Last mean reward per episode: -37.60
Num timesteps: 933000
Best mean reward: -33.91 - Last mean reward per episode: -35.12
Num timesteps: 936000
Best mean reward: -33.91 - Last mean reward per episode: -33.43
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 939000
Best mean reward: -33.43 - Last mean reward per episode: -34.74
Num timesteps: 942000
Best mean reward: -33.43 - Last mean reward per episode: -35.94
Num timesteps: 945000
Best mean reward: -33.43 - Last mean reward per episode: -36.60
Num timesteps: 948000
Best mean reward: -33.43 - Last mean reward per episode: -34.08
Num timesteps: 951000
Best mean reward: -33.43 - Last mean reward per episode: -36.82
Num timesteps: 954000
Best mean reward: -33.43 - Last mean reward per episode: -40.24
Num timesteps: 957000
Best mean reward: -33.43 - Last mean reward per episode: -36.45
Num timesteps: 960000
Best mean reward: -33.43 - Last mean reward per episode: -36.48
Num timesteps: 963000
Best mean reward: -33.43 - Last mean reward per episode: -37.37
Num timesteps: 966000
Best mean reward: -33.43 - Last mean reward per episode: -33.18
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 969000
Best mean reward: -33.18 - Last mean reward per episode: -35.31
Num timesteps: 972000
Best mean reward: -33.18 - Last mean reward per episode: -34.31
Num timesteps: 975000
Best mean reward: -33.18 - Last mean reward per episode: -36.34
Num timesteps: 978000
Best mean reward: -33.18 - Last mean reward per episode: -37.42
Num timesteps: 981000
Best mean reward: -33.18 - Last mean reward per episode: -35.07
Num timesteps: 984000
Best mean reward: -33.18 - Last mean reward per episode: -36.55
Num timesteps: 987000
Best mean reward: -33.18 - Last mean reward per episode: -35.16
Num timesteps: 990000
Best mean reward: -33.18 - Last mean reward per episode: -36.91
Num timesteps: 993000
Best mean reward: -33.18 - Last mean reward per episode: -32.97
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 996000
Best mean reward: -32.97 - Last mean reward per episode: -33.79
Num timesteps: 999000
Best mean reward: -32.97 - Last mean reward per episode: -34.52
Num timesteps: 1002000
Best mean reward: -32.97 - Last mean reward per episode: -32.22
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 1005000
Best mean reward: -32.22 - Last mean reward per episode: -30.77
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 1008000
Best mean reward: -30.77 - Last mean reward per episode: -33.37
Num timesteps: 1011000
Best mean reward: -30.77 - Last mean reward per episode: -34.52
Num timesteps: 1014000
Best mean reward: -30.77 - Last mean reward per episode: -30.02
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 1017000
Best mean reward: -30.02 - Last mean reward per episode: -35.07
Num timesteps: 1020000
Best mean reward: -30.02 - Last mean reward per episode: -36.00
Num timesteps: 1023000
Best mean reward: -30.02 - Last mean reward per episode: -28.96
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 1026000
Best mean reward: -28.96 - Last mean reward per episode: -32.45
Num timesteps: 1029000
Best mean reward: -28.96 - Last mean reward per episode: -33.10
Num timesteps: 1032000
Best mean reward: -28.96 - Last mean reward per episode: -34.56
Num timesteps: 1035000
Best mean reward: -28.96 - Last mean reward per episode: -33.58
Num timesteps: 1038000
Best mean reward: -28.96 - Last mean reward per episode: -35.05
Num timesteps: 1041000
Best mean reward: -28.96 - Last mean reward per episode: -34.40
Num timesteps: 1044000
Best mean reward: -28.96 - Last mean reward per episode: -33.74
Num timesteps: 1047000
Best mean reward: -28.96 - Last mean reward per episode: -29.73
Num timesteps: 1050000
Best mean reward: -28.96 - Last mean reward per episode: -33.11
Num timesteps: 1053000
Best mean reward: -28.96 - Last mean reward per episode: -34.20
Num timesteps: 1056000
Best mean reward: -28.96 - Last mean reward per episode: -35.19
Num timesteps: 1059000
Best mean reward: -28.96 - Last mean reward per episode: -33.30
Num timesteps: 1062000
Best mean reward: -28.96 - Last mean reward per episode: -33.14
Num timesteps: 1065000
Best mean reward: -28.96 - Last mean reward per episode: -31.50
Num timesteps: 1068000
Best mean reward: -28.96 - Last mean reward per episode: -32.20
Num timesteps: 1071000
Best mean reward: -28.96 - Last mean reward per episode: -30.49
Num timesteps: 1074000
Best mean reward: -28.96 - Last mean reward per episode: -32.84
Num timesteps: 1077000
Best mean reward: -28.96 - Last mean reward per episode: -31.11
Num timesteps: 1080000
Best mean reward: -28.96 - Last mean reward per episode: -31.76
Num timesteps: 1083000
Best mean reward: -28.96 - Last mean reward per episode: -33.75
Num timesteps: 1086000
Best mean reward: -28.96 - Last mean reward per episode: -30.35
Num timesteps: 1089000
Best mean reward: -28.96 - Last mean reward per episode: -28.48
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 1092000
Best mean reward: -28.48 - Last mean reward per episode: -29.65
Num timesteps: 1095000
Best mean reward: -28.48 - Last mean reward per episode: -31.59
Num timesteps: 1098000
Best mean reward: -28.48 - Last mean reward per episode: -29.10
Num timesteps: 1101000
Best mean reward: -28.48 - Last mean reward per episode: -28.58
Num timesteps: 1104000
Best mean reward: -28.48 - Last mean reward per episode: -30.81
Num timesteps: 1107000
Best mean reward: -28.48 - Last mean reward per episode: -28.31
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 1110000
Best mean reward: -28.31 - Last mean reward per episode: -26.94
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 1113000
Best mean reward: -26.94 - Last mean reward per episode: -31.16
Num timesteps: 1116000
Best mean reward: -26.94 - Last mean reward per episode: -30.58
Num timesteps: 1119000
Best mean reward: -26.94 - Last mean reward per episode: -28.61
Num timesteps: 1122000
Best mean reward: -26.94 - Last mean reward per episode: -32.79
Num timesteps: 1125000
Best mean reward: -26.94 - Last mean reward per episode: -32.39
Num timesteps: 1128000
Best mean reward: -26.94 - Last mean reward per episode: -31.70
Num timesteps: 1131000
Best mean reward: -26.94 - Last mean reward per episode: -32.30
Num timesteps: 1134000
Best mean reward: -26.94 - Last mean reward per episode: -32.49
Num timesteps: 1137000
Best mean reward: -26.94 - Last mean reward per episode: -30.59
Num timesteps: 1140000
Best mean reward: -26.94 - Last mean reward per episode: -29.77
Num timesteps: 1143000
Best mean reward: -26.94 - Last mean reward per episode: -29.53
Num timesteps: 1146000
Best mean reward: -26.94 - Last mean reward per episode: -30.24
Num timesteps: 1149000
Best mean reward: -26.94 - Last mean reward per episode: -30.59
Num timesteps: 1152000
Best mean reward: -26.94 - Last mean reward per episode: -30.39
Num timesteps: 1155000
Best mean reward: -26.94 - Last mean reward per episode: -27.62
Num timesteps: 1158000
Best mean reward: -26.94 - Last mean reward per episode: -26.76
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 1161000
Best mean reward: -26.76 - Last mean reward per episode: -32.29
Num timesteps: 1164000
Best mean reward: -26.76 - Last mean reward per episode: -27.80
Num timesteps: 1167000
Best mean reward: -26.76 - Last mean reward per episode: -31.35
Num timesteps: 1170000
Best mean reward: -26.76 - Last mean reward per episode: -35.59
Num timesteps: 1173000
Best mean reward: -26.76 - Last mean reward per episode: -30.42
Num timesteps: 1176000
Best mean reward: -26.76 - Last mean reward per episode: -28.57
Num timesteps: 1179000
Best mean reward: -26.76 - Last mean reward per episode: -29.84
Num timesteps: 1182000
Best mean reward: -26.76 - Last mean reward per episode: -27.51
Num timesteps: 1185000
Best mean reward: -26.76 - Last mean reward per episode: -25.62
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 1188000
Best mean reward: -25.62 - Last mean reward per episode: -28.08
Num timesteps: 1191000
Best mean reward: -25.62 - Last mean reward per episode: -26.08
Num timesteps: 1194000
Best mean reward: -25.62 - Last mean reward per episode: -25.42
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 1197000
Best mean reward: -25.42 - Last mean reward per episode: -28.56
Num timesteps: 1200000
Best mean reward: -25.42 - Last mean reward per episode: -30.08
Num timesteps: 1203000
Best mean reward: -25.42 - Last mean reward per episode: -31.34
Num timesteps: 1206000
Best mean reward: -25.42 - Last mean reward per episode: -26.46
Num timesteps: 1209000
Best mean reward: -25.42 - Last mean reward per episode: -27.01
Num timesteps: 1212000
Best mean reward: -25.42 - Last mean reward per episode: -29.35
Num timesteps: 1215000
Best mean reward: -25.42 - Last mean reward per episode: -25.50
Num timesteps: 1218000
Best mean reward: -25.42 - Last mean reward per episode: -28.62
Num timesteps: 1221000
Best mean reward: -25.42 - Last mean reward per episode: -24.89
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 1224000
Best mean reward: -24.89 - Last mean reward per episode: -30.86
Num timesteps: 1227000
Best mean reward: -24.89 - Last mean reward per episode: -27.44
Num timesteps: 1230000
Best mean reward: -24.89 - Last mean reward per episode: -30.24
Num timesteps: 1233000
Best mean reward: -24.89 - Last mean reward per episode: -28.47
Num timesteps: 1236000
Best mean reward: -24.89 - Last mean reward per episode: -27.33
Num timesteps: 1239000
Best mean reward: -24.89 - Last mean reward per episode: -25.60
Num timesteps: 1242000
Best mean reward: -24.89 - Last mean reward per episode: -24.85
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 1245000
Best mean reward: -24.85 - Last mean reward per episode: -27.27
Num timesteps: 1248000
Best mean reward: -24.85 - Last mean reward per episode: -27.28
Num timesteps: 1251000
Best mean reward: -24.85 - Last mean reward per episode: -26.21
Num timesteps: 1254000
Best mean reward: -24.85 - Last mean reward per episode: -28.91
Num timesteps: 1257000
Best mean reward: -24.85 - Last mean reward per episode: -29.80
Num timesteps: 1260000
Best mean reward: -24.85 - Last mean reward per episode: -28.93
Num timesteps: 1263000
Best mean reward: -24.85 - Last mean reward per episode: -28.15
Num timesteps: 1266000
Best mean reward: -24.85 - Last mean reward per episode: -26.64
Num timesteps: 1269000
Best mean reward: -24.85 - Last mean reward per episode: -25.23
Num timesteps: 1272000
Best mean reward: -24.85 - Last mean reward per episode: -24.74
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 1275000
Best mean reward: -24.74 - Last mean reward per episode: -27.10
Num timesteps: 1278000
Best mean reward: -24.74 - Last mean reward per episode: -29.54
Num timesteps: 1281000
Best mean reward: -24.74 - Last mean reward per episode: -25.25
Num timesteps: 1284000
Best mean reward: -24.74 - Last mean reward per episode: -22.01
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 1287000
Best mean reward: -22.01 - Last mean reward per episode: -21.39
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 1290000
Best mean reward: -21.39 - Last mean reward per episode: -29.13
Num timesteps: 1293000
Best mean reward: -21.39 - Last mean reward per episode: -28.58
Num timesteps: 1296000
Best mean reward: -21.39 - Last mean reward per episode: -27.07
Num timesteps: 1299000
Best mean reward: -21.39 - Last mean reward per episode: -26.55
Num timesteps: 1302000
Best mean reward: -21.39 - Last mean reward per episode: -24.01
Num timesteps: 1305000
Best mean reward: -21.39 - Last mean reward per episode: -26.10
Num timesteps: 1308000
Best mean reward: -21.39 - Last mean reward per episode: -21.48
Num timesteps: 1311000
Best mean reward: -21.39 - Last mean reward per episode: -25.33
Num timesteps: 1314000
Best mean reward: -21.39 - Last mean reward per episode: -25.62
Num timesteps: 1317000
Best mean reward: -21.39 - Last mean reward per episode: -23.81
Num timesteps: 1320000
Best mean reward: -21.39 - Last mean reward per episode: -22.19
Num timesteps: 1323000
Best mean reward: -21.39 - Last mean reward per episode: -24.96
Num timesteps: 1326000
Best mean reward: -21.39 - Last mean reward per episode: -24.36
Num timesteps: 1329000
Best mean reward: -21.39 - Last mean reward per episode: -25.51
Num timesteps: 1332000
Best mean reward: -21.39 - Last mean reward per episode: -24.34
Num timesteps: 1335000
Best mean reward: -21.39 - Last mean reward per episode: -25.71
Num timesteps: 1338000
Best mean reward: -21.39 - Last mean reward per episode: -24.87
Num timesteps: 1341000
Best mean reward: -21.39 - Last mean reward per episode: -25.12
Num timesteps: 1344000
Best mean reward: -21.39 - Last mean reward per episode: -25.13
Num timesteps: 1347000
Best mean reward: -21.39 - Last mean reward per episode: -24.87
Num timesteps: 1350000
Best mean reward: -21.39 - Last mean reward per episode: -23.87
Num timesteps: 1353000
Best mean reward: -21.39 - Last mean reward per episode: -23.25
Num timesteps: 1356000
Best mean reward: -21.39 - Last mean reward per episode: -25.12
Num timesteps: 1359000
Best mean reward: -21.39 - Last mean reward per episode: -26.53
Num timesteps: 1362000
Best mean reward: -21.39 - Last mean reward per episode: -26.71
Num timesteps: 1365000
Best mean reward: -21.39 - Last mean reward per episode: -27.06
Num timesteps: 1368000
Best mean reward: -21.39 - Last mean reward per episode: -25.76
Num timesteps: 1371000
Best mean reward: -21.39 - Last mean reward per episode: -23.80
Num timesteps: 1374000
Best mean reward: -21.39 - Last mean reward per episode: -24.87
Num timesteps: 1377000
Best mean reward: -21.39 - Last mean reward per episode: -21.20
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 1380000
Best mean reward: -21.20 - Last mean reward per episode: -20.79
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 1383000
Best mean reward: -20.79 - Last mean reward per episode: -22.79
Num timesteps: 1386000
Best mean reward: -20.79 - Last mean reward per episode: -24.31
Num timesteps: 1389000
Best mean reward: -20.79 - Last mean reward per episode: -25.10
Num timesteps: 1392000
Best mean reward: -20.79 - Last mean reward per episode: -20.65
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 1395000
Best mean reward: -20.65 - Last mean reward per episode: -20.57
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 1398000
Best mean reward: -20.57 - Last mean reward per episode: -26.77
Num timesteps: 1401000
Best mean reward: -20.57 - Last mean reward per episode: -21.29
Num timesteps: 1404000
Best mean reward: -20.57 - Last mean reward per episode: -23.45
Num timesteps: 1407000
Best mean reward: -20.57 - Last mean reward per episode: -23.77
Num timesteps: 1410000
Best mean reward: -20.57 - Last mean reward per episode: -23.58
Num timesteps: 1413000
Best mean reward: -20.57 - Last mean reward per episode: -24.89
Num timesteps: 1416000
Best mean reward: -20.57 - Last mean reward per episode: -22.36
Num timesteps: 1419000
Best mean reward: -20.57 - Last mean reward per episode: -21.37
Num timesteps: 1422000
Best mean reward: -20.57 - Last mean reward per episode: -22.56
Num timesteps: 1425000
Best mean reward: -20.57 - Last mean reward per episode: -23.90
Num timesteps: 1428000
Best mean reward: -20.57 - Last mean reward per episode: -22.00
Num timesteps: 1431000
Best mean reward: -20.57 - Last mean reward per episode: -22.51
Num timesteps: 1434000
Best mean reward: -20.57 - Last mean reward per episode: -22.59
Num timesteps: 1437000
Best mean reward: -20.57 - Last mean reward per episode: -20.10
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 1440000
Best mean reward: -20.10 - Last mean reward per episode: -18.99
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 1443000
Best mean reward: -18.99 - Last mean reward per episode: -26.13
Num timesteps: 1446000
Best mean reward: -18.99 - Last mean reward per episode: -25.06
Num timesteps: 1449000
Best mean reward: -18.99 - Last mean reward per episode: -22.08
Num timesteps: 1452000
Best mean reward: -18.99 - Last mean reward per episode: -24.43
Num timesteps: 1455000
Best mean reward: -18.99 - Last mean reward per episode: -22.73
Num timesteps: 1458000
Best mean reward: -18.99 - Last mean reward per episode: -24.65
Num timesteps: 1461000
Best mean reward: -18.99 - Last mean reward per episode: -24.61
Num timesteps: 1464000
Best mean reward: -18.99 - Last mean reward per episode: -18.25
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 1467000
Best mean reward: -18.25 - Last mean reward per episode: -21.20
Num timesteps: 1470000
Best mean reward: -18.25 - Last mean reward per episode: -23.58
Num timesteps: 1473000
Best mean reward: -18.25 - Last mean reward per episode: -21.70
Num timesteps: 1476000
Best mean reward: -18.25 - Last mean reward per episode: -18.57
Num timesteps: 1479000
Best mean reward: -18.25 - Last mean reward per episode: -21.19
Num timesteps: 1482000
Best mean reward: -18.25 - Last mean reward per episode: -20.46
Num timesteps: 1485000
Best mean reward: -18.25 - Last mean reward per episode: -16.04
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 1488000
Best mean reward: -16.04 - Last mean reward per episode: -22.85
Num timesteps: 1491000
Best mean reward: -16.04 - Last mean reward per episode: -19.99
Num timesteps: 1494000
Best mean reward: -16.04 - Last mean reward per episode: -21.81
Num timesteps: 1497000
Best mean reward: -16.04 - Last mean reward per episode: -22.05
Num timesteps: 1500000
Best mean reward: -16.04 - Last mean reward per episode: -20.41
Num timesteps: 1503000
Best mean reward: -16.04 - Last mean reward per episode: -19.87
Num timesteps: 1506000
Best mean reward: -16.04 - Last mean reward per episode: -20.72
Num timesteps: 1509000
Best mean reward: -16.04 - Last mean reward per episode: -18.04
Num timesteps: 1512000
Best mean reward: -16.04 - Last mean reward per episode: -19.53
Num timesteps: 1515000
Best mean reward: -16.04 - Last mean reward per episode: -20.65
Num timesteps: 1518000
Best mean reward: -16.04 - Last mean reward per episode: -22.45
Num timesteps: 1521000
Best mean reward: -16.04 - Last mean reward per episode: -23.32
Num timesteps: 1524000
Best mean reward: -16.04 - Last mean reward per episode: -19.51
Num timesteps: 1527000
Best mean reward: -16.04 - Last mean reward per episode: -18.75
Num timesteps: 1530000
Best mean reward: -16.04 - Last mean reward per episode: -17.93
Num timesteps: 1533000
Best mean reward: -16.04 - Last mean reward per episode: -19.72
Num timesteps: 1536000
Best mean reward: -16.04 - Last mean reward per episode: -22.18
Num timesteps: 1539000
Best mean reward: -16.04 - Last mean reward per episode: -21.52
Num timesteps: 1542000
Best mean reward: -16.04 - Last mean reward per episode: -22.19
Num timesteps: 1545000
Best mean reward: -16.04 - Last mean reward per episode: -21.06
Num timesteps: 1548000
Best mean reward: -16.04 - Last mean reward per episode: -20.80
Num timesteps: 1551000
Best mean reward: -16.04 - Last mean reward per episode: -17.64
Num timesteps: 1554000
Best mean reward: -16.04 - Last mean reward per episode: -19.55
Num timesteps: 1557000
Best mean reward: -16.04 - Last mean reward per episode: -19.59
Num timesteps: 1560000
Best mean reward: -16.04 - Last mean reward per episode: -17.75
Num timesteps: 1563000
Best mean reward: -16.04 - Last mean reward per episode: -18.19
Num timesteps: 1566000
Best mean reward: -16.04 - Last mean reward per episode: -18.24
Num timesteps: 1569000
Best mean reward: -16.04 - Last mean reward per episode: -18.58
Num timesteps: 1572000
Best mean reward: -16.04 - Last mean reward per episode: -19.47
Num timesteps: 1575000
Best mean reward: -16.04 - Last mean reward per episode: -15.26
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 1578000
Best mean reward: -15.26 - Last mean reward per episode: -16.09
Num timesteps: 1581000
Best mean reward: -15.26 - Last mean reward per episode: -16.82
Num timesteps: 1584000
Best mean reward: -15.26 - Last mean reward per episode: -15.59
Num timesteps: 1587000
Best mean reward: -15.26 - Last mean reward per episode: -20.22
Num timesteps: 1590000
Best mean reward: -15.26 - Last mean reward per episode: -17.52
Num timesteps: 1593000
Best mean reward: -15.26 - Last mean reward per episode: -16.51
Num timesteps: 1596000
Best mean reward: -15.26 - Last mean reward per episode: -16.66
Num timesteps: 1599000
Best mean reward: -15.26 - Last mean reward per episode: -17.06
Num timesteps: 1602000
Best mean reward: -15.26 - Last mean reward per episode: -16.20
Num timesteps: 1605000
Best mean reward: -15.26 - Last mean reward per episode: -17.04
Num timesteps: 1608000
Best mean reward: -15.26 - Last mean reward per episode: -20.71
Num timesteps: 1611000
Best mean reward: -15.26 - Last mean reward per episode: -18.32
Num timesteps: 1614000
Best mean reward: -15.26 - Last mean reward per episode: -19.13
Num timesteps: 1617000
Best mean reward: -15.26 - Last mean reward per episode: -16.53
Num timesteps: 1620000
Best mean reward: -15.26 - Last mean reward per episode: -20.81
Num timesteps: 1623000
Best mean reward: -15.26 - Last mean reward per episode: -17.71
Num timesteps: 1626000
Best mean reward: -15.26 - Last mean reward per episode: -17.84
Num timesteps: 1629000
Best mean reward: -15.26 - Last mean reward per episode: -19.69
Num timesteps: 1632000
Best mean reward: -15.26 - Last mean reward per episode: -17.76
Num timesteps: 1635000
Best mean reward: -15.26 - Last mean reward per episode: -21.50
Num timesteps: 1638000
Best mean reward: -15.26 - Last mean reward per episode: -16.86
Num timesteps: 1641000
Best mean reward: -15.26 - Last mean reward per episode: -19.42
Num timesteps: 1644000
Best mean reward: -15.26 - Last mean reward per episode: -16.31
Num timesteps: 1647000
Best mean reward: -15.26 - Last mean reward per episode: -16.09
Num timesteps: 1650000
Best mean reward: -15.26 - Last mean reward per episode: -18.72
Num timesteps: 1653000
Best mean reward: -15.26 - Last mean reward per episode: -19.48
Num timesteps: 1656000
Best mean reward: -15.26 - Last mean reward per episode: -14.95
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 1659000
Best mean reward: -14.95 - Last mean reward per episode: -19.66
Num timesteps: 1662000
Best mean reward: -14.95 - Last mean reward per episode: -18.11
Num timesteps: 1665000
Best mean reward: -14.95 - Last mean reward per episode: -19.34
Num timesteps: 1668000
Best mean reward: -14.95 - Last mean reward per episode: -18.27
Num timesteps: 1671000
Best mean reward: -14.95 - Last mean reward per episode: -17.75
Num timesteps: 1674000
Best mean reward: -14.95 - Last mean reward per episode: -16.62
Num timesteps: 1677000
Best mean reward: -14.95 - Last mean reward per episode: -14.10
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 1680000
Best mean reward: -14.10 - Last mean reward per episode: -16.77
Num timesteps: 1683000
Best mean reward: -14.10 - Last mean reward per episode: -15.71
Num timesteps: 1686000
Best mean reward: -14.10 - Last mean reward per episode: -14.63
Num timesteps: 1689000
Best mean reward: -14.10 - Last mean reward per episode: -18.80
Num timesteps: 1692000
Best mean reward: -14.10 - Last mean reward per episode: -15.44
Num timesteps: 1695000
Best mean reward: -14.10 - Last mean reward per episode: -18.97
Num timesteps: 1698000
Best mean reward: -14.10 - Last mean reward per episode: -17.84
Num timesteps: 1701000
Best mean reward: -14.10 - Last mean reward per episode: -15.05
Num timesteps: 1704000
Best mean reward: -14.10 - Last mean reward per episode: -17.79
Num timesteps: 1707000
Best mean reward: -14.10 - Last mean reward per episode: -17.28
Num timesteps: 1710000
Best mean reward: -14.10 - Last mean reward per episode: -16.50
Num timesteps: 1713000
Best mean reward: -14.10 - Last mean reward per episode: -15.12
Num timesteps: 1716000
Best mean reward: -14.10 - Last mean reward per episode: -15.09
Num timesteps: 1719000
Best mean reward: -14.10 - Last mean reward per episode: -14.57
Num timesteps: 1722000
Best mean reward: -14.10 - Last mean reward per episode: -16.76
Num timesteps: 1725000
Best mean reward: -14.10 - Last mean reward per episode: -14.90
Num timesteps: 1728000
Best mean reward: -14.10 - Last mean reward per episode: -13.63
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 1731000
Best mean reward: -13.63 - Last mean reward per episode: -14.43
Num timesteps: 1734000
Best mean reward: -13.63 - Last mean reward per episode: -17.02
Num timesteps: 1737000
Best mean reward: -13.63 - Last mean reward per episode: -17.23
Num timesteps: 1740000
Best mean reward: -13.63 - Last mean reward per episode: -15.58
Num timesteps: 1743000
Best mean reward: -13.63 - Last mean reward per episode: -15.47
Num timesteps: 1746000
Best mean reward: -13.63 - Last mean reward per episode: -14.89
Num timesteps: 1749000
Best mean reward: -13.63 - Last mean reward per episode: -14.66
Num timesteps: 1752000
Best mean reward: -13.63 - Last mean reward per episode: -14.11
Num timesteps: 1755000
Best mean reward: -13.63 - Last mean reward per episode: -18.18
Num timesteps: 1758000
Best mean reward: -13.63 - Last mean reward per episode: -15.72
Num timesteps: 1761000
Best mean reward: -13.63 - Last mean reward per episode: -12.15
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 1764000
Best mean reward: -12.15 - Last mean reward per episode: -15.52
Num timesteps: 1767000
Best mean reward: -12.15 - Last mean reward per episode: -16.08
Num timesteps: 1770000
Best mean reward: -12.15 - Last mean reward per episode: -21.01
Num timesteps: 1773000
Best mean reward: -12.15 - Last mean reward per episode: -14.29
Num timesteps: 1776000
Best mean reward: -12.15 - Last mean reward per episode: -15.05
Num timesteps: 1779000
Best mean reward: -12.15 - Last mean reward per episode: -11.35
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 1782000
Best mean reward: -11.35 - Last mean reward per episode: -10.95
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 1785000
Best mean reward: -10.95 - Last mean reward per episode: -16.22
Num timesteps: 1788000
Best mean reward: -10.95 - Last mean reward per episode: -14.75
Num timesteps: 1791000
Best mean reward: -10.95 - Last mean reward per episode: -13.84
Num timesteps: 1794000
Best mean reward: -10.95 - Last mean reward per episode: -12.92
Num timesteps: 1797000
Best mean reward: -10.95 - Last mean reward per episode: -11.61
Num timesteps: 1800000
Best mean reward: -10.95 - Last mean reward per episode: -15.85
Num timesteps: 1803000
Best mean reward: -10.95 - Last mean reward per episode: -15.78
Num timesteps: 1806000
Best mean reward: -10.95 - Last mean reward per episode: -15.54
Num timesteps: 1809000
Best mean reward: -10.95 - Last mean reward per episode: -15.39
Num timesteps: 1812000
Best mean reward: -10.95 - Last mean reward per episode: -16.59
Num timesteps: 1815000
Best mean reward: -10.95 - Last mean reward per episode: -11.53
Num timesteps: 1818000
Best mean reward: -10.95 - Last mean reward per episode: -13.15
Num timesteps: 1821000
Best mean reward: -10.95 - Last mean reward per episode: -13.43
Num timesteps: 1824000
Best mean reward: -10.95 - Last mean reward per episode: -11.98
Num timesteps: 1827000
Best mean reward: -10.95 - Last mean reward per episode: -12.38
Num timesteps: 1830000
Best mean reward: -10.95 - Last mean reward per episode: -17.28
Num timesteps: 1833000
Best mean reward: -10.95 - Last mean reward per episode: -14.81
Num timesteps: 1836000
Best mean reward: -10.95 - Last mean reward per episode: -13.94
Num timesteps: 1839000
Best mean reward: -10.95 - Last mean reward per episode: -15.27
Num timesteps: 1842000
Best mean reward: -10.95 - Last mean reward per episode: -12.36
Num timesteps: 1845000
Best mean reward: -10.95 - Last mean reward per episode: -13.14
Num timesteps: 1848000
Best mean reward: -10.95 - Last mean reward per episode: -13.06
Num timesteps: 1851000
Best mean reward: -10.95 - Last mean reward per episode: -15.27
Num timesteps: 1854000
Best mean reward: -10.95 - Last mean reward per episode: -14.74
Num timesteps: 1857000
Best mean reward: -10.95 - Last mean reward per episode: -12.92
Num timesteps: 1860000
Best mean reward: -10.95 - Last mean reward per episode: -13.45
Num timesteps: 1863000
Best mean reward: -10.95 - Last mean reward per episode: -12.48
Num timesteps: 1866000
Best mean reward: -10.95 - Last mean reward per episode: -14.78
Num timesteps: 1869000
Best mean reward: -10.95 - Last mean reward per episode: -11.56
Num timesteps: 1872000
Best mean reward: -10.95 - Last mean reward per episode: -13.15
Num timesteps: 1875000
Best mean reward: -10.95 - Last mean reward per episode: -11.45
Num timesteps: 1878000
Best mean reward: -10.95 - Last mean reward per episode: -11.24
Num timesteps: 1881000
Best mean reward: -10.95 - Last mean reward per episode: -12.70
Num timesteps: 1884000
Best mean reward: -10.95 - Last mean reward per episode: -12.71
Num timesteps: 1887000
Best mean reward: -10.95 - Last mean reward per episode: -13.52
Num timesteps: 1890000
Best mean reward: -10.95 - Last mean reward per episode: -13.11
Num timesteps: 1893000
Best mean reward: -10.95 - Last mean reward per episode: -15.14
Num timesteps: 1896000
Best mean reward: -10.95 - Last mean reward per episode: -10.76
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 1899000
Best mean reward: -10.76 - Last mean reward per episode: -11.08
Num timesteps: 1902000
Best mean reward: -10.76 - Last mean reward per episode: -9.92
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 1905000
Best mean reward: -9.92 - Last mean reward per episode: -12.01
Num timesteps: 1908000
Best mean reward: -9.92 - Last mean reward per episode: -11.91
Num timesteps: 1911000
Best mean reward: -9.92 - Last mean reward per episode: -13.83
Num timesteps: 1914000
Best mean reward: -9.92 - Last mean reward per episode: -14.08
Num timesteps: 1917000
Best mean reward: -9.92 - Last mean reward per episode: -9.53
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 1920000
Best mean reward: -9.53 - Last mean reward per episode: -11.45
Num timesteps: 1923000
Best mean reward: -9.53 - Last mean reward per episode: -11.72
Num timesteps: 1926000
Best mean reward: -9.53 - Last mean reward per episode: -11.18
Num timesteps: 1929000
Best mean reward: -9.53 - Last mean reward per episode: -10.85
Num timesteps: 1932000
Best mean reward: -9.53 - Last mean reward per episode: -10.69
Num timesteps: 1935000
Best mean reward: -9.53 - Last mean reward per episode: -12.27
Num timesteps: 1938000
Best mean reward: -9.53 - Last mean reward per episode: -14.66
Num timesteps: 1941000
Best mean reward: -9.53 - Last mean reward per episode: -12.75
Num timesteps: 1944000
Best mean reward: -9.53 - Last mean reward per episode: -11.66
Num timesteps: 1947000
Best mean reward: -9.53 - Last mean reward per episode: -13.80
Num timesteps: 1950000
Best mean reward: -9.53 - Last mean reward per episode: -10.86
Num timesteps: 1953000
Best mean reward: -9.53 - Last mean reward per episode: -10.77
Num timesteps: 1956000
Best mean reward: -9.53 - Last mean reward per episode: -12.41
Num timesteps: 1959000
Best mean reward: -9.53 - Last mean reward per episode: -10.83
Num timesteps: 1962000
Best mean reward: -9.53 - Last mean reward per episode: -13.32
Num timesteps: 1965000
Best mean reward: -9.53 - Last mean reward per episode: -9.75
Num timesteps: 1968000
Best mean reward: -9.53 - Last mean reward per episode: -9.43
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 1971000
Best mean reward: -9.43 - Last mean reward per episode: -11.65
Num timesteps: 1974000
Best mean reward: -9.43 - Last mean reward per episode: -11.56
Num timesteps: 1977000
Best mean reward: -9.43 - Last mean reward per episode: -12.83
Num timesteps: 1980000
Best mean reward: -9.43 - Last mean reward per episode: -9.55
Num timesteps: 1983000
Best mean reward: -9.43 - Last mean reward per episode: -10.78
Num timesteps: 1986000
Best mean reward: -9.43 - Last mean reward per episode: -11.67
Num timesteps: 1989000
Best mean reward: -9.43 - Last mean reward per episode: -9.75
Num timesteps: 1992000
Best mean reward: -9.43 - Last mean reward per episode: -14.02
Num timesteps: 1995000
Best mean reward: -9.43 - Last mean reward per episode: -10.48
Num timesteps: 1998000
Best mean reward: -9.43 - Last mean reward per episode: -12.28
Num timesteps: 2001000
Best mean reward: -9.43 - Last mean reward per episode: -10.50
Num timesteps: 2004000
Best mean reward: -9.43 - Last mean reward per episode: -14.44
Num timesteps: 2007000
Best mean reward: -9.43 - Last mean reward per episode: -10.85
Num timesteps: 2010000
Best mean reward: -9.43 - Last mean reward per episode: -10.42
Num timesteps: 2013000
Best mean reward: -9.43 - Last mean reward per episode: -10.67
Num timesteps: 2016000
Best mean reward: -9.43 - Last mean reward per episode: -9.07
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 2019000
Best mean reward: -9.07 - Last mean reward per episode: -10.58
Num timesteps: 2022000
Best mean reward: -9.07 - Last mean reward per episode: -10.59
Num timesteps: 2025000
Best mean reward: -9.07 - Last mean reward per episode: -14.50
Num timesteps: 2028000
Best mean reward: -9.07 - Last mean reward per episode: -10.00
Num timesteps: 2031000
Best mean reward: -9.07 - Last mean reward per episode: -10.53
Num timesteps: 2034000
Best mean reward: -9.07 - Last mean reward per episode: -11.14
Num timesteps: 2037000
Best mean reward: -9.07 - Last mean reward per episode: -9.72
Num timesteps: 2040000
Best mean reward: -9.07 - Last mean reward per episode: -11.81
Num timesteps: 2043000
Best mean reward: -9.07 - Last mean reward per episode: -10.53
Num timesteps: 2046000
Best mean reward: -9.07 - Last mean reward per episode: -9.88
Num timesteps: 2049000
Best mean reward: -9.07 - Last mean reward per episode: -10.92
Num timesteps: 2052000
Best mean reward: -9.07 - Last mean reward per episode: -10.71
Num timesteps: 2055000
Best mean reward: -9.07 - Last mean reward per episode: -11.25
Num timesteps: 2058000
Best mean reward: -9.07 - Last mean reward per episode: -11.00
Num timesteps: 2061000
Best mean reward: -9.07 - Last mean reward per episode: -11.66
Num timesteps: 2064000
Best mean reward: -9.07 - Last mean reward per episode: -9.50
Num timesteps: 2067000
Best mean reward: -9.07 - Last mean reward per episode: -9.59
Num timesteps: 2070000
Best mean reward: -9.07 - Last mean reward per episode: -9.37
Num timesteps: 2073000
Best mean reward: -9.07 - Last mean reward per episode: -12.04
Num timesteps: 2076000
Best mean reward: -9.07 - Last mean reward per episode: -11.61
Num timesteps: 2079000
Best mean reward: -9.07 - Last mean reward per episode: -10.40
Num timesteps: 2082000
Best mean reward: -9.07 - Last mean reward per episode: -10.90
Num timesteps: 2085000
Best mean reward: -9.07 - Last mean reward per episode: -9.69
Num timesteps: 2088000
Best mean reward: -9.07 - Last mean reward per episode: -9.63
Num timesteps: 2091000
Best mean reward: -9.07 - Last mean reward per episode: -12.62
Num timesteps: 2094000
Best mean reward: -9.07 - Last mean reward per episode: -9.55
Num timesteps: 2097000
Best mean reward: -9.07 - Last mean reward per episode: -10.45
Num timesteps: 2100000
Best mean reward: -9.07 - Last mean reward per episode: -10.10
Num timesteps: 2103000
Best mean reward: -9.07 - Last mean reward per episode: -11.11
Num timesteps: 2106000
Best mean reward: -9.07 - Last mean reward per episode: -9.10
Num timesteps: 2109000
Best mean reward: -9.07 - Last mean reward per episode: -9.80
Num timesteps: 2112000
Best mean reward: -9.07 - Last mean reward per episode: -8.40
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 2115000
Best mean reward: -8.40 - Last mean reward per episode: -8.39
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 2118000
Best mean reward: -8.39 - Last mean reward per episode: -10.26
Num timesteps: 2121000
Best mean reward: -8.39 - Last mean reward per episode: -10.07
Num timesteps: 2124000
Best mean reward: -8.39 - Last mean reward per episode: -10.21
Num timesteps: 2127000
Best mean reward: -8.39 - Last mean reward per episode: -13.57
Num timesteps: 2130000
Best mean reward: -8.39 - Last mean reward per episode: -9.34
Num timesteps: 2133000
Best mean reward: -8.39 - Last mean reward per episode: -9.40
Num timesteps: 2136000
Best mean reward: -8.39 - Last mean reward per episode: -7.95
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 2139000
Best mean reward: -7.95 - Last mean reward per episode: -9.46
Num timesteps: 2142000
Best mean reward: -7.95 - Last mean reward per episode: -11.14
Num timesteps: 2145000
Best mean reward: -7.95 - Last mean reward per episode: -9.79
Num timesteps: 2148000
Best mean reward: -7.95 - Last mean reward per episode: -10.47
Num timesteps: 2151000
Best mean reward: -7.95 - Last mean reward per episode: -9.66
Num timesteps: 2154000
Best mean reward: -7.95 - Last mean reward per episode: -10.16
Num timesteps: 2157000
Best mean reward: -7.95 - Last mean reward per episode: -10.71
Num timesteps: 2160000
Best mean reward: -7.95 - Last mean reward per episode: -8.00
Num timesteps: 2163000
Best mean reward: -7.95 - Last mean reward per episode: -9.92
Num timesteps: 2166000
Best mean reward: -7.95 - Last mean reward per episode: -9.33
Num timesteps: 2169000
Best mean reward: -7.95 - Last mean reward per episode: -8.45
Num timesteps: 2172000
Best mean reward: -7.95 - Last mean reward per episode: -9.10
Num timesteps: 2175000
Best mean reward: -7.95 - Last mean reward per episode: -8.21
Num timesteps: 2178000
Best mean reward: -7.95 - Last mean reward per episode: -9.90
Num timesteps: 2181000
Best mean reward: -7.95 - Last mean reward per episode: -7.41
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 2184000
Best mean reward: -7.41 - Last mean reward per episode: -8.57
Num timesteps: 2187000
Best mean reward: -7.41 - Last mean reward per episode: -7.70
Num timesteps: 2190000
Best mean reward: -7.41 - Last mean reward per episode: -7.37
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 2193000
Best mean reward: -7.37 - Last mean reward per episode: -7.27
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 2196000
Best mean reward: -7.27 - Last mean reward per episode: -8.97
Num timesteps: 2199000
Best mean reward: -7.27 - Last mean reward per episode: -7.09
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 2202000
Best mean reward: -7.09 - Last mean reward per episode: -7.70
Num timesteps: 2205000
Best mean reward: -7.09 - Last mean reward per episode: -7.13
Num timesteps: 2208000
Best mean reward: -7.09 - Last mean reward per episode: -8.16
Num timesteps: 2211000
Best mean reward: -7.09 - Last mean reward per episode: -8.97
Num timesteps: 2214000
Best mean reward: -7.09 - Last mean reward per episode: -9.42
Num timesteps: 2217000
Best mean reward: -7.09 - Last mean reward per episode: -9.88
Num timesteps: 2220000
Best mean reward: -7.09 - Last mean reward per episode: -9.80
Num timesteps: 2223000
Best mean reward: -7.09 - Last mean reward per episode: -8.18
Num timesteps: 2226000
Best mean reward: -7.09 - Last mean reward per episode: -7.49
Num timesteps: 2229000
Best mean reward: -7.09 - Last mean reward per episode: -7.95
Num timesteps: 2232000
Best mean reward: -7.09 - Last mean reward per episode: -9.56
Num timesteps: 2235000
Best mean reward: -7.09 - Last mean reward per episode: -9.56
Num timesteps: 2238000
Best mean reward: -7.09 - Last mean reward per episode: -6.77
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 2241000
Best mean reward: -6.77 - Last mean reward per episode: -7.02
Num timesteps: 2244000
Best mean reward: -6.77 - Last mean reward per episode: -7.86
Num timesteps: 2247000
Best mean reward: -6.77 - Last mean reward per episode: -8.19
Num timesteps: 2250000
Best mean reward: -6.77 - Last mean reward per episode: -8.78
Num timesteps: 2253000
Best mean reward: -6.77 - Last mean reward per episode: -9.87
Num timesteps: 2256000
Best mean reward: -6.77 - Last mean reward per episode: -8.59
Num timesteps: 2259000
Best mean reward: -6.77 - Last mean reward per episode: -8.77
Num timesteps: 2262000
Best mean reward: -6.77 - Last mean reward per episode: -7.01
Num timesteps: 2265000
Best mean reward: -6.77 - Last mean reward per episode: -6.88
Num timesteps: 2268000
Best mean reward: -6.77 - Last mean reward per episode: -7.84
Num timesteps: 2271000
Best mean reward: -6.77 - Last mean reward per episode: -7.39
Num timesteps: 2274000
Best mean reward: -6.77 - Last mean reward per episode: -7.67
Num timesteps: 2277000
Best mean reward: -6.77 - Last mean reward per episode: -5.60
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 2280000
Best mean reward: -5.60 - Last mean reward per episode: -7.13
Num timesteps: 2283000
Best mean reward: -5.60 - Last mean reward per episode: -7.40
Num timesteps: 2286000
Best mean reward: -5.60 - Last mean reward per episode: -6.40
Num timesteps: 2289000
Best mean reward: -5.60 - Last mean reward per episode: -6.48
Num timesteps: 2292000
Best mean reward: -5.60 - Last mean reward per episode: -8.23
Num timesteps: 2295000
Best mean reward: -5.60 - Last mean reward per episode: -9.11
Num timesteps: 2298000
Best mean reward: -5.60 - Last mean reward per episode: -7.93
Num timesteps: 2301000
Best mean reward: -5.60 - Last mean reward per episode: -8.42
Num timesteps: 2304000
Best mean reward: -5.60 - Last mean reward per episode: -8.52
Num timesteps: 2307000
Best mean reward: -5.60 - Last mean reward per episode: -7.61
Num timesteps: 2310000
Best mean reward: -5.60 - Last mean reward per episode: -6.88
Num timesteps: 2313000
Best mean reward: -5.60 - Last mean reward per episode: -8.71
Num timesteps: 2316000
Best mean reward: -5.60 - Last mean reward per episode: -8.03
Num timesteps: 2319000
Best mean reward: -5.60 - Last mean reward per episode: -7.64
Num timesteps: 2322000
Best mean reward: -5.60 - Last mean reward per episode: -6.07
Num timesteps: 2325000
Best mean reward: -5.60 - Last mean reward per episode: -9.35
Num timesteps: 2328000
Best mean reward: -5.60 - Last mean reward per episode: -7.35
Num timesteps: 2331000
Best mean reward: -5.60 - Last mean reward per episode: -7.70
Num timesteps: 2334000
Best mean reward: -5.60 - Last mean reward per episode: -7.45
Num timesteps: 2337000
Best mean reward: -5.60 - Last mean reward per episode: -8.35
Num timesteps: 2340000
Best mean reward: -5.60 - Last mean reward per episode: -8.85
Num timesteps: 2343000
Best mean reward: -5.60 - Last mean reward per episode: -9.91
Num timesteps: 2346000
Best mean reward: -5.60 - Last mean reward per episode: -6.93
Num timesteps: 2349000
Best mean reward: -5.60 - Last mean reward per episode: -6.64
Num timesteps: 2352000
Best mean reward: -5.60 - Last mean reward per episode: -8.32
Num timesteps: 2355000
Best mean reward: -5.60 - Last mean reward per episode: -5.92
Num timesteps: 2358000
Best mean reward: -5.60 - Last mean reward per episode: -7.60
Num timesteps: 2361000
Best mean reward: -5.60 - Last mean reward per episode: -6.47
Num timesteps: 2364000
Best mean reward: -5.60 - Last mean reward per episode: -5.73
Num timesteps: 2367000
Best mean reward: -5.60 - Last mean reward per episode: -6.17
Num timesteps: 2370000
Best mean reward: -5.60 - Last mean reward per episode: -6.19
Num timesteps: 2373000
Best mean reward: -5.60 - Last mean reward per episode: -7.50
Num timesteps: 2376000
Best mean reward: -5.60 - Last mean reward per episode: -7.75
Num timesteps: 2379000
Best mean reward: -5.60 - Last mean reward per episode: -6.21
Num timesteps: 2382000
Best mean reward: -5.60 - Last mean reward per episode: -6.80
Num timesteps: 2385000
Best mean reward: -5.60 - Last mean reward per episode: -7.11
Num timesteps: 2388000
Best mean reward: -5.60 - Last mean reward per episode: -5.64
Num timesteps: 2391000
Best mean reward: -5.60 - Last mean reward per episode: -6.27
Num timesteps: 2394000
Best mean reward: -5.60 - Last mean reward per episode: -7.99
Num timesteps: 2397000
Best mean reward: -5.60 - Last mean reward per episode: -6.49
Num timesteps: 2400000
Best mean reward: -5.60 - Last mean reward per episode: -6.07
Num timesteps: 2403000
Best mean reward: -5.60 - Last mean reward per episode: -6.23
Num timesteps: 2406000
Best mean reward: -5.60 - Last mean reward per episode: -7.58
Num timesteps: 2409000
Best mean reward: -5.60 - Last mean reward per episode: -7.69
Num timesteps: 2412000
Best mean reward: -5.60 - Last mean reward per episode: -6.45
Num timesteps: 2415000
Best mean reward: -5.60 - Last mean reward per episode: -6.97
Num timesteps: 2418000
Best mean reward: -5.60 - Last mean reward per episode: -6.49
Num timesteps: 2421000
Best mean reward: -5.60 - Last mean reward per episode: -7.99
Num timesteps: 2424000
Best mean reward: -5.60 - Last mean reward per episode: -7.06
Num timesteps: 2427000
Best mean reward: -5.60 - Last mean reward per episode: -6.78
Num timesteps: 2430000
Best mean reward: -5.60 - Last mean reward per episode: -6.94
Num timesteps: 2433000
Best mean reward: -5.60 - Last mean reward per episode: -6.84
Num timesteps: 2436000
Best mean reward: -5.60 - Last mean reward per episode: -5.86
Num timesteps: 2439000
Best mean reward: -5.60 - Last mean reward per episode: -5.89
Num timesteps: 2442000
Best mean reward: -5.60 - Last mean reward per episode: -8.28
Num timesteps: 2445000
Best mean reward: -5.60 - Last mean reward per episode: -6.19
Num timesteps: 2448000
Best mean reward: -5.60 - Last mean reward per episode: -7.27
Num timesteps: 2451000
Best mean reward: -5.60 - Last mean reward per episode: -6.25
Num timesteps: 2454000
Best mean reward: -5.60 - Last mean reward per episode: -8.21
Num timesteps: 2457000
Best mean reward: -5.60 - Last mean reward per episode: -8.57
Num timesteps: 2460000
Best mean reward: -5.60 - Last mean reward per episode: -6.68
Num timesteps: 2463000
Best mean reward: -5.60 - Last mean reward per episode: -6.19
Num timesteps: 2466000
Best mean reward: -5.60 - Last mean reward per episode: -7.00
Num timesteps: 2469000
Best mean reward: -5.60 - Last mean reward per episode: -6.55
Num timesteps: 2472000
Best mean reward: -5.60 - Last mean reward per episode: -6.96
Num timesteps: 2475000
Best mean reward: -5.60 - Last mean reward per episode: -6.94
Num timesteps: 2478000
Best mean reward: -5.60 - Last mean reward per episode: -6.58
Num timesteps: 2481000
Best mean reward: -5.60 - Last mean reward per episode: -6.43
Num timesteps: 2484000
Best mean reward: -5.60 - Last mean reward per episode: -6.21
Num timesteps: 2487000
Best mean reward: -5.60 - Last mean reward per episode: -6.38
Num timesteps: 2490000
Best mean reward: -5.60 - Last mean reward per episode: -7.10
Num timesteps: 2493000
Best mean reward: -5.60 - Last mean reward per episode: -7.14
Num timesteps: 2496000
Best mean reward: -5.60 - Last mean reward per episode: -7.85
Num timesteps: 2499000
Best mean reward: -5.60 - Last mean reward per episode: -5.83
Num timesteps: 2502000
Best mean reward: -5.60 - Last mean reward per episode: -6.27
Num timesteps: 2505000
Best mean reward: -5.60 - Last mean reward per episode: -5.40
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 2508000
Best mean reward: -5.40 - Last mean reward per episode: -7.94
Num timesteps: 2511000
Best mean reward: -5.40 - Last mean reward per episode: -6.64
Num timesteps: 2514000
Best mean reward: -5.40 - Last mean reward per episode: -6.49
Num timesteps: 2517000
Best mean reward: -5.40 - Last mean reward per episode: -6.98
Num timesteps: 2520000
Best mean reward: -5.40 - Last mean reward per episode: -5.07
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 2523000
Best mean reward: -5.07 - Last mean reward per episode: -7.12
Num timesteps: 2526000
Best mean reward: -5.07 - Last mean reward per episode: -5.69
Num timesteps: 2529000
Best mean reward: -5.07 - Last mean reward per episode: -6.51
Num timesteps: 2532000
Best mean reward: -5.07 - Last mean reward per episode: -7.27
Num timesteps: 2535000
Best mean reward: -5.07 - Last mean reward per episode: -6.47
Num timesteps: 2538000
Best mean reward: -5.07 - Last mean reward per episode: -6.85
Num timesteps: 2541000
Best mean reward: -5.07 - Last mean reward per episode: -6.56
Num timesteps: 2544000
Best mean reward: -5.07 - Last mean reward per episode: -4.69
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 2547000
Best mean reward: -4.69 - Last mean reward per episode: -6.05
Num timesteps: 2550000
Best mean reward: -4.69 - Last mean reward per episode: -5.68
Num timesteps: 2553000
Best mean reward: -4.69 - Last mean reward per episode: -6.45
Num timesteps: 2556000
Best mean reward: -4.69 - Last mean reward per episode: -6.05
Num timesteps: 2559000
Best mean reward: -4.69 - Last mean reward per episode: -4.93
Num timesteps: 2562000
Best mean reward: -4.69 - Last mean reward per episode: -6.76
Num timesteps: 2565000
Best mean reward: -4.69 - Last mean reward per episode: -5.02
Num timesteps: 2568000
Best mean reward: -4.69 - Last mean reward per episode: -6.31
Num timesteps: 2571000
Best mean reward: -4.69 - Last mean reward per episode: -5.51
Num timesteps: 2574000
Best mean reward: -4.69 - Last mean reward per episode: -4.86
Num timesteps: 2577000
Best mean reward: -4.69 - Last mean reward per episode: -5.95
Num timesteps: 2580000
Best mean reward: -4.69 - Last mean reward per episode: -5.79
Num timesteps: 2583000
Best mean reward: -4.69 - Last mean reward per episode: -5.59
Num timesteps: 2586000
Best mean reward: -4.69 - Last mean reward per episode: -6.08
Num timesteps: 2589000
Best mean reward: -4.69 - Last mean reward per episode: -5.88
Num timesteps: 2592000
Best mean reward: -4.69 - Last mean reward per episode: -5.90
Num timesteps: 2595000
Best mean reward: -4.69 - Last mean reward per episode: -5.20
Num timesteps: 2598000
Best mean reward: -4.69 - Last mean reward per episode: -5.75
Num timesteps: 2601000
Best mean reward: -4.69 - Last mean reward per episode: -6.27
Num timesteps: 2604000
Best mean reward: -4.69 - Last mean reward per episode: -4.55
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 2607000
Best mean reward: -4.55 - Last mean reward per episode: -6.30
Num timesteps: 2610000
Best mean reward: -4.55 - Last mean reward per episode: -6.59
Num timesteps: 2613000
Best mean reward: -4.55 - Last mean reward per episode: -5.32
Num timesteps: 2616000
Best mean reward: -4.55 - Last mean reward per episode: -4.91
Num timesteps: 2619000
Best mean reward: -4.55 - Last mean reward per episode: -7.83
Num timesteps: 2622000
Best mean reward: -4.55 - Last mean reward per episode: -6.28
Num timesteps: 2625000
Best mean reward: -4.55 - Last mean reward per episode: -5.56
Num timesteps: 2628000
Best mean reward: -4.55 - Last mean reward per episode: -5.79
Num timesteps: 2631000
Best mean reward: -4.55 - Last mean reward per episode: -5.40
Num timesteps: 2634000
Best mean reward: -4.55 - Last mean reward per episode: -5.71
Num timesteps: 2637000
Best mean reward: -4.55 - Last mean reward per episode: -5.53
Num timesteps: 2640000
Best mean reward: -4.55 - Last mean reward per episode: -6.72
Num timesteps: 2643000
Best mean reward: -4.55 - Last mean reward per episode: -4.50
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 2646000
Best mean reward: -4.50 - Last mean reward per episode: -4.66
Num timesteps: 2649000
Best mean reward: -4.50 - Last mean reward per episode: -5.03
Num timesteps: 2652000
Best mean reward: -4.50 - Last mean reward per episode: -5.41
Num timesteps: 2655000
Best mean reward: -4.50 - Last mean reward per episode: -5.27
Num timesteps: 2658000
Best mean reward: -4.50 - Last mean reward per episode: -6.08
Num timesteps: 2661000
Best mean reward: -4.50 - Last mean reward per episode: -5.54
Num timesteps: 2664000
Best mean reward: -4.50 - Last mean reward per episode: -5.19
Num timesteps: 2667000
Best mean reward: -4.50 - Last mean reward per episode: -4.85
Num timesteps: 2670000
Best mean reward: -4.50 - Last mean reward per episode: -5.87
Num timesteps: 2673000
Best mean reward: -4.50 - Last mean reward per episode: -4.90
Num timesteps: 2676000
Best mean reward: -4.50 - Last mean reward per episode: -5.68
Num timesteps: 2679000
Best mean reward: -4.50 - Last mean reward per episode: -6.58
Num timesteps: 2682000
Best mean reward: -4.50 - Last mean reward per episode: -4.04
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 2685000
Best mean reward: -4.04 - Last mean reward per episode: -6.79
Num timesteps: 2688000
Best mean reward: -4.04 - Last mean reward per episode: -4.89
Num timesteps: 2691000
Best mean reward: -4.04 - Last mean reward per episode: -4.71
Num timesteps: 2694000
Best mean reward: -4.04 - Last mean reward per episode: -4.64
Num timesteps: 2697000
Best mean reward: -4.04 - Last mean reward per episode: -5.46
Num timesteps: 2700000
Best mean reward: -4.04 - Last mean reward per episode: -4.81
Num timesteps: 2703000
Best mean reward: -4.04 - Last mean reward per episode: -5.18
Num timesteps: 2706000
Best mean reward: -4.04 - Last mean reward per episode: -5.41
Num timesteps: 2709000
Best mean reward: -4.04 - Last mean reward per episode: -3.95
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 2712000
Best mean reward: -3.95 - Last mean reward per episode: -4.68
Num timesteps: 2715000
Best mean reward: -3.95 - Last mean reward per episode: -6.88
Num timesteps: 2718000
Best mean reward: -3.95 - Last mean reward per episode: -4.62
Num timesteps: 2721000
Best mean reward: -3.95 - Last mean reward per episode: -6.21
Num timesteps: 2724000
Best mean reward: -3.95 - Last mean reward per episode: -5.11
Num timesteps: 2727000
Best mean reward: -3.95 - Last mean reward per episode: -5.86
Num timesteps: 2730000
Best mean reward: -3.95 - Last mean reward per episode: -6.22
Num timesteps: 2733000
Best mean reward: -3.95 - Last mean reward per episode: -5.26
Num timesteps: 2736000
Best mean reward: -3.95 - Last mean reward per episode: -5.38
Num timesteps: 2739000
Best mean reward: -3.95 - Last mean reward per episode: -6.72
Num timesteps: 2742000
Best mean reward: -3.95 - Last mean reward per episode: -6.21
Num timesteps: 2745000
Best mean reward: -3.95 - Last mean reward per episode: -5.33
Num timesteps: 2748000
Best mean reward: -3.95 - Last mean reward per episode: -5.10
Num timesteps: 2751000
Best mean reward: -3.95 - Last mean reward per episode: -4.47
Num timesteps: 2754000
Best mean reward: -3.95 - Last mean reward per episode: -5.59
Num timesteps: 2757000
Best mean reward: -3.95 - Last mean reward per episode: -5.01
Num timesteps: 2760000
Best mean reward: -3.95 - Last mean reward per episode: -4.30
Num timesteps: 2763000
Best mean reward: -3.95 - Last mean reward per episode: -4.52
Num timesteps: 2766000
Best mean reward: -3.95 - Last mean reward per episode: -4.92
Num timesteps: 2769000
Best mean reward: -3.95 - Last mean reward per episode: -4.79
Num timesteps: 2772000
Best mean reward: -3.95 - Last mean reward per episode: -4.62
Num timesteps: 2775000
Best mean reward: -3.95 - Last mean reward per episode: -4.29
Num timesteps: 2778000
Best mean reward: -3.95 - Last mean reward per episode: -4.60
Num timesteps: 2781000
Best mean reward: -3.95 - Last mean reward per episode: -4.25
Num timesteps: 2784000
Best mean reward: -3.95 - Last mean reward per episode: -4.17
Num timesteps: 2787000
Best mean reward: -3.95 - Last mean reward per episode: -6.35
Num timesteps: 2790000
Best mean reward: -3.95 - Last mean reward per episode: -3.95
Num timesteps: 2793000
Best mean reward: -3.95 - Last mean reward per episode: -4.65
Num timesteps: 2796000
Best mean reward: -3.95 - Last mean reward per episode: -4.74
Num timesteps: 2799000
Best mean reward: -3.95 - Last mean reward per episode: -4.81
Num timesteps: 2802000
Best mean reward: -3.95 - Last mean reward per episode: -4.43
Num timesteps: 2805000
Best mean reward: -3.95 - Last mean reward per episode: -6.71
Num timesteps: 2808000
Best mean reward: -3.95 - Last mean reward per episode: -5.51
Num timesteps: 2811000
Best mean reward: -3.95 - Last mean reward per episode: -4.61
Num timesteps: 2814000
Best mean reward: -3.95 - Last mean reward per episode: -4.27
Num timesteps: 2817000
Best mean reward: -3.95 - Last mean reward per episode: -4.79
Num timesteps: 2820000
Best mean reward: -3.95 - Last mean reward per episode: -4.99
Num timesteps: 2823000
Best mean reward: -3.95 - Last mean reward per episode: -4.85
Num timesteps: 2826000
Best mean reward: -3.95 - Last mean reward per episode: -4.09
Num timesteps: 2829000
Best mean reward: -3.95 - Last mean reward per episode: -5.02
Num timesteps: 2832000
Best mean reward: -3.95 - Last mean reward per episode: -5.39
Num timesteps: 2835000
Best mean reward: -3.95 - Last mean reward per episode: -4.38
Num timesteps: 2838000
Best mean reward: -3.95 - Last mean reward per episode: -3.88
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 2841000
Best mean reward: -3.88 - Last mean reward per episode: -5.85
Num timesteps: 2844000
Best mean reward: -3.88 - Last mean reward per episode: -5.72
Num timesteps: 2847000
Best mean reward: -3.88 - Last mean reward per episode: -4.53
Num timesteps: 2850000
Best mean reward: -3.88 - Last mean reward per episode: -5.07
Num timesteps: 2853000
Best mean reward: -3.88 - Last mean reward per episode: -3.84
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 2856000
Best mean reward: -3.84 - Last mean reward per episode: -5.22
Num timesteps: 2859000
Best mean reward: -3.84 - Last mean reward per episode: -4.67
Num timesteps: 2862000
Best mean reward: -3.84 - Last mean reward per episode: -4.30
Num timesteps: 2865000
Best mean reward: -3.84 - Last mean reward per episode: -3.91
Num timesteps: 2868000
Best mean reward: -3.84 - Last mean reward per episode: -4.31
Num timesteps: 2871000
Best mean reward: -3.84 - Last mean reward per episode: -3.84
Num timesteps: 2874000
Best mean reward: -3.84 - Last mean reward per episode: -4.05
Num timesteps: 2877000
Best mean reward: -3.84 - Last mean reward per episode: -4.68
Num timesteps: 2880000
Best mean reward: -3.84 - Last mean reward per episode: -4.45
Num timesteps: 2883000
Best mean reward: -3.84 - Last mean reward per episode: -4.92
Num timesteps: 2886000
Best mean reward: -3.84 - Last mean reward per episode: -4.37
Num timesteps: 2889000
Best mean reward: -3.84 - Last mean reward per episode: -4.67
Num timesteps: 2892000
Best mean reward: -3.84 - Last mean reward per episode: -5.04
Num timesteps: 2895000
Best mean reward: -3.84 - Last mean reward per episode: -4.03
Num timesteps: 2898000
Best mean reward: -3.84 - Last mean reward per episode: -4.85
Num timesteps: 2901000
Best mean reward: -3.84 - Last mean reward per episode: -3.93
Num timesteps: 2904000
Best mean reward: -3.84 - Last mean reward per episode: -4.43
Num timesteps: 2907000
Best mean reward: -3.84 - Last mean reward per episode: -4.92
Num timesteps: 2910000
Best mean reward: -3.84 - Last mean reward per episode: -3.94
Num timesteps: 2913000
Best mean reward: -3.84 - Last mean reward per episode: -5.30
Num timesteps: 2916000
Best mean reward: -3.84 - Last mean reward per episode: -3.95
Num timesteps: 2919000
Best mean reward: -3.84 - Last mean reward per episode: -3.91
Num timesteps: 2922000
Best mean reward: -3.84 - Last mean reward per episode: -4.34
Num timesteps: 2925000
Best mean reward: -3.84 - Last mean reward per episode: -4.98
Num timesteps: 2928000
Best mean reward: -3.84 - Last mean reward per episode: -3.86
Num timesteps: 2931000
Best mean reward: -3.84 - Last mean reward per episode: -4.51
Num timesteps: 2934000
Best mean reward: -3.84 - Last mean reward per episode: -4.26
Num timesteps: 2937000
Best mean reward: -3.84 - Last mean reward per episode: -4.65
Num timesteps: 2940000
Best mean reward: -3.84 - Last mean reward per episode: -3.99
Num timesteps: 2943000
Best mean reward: -3.84 - Last mean reward per episode: -4.61
Num timesteps: 2946000
Best mean reward: -3.84 - Last mean reward per episode: -3.99
Num timesteps: 2949000
Best mean reward: -3.84 - Last mean reward per episode: -4.82
Num timesteps: 2952000
Best mean reward: -3.84 - Last mean reward per episode: -4.17
Num timesteps: 2955000
Best mean reward: -3.84 - Last mean reward per episode: -4.10
Num timesteps: 2958000
Best mean reward: -3.84 - Last mean reward per episode: -3.96
Num timesteps: 2961000
Best mean reward: -3.84 - Last mean reward per episode: -3.79
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 2964000
Best mean reward: -3.79 - Last mean reward per episode: -4.31
Num timesteps: 2967000
Best mean reward: -3.79 - Last mean reward per episode: -4.41
Num timesteps: 2970000
Best mean reward: -3.79 - Last mean reward per episode: -3.66
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 2973000
Best mean reward: -3.66 - Last mean reward per episode: -5.51
Num timesteps: 2976000
Best mean reward: -3.66 - Last mean reward per episode: -3.86
Num timesteps: 2979000
Best mean reward: -3.66 - Last mean reward per episode: -4.05
Num timesteps: 2982000
Best mean reward: -3.66 - Last mean reward per episode: -4.62
Num timesteps: 2985000
Best mean reward: -3.66 - Last mean reward per episode: -3.96
Num timesteps: 2988000
Best mean reward: -3.66 - Last mean reward per episode: -3.88
Num timesteps: 2991000
Best mean reward: -3.66 - Last mean reward per episode: -4.23
Num timesteps: 2994000
Best mean reward: -3.66 - Last mean reward per episode: -4.44
Num timesteps: 2997000
Best mean reward: -3.66 - Last mean reward per episode: -4.23
Num timesteps: 3000000
Best mean reward: -3.66 - Last mean reward per episode: -3.71
Num timesteps: 3003000
Best mean reward: -3.66 - Last mean reward per episode: -3.72
Num timesteps: 3006000
Best mean reward: -3.66 - Last mean reward per episode: -3.72
Num timesteps: 3009000
Best mean reward: -3.66 - Last mean reward per episode: -4.21
Num timesteps: 3012000
Best mean reward: -3.66 - Last mean reward per episode: -3.78
Num timesteps: 3015000
Best mean reward: -3.66 - Last mean reward per episode: -3.61
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 3018000
Best mean reward: -3.61 - Last mean reward per episode: -3.71
Num timesteps: 3021000
Best mean reward: -3.61 - Last mean reward per episode: -3.57
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 3024000
Best mean reward: -3.57 - Last mean reward per episode: -4.02
Num timesteps: 3027000
Best mean reward: -3.57 - Last mean reward per episode: -5.27
Num timesteps: 3030000
Best mean reward: -3.57 - Last mean reward per episode: -4.12
Num timesteps: 3033000
Best mean reward: -3.57 - Last mean reward per episode: -4.20
Num timesteps: 3036000
Best mean reward: -3.57 - Last mean reward per episode: -4.92
Num timesteps: 3039000
Best mean reward: -3.57 - Last mean reward per episode: -3.82
Num timesteps: 3042000
Best mean reward: -3.57 - Last mean reward per episode: -3.78
Num timesteps: 3045000
Best mean reward: -3.57 - Last mean reward per episode: -3.84
Num timesteps: 3048000
Best mean reward: -3.57 - Last mean reward per episode: -4.13
Num timesteps: 3051000
Best mean reward: -3.57 - Last mean reward per episode: -4.09
Num timesteps: 3054000
Best mean reward: -3.57 - Last mean reward per episode: -5.11
Num timesteps: 3057000
Best mean reward: -3.57 - Last mean reward per episode: -3.97
Num timesteps: 3060000
Best mean reward: -3.57 - Last mean reward per episode: -3.64
Num timesteps: 3063000
Best mean reward: -3.57 - Last mean reward per episode: -3.60
Num timesteps: 3066000
Best mean reward: -3.57 - Last mean reward per episode: -3.77
Num timesteps: 3069000
Best mean reward: -3.57 - Last mean reward per episode: -3.58
Num timesteps: 3072000
Best mean reward: -3.57 - Last mean reward per episode: -3.92
Num timesteps: 3075000
Best mean reward: -3.57 - Last mean reward per episode: -3.97
Num timesteps: 3078000
Best mean reward: -3.57 - Last mean reward per episode: -3.67
Num timesteps: 3081000
Best mean reward: -3.57 - Last mean reward per episode: -5.33
Num timesteps: 3084000
Best mean reward: -3.57 - Last mean reward per episode: -4.06
Num timesteps: 3087000
Best mean reward: -3.57 - Last mean reward per episode: -4.21
Num timesteps: 3090000
Best mean reward: -3.57 - Last mean reward per episode: -3.87
Num timesteps: 3093000
Best mean reward: -3.57 - Last mean reward per episode: -3.76
Num timesteps: 3096000
Best mean reward: -3.57 - Last mean reward per episode: -4.50
Num timesteps: 3099000
Best mean reward: -3.57 - Last mean reward per episode: -3.77
Num timesteps: 3102000
Best mean reward: -3.57 - Last mean reward per episode: -3.60
Num timesteps: 3105000
Best mean reward: -3.57 - Last mean reward per episode: -3.73
Num timesteps: 3108000
Best mean reward: -3.57 - Last mean reward per episode: -4.78
Num timesteps: 3111000
Best mean reward: -3.57 - Last mean reward per episode: -4.22
Num timesteps: 3114000
Best mean reward: -3.57 - Last mean reward per episode: -3.56
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 3117000
Best mean reward: -3.56 - Last mean reward per episode: -4.16
Num timesteps: 3120000
Best mean reward: -3.56 - Last mean reward per episode: -4.05
Num timesteps: 3123000
Best mean reward: -3.56 - Last mean reward per episode: -3.66
Num timesteps: 3126000
Best mean reward: -3.56 - Last mean reward per episode: -3.58
Num timesteps: 3129000
Best mean reward: -3.56 - Last mean reward per episode: -4.07
Num timesteps: 3132000
Best mean reward: -3.56 - Last mean reward per episode: -3.48
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 3135000
Best mean reward: -3.48 - Last mean reward per episode: -3.85
Num timesteps: 3138000
Best mean reward: -3.48 - Last mean reward per episode: -3.80
Num timesteps: 3141000
Best mean reward: -3.48 - Last mean reward per episode: -3.50
Num timesteps: 3144000
Best mean reward: -3.48 - Last mean reward per episode: -3.61
Num timesteps: 3147000
Best mean reward: -3.48 - Last mean reward per episode: -4.01
Num timesteps: 3150000
Best mean reward: -3.48 - Last mean reward per episode: -3.54
Num timesteps: 3153000
Best mean reward: -3.48 - Last mean reward per episode: -3.89
Num timesteps: 3156000
Best mean reward: -3.48 - Last mean reward per episode: -3.45
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 3159000
Best mean reward: -3.45 - Last mean reward per episode: -4.08
Num timesteps: 3162000
Best mean reward: -3.45 - Last mean reward per episode: -3.91
Num timesteps: 3165000
Best mean reward: -3.45 - Last mean reward per episode: -3.44
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 3168000
Best mean reward: -3.44 - Last mean reward per episode: -4.21
Num timesteps: 3171000
Best mean reward: -3.44 - Last mean reward per episode: -4.21
Num timesteps: 3174000
Best mean reward: -3.44 - Last mean reward per episode: -3.59
Num timesteps: 3177000
Best mean reward: -3.44 - Last mean reward per episode: -3.58
Num timesteps: 3180000
Best mean reward: -3.44 - Last mean reward per episode: -3.55
Num timesteps: 3183000
Best mean reward: -3.44 - Last mean reward per episode: -3.57
Num timesteps: 3186000
Best mean reward: -3.44 - Last mean reward per episode: -3.99
Num timesteps: 3189000
Best mean reward: -3.44 - Last mean reward per episode: -3.83
Num timesteps: 3192000
Best mean reward: -3.44 - Last mean reward per episode: -3.43
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 3195000
Best mean reward: -3.43 - Last mean reward per episode: -4.04
Num timesteps: 3198000
Best mean reward: -3.43 - Last mean reward per episode: -3.45
Num timesteps: 3201000
Best mean reward: -3.43 - Last mean reward per episode: -4.81
Num timesteps: 3204000
Best mean reward: -3.43 - Last mean reward per episode: -3.49
Num timesteps: 3207000
Best mean reward: -3.43 - Last mean reward per episode: -4.16
Num timesteps: 3210000
Best mean reward: -3.43 - Last mean reward per episode: -3.42
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 3213000
Best mean reward: -3.42 - Last mean reward per episode: -3.54
Num timesteps: 3216000
Best mean reward: -3.42 - Last mean reward per episode: -3.50
Num timesteps: 3219000
Best mean reward: -3.42 - Last mean reward per episode: -3.64
Num timesteps: 3222000
Best mean reward: -3.42 - Last mean reward per episode: -3.54
Num timesteps: 3225000
Best mean reward: -3.42 - Last mean reward per episode: -3.96
Num timesteps: 3228000
Best mean reward: -3.42 - Last mean reward per episode: -3.59
Num timesteps: 3231000
Best mean reward: -3.42 - Last mean reward per episode: -3.64
Num timesteps: 3234000
Best mean reward: -3.42 - Last mean reward per episode: -4.29
Num timesteps: 3237000
Best mean reward: -3.42 - Last mean reward per episode: -3.43
Num timesteps: 3240000
Best mean reward: -3.42 - Last mean reward per episode: -3.52
Num timesteps: 3243000
Best mean reward: -3.42 - Last mean reward per episode: -3.57
Num timesteps: 3246000
Best mean reward: -3.42 - Last mean reward per episode: -3.67
Num timesteps: 3249000
Best mean reward: -3.42 - Last mean reward per episode: -3.52
Num timesteps: 3252000
Best mean reward: -3.42 - Last mean reward per episode: -3.83
Num timesteps: 3255000
Best mean reward: -3.42 - Last mean reward per episode: -4.76
Num timesteps: 3258000
Best mean reward: -3.42 - Last mean reward per episode: -3.52
Num timesteps: 3261000
Best mean reward: -3.42 - Last mean reward per episode: -3.33
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 3264000
Best mean reward: -3.33 - Last mean reward per episode: -3.45
Num timesteps: 3267000
Best mean reward: -3.33 - Last mean reward per episode: -3.94
Num timesteps: 3270000
Best mean reward: -3.33 - Last mean reward per episode: -3.28
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 3273000
Best mean reward: -3.28 - Last mean reward per episode: -4.20
Num timesteps: 3276000
Best mean reward: -3.28 - Last mean reward per episode: -3.85
Num timesteps: 3279000
Best mean reward: -3.28 - Last mean reward per episode: -3.43
Num timesteps: 3282000
Best mean reward: -3.28 - Last mean reward per episode: -3.41
Num timesteps: 3285000
Best mean reward: -3.28 - Last mean reward per episode: -3.28
Num timesteps: 3288000
Best mean reward: -3.28 - Last mean reward per episode: -3.34
Num timesteps: 3291000
Best mean reward: -3.28 - Last mean reward per episode: -3.93
Num timesteps: 3294000
Best mean reward: -3.28 - Last mean reward per episode: -3.98
Num timesteps: 3297000
Best mean reward: -3.28 - Last mean reward per episode: -3.34
Num timesteps: 3300000
Best mean reward: -3.28 - Last mean reward per episode: -3.32
Num timesteps: 3303000
Best mean reward: -3.28 - Last mean reward per episode: -3.42
Num timesteps: 3306000
Best mean reward: -3.28 - Last mean reward per episode: -3.38
Num timesteps: 3309000
Best mean reward: -3.28 - Last mean reward per episode: -3.54
Num timesteps: 3312000
Best mean reward: -3.28 - Last mean reward per episode: -3.46
Num timesteps: 3315000
Best mean reward: -3.28 - Last mean reward per episode: -3.34
Num timesteps: 3318000
Best mean reward: -3.28 - Last mean reward per episode: -3.42
Num timesteps: 3321000
Best mean reward: -3.28 - Last mean reward per episode: -3.90
Num timesteps: 3324000
Best mean reward: -3.28 - Last mean reward per episode: -3.77
Num timesteps: 3327000
Best mean reward: -3.28 - Last mean reward per episode: -3.49
Num timesteps: 3330000
Best mean reward: -3.28 - Last mean reward per episode: -4.01
Num timesteps: 3333000
Best mean reward: -3.28 - Last mean reward per episode: -3.52
Num timesteps: 3336000
Best mean reward: -3.28 - Last mean reward per episode: -4.00
Num timesteps: 3339000
Best mean reward: -3.28 - Last mean reward per episode: -3.98
Num timesteps: 3342000
Best mean reward: -3.28 - Last mean reward per episode: -3.40
Num timesteps: 3345000
Best mean reward: -3.28 - Last mean reward per episode: -3.70
Num timesteps: 3348000
Best mean reward: -3.28 - Last mean reward per episode: -3.65
Num timesteps: 3351000
Best mean reward: -3.28 - Last mean reward per episode: -3.26
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 3354000
Best mean reward: -3.26 - Last mean reward per episode: -3.26
Num timesteps: 3357000
Best mean reward: -3.26 - Last mean reward per episode: -3.28
Num timesteps: 3360000
Best mean reward: -3.26 - Last mean reward per episode: -3.86
Num timesteps: 3363000
Best mean reward: -3.26 - Last mean reward per episode: -4.50
Num timesteps: 3366000
Best mean reward: -3.26 - Last mean reward per episode: -3.41
Num timesteps: 3369000
Best mean reward: -3.26 - Last mean reward per episode: -3.21
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 3372000
Best mean reward: -3.21 - Last mean reward per episode: -3.31
Num timesteps: 3375000
Best mean reward: -3.21 - Last mean reward per episode: -3.65
Num timesteps: 3378000
Best mean reward: -3.21 - Last mean reward per episode: -3.33
Num timesteps: 3381000
Best mean reward: -3.21 - Last mean reward per episode: -3.37
Num timesteps: 3384000
Best mean reward: -3.21 - Last mean reward per episode: -3.77
Num timesteps: 3387000
Best mean reward: -3.21 - Last mean reward per episode: -3.79
Num timesteps: 3390000
Best mean reward: -3.21 - Last mean reward per episode: -3.32
Num timesteps: 3393000
Best mean reward: -3.21 - Last mean reward per episode: -3.38
Num timesteps: 3396000
Best mean reward: -3.21 - Last mean reward per episode: -3.35
Num timesteps: 3399000
Best mean reward: -3.21 - Last mean reward per episode: -3.32
Num timesteps: 3402000
Best mean reward: -3.21 - Last mean reward per episode: -3.31
Num timesteps: 3405000
Best mean reward: -3.21 - Last mean reward per episode: -3.29
Num timesteps: 3408000
Best mean reward: -3.21 - Last mean reward per episode: -3.45
Num timesteps: 3411000
Best mean reward: -3.21 - Last mean reward per episode: -3.30
Num timesteps: 3414000
Best mean reward: -3.21 - Last mean reward per episode: -3.28
Num timesteps: 3417000
Best mean reward: -3.21 - Last mean reward per episode: -3.32
Num timesteps: 3420000
Best mean reward: -3.21 - Last mean reward per episode: -3.32
Num timesteps: 3423000
Best mean reward: -3.21 - Last mean reward per episode: -3.83
Num timesteps: 3426000
Best mean reward: -3.21 - Last mean reward per episode: -3.46
Num timesteps: 3429000
Best mean reward: -3.21 - Last mean reward per episode: -3.33
Num timesteps: 3432000
Best mean reward: -3.21 - Last mean reward per episode: -3.99
Num timesteps: 3435000
Best mean reward: -3.21 - Last mean reward per episode: -3.32
Num timesteps: 3438000
Best mean reward: -3.21 - Last mean reward per episode: -3.31
Num timesteps: 3441000
Best mean reward: -3.21 - Last mean reward per episode: -3.68
Num timesteps: 3444000
Best mean reward: -3.21 - Last mean reward per episode: -3.31
Num timesteps: 3447000
Best mean reward: -3.21 - Last mean reward per episode: -3.27
Num timesteps: 3450000
Best mean reward: -3.21 - Last mean reward per episode: -3.22
Num timesteps: 3453000
Best mean reward: -3.21 - Last mean reward per episode: -3.19
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 3456000
Best mean reward: -3.19 - Last mean reward per episode: -3.64
Num timesteps: 3459000
Best mean reward: -3.19 - Last mean reward per episode: -3.27
Num timesteps: 3462000
Best mean reward: -3.19 - Last mean reward per episode: -3.29
Num timesteps: 3465000
Best mean reward: -3.19 - Last mean reward per episode: -3.21
Num timesteps: 3468000
Best mean reward: -3.19 - Last mean reward per episode: -3.24
Num timesteps: 3471000
Best mean reward: -3.19 - Last mean reward per episode: -3.20
Num timesteps: 3474000
Best mean reward: -3.19 - Last mean reward per episode: -3.23
Num timesteps: 3477000
Best mean reward: -3.19 - Last mean reward per episode: -3.40
Num timesteps: 3480000
Best mean reward: -3.19 - Last mean reward per episode: -3.81
Num timesteps: 3483000
Best mean reward: -3.19 - Last mean reward per episode: -3.73
Num timesteps: 3486000
Best mean reward: -3.19 - Last mean reward per episode: -3.68
Num timesteps: 3489000
Best mean reward: -3.19 - Last mean reward per episode: -3.70
Num timesteps: 3492000
Best mean reward: -3.19 - Last mean reward per episode: -3.27
Num timesteps: 3495000
Best mean reward: -3.19 - Last mean reward per episode: -3.23
Num timesteps: 3498000
Best mean reward: -3.19 - Last mean reward per episode: -3.37
Num timesteps: 3501000
Best mean reward: -3.19 - Last mean reward per episode: -3.30
Num timesteps: 3504000
Best mean reward: -3.19 - Last mean reward per episode: -3.26
Num timesteps: 3507000
Best mean reward: -3.19 - Last mean reward per episode: -3.27
Num timesteps: 3510000
Best mean reward: -3.19 - Last mean reward per episode: -3.76
Num timesteps: 3513000
Best mean reward: -3.19 - Last mean reward per episode: -3.22
Num timesteps: 3516000
Best mean reward: -3.19 - Last mean reward per episode: -3.21
Num timesteps: 3519000
Best mean reward: -3.19 - Last mean reward per episode: -3.80
Num timesteps: 3522000
Best mean reward: -3.19 - Last mean reward per episode: -3.22
Num timesteps: 3525000
Best mean reward: -3.19 - Last mean reward per episode: -3.27
Num timesteps: 3528000
Best mean reward: -3.19 - Last mean reward per episode: -3.28
Num timesteps: 3531000
Best mean reward: -3.19 - Last mean reward per episode: -3.17
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 3534000
Best mean reward: -3.17 - Last mean reward per episode: -3.26
Num timesteps: 3537000
Best mean reward: -3.17 - Last mean reward per episode: -3.75
Num timesteps: 3540000
Best mean reward: -3.17 - Last mean reward per episode: -3.04
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 3543000
Best mean reward: -3.04 - Last mean reward per episode: -3.28
Num timesteps: 3546000
Best mean reward: -3.04 - Last mean reward per episode: -3.13
Num timesteps: 3549000
Best mean reward: -3.04 - Last mean reward per episode: -3.20
Num timesteps: 3552000
Best mean reward: -3.04 - Last mean reward per episode: -3.07
Num timesteps: 3555000
Best mean reward: -3.04 - Last mean reward per episode: -3.24
Num timesteps: 3558000
Best mean reward: -3.04 - Last mean reward per episode: -3.17
Num timesteps: 3561000
Best mean reward: -3.04 - Last mean reward per episode: -3.07
Num timesteps: 3564000
Best mean reward: -3.04 - Last mean reward per episode: -3.14
Num timesteps: 3567000
Best mean reward: -3.04 - Last mean reward per episode: -3.18
Num timesteps: 3570000
Best mean reward: -3.04 - Last mean reward per episode: -3.22
Num timesteps: 3573000
Best mean reward: -3.04 - Last mean reward per episode: -3.81
Num timesteps: 3576000
Best mean reward: -3.04 - Last mean reward per episode: -3.29
Num timesteps: 3579000
Best mean reward: -3.04 - Last mean reward per episode: -3.10
Num timesteps: 3582000
Best mean reward: -3.04 - Last mean reward per episode: -3.20
Num timesteps: 3585000
Best mean reward: -3.04 - Last mean reward per episode: -3.26
Num timesteps: 3588000
Best mean reward: -3.04 - Last mean reward per episode: -3.17
Num timesteps: 3591000
Best mean reward: -3.04 - Last mean reward per episode: -3.23
Num timesteps: 3594000
Best mean reward: -3.04 - Last mean reward per episode: -3.53
Num timesteps: 3597000
Best mean reward: -3.04 - Last mean reward per episode: -3.26
Num timesteps: 3600000
Best mean reward: -3.04 - Last mean reward per episode: -3.26
Num timesteps: 3603000
Best mean reward: -3.04 - Last mean reward per episode: -3.32
Num timesteps: 3606000
Best mean reward: -3.04 - Last mean reward per episode: -3.13
Num timesteps: 3609000
Best mean reward: -3.04 - Last mean reward per episode: -3.26
Num timesteps: 3612000
Best mean reward: -3.04 - Last mean reward per episode: -3.11
Num timesteps: 3615000
Best mean reward: -3.04 - Last mean reward per episode: -3.20
Num timesteps: 3618000
Best mean reward: -3.04 - Last mean reward per episode: -3.64
Num timesteps: 3621000
Best mean reward: -3.04 - Last mean reward per episode: -3.13
Num timesteps: 3624000
Best mean reward: -3.04 - Last mean reward per episode: -3.12
Num timesteps: 3627000
Best mean reward: -3.04 - Last mean reward per episode: -3.24
Num timesteps: 3630000
Best mean reward: -3.04 - Last mean reward per episode: -3.18
Num timesteps: 3633000
Best mean reward: -3.04 - Last mean reward per episode: -3.31
Num timesteps: 3636000
Best mean reward: -3.04 - Last mean reward per episode: -3.19
Num timesteps: 3639000
Best mean reward: -3.04 - Last mean reward per episode: -3.26
Num timesteps: 3642000
Best mean reward: -3.04 - Last mean reward per episode: -3.19
Num timesteps: 3645000
Best mean reward: -3.04 - Last mean reward per episode: -3.16
Num timesteps: 3648000
Best mean reward: -3.04 - Last mean reward per episode: -3.18
Num timesteps: 3651000
Best mean reward: -3.04 - Last mean reward per episode: -3.28
Num timesteps: 3654000
Best mean reward: -3.04 - Last mean reward per episode: -3.09
Num timesteps: 3657000
Best mean reward: -3.04 - Last mean reward per episode: -3.14
Num timesteps: 3660000
Best mean reward: -3.04 - Last mean reward per episode: -3.25
Num timesteps: 3663000
Best mean reward: -3.04 - Last mean reward per episode: -3.20
Num timesteps: 3666000
Best mean reward: -3.04 - Last mean reward per episode: -3.22
Num timesteps: 3669000
Best mean reward: -3.04 - Last mean reward per episode: -3.16
Num timesteps: 3672000
Best mean reward: -3.04 - Last mean reward per episode: -3.13
Num timesteps: 3675000
Best mean reward: -3.04 - Last mean reward per episode: -3.14
Num timesteps: 3678000
Best mean reward: -3.04 - Last mean reward per episode: -3.07
Num timesteps: 3681000
Best mean reward: -3.04 - Last mean reward per episode: -3.23
Num timesteps: 3684000
Best mean reward: -3.04 - Last mean reward per episode: -3.14
Num timesteps: 3687000
Best mean reward: -3.04 - Last mean reward per episode: -3.14
Num timesteps: 3690000
Best mean reward: -3.04 - Last mean reward per episode: -3.13
Num timesteps: 3693000
Best mean reward: -3.04 - Last mean reward per episode: -3.13
Num timesteps: 3696000
Best mean reward: -3.04 - Last mean reward per episode: -3.29
Num timesteps: 3699000
Best mean reward: -3.04 - Last mean reward per episode: -3.15
Num timesteps: 3702000
Best mean reward: -3.04 - Last mean reward per episode: -3.20
Num timesteps: 3705000
Best mean reward: -3.04 - Last mean reward per episode: -3.26
Num timesteps: 3708000
Best mean reward: -3.04 - Last mean reward per episode: -3.08
Num timesteps: 3711000
Best mean reward: -3.04 - Last mean reward per episode: -3.05
Num timesteps: 3714000
Best mean reward: -3.04 - Last mean reward per episode: -3.68
Num timesteps: 3717000
Best mean reward: -3.04 - Last mean reward per episode: -3.18
Num timesteps: 3720000
Best mean reward: -3.04 - Last mean reward per episode: -3.14
Num timesteps: 3723000
Best mean reward: -3.04 - Last mean reward per episode: -3.24
Num timesteps: 3726000
Best mean reward: -3.04 - Last mean reward per episode: -3.12
Num timesteps: 3729000
Best mean reward: -3.04 - Last mean reward per episode: -3.15
Num timesteps: 3732000
Best mean reward: -3.04 - Last mean reward per episode: -3.20
Num timesteps: 3735000
Best mean reward: -3.04 - Last mean reward per episode: -3.18
Num timesteps: 3738000
Best mean reward: -3.04 - Last mean reward per episode: -3.19
Num timesteps: 3741000
Best mean reward: -3.04 - Last mean reward per episode: -3.58
Num timesteps: 3744000
Best mean reward: -3.04 - Last mean reward per episode: -3.20
Num timesteps: 3747000
Best mean reward: -3.04 - Last mean reward per episode: -3.53
Num timesteps: 3750000
Best mean reward: -3.04 - Last mean reward per episode: -3.13
Num timesteps: 3753000
Best mean reward: -3.04 - Last mean reward per episode: -3.25
Num timesteps: 3756000
Best mean reward: -3.04 - Last mean reward per episode: -3.13
Num timesteps: 3759000
Best mean reward: -3.04 - Last mean reward per episode: -3.23
Num timesteps: 3762000
Best mean reward: -3.04 - Last mean reward per episode: -3.54
Num timesteps: 3765000
Best mean reward: -3.04 - Last mean reward per episode: -3.10
Num timesteps: 3768000
Best mean reward: -3.04 - Last mean reward per episode: -3.20
Num timesteps: 3771000
Best mean reward: -3.04 - Last mean reward per episode: -3.12
Num timesteps: 3774000
Best mean reward: -3.04 - Last mean reward per episode: -3.13
Num timesteps: 3777000
Best mean reward: -3.04 - Last mean reward per episode: -3.13
Num timesteps: 3780000
Best mean reward: -3.04 - Last mean reward per episode: -3.07
Num timesteps: 3783000
Best mean reward: -3.04 - Last mean reward per episode: -3.10
Num timesteps: 3786000
Best mean reward: -3.04 - Last mean reward per episode: -3.08
Num timesteps: 3789000
Best mean reward: -3.04 - Last mean reward per episode: -4.07
Num timesteps: 3792000
Best mean reward: -3.04 - Last mean reward per episode: -3.58
Num timesteps: 3795000
Best mean reward: -3.04 - Last mean reward per episode: -3.20
Num timesteps: 3798000
Best mean reward: -3.04 - Last mean reward per episode: -3.17
Num timesteps: 3801000
Best mean reward: -3.04 - Last mean reward per episode: -3.14
Num timesteps: 3804000
Best mean reward: -3.04 - Last mean reward per episode: -3.09
Num timesteps: 3807000
Best mean reward: -3.04 - Last mean reward per episode: -3.64
Num timesteps: 3810000
Best mean reward: -3.04 - Last mean reward per episode: -4.13
Num timesteps: 3813000
Best mean reward: -3.04 - Last mean reward per episode: -3.12
Num timesteps: 3816000
Best mean reward: -3.04 - Last mean reward per episode: -3.20
Num timesteps: 3819000
Best mean reward: -3.04 - Last mean reward per episode: -3.16
Num timesteps: 3822000
Best mean reward: -3.04 - Last mean reward per episode: -3.10
Num timesteps: 3825000
Best mean reward: -3.04 - Last mean reward per episode: -3.16
Num timesteps: 3828000
Best mean reward: -3.04 - Last mean reward per episode: -3.09
Num timesteps: 3831000
Best mean reward: -3.04 - Last mean reward per episode: -3.17
Num timesteps: 3834000
Best mean reward: -3.04 - Last mean reward per episode: -3.12
Num timesteps: 3837000
Best mean reward: -3.04 - Last mean reward per episode: -3.04
Num timesteps: 3840000
Best mean reward: -3.04 - Last mean reward per episode: -3.16
Num timesteps: 3843000
Best mean reward: -3.04 - Last mean reward per episode: -3.06
Num timesteps: 3846000
Best mean reward: -3.04 - Last mean reward per episode: -3.22
Num timesteps: 3849000
Best mean reward: -3.04 - Last mean reward per episode: -3.08
Num timesteps: 3852000
Best mean reward: -3.04 - Last mean reward per episode: -3.10
Num timesteps: 3855000
Best mean reward: -3.04 - Last mean reward per episode: -3.09
Num timesteps: 3858000
Best mean reward: -3.04 - Last mean reward per episode: -3.11
Num timesteps: 3861000
Best mean reward: -3.04 - Last mean reward per episode: -3.06
Num timesteps: 3864000
Best mean reward: -3.04 - Last mean reward per episode: -3.10
Num timesteps: 3867000
Best mean reward: -3.04 - Last mean reward per episode: -3.11
Num timesteps: 3870000
Best mean reward: -3.04 - Last mean reward per episode: -3.08
Num timesteps: 3873000
Best mean reward: -3.04 - Last mean reward per episode: -3.07
Num timesteps: 3876000
Best mean reward: -3.04 - Last mean reward per episode: -3.05
Num timesteps: 3879000
Best mean reward: -3.04 - Last mean reward per episode: -3.13
Num timesteps: 3882000
Best mean reward: -3.04 - Last mean reward per episode: -3.07
Num timesteps: 3885000
Best mean reward: -3.04 - Last mean reward per episode: -3.05
Num timesteps: 3888000
Best mean reward: -3.04 - Last mean reward per episode: -3.13
Num timesteps: 3891000
Best mean reward: -3.04 - Last mean reward per episode: -3.15
Num timesteps: 3894000
Best mean reward: -3.04 - Last mean reward per episode: -3.13
Num timesteps: 3897000
Best mean reward: -3.04 - Last mean reward per episode: -3.04
Num timesteps: 3900000
Best mean reward: -3.04 - Last mean reward per episode: -3.30
Num timesteps: 3903000
Best mean reward: -3.04 - Last mean reward per episode: -3.06
Num timesteps: 3906000
Best mean reward: -3.04 - Last mean reward per episode: -3.12
Num timesteps: 3909000
Best mean reward: -3.04 - Last mean reward per episode: -3.09
Num timesteps: 3912000
Best mean reward: -3.04 - Last mean reward per episode: -3.09
Num timesteps: 3915000
Best mean reward: -3.04 - Last mean reward per episode: -3.06
Num timesteps: 3918000
Best mean reward: -3.04 - Last mean reward per episode: -3.11
Num timesteps: 3921000
Best mean reward: -3.04 - Last mean reward per episode: -3.10
Num timesteps: 3924000
Best mean reward: -3.04 - Last mean reward per episode: -3.53
Num timesteps: 3927000
Best mean reward: -3.04 - Last mean reward per episode: -3.11
Num timesteps: 3930000
Best mean reward: -3.04 - Last mean reward per episode: -3.57
Num timesteps: 3933000
Best mean reward: -3.04 - Last mean reward per episode: -3.03
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 3936000
Best mean reward: -3.03 - Last mean reward per episode: -3.07
Num timesteps: 3939000
Best mean reward: -3.03 - Last mean reward per episode: -3.02
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 3942000
Best mean reward: -3.02 - Last mean reward per episode: -3.57
Num timesteps: 3945000
Best mean reward: -3.02 - Last mean reward per episode: -3.00
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 3948000
Best mean reward: -3.00 - Last mean reward per episode: -3.16
Num timesteps: 3951000
Best mean reward: -3.00 - Last mean reward per episode: -3.09
Num timesteps: 3954000
Best mean reward: -3.00 - Last mean reward per episode: -3.01
Num timesteps: 3957000
Best mean reward: -3.00 - Last mean reward per episode: -3.14
Num timesteps: 3960000
Best mean reward: -3.00 - Last mean reward per episode: -3.03
Num timesteps: 3963000
Best mean reward: -3.00 - Last mean reward per episode: -3.04
Num timesteps: 3966000
Best mean reward: -3.00 - Last mean reward per episode: -3.05
Num timesteps: 3969000
Best mean reward: -3.00 - Last mean reward per episode: -3.06
Num timesteps: 3972000
Best mean reward: -3.00 - Last mean reward per episode: -3.09
Num timesteps: 3975000
Best mean reward: -3.00 - Last mean reward per episode: -3.15
Num timesteps: 3978000
Best mean reward: -3.00 - Last mean reward per episode: -3.11
Num timesteps: 3981000
Best mean reward: -3.00 - Last mean reward per episode: -3.07
Num timesteps: 3984000
Best mean reward: -3.00 - Last mean reward per episode: -3.06
Num timesteps: 3987000
Best mean reward: -3.00 - Last mean reward per episode: -3.07
Num timesteps: 3990000
Best mean reward: -3.00 - Last mean reward per episode: -3.54
Num timesteps: 3993000
Best mean reward: -3.00 - Last mean reward per episode: -3.40
Num timesteps: 3996000
Best mean reward: -3.00 - Last mean reward per episode: -3.23
Num timesteps: 3999000
Best mean reward: -3.00 - Last mean reward per episode: -3.12
Num timesteps: 4002000
Best mean reward: -3.00 - Last mean reward per episode: -3.03
Num timesteps: 4005000
Best mean reward: -3.00 - Last mean reward per episode: -3.04
Num timesteps: 4008000
Best mean reward: -3.00 - Last mean reward per episode: -3.11
Num timesteps: 4011000
Best mean reward: -3.00 - Last mean reward per episode: -3.11
Num timesteps: 4014000
Best mean reward: -3.00 - Last mean reward per episode: -3.06
Num timesteps: 4017000
Best mean reward: -3.00 - Last mean reward per episode: -3.02
Num timesteps: 4020000
Best mean reward: -3.00 - Last mean reward per episode: -3.51
Num timesteps: 4023000
Best mean reward: -3.00 - Last mean reward per episode: -3.10
Num timesteps: 4026000
Best mean reward: -3.00 - Last mean reward per episode: -3.51
Num timesteps: 4029000
Best mean reward: -3.00 - Last mean reward per episode: -3.09
Num timesteps: 4032000
Best mean reward: -3.00 - Last mean reward per episode: -3.04
Num timesteps: 4035000
Best mean reward: -3.00 - Last mean reward per episode: -3.06
Num timesteps: 4038000
Best mean reward: -3.00 - Last mean reward per episode: -3.01
Num timesteps: 4041000
Best mean reward: -3.00 - Last mean reward per episode: -3.02
Num timesteps: 4044000
Best mean reward: -3.00 - Last mean reward per episode: -2.99
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 4047000
Best mean reward: -2.99 - Last mean reward per episode: -3.05
Num timesteps: 4050000
Best mean reward: -2.99 - Last mean reward per episode: -3.06
Num timesteps: 4053000
Best mean reward: -2.99 - Last mean reward per episode: -3.07
Num timesteps: 4056000
Best mean reward: -2.99 - Last mean reward per episode: -3.01
Num timesteps: 4059000
Best mean reward: -2.99 - Last mean reward per episode: -3.02
Num timesteps: 4062000
Best mean reward: -2.99 - Last mean reward per episode: -3.04
Num timesteps: 4065000
Best mean reward: -2.99 - Last mean reward per episode: -3.01
Num timesteps: 4068000
Best mean reward: -2.99 - Last mean reward per episode: -3.07
Num timesteps: 4071000
Best mean reward: -2.99 - Last mean reward per episode: -3.08
Num timesteps: 4074000
Best mean reward: -2.99 - Last mean reward per episode: -3.03
Num timesteps: 4077000
Best mean reward: -2.99 - Last mean reward per episode: -3.04
Num timesteps: 4080000
Best mean reward: -2.99 - Last mean reward per episode: -3.03
Num timesteps: 4083000
Best mean reward: -2.99 - Last mean reward per episode: -3.01
Num timesteps: 4086000
Best mean reward: -2.99 - Last mean reward per episode: -3.04
Num timesteps: 4089000
Best mean reward: -2.99 - Last mean reward per episode: -3.03
Num timesteps: 4092000
Best mean reward: -2.99 - Last mean reward per episode: -3.01
Num timesteps: 4095000
Best mean reward: -2.99 - Last mean reward per episode: -3.09
Num timesteps: 4098000
Best mean reward: -2.99 - Last mean reward per episode: -3.14
Num timesteps: 4101000
Best mean reward: -2.99 - Last mean reward per episode: -2.99
Num timesteps: 4104000
Best mean reward: -2.99 - Last mean reward per episode: -3.14
Num timesteps: 4107000
Best mean reward: -2.99 - Last mean reward per episode: -3.62
Num timesteps: 4110000
Best mean reward: -2.99 - Last mean reward per episode: -3.02
Num timesteps: 4113000
Best mean reward: -2.99 - Last mean reward per episode: -3.13
Num timesteps: 4116000
Best mean reward: -2.99 - Last mean reward per episode: -3.00
Num timesteps: 4119000
Best mean reward: -2.99 - Last mean reward per episode: -3.04
Num timesteps: 4122000
Best mean reward: -2.99 - Last mean reward per episode: -3.54
Num timesteps: 4125000
Best mean reward: -2.99 - Last mean reward per episode: -3.04
Num timesteps: 4128000
Best mean reward: -2.99 - Last mean reward per episode: -3.47
Num timesteps: 4131000
Best mean reward: -2.99 - Last mean reward per episode: -3.02
Num timesteps: 4134000
Best mean reward: -2.99 - Last mean reward per episode: -3.04
Num timesteps: 4137000
Best mean reward: -2.99 - Last mean reward per episode: -3.05
Num timesteps: 4140000
Best mean reward: -2.99 - Last mean reward per episode: -3.02
Num timesteps: 4143000
Best mean reward: -2.99 - Last mean reward per episode: -3.43
Num timesteps: 4146000
Best mean reward: -2.99 - Last mean reward per episode: -3.02
Num timesteps: 4149000
Best mean reward: -2.99 - Last mean reward per episode: -3.00
Num timesteps: 4152000
Best mean reward: -2.99 - Last mean reward per episode: -3.02
Num timesteps: 4155000
Best mean reward: -2.99 - Last mean reward per episode: -3.02
Num timesteps: 4158000
Best mean reward: -2.99 - Last mean reward per episode: -3.03
Num timesteps: 4161000
Best mean reward: -2.99 - Last mean reward per episode: -3.06
Num timesteps: 4164000
Best mean reward: -2.99 - Last mean reward per episode: -3.08
Num timesteps: 4167000
Best mean reward: -2.99 - Last mean reward per episode: -3.11
Num timesteps: 4170000
Best mean reward: -2.99 - Last mean reward per episode: -3.03
Num timesteps: 4173000
Best mean reward: -2.99 - Last mean reward per episode: -3.06
Num timesteps: 4176000
Best mean reward: -2.99 - Last mean reward per episode: -3.06
Num timesteps: 4179000
Best mean reward: -2.99 - Last mean reward per episode: -3.01
Num timesteps: 4182000
Best mean reward: -2.99 - Last mean reward per episode: -3.03
Num timesteps: 4185000
Best mean reward: -2.99 - Last mean reward per episode: -2.98
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 4188000
Best mean reward: -2.98 - Last mean reward per episode: -3.46
Num timesteps: 4191000
Best mean reward: -2.98 - Last mean reward per episode: -2.98
Num timesteps: 4194000
Best mean reward: -2.98 - Last mean reward per episode: -3.03
Num timesteps: 4197000
Best mean reward: -2.98 - Last mean reward per episode: -2.99
Num timesteps: 4200000
Best mean reward: -2.98 - Last mean reward per episode: -3.07
Num timesteps: 4203000
Best mean reward: -2.98 - Last mean reward per episode: -3.01
Num timesteps: 4206000
Best mean reward: -2.98 - Last mean reward per episode: -3.05
Num timesteps: 4209000
Best mean reward: -2.98 - Last mean reward per episode: -3.01
Num timesteps: 4212000
Best mean reward: -2.98 - Last mean reward per episode: -3.04
Num timesteps: 4215000
Best mean reward: -2.98 - Last mean reward per episode: -3.02
Num timesteps: 4218000
Best mean reward: -2.98 - Last mean reward per episode: -3.00
Num timesteps: 4221000
Best mean reward: -2.98 - Last mean reward per episode: -3.08
Num timesteps: 4224000
Best mean reward: -2.98 - Last mean reward per episode: -3.09
Num timesteps: 4227000
Best mean reward: -2.98 - Last mean reward per episode: -3.03
Num timesteps: 4230000
Best mean reward: -2.98 - Last mean reward per episode: -2.99
Num timesteps: 4233000
Best mean reward: -2.98 - Last mean reward per episode: -2.95
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 4236000
Best mean reward: -2.95 - Last mean reward per episode: -3.03
Num timesteps: 4239000
Best mean reward: -2.95 - Last mean reward per episode: -3.02
Num timesteps: 4242000
Best mean reward: -2.95 - Last mean reward per episode: -2.97
Num timesteps: 4245000
Best mean reward: -2.95 - Last mean reward per episode: -3.01
Num timesteps: 4248000
Best mean reward: -2.95 - Last mean reward per episode: -2.97
Num timesteps: 4251000
Best mean reward: -2.95 - Last mean reward per episode: -2.99
Num timesteps: 4254000
Best mean reward: -2.95 - Last mean reward per episode: -3.06
Num timesteps: 4257000
Best mean reward: -2.95 - Last mean reward per episode: -3.58
Num timesteps: 4260000
Best mean reward: -2.95 - Last mean reward per episode: -3.00
Num timesteps: 4263000
Best mean reward: -2.95 - Last mean reward per episode: -3.03
Num timesteps: 4266000
Best mean reward: -2.95 - Last mean reward per episode: -3.00
Num timesteps: 4269000
Best mean reward: -2.95 - Last mean reward per episode: -2.97
Num timesteps: 4272000
Best mean reward: -2.95 - Last mean reward per episode: -2.99
Num timesteps: 4275000
Best mean reward: -2.95 - Last mean reward per episode: -3.00
Num timesteps: 4278000
Best mean reward: -2.95 - Last mean reward per episode: -3.04
Num timesteps: 4281000
Best mean reward: -2.95 - Last mean reward per episode: -2.98
Num timesteps: 4284000
Best mean reward: -2.95 - Last mean reward per episode: -3.01
Num timesteps: 4287000
Best mean reward: -2.95 - Last mean reward per episode: -3.49
Num timesteps: 4290000
Best mean reward: -2.95 - Last mean reward per episode: -3.01
Num timesteps: 4293000
Best mean reward: -2.95 - Last mean reward per episode: -2.99
Num timesteps: 4296000
Best mean reward: -2.95 - Last mean reward per episode: -2.97
Num timesteps: 4299000
Best mean reward: -2.95 - Last mean reward per episode: -3.02
Num timesteps: 4302000
Best mean reward: -2.95 - Last mean reward per episode: -3.07
Num timesteps: 4305000
Best mean reward: -2.95 - Last mean reward per episode: -2.99
Num timesteps: 4308000
Best mean reward: -2.95 - Last mean reward per episode: -3.00
Num timesteps: 4311000
Best mean reward: -2.95 - Last mean reward per episode: -3.04
Num timesteps: 4314000
Best mean reward: -2.95 - Last mean reward per episode: -3.03
Num timesteps: 4317000
Best mean reward: -2.95 - Last mean reward per episode: -3.02
Num timesteps: 4320000
Best mean reward: -2.95 - Last mean reward per episode: -2.98
Num timesteps: 4323000
Best mean reward: -2.95 - Last mean reward per episode: -3.02
Num timesteps: 4326000
Best mean reward: -2.95 - Last mean reward per episode: -2.98
Num timesteps: 4329000
Best mean reward: -2.95 - Last mean reward per episode: -2.96
Num timesteps: 4332000
Best mean reward: -2.95 - Last mean reward per episode: -2.94
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 4335000
Best mean reward: -2.94 - Last mean reward per episode: -2.99
Num timesteps: 4338000
Best mean reward: -2.94 - Last mean reward per episode: -2.97
Num timesteps: 4341000
Best mean reward: -2.94 - Last mean reward per episode: -2.98
Num timesteps: 4344000
Best mean reward: -2.94 - Last mean reward per episode: -2.95
Num timesteps: 4347000
Best mean reward: -2.94 - Last mean reward per episode: -3.00
Num timesteps: 4350000
Best mean reward: -2.94 - Last mean reward per episode: -2.96
Num timesteps: 4353000
Best mean reward: -2.94 - Last mean reward per episode: -3.00
Num timesteps: 4356000
Best mean reward: -2.94 - Last mean reward per episode: -2.96
Num timesteps: 4359000
Best mean reward: -2.94 - Last mean reward per episode: -3.06
Num timesteps: 4362000
Best mean reward: -2.94 - Last mean reward per episode: -2.99
Num timesteps: 4365000
Best mean reward: -2.94 - Last mean reward per episode: -2.94
Num timesteps: 4368000
Best mean reward: -2.94 - Last mean reward per episode: -3.01
Num timesteps: 4371000
Best mean reward: -2.94 - Last mean reward per episode: -2.98
Num timesteps: 4374000
Best mean reward: -2.94 - Last mean reward per episode: -2.99
Num timesteps: 4377000
Best mean reward: -2.94 - Last mean reward per episode: -3.38
Num timesteps: 4380000
Best mean reward: -2.94 - Last mean reward per episode: -3.04
Num timesteps: 4383000
Best mean reward: -2.94 - Last mean reward per episode: -2.98
Num timesteps: 4386000
Best mean reward: -2.94 - Last mean reward per episode: -2.99
Num timesteps: 4389000
Best mean reward: -2.94 - Last mean reward per episode: -3.00
Num timesteps: 4392000
Best mean reward: -2.94 - Last mean reward per episode: -2.99
Num timesteps: 4395000
Best mean reward: -2.94 - Last mean reward per episode: -2.97
Num timesteps: 4398000
Best mean reward: -2.94 - Last mean reward per episode: -2.97
Num timesteps: 4401000
Best mean reward: -2.94 - Last mean reward per episode: -3.02
Num timesteps: 4404000
Best mean reward: -2.94 - Last mean reward per episode: -3.48
Num timesteps: 4407000
Best mean reward: -2.94 - Last mean reward per episode: -3.09
Num timesteps: 4410000
Best mean reward: -2.94 - Last mean reward per episode: -3.01
Num timesteps: 4413000
Best mean reward: -2.94 - Last mean reward per episode: -2.96
Num timesteps: 4416000
Best mean reward: -2.94 - Last mean reward per episode: -3.46
Num timesteps: 4419000
Best mean reward: -2.94 - Last mean reward per episode: -3.10
Num timesteps: 4422000
Best mean reward: -2.94 - Last mean reward per episode: -2.99
Num timesteps: 4425000
Best mean reward: -2.94 - Last mean reward per episode: -2.99
Num timesteps: 4428000
Best mean reward: -2.94 - Last mean reward per episode: -3.10
Num timesteps: 4431000
Best mean reward: -2.94 - Last mean reward per episode: -2.99
Num timesteps: 4434000
Best mean reward: -2.94 - Last mean reward per episode: -3.03
Num timesteps: 4437000
Best mean reward: -2.94 - Last mean reward per episode: -2.94
Num timesteps: 4440000
Best mean reward: -2.94 - Last mean reward per episode: -2.99
Num timesteps: 4443000
Best mean reward: -2.94 - Last mean reward per episode: -3.00
Num timesteps: 4446000
Best mean reward: -2.94 - Last mean reward per episode: -3.46
Num timesteps: 4449000
Best mean reward: -2.94 - Last mean reward per episode: -3.00
Num timesteps: 4452000
Best mean reward: -2.94 - Last mean reward per episode: -2.98
Num timesteps: 4455000
Best mean reward: -2.94 - Last mean reward per episode: -2.97
Num timesteps: 4458000
Best mean reward: -2.94 - Last mean reward per episode: -2.96
Num timesteps: 4461000
Best mean reward: -2.94 - Last mean reward per episode: -3.01
Num timesteps: 4464000
Best mean reward: -2.94 - Last mean reward per episode: -3.03
Num timesteps: 4467000
Best mean reward: -2.94 - Last mean reward per episode: -2.99
Num timesteps: 4470000
Best mean reward: -2.94 - Last mean reward per episode: -2.95
Num timesteps: 4473000
Best mean reward: -2.94 - Last mean reward per episode: -2.95
Num timesteps: 4476000
Best mean reward: -2.94 - Last mean reward per episode: -2.95
Num timesteps: 4479000
Best mean reward: -2.94 - Last mean reward per episode: -2.98
Num timesteps: 4482000
Best mean reward: -2.94 - Last mean reward per episode: -2.98
Num timesteps: 4485000
Best mean reward: -2.94 - Last mean reward per episode: -2.95
Num timesteps: 4488000
Best mean reward: -2.94 - Last mean reward per episode: -3.01
Num timesteps: 4491000
Best mean reward: -2.94 - Last mean reward per episode: -2.98
Num timesteps: 4494000
Best mean reward: -2.94 - Last mean reward per episode: -2.97
Num timesteps: 4497000
Best mean reward: -2.94 - Last mean reward per episode: -2.95
Num timesteps: 4500000
Best mean reward: -2.94 - Last mean reward per episode: -2.99
Num timesteps: 4503000
Best mean reward: -2.94 - Last mean reward per episode: -3.00
Num timesteps: 4506000
Best mean reward: -2.94 - Last mean reward per episode: -3.00
Num timesteps: 4509000
Best mean reward: -2.94 - Last mean reward per episode: -2.99
Num timesteps: 4512000
Best mean reward: -2.94 - Last mean reward per episode: -2.94
Num timesteps: 4515000
Best mean reward: -2.94 - Last mean reward per episode: -3.05
Num timesteps: 4518000
Best mean reward: -2.94 - Last mean reward per episode: -3.00
Num timesteps: 4521000
Best mean reward: -2.94 - Last mean reward per episode: -3.03
Num timesteps: 4524000
Best mean reward: -2.94 - Last mean reward per episode: -2.96
Num timesteps: 4527000
Best mean reward: -2.94 - Last mean reward per episode: -2.96
Num timesteps: 4530000
Best mean reward: -2.94 - Last mean reward per episode: -2.99
Num timesteps: 4533000
Best mean reward: -2.94 - Last mean reward per episode: -2.98
Num timesteps: 4536000
Best mean reward: -2.94 - Last mean reward per episode: -2.96
Num timesteps: 4539000
Best mean reward: -2.94 - Last mean reward per episode: -2.94
Num timesteps: 4542000
Best mean reward: -2.94 - Last mean reward per episode: -2.97
Num timesteps: 4545000
Best mean reward: -2.94 - Last mean reward per episode: -2.98
Num timesteps: 4548000
Best mean reward: -2.94 - Last mean reward per episode: -3.11
Num timesteps: 4551000
Best mean reward: -2.94 - Last mean reward per episode: -2.96
Num timesteps: 4554000
Best mean reward: -2.94 - Last mean reward per episode: -2.96
Num timesteps: 4557000
Best mean reward: -2.94 - Last mean reward per episode: -2.99
Num timesteps: 4560000
Best mean reward: -2.94 - Last mean reward per episode: -2.96
Num timesteps: 4563000
Best mean reward: -2.94 - Last mean reward per episode: -2.98
Num timesteps: 4566000
Best mean reward: -2.94 - Last mean reward per episode: -2.98
Num timesteps: 4569000
Best mean reward: -2.94 - Last mean reward per episode: -2.96
Num timesteps: 4572000
Best mean reward: -2.94 - Last mean reward per episode: -3.00
Num timesteps: 4575000
Best mean reward: -2.94 - Last mean reward per episode: -2.93
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 4578000
Best mean reward: -2.93 - Last mean reward per episode: -2.99
Num timesteps: 4581000
Best mean reward: -2.93 - Last mean reward per episode: -2.99
Num timesteps: 4584000
Best mean reward: -2.93 - Last mean reward per episode: -2.96
Num timesteps: 4587000
Best mean reward: -2.93 - Last mean reward per episode: -2.98
Num timesteps: 4590000
Best mean reward: -2.93 - Last mean reward per episode: -2.93
Num timesteps: 4593000
Best mean reward: -2.93 - Last mean reward per episode: -2.96
Num timesteps: 4596000
Best mean reward: -2.93 - Last mean reward per episode: -2.91
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 4599000
Best mean reward: -2.91 - Last mean reward per episode: -3.45
Num timesteps: 4602000
Best mean reward: -2.91 - Last mean reward per episode: -2.99
Num timesteps: 4605000
Best mean reward: -2.91 - Last mean reward per episode: -2.97
Num timesteps: 4608000
Best mean reward: -2.91 - Last mean reward per episode: -2.97
Num timesteps: 4611000
Best mean reward: -2.91 - Last mean reward per episode: -2.99
Num timesteps: 4614000
Best mean reward: -2.91 - Last mean reward per episode: -2.99
Num timesteps: 4617000
Best mean reward: -2.91 - Last mean reward per episode: -2.97
Num timesteps: 4620000
Best mean reward: -2.91 - Last mean reward per episode: -3.00
Num timesteps: 4623000
Best mean reward: -2.91 - Last mean reward per episode: -2.97
Num timesteps: 4626000
Best mean reward: -2.91 - Last mean reward per episode: -2.92
Num timesteps: 4629000
Best mean reward: -2.91 - Last mean reward per episode: -2.95
Num timesteps: 4632000
Best mean reward: -2.91 - Last mean reward per episode: -2.99
Num timesteps: 4635000
Best mean reward: -2.91 - Last mean reward per episode: -2.97
Num timesteps: 4638000
Best mean reward: -2.91 - Last mean reward per episode: -2.99
Num timesteps: 4641000
Best mean reward: -2.91 - Last mean reward per episode: -2.96
Num timesteps: 4644000
Best mean reward: -2.91 - Last mean reward per episode: -3.01
Num timesteps: 4647000
Best mean reward: -2.91 - Last mean reward per episode: -2.98
Num timesteps: 4650000
Best mean reward: -2.91 - Last mean reward per episode: -2.96
Num timesteps: 4653000
Best mean reward: -2.91 - Last mean reward per episode: -2.98
Num timesteps: 4656000
Best mean reward: -2.91 - Last mean reward per episode: -3.38
Num timesteps: 4659000
Best mean reward: -2.91 - Last mean reward per episode: -2.97
Num timesteps: 4662000
Best mean reward: -2.91 - Last mean reward per episode: -2.98
Num timesteps: 4665000
Best mean reward: -2.91 - Last mean reward per episode: -2.97
Num timesteps: 4668000
Best mean reward: -2.91 - Last mean reward per episode: -2.99
Num timesteps: 4671000
Best mean reward: -2.91 - Last mean reward per episode: -2.97
Num timesteps: 4674000
Best mean reward: -2.91 - Last mean reward per episode: -2.98
Num timesteps: 4677000
Best mean reward: -2.91 - Last mean reward per episode: -2.95
Num timesteps: 4680000
Best mean reward: -2.91 - Last mean reward per episode: -2.98
Num timesteps: 4683000
Best mean reward: -2.91 - Last mean reward per episode: -2.99
Num timesteps: 4686000
Best mean reward: -2.91 - Last mean reward per episode: -2.96
Num timesteps: 4689000
Best mean reward: -2.91 - Last mean reward per episode: -2.96
Num timesteps: 4692000
Best mean reward: -2.91 - Last mean reward per episode: -2.99
Num timesteps: 4695000
Best mean reward: -2.91 - Last mean reward per episode: -2.97
Num timesteps: 4698000
Best mean reward: -2.91 - Last mean reward per episode: -2.99
Num timesteps: 4701000
Best mean reward: -2.91 - Last mean reward per episode: -3.24
Num timesteps: 4704000
Best mean reward: -2.91 - Last mean reward per episode: -3.03
Num timesteps: 4707000
Best mean reward: -2.91 - Last mean reward per episode: -2.97
Num timesteps: 4710000
Best mean reward: -2.91 - Last mean reward per episode: -3.11
Num timesteps: 4713000
Best mean reward: -2.91 - Last mean reward per episode: -2.94
Num timesteps: 4716000
Best mean reward: -2.91 - Last mean reward per episode: -2.98
Num timesteps: 4719000
Best mean reward: -2.91 - Last mean reward per episode: -2.93
Num timesteps: 4722000
Best mean reward: -2.91 - Last mean reward per episode: -3.00
Num timesteps: 4725000
Best mean reward: -2.91 - Last mean reward per episode: -2.96
Num timesteps: 4728000
Best mean reward: -2.91 - Last mean reward per episode: -3.00
Num timesteps: 4731000
Best mean reward: -2.91 - Last mean reward per episode: -2.99
Num timesteps: 4734000
Best mean reward: -2.91 - Last mean reward per episode: -2.94
Num timesteps: 4737000
Best mean reward: -2.91 - Last mean reward per episode: -3.04
Num timesteps: 4740000
Best mean reward: -2.91 - Last mean reward per episode: -2.98
Num timesteps: 4743000
Best mean reward: -2.91 - Last mean reward per episode: -3.00
Num timesteps: 4746000
Best mean reward: -2.91 - Last mean reward per episode: -2.98
Num timesteps: 4749000
Best mean reward: -2.91 - Last mean reward per episode: -3.02
Num timesteps: 4752000
Best mean reward: -2.91 - Last mean reward per episode: -2.96
Num timesteps: 4755000
Best mean reward: -2.91 - Last mean reward per episode: -2.96
Num timesteps: 4758000
Best mean reward: -2.91 - Last mean reward per episode: -2.97
Num timesteps: 4761000
Best mean reward: -2.91 - Last mean reward per episode: -2.95
Num timesteps: 4764000
Best mean reward: -2.91 - Last mean reward per episode: -3.08
Num timesteps: 4767000
Best mean reward: -2.91 - Last mean reward per episode: -2.97
Num timesteps: 4770000
Best mean reward: -2.91 - Last mean reward per episode: -2.98
Num timesteps: 4773000
Best mean reward: -2.91 - Last mean reward per episode: -3.44
Num timesteps: 4776000
Best mean reward: -2.91 - Last mean reward per episode: -2.99
Num timesteps: 4779000
Best mean reward: -2.91 - Last mean reward per episode: -2.91
Num timesteps: 4782000
Best mean reward: -2.91 - Last mean reward per episode: -2.97
Num timesteps: 4785000
Best mean reward: -2.91 - Last mean reward per episode: -3.41
Num timesteps: 4788000
Best mean reward: -2.91 - Last mean reward per episode: -2.90
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 4791000
Best mean reward: -2.90 - Last mean reward per episode: -2.94
Num timesteps: 4794000
Best mean reward: -2.90 - Last mean reward per episode: -3.02
Num timesteps: 4797000
Best mean reward: -2.90 - Last mean reward per episode: -2.98
Num timesteps: 4800000
Best mean reward: -2.90 - Last mean reward per episode: -2.90
Num timesteps: 4803000
Best mean reward: -2.90 - Last mean reward per episode: -2.94
Num timesteps: 4806000
Best mean reward: -2.90 - Last mean reward per episode: -2.95
Num timesteps: 4809000
Best mean reward: -2.90 - Last mean reward per episode: -2.96
Num timesteps: 4812000
Best mean reward: -2.90 - Last mean reward per episode: -2.92
Num timesteps: 4815000
Best mean reward: -2.90 - Last mean reward per episode: -2.97
Num timesteps: 4818000
Best mean reward: -2.90 - Last mean reward per episode: -2.90
Num timesteps: 4821000
Best mean reward: -2.90 - Last mean reward per episode: -2.88
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 4824000
Best mean reward: -2.88 - Last mean reward per episode: -2.98
Num timesteps: 4827000
Best mean reward: -2.88 - Last mean reward per episode: -2.94
Num timesteps: 4830000
Best mean reward: -2.88 - Last mean reward per episode: -2.92
Num timesteps: 4833000
Best mean reward: -2.88 - Last mean reward per episode: -2.94
Num timesteps: 4836000
Best mean reward: -2.88 - Last mean reward per episode: -2.93
Num timesteps: 4839000
Best mean reward: -2.88 - Last mean reward per episode: -2.95
Num timesteps: 4842000
Best mean reward: -2.88 - Last mean reward per episode: -3.90
Num timesteps: 4845000
Best mean reward: -2.88 - Last mean reward per episode: -2.93
Num timesteps: 4848000
Best mean reward: -2.88 - Last mean reward per episode: -2.90
Num timesteps: 4851000
Best mean reward: -2.88 - Last mean reward per episode: -2.88
Num timesteps: 4854000
Best mean reward: -2.88 - Last mean reward per episode: -2.96
Num timesteps: 4857000
Best mean reward: -2.88 - Last mean reward per episode: -3.38
Num timesteps: 4860000
Best mean reward: -2.88 - Last mean reward per episode: -2.94
Num timesteps: 4863000
Best mean reward: -2.88 - Last mean reward per episode: -2.96
Num timesteps: 4866000
Best mean reward: -2.88 - Last mean reward per episode: -2.90
Num timesteps: 4869000
Best mean reward: -2.88 - Last mean reward per episode: -2.94
Num timesteps: 4872000
Best mean reward: -2.88 - Last mean reward per episode: -2.92
Num timesteps: 4875000
Best mean reward: -2.88 - Last mean reward per episode: -2.91
Num timesteps: 4878000
Best mean reward: -2.88 - Last mean reward per episode: -2.93
Num timesteps: 4881000
Best mean reward: -2.88 - Last mean reward per episode: -2.93
Num timesteps: 4884000
Best mean reward: -2.88 - Last mean reward per episode: -2.97
Num timesteps: 4887000
Best mean reward: -2.88 - Last mean reward per episode: -2.93
Num timesteps: 4890000
Best mean reward: -2.88 - Last mean reward per episode: -3.41
Num timesteps: 4893000
Best mean reward: -2.88 - Last mean reward per episode: -2.95
Num timesteps: 4896000
Best mean reward: -2.88 - Last mean reward per episode: -2.93
Num timesteps: 4899000
Best mean reward: -2.88 - Last mean reward per episode: -2.92
Num timesteps: 4902000
Best mean reward: -2.88 - Last mean reward per episode: -2.97
Num timesteps: 4905000
Best mean reward: -2.88 - Last mean reward per episode: -3.03
Num timesteps: 4908000
Best mean reward: -2.88 - Last mean reward per episode: -2.96
Num timesteps: 4911000
Best mean reward: -2.88 - Last mean reward per episode: -2.94
Num timesteps: 4914000
Best mean reward: -2.88 - Last mean reward per episode: -2.95
Num timesteps: 4917000
Best mean reward: -2.88 - Last mean reward per episode: -3.40
Num timesteps: 4920000
Best mean reward: -2.88 - Last mean reward per episode: -2.92
Num timesteps: 4923000
Best mean reward: -2.88 - Last mean reward per episode: -3.39
Num timesteps: 4926000
Best mean reward: -2.88 - Last mean reward per episode: -2.94
Num timesteps: 4929000
Best mean reward: -2.88 - Last mean reward per episode: -2.97
Num timesteps: 4932000
Best mean reward: -2.88 - Last mean reward per episode: -2.95
Num timesteps: 4935000
Best mean reward: -2.88 - Last mean reward per episode: -2.97
Num timesteps: 4938000
Best mean reward: -2.88 - Last mean reward per episode: -2.93
Num timesteps: 4941000
Best mean reward: -2.88 - Last mean reward per episode: -2.94
Num timesteps: 4944000
Best mean reward: -2.88 - Last mean reward per episode: -2.93
Num timesteps: 4947000
Best mean reward: -2.88 - Last mean reward per episode: -3.04
Num timesteps: 4950000
Best mean reward: -2.88 - Last mean reward per episode: -2.99
Num timesteps: 4953000
Best mean reward: -2.88 - Last mean reward per episode: -2.90
Num timesteps: 4956000
Best mean reward: -2.88 - Last mean reward per episode: -2.94
Num timesteps: 4959000
Best mean reward: -2.88 - Last mean reward per episode: -3.00
Num timesteps: 4962000
Best mean reward: -2.88 - Last mean reward per episode: -2.98
Num timesteps: 4965000
Best mean reward: -2.88 - Last mean reward per episode: -2.93
Num timesteps: 4968000
Best mean reward: -2.88 - Last mean reward per episode: -2.96
Num timesteps: 4971000
Best mean reward: -2.88 - Last mean reward per episode: -2.95
Num timesteps: 4974000
Best mean reward: -2.88 - Last mean reward per episode: -2.92
Num timesteps: 4977000
Best mean reward: -2.88 - Last mean reward per episode: -2.95
Num timesteps: 4980000
Best mean reward: -2.88 - Last mean reward per episode: -2.96
Num timesteps: 4983000
Best mean reward: -2.88 - Last mean reward per episode: -2.94
Num timesteps: 4986000
Best mean reward: -2.88 - Last mean reward per episode: -2.91
Num timesteps: 4989000
Best mean reward: -2.88 - Last mean reward per episode: -2.98
Num timesteps: 4992000
Best mean reward: -2.88 - Last mean reward per episode: -2.93
Num timesteps: 4995000
Best mean reward: -2.88 - Last mean reward per episode: -2.94
Num timesteps: 4998000
Best mean reward: -2.88 - Last mean reward per episode: -2.95
Num timesteps: 5001000
Best mean reward: -2.88 - Last mean reward per episode: -2.98
Num timesteps: 5004000
Best mean reward: -2.88 - Last mean reward per episode: -2.96
Num timesteps: 5007000
Best mean reward: -2.88 - Last mean reward per episode: -2.93
Num timesteps: 5010000
Best mean reward: -2.88 - Last mean reward per episode: -2.97
Num timesteps: 5013000
Best mean reward: -2.88 - Last mean reward per episode: -2.93
Num timesteps: 5016000
Best mean reward: -2.88 - Last mean reward per episode: -2.93
Num timesteps: 5019000
Best mean reward: -2.88 - Last mean reward per episode: -2.84
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 5022000
Best mean reward: -2.84 - Last mean reward per episode: -2.96
Num timesteps: 5025000
Best mean reward: -2.84 - Last mean reward per episode: -2.95
Num timesteps: 5028000
Best mean reward: -2.84 - Last mean reward per episode: -2.95
Num timesteps: 5031000
Best mean reward: -2.84 - Last mean reward per episode: -2.92
Num timesteps: 5034000
Best mean reward: -2.84 - Last mean reward per episode: -2.93
Num timesteps: 5037000
Best mean reward: -2.84 - Last mean reward per episode: -2.95
Num timesteps: 5040000
Best mean reward: -2.84 - Last mean reward per episode: -2.95
Num timesteps: 5043000
Best mean reward: -2.84 - Last mean reward per episode: -2.94
Num timesteps: 5046000
Best mean reward: -2.84 - Last mean reward per episode: -2.89
Num timesteps: 5049000
Best mean reward: -2.84 - Last mean reward per episode: -2.97
Num timesteps: 5052000
Best mean reward: -2.84 - Last mean reward per episode: -2.95
Num timesteps: 5055000
Best mean reward: -2.84 - Last mean reward per episode: -2.88
Num timesteps: 5058000
Best mean reward: -2.84 - Last mean reward per episode: -2.86
Num timesteps: 5061000
Best mean reward: -2.84 - Last mean reward per episode: -3.06
Num timesteps: 5064000
Best mean reward: -2.84 - Last mean reward per episode: -2.91
Num timesteps: 5067000
Best mean reward: -2.84 - Last mean reward per episode: -2.93
Num timesteps: 5070000
Best mean reward: -2.84 - Last mean reward per episode: -2.94
Num timesteps: 5073000
Best mean reward: -2.84 - Last mean reward per episode: -2.94
Num timesteps: 5076000
Best mean reward: -2.84 - Last mean reward per episode: -2.92
Num timesteps: 5079000
Best mean reward: -2.84 - Last mean reward per episode: -2.95
Num timesteps: 5082000
Best mean reward: -2.84 - Last mean reward per episode: -2.97
Num timesteps: 5085000
Best mean reward: -2.84 - Last mean reward per episode: -2.89
Num timesteps: 5088000
Best mean reward: -2.84 - Last mean reward per episode: -2.90
Num timesteps: 5091000
Best mean reward: -2.84 - Last mean reward per episode: -2.93
Num timesteps: 5094000
Best mean reward: -2.84 - Last mean reward per episode: -2.93
Num timesteps: 5097000
Best mean reward: -2.84 - Last mean reward per episode: -2.91
Num timesteps: 5100000
Best mean reward: -2.84 - Last mean reward per episode: -2.87
Num timesteps: 5103000
Best mean reward: -2.84 - Last mean reward per episode: -2.92
Num timesteps: 5106000
Best mean reward: -2.84 - Last mean reward per episode: -2.91
Num timesteps: 5109000
Best mean reward: -2.84 - Last mean reward per episode: -2.94
Num timesteps: 5112000
Best mean reward: -2.84 - Last mean reward per episode: -2.89
Num timesteps: 5115000
Best mean reward: -2.84 - Last mean reward per episode: -2.87
Num timesteps: 5118000
Best mean reward: -2.84 - Last mean reward per episode: -2.91
Num timesteps: 5121000
Best mean reward: -2.84 - Last mean reward per episode: -2.89
Num timesteps: 5124000
Best mean reward: -2.84 - Last mean reward per episode: -2.90
Num timesteps: 5127000
Best mean reward: -2.84 - Last mean reward per episode: -2.94
Num timesteps: 5130000
Best mean reward: -2.84 - Last mean reward per episode: -2.93
Num timesteps: 5133000
Best mean reward: -2.84 - Last mean reward per episode: -2.95
Num timesteps: 5136000
Best mean reward: -2.84 - Last mean reward per episode: -2.98
Num timesteps: 5139000
Best mean reward: -2.84 - Last mean reward per episode: -2.93
Num timesteps: 5142000
Best mean reward: -2.84 - Last mean reward per episode: -2.88
Num timesteps: 5145000
Best mean reward: -2.84 - Last mean reward per episode: -2.93
Num timesteps: 5148000
Best mean reward: -2.84 - Last mean reward per episode: -2.93
Num timesteps: 5151000
Best mean reward: -2.84 - Last mean reward per episode: -2.90
Num timesteps: 5154000
Best mean reward: -2.84 - Last mean reward per episode: -2.92
Num timesteps: 5157000
Best mean reward: -2.84 - Last mean reward per episode: -2.95
Num timesteps: 5160000
Best mean reward: -2.84 - Last mean reward per episode: -2.92
Num timesteps: 5163000
Best mean reward: -2.84 - Last mean reward per episode: -2.88
Num timesteps: 5166000
Best mean reward: -2.84 - Last mean reward per episode: -2.93
Num timesteps: 5169000
Best mean reward: -2.84 - Last mean reward per episode: -2.95
Num timesteps: 5172000
Best mean reward: -2.84 - Last mean reward per episode: -2.88
Num timesteps: 5175000
Best mean reward: -2.84 - Last mean reward per episode: -2.97
Num timesteps: 5178000
Best mean reward: -2.84 - Last mean reward per episode: -2.94
Num timesteps: 5181000
Best mean reward: -2.84 - Last mean reward per episode: -2.93
Num timesteps: 5184000
Best mean reward: -2.84 - Last mean reward per episode: -2.91
Num timesteps: 5187000
Best mean reward: -2.84 - Last mean reward per episode: -2.93
Num timesteps: 5190000
Best mean reward: -2.84 - Last mean reward per episode: -2.96
Num timesteps: 5193000
Best mean reward: -2.84 - Last mean reward per episode: -2.88
Num timesteps: 5196000
Best mean reward: -2.84 - Last mean reward per episode: -3.36
Num timesteps: 5199000
Best mean reward: -2.84 - Last mean reward per episode: -2.88
Num timesteps: 5202000
Best mean reward: -2.84 - Last mean reward per episode: -2.86
Num timesteps: 5205000
Best mean reward: -2.84 - Last mean reward per episode: -2.89
Num timesteps: 5208000
Best mean reward: -2.84 - Last mean reward per episode: -2.93
Num timesteps: 5211000
Best mean reward: -2.84 - Last mean reward per episode: -2.96
Num timesteps: 5214000
Best mean reward: -2.84 - Last mean reward per episode: -2.88
Num timesteps: 5217000
Best mean reward: -2.84 - Last mean reward per episode: -2.89
Num timesteps: 5220000
Best mean reward: -2.84 - Last mean reward per episode: -2.93
Num timesteps: 5223000
Best mean reward: -2.84 - Last mean reward per episode: -2.87
Num timesteps: 5226000
Best mean reward: -2.84 - Last mean reward per episode: -2.93
Num timesteps: 5229000
Best mean reward: -2.84 - Last mean reward per episode: -2.93
Num timesteps: 5232000
Best mean reward: -2.84 - Last mean reward per episode: -2.90
Num timesteps: 5235000
Best mean reward: -2.84 - Last mean reward per episode: -2.87
Num timesteps: 5238000
Best mean reward: -2.84 - Last mean reward per episode: -2.92
Num timesteps: 5241000
Best mean reward: -2.84 - Last mean reward per episode: -2.92
Num timesteps: 5244000
Best mean reward: -2.84 - Last mean reward per episode: -2.86
Num timesteps: 5247000
Best mean reward: -2.84 - Last mean reward per episode: -2.92
Num timesteps: 5250000
Best mean reward: -2.84 - Last mean reward per episode: -2.95
Num timesteps: 5253000
Best mean reward: -2.84 - Last mean reward per episode: -2.91
Num timesteps: 5256000
Best mean reward: -2.84 - Last mean reward per episode: -2.93
Num timesteps: 5259000
Best mean reward: -2.84 - Last mean reward per episode: -2.89
Num timesteps: 5262000
Best mean reward: -2.84 - Last mean reward per episode: -2.91
Num timesteps: 5265000
Best mean reward: -2.84 - Last mean reward per episode: -2.88
Num timesteps: 5268000
Best mean reward: -2.84 - Last mean reward per episode: -2.90
Num timesteps: 5271000
Best mean reward: -2.84 - Last mean reward per episode: -2.97
Num timesteps: 5274000
Best mean reward: -2.84 - Last mean reward per episode: -2.89
Num timesteps: 5277000
Best mean reward: -2.84 - Last mean reward per episode: -2.91
Num timesteps: 5280000
Best mean reward: -2.84 - Last mean reward per episode: -2.86
Num timesteps: 5283000
Best mean reward: -2.84 - Last mean reward per episode: -2.88
Num timesteps: 5286000
Best mean reward: -2.84 - Last mean reward per episode: -2.90
Num timesteps: 5289000
Best mean reward: -2.84 - Last mean reward per episode: -3.41
Num timesteps: 5292000
Best mean reward: -2.84 - Last mean reward per episode: -3.37
Num timesteps: 5295000
Best mean reward: -2.84 - Last mean reward per episode: -2.87
Num timesteps: 5298000
Best mean reward: -2.84 - Last mean reward per episode: -2.88
Num timesteps: 5301000
Best mean reward: -2.84 - Last mean reward per episode: -2.91
Num timesteps: 5304000
Best mean reward: -2.84 - Last mean reward per episode: -2.93
Num timesteps: 5307000
Best mean reward: -2.84 - Last mean reward per episode: -3.00
Num timesteps: 5310000
Best mean reward: -2.84 - Last mean reward per episode: -2.95
Num timesteps: 5313000
Best mean reward: -2.84 - Last mean reward per episode: -2.91
Num timesteps: 5316000
Best mean reward: -2.84 - Last mean reward per episode: -2.91
Num timesteps: 5319000
Best mean reward: -2.84 - Last mean reward per episode: -2.89
Num timesteps: 5322000
Best mean reward: -2.84 - Last mean reward per episode: -2.91
Num timesteps: 5325000
Best mean reward: -2.84 - Last mean reward per episode: -2.82
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 5328000
Best mean reward: -2.82 - Last mean reward per episode: -2.91
Num timesteps: 5331000
Best mean reward: -2.82 - Last mean reward per episode: -2.90
Num timesteps: 5334000
Best mean reward: -2.82 - Last mean reward per episode: -2.90
Num timesteps: 5337000
Best mean reward: -2.82 - Last mean reward per episode: -2.91
Num timesteps: 5340000
Best mean reward: -2.82 - Last mean reward per episode: -2.91
Num timesteps: 5343000
Best mean reward: -2.82 - Last mean reward per episode: -2.93
Num timesteps: 5346000
Best mean reward: -2.82 - Last mean reward per episode: -2.92
Num timesteps: 5349000
Best mean reward: -2.82 - Last mean reward per episode: -2.85
Num timesteps: 5352000
Best mean reward: -2.82 - Last mean reward per episode: -2.89
Num timesteps: 5355000
Best mean reward: -2.82 - Last mean reward per episode: -3.34
Num timesteps: 5358000
Best mean reward: -2.82 - Last mean reward per episode: -2.91
Num timesteps: 5361000
Best mean reward: -2.82 - Last mean reward per episode: -2.92
Num timesteps: 5364000
Best mean reward: -2.82 - Last mean reward per episode: -2.87
Num timesteps: 5367000
Best mean reward: -2.82 - Last mean reward per episode: -2.88
Num timesteps: 5370000
Best mean reward: -2.82 - Last mean reward per episode: -2.90
Num timesteps: 5373000
Best mean reward: -2.82 - Last mean reward per episode: -2.95
Num timesteps: 5376000
Best mean reward: -2.82 - Last mean reward per episode: -2.89
Num timesteps: 5379000
Best mean reward: -2.82 - Last mean reward per episode: -2.84
Num timesteps: 5382000
Best mean reward: -2.82 - Last mean reward per episode: -2.93
Num timesteps: 5385000
Best mean reward: -2.82 - Last mean reward per episode: -2.91
Num timesteps: 5388000
Best mean reward: -2.82 - Last mean reward per episode: -2.90
Num timesteps: 5391000
Best mean reward: -2.82 - Last mean reward per episode: -2.88
Num timesteps: 5394000
Best mean reward: -2.82 - Last mean reward per episode: -2.87
Num timesteps: 5397000
Best mean reward: -2.82 - Last mean reward per episode: -2.83
Num timesteps: 5400000
Best mean reward: -2.82 - Last mean reward per episode: -3.35
Num timesteps: 5403000
Best mean reward: -2.82 - Last mean reward per episode: -3.37
Num timesteps: 5406000
Best mean reward: -2.82 - Last mean reward per episode: -2.91
Num timesteps: 5409000
Best mean reward: -2.82 - Last mean reward per episode: -2.85
Num timesteps: 5412000
Best mean reward: -2.82 - Last mean reward per episode: -2.85
Num timesteps: 5415000
Best mean reward: -2.82 - Last mean reward per episode: -2.90
Num timesteps: 5418000
Best mean reward: -2.82 - Last mean reward per episode: -2.89
Num timesteps: 5421000
Best mean reward: -2.82 - Last mean reward per episode: -2.82
Num timesteps: 5424000
Best mean reward: -2.82 - Last mean reward per episode: -2.89
Num timesteps: 5427000
Best mean reward: -2.82 - Last mean reward per episode: -2.88
Num timesteps: 5430000
Best mean reward: -2.82 - Last mean reward per episode: -2.87
Num timesteps: 5433000
Best mean reward: -2.82 - Last mean reward per episode: -2.86
Num timesteps: 5436000
Best mean reward: -2.82 - Last mean reward per episode: -2.86
Num timesteps: 5439000
Best mean reward: -2.82 - Last mean reward per episode: -2.90
Num timesteps: 5442000
Best mean reward: -2.82 - Last mean reward per episode: -2.90
Num timesteps: 5445000
Best mean reward: -2.82 - Last mean reward per episode: -2.93
Num timesteps: 5448000
Best mean reward: -2.82 - Last mean reward per episode: -2.85
Num timesteps: 5451000
Best mean reward: -2.82 - Last mean reward per episode: -2.94
Num timesteps: 5454000
Best mean reward: -2.82 - Last mean reward per episode: -2.93
Num timesteps: 5457000
Best mean reward: -2.82 - Last mean reward per episode: -2.87
Num timesteps: 5460000
Best mean reward: -2.82 - Last mean reward per episode: -2.85
Num timesteps: 5463000
Best mean reward: -2.82 - Last mean reward per episode: -2.85
Num timesteps: 5466000
Best mean reward: -2.82 - Last mean reward per episode: -2.87
Num timesteps: 5469000
Best mean reward: -2.82 - Last mean reward per episode: -2.84
Num timesteps: 5472000
Best mean reward: -2.82 - Last mean reward per episode: -2.90
Num timesteps: 5475000
Best mean reward: -2.82 - Last mean reward per episode: -2.83
Num timesteps: 5478000
Best mean reward: -2.82 - Last mean reward per episode: -2.89
Num timesteps: 5481000
Best mean reward: -2.82 - Last mean reward per episode: -2.90
Num timesteps: 5484000
Best mean reward: -2.82 - Last mean reward per episode: -2.91
Num timesteps: 5487000
Best mean reward: -2.82 - Last mean reward per episode: -2.88
Num timesteps: 5490000
Best mean reward: -2.82 - Last mean reward per episode: -2.85
Num timesteps: 5493000
Best mean reward: -2.82 - Last mean reward per episode: -2.86
Num timesteps: 5496000
Best mean reward: -2.82 - Last mean reward per episode: -2.92
Num timesteps: 5499000
Best mean reward: -2.82 - Last mean reward per episode: -2.88
Num timesteps: 5502000
Best mean reward: -2.82 - Last mean reward per episode: -2.86
Num timesteps: 5505000
Best mean reward: -2.82 - Last mean reward per episode: -2.93
Num timesteps: 5508000
Best mean reward: -2.82 - Last mean reward per episode: -2.91
Num timesteps: 5511000
Best mean reward: -2.82 - Last mean reward per episode: -2.87
Num timesteps: 5514000
Best mean reward: -2.82 - Last mean reward per episode: -2.90
Num timesteps: 5517000
Best mean reward: -2.82 - Last mean reward per episode: -2.91
Num timesteps: 5520000
Best mean reward: -2.82 - Last mean reward per episode: -2.89
Num timesteps: 5523000
Best mean reward: -2.82 - Last mean reward per episode: -2.84
Num timesteps: 5526000
Best mean reward: -2.82 - Last mean reward per episode: -2.86
Num timesteps: 5529000
Best mean reward: -2.82 - Last mean reward per episode: -2.97
Num timesteps: 5532000
Best mean reward: -2.82 - Last mean reward per episode: -2.83
Num timesteps: 5535000
Best mean reward: -2.82 - Last mean reward per episode: -2.90
Num timesteps: 5538000
Best mean reward: -2.82 - Last mean reward per episode: -2.85
Num timesteps: 5541000
Best mean reward: -2.82 - Last mean reward per episode: -2.87
Num timesteps: 5544000
Best mean reward: -2.82 - Last mean reward per episode: -2.85
Num timesteps: 5547000
Best mean reward: -2.82 - Last mean reward per episode: -2.89
Num timesteps: 5550000
Best mean reward: -2.82 - Last mean reward per episode: -2.91
Num timesteps: 5553000
Best mean reward: -2.82 - Last mean reward per episode: -2.86
Num timesteps: 5556000
Best mean reward: -2.82 - Last mean reward per episode: -2.99
Num timesteps: 5559000
Best mean reward: -2.82 - Last mean reward per episode: -3.37
Num timesteps: 5562000
Best mean reward: -2.82 - Last mean reward per episode: -2.92
Num timesteps: 5565000
Best mean reward: -2.82 - Last mean reward per episode: -2.79
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 5568000
Best mean reward: -2.79 - Last mean reward per episode: -2.92
Num timesteps: 5571000
Best mean reward: -2.79 - Last mean reward per episode: -3.31
Num timesteps: 5574000
Best mean reward: -2.79 - Last mean reward per episode: -2.83
Num timesteps: 5577000
Best mean reward: -2.79 - Last mean reward per episode: -2.87
Num timesteps: 5580000
Best mean reward: -2.79 - Last mean reward per episode: -2.95
Num timesteps: 5583000
Best mean reward: -2.79 - Last mean reward per episode: -2.90
Num timesteps: 5586000
Best mean reward: -2.79 - Last mean reward per episode: -2.83
Num timesteps: 5589000
Best mean reward: -2.79 - Last mean reward per episode: -2.76
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 5592000
Best mean reward: -2.76 - Last mean reward per episode: -2.84
Num timesteps: 5595000
Best mean reward: -2.76 - Last mean reward per episode: -2.88
Num timesteps: 5598000
Best mean reward: -2.76 - Last mean reward per episode: -2.86
Num timesteps: 5601000
Best mean reward: -2.76 - Last mean reward per episode: -2.93
Num timesteps: 5604000
Best mean reward: -2.76 - Last mean reward per episode: -3.38
Num timesteps: 5607000
Best mean reward: -2.76 - Last mean reward per episode: -3.04
Num timesteps: 5610000
Best mean reward: -2.76 - Last mean reward per episode: -3.13
Num timesteps: 5613000
Best mean reward: -2.76 - Last mean reward per episode: -3.43
Num timesteps: 5616000
Best mean reward: -2.76 - Last mean reward per episode: -3.09
Num timesteps: 5619000
Best mean reward: -2.76 - Last mean reward per episode: -3.11
Num timesteps: 5622000
Best mean reward: -2.76 - Last mean reward per episode: -4.08
Num timesteps: 5625000
Best mean reward: -2.76 - Last mean reward per episode: -2.98
Num timesteps: 5628000
Best mean reward: -2.76 - Last mean reward per episode: -3.58
Num timesteps: 5631000
Best mean reward: -2.76 - Last mean reward per episode: -3.14
Num timesteps: 5634000
Best mean reward: -2.76 - Last mean reward per episode: -2.98
Num timesteps: 5637000
Best mean reward: -2.76 - Last mean reward per episode: -3.46
Num timesteps: 5640000
Best mean reward: -2.76 - Last mean reward per episode: -3.59
Num timesteps: 5643000
Best mean reward: -2.76 - Last mean reward per episode: -3.51
Num timesteps: 5646000
Best mean reward: -2.76 - Last mean reward per episode: -4.03
Num timesteps: 5649000
Best mean reward: -2.76 - Last mean reward per episode: -3.16
Num timesteps: 5652000
Best mean reward: -2.76 - Last mean reward per episode: -3.48
Num timesteps: 5655000
Best mean reward: -2.76 - Last mean reward per episode: -3.00
Num timesteps: 5658000
Best mean reward: -2.76 - Last mean reward per episode: -3.13
Num timesteps: 5661000
Best mean reward: -2.76 - Last mean reward per episode: -3.13
Num timesteps: 5664000
Best mean reward: -2.76 - Last mean reward per episode: -3.01
Num timesteps: 5667000
Best mean reward: -2.76 - Last mean reward per episode: -4.03
Num timesteps: 5670000
Best mean reward: -2.76 - Last mean reward per episode: -3.07
Num timesteps: 5673000
Best mean reward: -2.76 - Last mean reward per episode: -3.97
Num timesteps: 5676000
Best mean reward: -2.76 - Last mean reward per episode: -3.52
Num timesteps: 5679000
Best mean reward: -2.76 - Last mean reward per episode: -3.17
Num timesteps: 5682000
Best mean reward: -2.76 - Last mean reward per episode: -2.99
Num timesteps: 5685000
Best mean reward: -2.76 - Last mean reward per episode: -3.01
Num timesteps: 5688000
Best mean reward: -2.76 - Last mean reward per episode: -3.02
Num timesteps: 5691000
Best mean reward: -2.76 - Last mean reward per episode: -3.12
Num timesteps: 5694000
Best mean reward: -2.76 - Last mean reward per episode: -3.01
Num timesteps: 5697000
Best mean reward: -2.76 - Last mean reward per episode: -3.44
Num timesteps: 5700000
Best mean reward: -2.76 - Last mean reward per episode: -3.04
Num timesteps: 5703000
Best mean reward: -2.76 - Last mean reward per episode: -3.39
Num timesteps: 5706000
Best mean reward: -2.76 - Last mean reward per episode: -2.93
Num timesteps: 5709000
Best mean reward: -2.76 - Last mean reward per episode: -3.01
Num timesteps: 5712000
Best mean reward: -2.76 - Last mean reward per episode: -2.88
Num timesteps: 5715000
Best mean reward: -2.76 - Last mean reward per episode: -2.94
Num timesteps: 5718000
Best mean reward: -2.76 - Last mean reward per episode: -3.32
Num timesteps: 5721000
Best mean reward: -2.76 - Last mean reward per episode: -2.97
Num timesteps: 5724000
Best mean reward: -2.76 - Last mean reward per episode: -2.84
Num timesteps: 5727000
Best mean reward: -2.76 - Last mean reward per episode: -2.96
Num timesteps: 5730000
Best mean reward: -2.76 - Last mean reward per episode: -2.95
Num timesteps: 5733000
Best mean reward: -2.76 - Last mean reward per episode: -2.90
Num timesteps: 5736000
Best mean reward: -2.76 - Last mean reward per episode: -2.87
Num timesteps: 5739000
Best mean reward: -2.76 - Last mean reward per episode: -2.93
Num timesteps: 5742000
Best mean reward: -2.76 - Last mean reward per episode: -3.30
Num timesteps: 5745000
Best mean reward: -2.76 - Last mean reward per episode: -2.95
Num timesteps: 5748000
Best mean reward: -2.76 - Last mean reward per episode: -2.97
Num timesteps: 5751000
Best mean reward: -2.76 - Last mean reward per episode: -2.98
Num timesteps: 5754000
Best mean reward: -2.76 - Last mean reward per episode: -2.87
Num timesteps: 5757000
Best mean reward: -2.76 - Last mean reward per episode: -2.97
Num timesteps: 5760000
Best mean reward: -2.76 - Last mean reward per episode: -2.92
Num timesteps: 5763000
Best mean reward: -2.76 - Last mean reward per episode: -3.39
Num timesteps: 5766000
Best mean reward: -2.76 - Last mean reward per episode: -2.97
Num timesteps: 5769000
Best mean reward: -2.76 - Last mean reward per episode: -2.92
Num timesteps: 5772000
Best mean reward: -2.76 - Last mean reward per episode: -3.40
Num timesteps: 5775000
Best mean reward: -2.76 - Last mean reward per episode: -3.02
Num timesteps: 5778000
Best mean reward: -2.76 - Last mean reward per episode: -3.04
Num timesteps: 5781000
Best mean reward: -2.76 - Last mean reward per episode: -2.94
Num timesteps: 5784000
Best mean reward: -2.76 - Last mean reward per episode: -3.00
Num timesteps: 5787000
Best mean reward: -2.76 - Last mean reward per episode: -2.92
Num timesteps: 5790000
Best mean reward: -2.76 - Last mean reward per episode: -2.93
Num timesteps: 5793000
Best mean reward: -2.76 - Last mean reward per episode: -3.00
Num timesteps: 5796000
Best mean reward: -2.76 - Last mean reward per episode: -3.52
Num timesteps: 5799000
Best mean reward: -2.76 - Last mean reward per episode: -2.99
Num timesteps: 5802000
Best mean reward: -2.76 - Last mean reward per episode: -2.89
Num timesteps: 5805000
Best mean reward: -2.76 - Last mean reward per episode: -2.91
Num timesteps: 5808000
Best mean reward: -2.76 - Last mean reward per episode: -2.93
Num timesteps: 5811000
Best mean reward: -2.76 - Last mean reward per episode: -2.93
Num timesteps: 5814000
Best mean reward: -2.76 - Last mean reward per episode: -2.89
Num timesteps: 5817000
Best mean reward: -2.76 - Last mean reward per episode: -2.87
Num timesteps: 5820000
Best mean reward: -2.76 - Last mean reward per episode: -2.92
Num timesteps: 5823000
Best mean reward: -2.76 - Last mean reward per episode: -2.86
Num timesteps: 5826000
Best mean reward: -2.76 - Last mean reward per episode: -2.84
Num timesteps: 5829000
Best mean reward: -2.76 - Last mean reward per episode: -3.39
Num timesteps: 5832000
Best mean reward: -2.76 - Last mean reward per episode: -2.88
Num timesteps: 5835000
Best mean reward: -2.76 - Last mean reward per episode: -2.97
Num timesteps: 5838000
Best mean reward: -2.76 - Last mean reward per episode: -2.88
Num timesteps: 5841000
Best mean reward: -2.76 - Last mean reward per episode: -2.89
Num timesteps: 5844000
Best mean reward: -2.76 - Last mean reward per episode: -2.93
Num timesteps: 5847000
Best mean reward: -2.76 - Last mean reward per episode: -2.87
Num timesteps: 5850000
Best mean reward: -2.76 - Last mean reward per episode: -2.90
Num timesteps: 5853000
Best mean reward: -2.76 - Last mean reward per episode: -2.91
Num timesteps: 5856000
Best mean reward: -2.76 - Last mean reward per episode: -2.90
Num timesteps: 5859000
Best mean reward: -2.76 - Last mean reward per episode: -2.91
Num timesteps: 5862000
Best mean reward: -2.76 - Last mean reward per episode: -3.33
Num timesteps: 5865000
Best mean reward: -2.76 - Last mean reward per episode: -2.85
Num timesteps: 5868000
Best mean reward: -2.76 - Last mean reward per episode: -2.93
Num timesteps: 5871000
Best mean reward: -2.76 - Last mean reward per episode: -2.85
Num timesteps: 5874000
Best mean reward: -2.76 - Last mean reward per episode: -2.84
Num timesteps: 5877000
Best mean reward: -2.76 - Last mean reward per episode: -2.92
Num timesteps: 5880000
Best mean reward: -2.76 - Last mean reward per episode: -3.29
Num timesteps: 5883000
Best mean reward: -2.76 - Last mean reward per episode: -2.87
Num timesteps: 5886000
Best mean reward: -2.76 - Last mean reward per episode: -3.37
Num timesteps: 5889000
Best mean reward: -2.76 - Last mean reward per episode: -2.89
Num timesteps: 5892000
Best mean reward: -2.76 - Last mean reward per episode: -2.88
Num timesteps: 5895000
Best mean reward: -2.76 - Last mean reward per episode: -2.92
Num timesteps: 5898000
Best mean reward: -2.76 - Last mean reward per episode: -3.35
Num timesteps: 5901000
Best mean reward: -2.76 - Last mean reward per episode: -2.88
Num timesteps: 5904000
Best mean reward: -2.76 - Last mean reward per episode: -2.82
Num timesteps: 5907000
Best mean reward: -2.76 - Last mean reward per episode: -2.85
Num timesteps: 5910000
Best mean reward: -2.76 - Last mean reward per episode: -2.92
Num timesteps: 5913000
Best mean reward: -2.76 - Last mean reward per episode: -2.94
Num timesteps: 5916000
Best mean reward: -2.76 - Last mean reward per episode: -2.96
Num timesteps: 5919000
Best mean reward: -2.76 - Last mean reward per episode: -2.82
Num timesteps: 5922000
Best mean reward: -2.76 - Last mean reward per episode: -2.88
Num timesteps: 5925000
Best mean reward: -2.76 - Last mean reward per episode: -2.81
Num timesteps: 5928000
Best mean reward: -2.76 - Last mean reward per episode: -2.87
Num timesteps: 5931000
Best mean reward: -2.76 - Last mean reward per episode: -2.85
Num timesteps: 5934000
Best mean reward: -2.76 - Last mean reward per episode: -2.85
Num timesteps: 5937000
Best mean reward: -2.76 - Last mean reward per episode: -2.81
Num timesteps: 5940000
Best mean reward: -2.76 - Last mean reward per episode: -2.85
Num timesteps: 5943000
Best mean reward: -2.76 - Last mean reward per episode: -2.87
Num timesteps: 5946000
Best mean reward: -2.76 - Last mean reward per episode: -3.28
Num timesteps: 5949000
Best mean reward: -2.76 - Last mean reward per episode: -2.79
Num timesteps: 5952000
Best mean reward: -2.76 - Last mean reward per episode: -2.89
Num timesteps: 5955000
Best mean reward: -2.76 - Last mean reward per episode: -2.92
Num timesteps: 5958000
Best mean reward: -2.76 - Last mean reward per episode: -2.89
Num timesteps: 5961000
Best mean reward: -2.76 - Last mean reward per episode: -2.86
Num timesteps: 5964000
Best mean reward: -2.76 - Last mean reward per episode: -2.96
Num timesteps: 5967000
Best mean reward: -2.76 - Last mean reward per episode: -2.87
Num timesteps: 5970000
Best mean reward: -2.76 - Last mean reward per episode: -2.76
Num timesteps: 5973000
Best mean reward: -2.76 - Last mean reward per episode: -2.92
Num timesteps: 5976000
Best mean reward: -2.76 - Last mean reward per episode: -2.86
Num timesteps: 5979000
Best mean reward: -2.76 - Last mean reward per episode: -2.77
Num timesteps: 5982000
Best mean reward: -2.76 - Last mean reward per episode: -2.88
Num timesteps: 5985000
Best mean reward: -2.76 - Last mean reward per episode: -2.82
Num timesteps: 5988000
Best mean reward: -2.76 - Last mean reward per episode: -2.89
Num timesteps: 5991000
Best mean reward: -2.76 - Last mean reward per episode: -2.87
Num timesteps: 5994000
Best mean reward: -2.76 - Last mean reward per episode: -2.82
Num timesteps: 5997000
Best mean reward: -2.76 - Last mean reward per episode: -2.79
Num timesteps: 6000000
Best mean reward: -2.76 - Last mean reward per episode: -2.74
Saving new best model to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/tmpMulti/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 6003000
Best mean reward: -2.74 - Last mean reward per episode: -2.81
Num timesteps: 6006000
Best mean reward: -2.74 - Last mean reward per episode: -2.91
Num timesteps: 6009000
Best mean reward: -2.74 - Last mean reward per episode: -2.79
Num timesteps: 6012000
Best mean reward: -2.74 - Last mean reward per episode: -2.83
Num timesteps: 6015000
Best mean reward: -2.74 - Last mean reward per episode: -2.88
Num timesteps: 6018000
Best mean reward: -2.74 - Last mean reward per episode: -2.81
Num timesteps: 6021000
Best mean reward: -2.74 - Last mean reward per episode: -2.84
Num timesteps: 6024000
Best mean reward: -2.74 - Last mean reward per episode: -2.89
Num timesteps: 6027000
Best mean reward: -2.74 - Last mean reward per episode: -2.82
Num timesteps: 6030000
Best mean reward: -2.74 - Last mean reward per episode: -2.87
Num timesteps: 6033000
Best mean reward: -2.74 - Last mean reward per episode: -2.90
Num timesteps: 6036000
Best mean reward: -2.74 - Last mean reward per episode: -2.87
Num timesteps: 6039000
Best mean reward: -2.74 - Last mean reward per episode: -2.91
Num timesteps: 6042000
Best mean reward: -2.74 - Last mean reward per episode: -2.84
Num timesteps: 6045000
Best mean reward: -2.74 - Last mean reward per episode: -2.81
Num timesteps: 6048000
Best mean reward: -2.74 - Last mean reward per episode: -2.86
Num timesteps: 6051000
Best mean reward: -2.74 - Last mean reward per episode: -2.91
Num timesteps: 6054000
Best mean reward: -2.74 - Last mean reward per episode: -2.85
Num timesteps: 6057000
Best mean reward: -2.74 - Last mean reward per episode: -2.90
Num timesteps: 6060000
Best mean reward: -2.74 - Last mean reward per episode: -2.82
Num timesteps: 6063000
Best mean reward: -2.74 - Last mean reward per episode: -2.76
Num timesteps: 6066000
Best mean reward: -2.74 - Last mean reward per episode: -2.80
Num timesteps: 6069000
Best mean reward: -2.74 - Last mean reward per episode: -2.80
Num timesteps: 6072000
Best mean reward: -2.74 - Last mean reward per episode: -2.80
Num timesteps: 6075000
Best mean reward: -2.74 - Last mean reward per episode: -2.85
Num timesteps: 6078000
Best mean reward: -2.74 - Last mean reward per episode: -2.77
Num timesteps: 6081000
Best mean reward: -2.74 - Last mean reward per episode: -3.21
Num timesteps: 6084000
Best mean reward: -2.74 - Last mean reward per episode: -2.89
Num timesteps: 6087000
Best mean reward: -2.74 - Last mean reward per episode: -2.88
Num timesteps: 6090000
Best mean reward: -2.74 - Last mean reward per episode: -2.77
Num timesteps: 6093000
Best mean reward: -2.74 - Last mean reward per episode: -2.87

 
 
 

-----------------------------------------------------------------------------------

 
 
 

RECORDING OPTIMIZED STUDY
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/scripts/stable_baselines/pandaAndrew/pybulletCust.py:94: UserWarning: The use of the render method is not recommended when the environment has not been created with render=True. The rendering will probably be weird. Prefer making the environment with option `render=True`. For example: `env = gym.make('PandaReach-v2', render=True)`.
  warnings.warn(
/home/hjkwon/scripts/stable_baselines/pandaAndrew/pybulletCust.py:94: UserWarning: The use of the render method is not recommended when the environment has not been created with render=True. The rendering will probably be weird. Prefer making the environment with option `render=True`. For example: `env = gym.make('PandaReach-v2', render=True)`.
  warnings.warn(
Saving video to /home/hjkwon/scripts/stable_baselines/pandaAndrew/2022-04-24/videosDense/-step-0-to-step-400.mp4
Exception ignored in: <function VecVideoRecorder.__del__ at 0x7fb2d3b98b80>
Traceback (most recent call last):
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/stable_baselines3/common/vec_env/vec_video_recorder.py", line 113, in __del__
    self.close()
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/stable_baselines3/common/vec_env/vec_video_recorder.py", line 109, in close
    VecEnvWrapper.close(self)
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/stable_baselines3/common/vec_env/base_vec_env.py", line 278, in close
    return self.venv.close()
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/stable_baselines3/common/vec_env/vec_frame_stack.py", line 64, in close
    self.venv.close()
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 67, in close
    env.close()
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/gym/core.py", line 257, in close
    return self.env.close()
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/stable_baselines3/common/monitor.py", line 113, in close
    super(Monitor, self).close()
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/gym/core.py", line 257, in close
    return self.env.close()
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/gym/core.py", line 257, in close
    return self.env.close()
  File "/home/hjkwon/scripts/stable_baselines/pandaAndrew/robottaskenv.py", line 73, in close
    self.sim.close()
  File "/home/hjkwon/scripts/stable_baselines/pandaAndrew/pybulletCust.py", line 59, in close
    self.physics_client.disconnect()
pybullet.error: Not connected to physics server.
(rl_env) ]0;hjkwon@HJK-AI-Robotics: ~/scripts/stable_baselines/pandaAndrew[01;32mhjkwon@HJK-AI-Robotics[00m:[01;34m~/scripts/stable_baselines/pandaAndrew[00m$ exit
exit

Script done on 2022-04-25 13:55:16-04:00 [COMMAND_EXIT_CODE="0"]
