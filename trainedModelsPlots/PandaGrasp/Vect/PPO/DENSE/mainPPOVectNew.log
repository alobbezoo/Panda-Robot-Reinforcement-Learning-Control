Script started on 2022-06-27 00:15:02-04:00 [TERM="xterm-256color" TTY="/dev/pts/0" COLUMNS="211" LINES="55"]
bash: devel/setup.bash: No such file or directory
bash: /home/hjkwon/catkin_ws/src/moveit/devel/setup.bash: No such file or directory
]0;hjkwon@HJK-AI-Robotics: ~/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-26[01;32mhjkwon@HJK-AI-Robotics[00m:[01;34m~/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-26[00m$ cd ..
]0;hjkwon@HJK-AI-Robotics: ~/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining[01;32mhjkwon@HJK-AI-Robotics[00m:[01;34m~/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining[00m$ ls
[0m[01;34m2022-04-22[0m  [01;34m2022-04-24[0m  [01;34m2022-04-27[0m  [01;34m2022-06-25[0m  [01;34m2022-06-26[0m  [01;34m21April2022[0m  cpuMonitor.log  gpuMonitor.log  mainHERVect.py  mainPPOVect.py  mainSACVectTuner.py  mainTD3VectTuner.py  [01;34m__pycache__[0m  setDirectories.py
]0;hjkwon@HJK-AI-Robotics: ~/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining[01;32mhjkwon@HJK-AI-Robotics[00m:[01;34m~/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining[00m$ python3 mainPPOVect.py [K
Traceback (most recent call last):
  File "mainPPOVect.py", line 41, in <module>
    from SB3.vectEnvs import make_panda_env
  File "/home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/../SB3/vectEnvs.py", line 3, in <module>
    from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv, VecEnv
ModuleNotFoundError: No module named 'stable_baselines3'
]0;hjkwon@HJK-AI-Robotics: ~/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining[01;32mhjkwon@HJK-AI-Robotics[00m:[01;34m~/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining[00m$ conda activate rl_env
(rl_env) ]0;hjkwon@HJK-AI-Robotics: ~/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining[01;32mhjkwon@HJK-AI-Robotics[00m:[01;34m~/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining[00m$ scrip[K[K[K[K[Kpyhto[K[K[K[K[Kconda activate rl_envpython3 mainPPOVect.py
pybullet build time: Dec  1 2021 18:34:28
network arch:  [{'pi': [128, 128, 128, 128, 128, 128], 'vf': [128, 128, 128, 128, 128, 128]}]
_XSERVTransSocketUNIXCreateListener: ...SocketCreateListener() failed
_XSERVTransMakeAllCOTSServerListeners: server already running
(EE) 
Fatal server error:
(EE) Cannot establish any listening sockets - Make sure an X server isn't already running(EE) 
Traceback (most recent call last):
  File "/home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/mainPPOVect.py", line 171, in <module>
    vec_env = make_panda_env(env_id=ENV_ID, n_envs=N_ENVS, seed=0, monitor_dir=callbackDir)
TypeError: make_panda_env() missing 1 required positional argument: 'wrapper'
(rl_env) ]0;hjkwon@HJK-AI-Robotics: ~/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining[01;32mhjkwon@HJK-AI-Robotics[00m:[01;34m~/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining[00m$ python3 mainPPOVect.py
pybullet build time: Dec  1 2021 18:34:28
network arch:  [{'pi': [128, 128, 128, 128, 128, 128], 'vf': [128, 128, 128, 128, 128, 128]}]
_XSERVTransSocketUNIXCreateListener: ...SocketCreateListener() failed
_XSERVTransMakeAllCOTSServerListeners: server already running
(EE) 
Fatal server error:
(EE) Cannot establish any listening sockets - Make sure an X server isn't already running(EE) 
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413

 
 kwargs:  {'policy': 'MlpPolicy', 'device': 'cuda', 'verbose': 1, 'gamma': 0.9270335099881851, 'gae_lambda': 0.8237685826071883, 'learning_rate': 0.0013378119211869901, 'ent_coef': 1e-06, 'max_grad_norm': 0.5, 'vf_coef': 0.75, 'clip_range': 0.075, 'batch_size': 1024, 'n_steps': 32768, 'n_epochs': 9, 'policy_kwargs': {'net_arch': [{'pi': [128, 128, 128, 128, 128, 128], 'vf': [128, 128, 128, 128, 128, 128]}]}}
n_envs:  3
NOTE: Testing the model which performed well for Pick and Place with the grasp problem


Using cuda device
Traceback (most recent call last):
  File "/home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/mainPPOVect.py", line 183, in <module>
    model.learn(total_timesteps=int(OPTIMIZED_N_TIMESTEPS), callback=callback)
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py", line 299, in learn
    return super(PPO, self).learn(
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 242, in learn
    total_timesteps, callback = self._setup_learn(
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/stable_baselines3/common/base_class.py", line 429, in _setup_learn
    self._last_obs = self.env.reset()  # pytype: disable=annotation-type-mismatch
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/stable_baselines3/common/vec_env/vec_frame_stack.py", line 58, in reset
    observation = self.venv.reset()  # pytype:disable=annotation-type-mismatch
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 62, in reset
    self._save_obs(env_idx, obs)
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 92, in _save_obs
    self.buf_obs[key][env_idx] = obs
ValueError: could not broadcast input array from shape (7,) into shape (13,)
Exception ignored in: <function BulletClient.__del__ at 0x7fea18aefa60>
Traceback (most recent call last):
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/pybullet_utils/bullet_client.py", line 39, in __del__
AttributeError: 'NoneType' object has no attribute 'getpid'
Exception ignored in: <function BulletClient.__del__ at 0x7fea18aefa60>
Traceback (most recent call last):
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/pybullet_utils/bullet_client.py", line 39, in __del__
AttributeError: 'NoneType' object has no attribute 'getpid'
Exception ignored in: <function BulletClient.__del__ at 0x7fea18aefa60>
Traceback (most recent call last):
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/pybullet_utils/bullet_client.py", line 39, in __del__
AttributeError: 'NoneType' object has no attribute 'getpid'
(rl_env) ]0;hjkwon@HJK-AI-Robotics: ~/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining[01;32mhjkwon@HJK-AI-Robotics[00m:[01;34m~/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining[00m$ python3 mainPPOVect.py
pybullet build time: Dec  1 2021 18:34:28
network arch:  [{'pi': [256, 256, 256, 256, 256], 'vf': [256, 256, 256, 256, 256]}]
_XSERVTransSocketUNIXCreateListener: ...SocketCreateListener() failed
_XSERVTransMakeAllCOTSServerListeners: server already running
(EE) 
Fatal server error:
(EE) Cannot establish any listening sockets - Make sure an X server isn't already running(EE) 
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413

 
 kwargs:  {'policy': 'MlpPolicy', 'device': 'cuda', 'verbose': 1, 'gamma': 0.9022475058621503, 'gae_lambda': 0.9492095704977984, 'learning_rate': 0.0005124522762027331, 'ent_coef': 1e-06, 'max_grad_norm': 0.5, 'vf_coef': 0.75, 'clip_range': 0.075, 'batch_size': 512, 'n_steps': 4096, 'n_epochs': 12, 'policy_kwargs': {'net_arch': [{'pi': [256, 256, 256, 256, 256], 'vf': [256, 256, 256, 256, 256]}]}}
n_envs:  3
NOTE: Testing the model which performed well for Grasping with the grasp problem


Using cuda device
Num timesteps: 3000
Best mean reward: -inf - Last mean reward per episode: -11.53

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_002948_numTimesteps_3000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 6000
Best mean reward: -11.53 - Last mean reward per episode: -10.61

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_003000_numTimesteps_6000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
Num timesteps: 9000
Best mean reward: -10.61 - Last mean reward per episode: -11.01
Num timesteps: 12000
Best mean reward: -10.61 - Last mean reward per episode: -11.13
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.3     |
|    ep_rew_mean     | -11      |
| time/              |          |
|    fps             | 271      |
|    iterations      | 1        |
|    time_elapsed    | 45       |
|    total_timesteps | 12288    |
---------------------------------
Num timesteps: 15000
Best mean reward: -10.61 - Last mean reward per episode: -11.12
Num timesteps: 18000
Best mean reward: -10.61 - Last mean reward per episode: -10.52

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_003050_numTimesteps_18000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
Num timesteps: 21000
Best mean reward: -10.52 - Last mean reward per episode: -10.09

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_003107_numTimesteps_21000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
Num timesteps: 24000
Best mean reward: -10.09 - Last mean reward per episode: -9.64

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_003127_numTimesteps_24000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 45.7         |
|    ep_rew_mean          | -9.92        |
| time/                   |              |
|    fps                  | 217          |
|    iterations           | 2            |
|    time_elapsed         | 112          |
|    total_timesteps      | 24576        |
| train/                  |              |
|    approx_kl            | 0.0015662598 |
|    clip_fraction        | 0.117        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.68        |
|    explained_variance   | -0.101       |
|    learning_rate        | 0.000512     |
|    loss                 | 0.181        |
|    n_updates            | 12           |
|    policy_gradient_loss | -0.00379     |
|    std                  | 1            |
|    value_loss           | 0.369        |
------------------------------------------
Num timesteps: 27000
Best mean reward: -9.64 - Last mean reward per episode: -9.31

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_003146_numTimesteps_27000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
Num timesteps: 30000
Best mean reward: -9.31 - Last mean reward per episode: -9.70
Num timesteps: 33000
Best mean reward: -9.31 - Last mean reward per episode: -9.32
Num timesteps: 36000
Best mean reward: -9.31 - Last mean reward per episode: -9.54
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 45.9         |
|    ep_rew_mean          | -9.44        |
| time/                   |              |
|    fps                  | 196          |
|    iterations           | 3            |
|    time_elapsed         | 187          |
|    total_timesteps      | 36864        |
| train/                  |              |
|    approx_kl            | 0.0027222729 |
|    clip_fraction        | 0.274        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.67        |
|    explained_variance   | 0.602        |
|    learning_rate        | 0.000512     |
|    loss                 | 0.106        |
|    n_updates            | 24           |
|    policy_gradient_loss | -0.0114      |
|    std                  | 0.997        |
|    value_loss           | 0.195        |
------------------------------------------
Num timesteps: 39000
Best mean reward: -9.31 - Last mean reward per episode: -9.29

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_003301_numTimesteps_39000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
Num timesteps: 42000
Best mean reward: -9.29 - Last mean reward per episode: -8.84

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_003322_numTimesteps_42000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
Num timesteps: 45000
Best mean reward: -8.84 - Last mean reward per episode: -8.02

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_003333_numTimesteps_45000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
Num timesteps: 48000
Best mean reward: -8.02 - Last mean reward per episode: -8.02

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_003346_numTimesteps_48000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 44.1         |
|    ep_rew_mean          | -8.32        |
| time/                   |              |
|    fps                  | 193          |
|    iterations           | 4            |
|    time_elapsed         | 253          |
|    total_timesteps      | 49152        |
| train/                  |              |
|    approx_kl            | 0.0036191673 |
|    clip_fraction        | 0.339        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.65        |
|    explained_variance   | 0.731        |
|    learning_rate        | 0.000512     |
|    loss                 | 0.145        |
|    n_updates            | 36           |
|    policy_gradient_loss | -0.0116      |
|    std                  | 0.994        |
|    value_loss           | 0.208        |
------------------------------------------
Num timesteps: 51000
Best mean reward: -8.02 - Last mean reward per episode: -8.80
Num timesteps: 54000
Best mean reward: -8.02 - Last mean reward per episode: -8.94
Num timesteps: 57000
Best mean reward: -8.02 - Last mean reward per episode: -7.54

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_003442_numTimesteps_57000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
Num timesteps: 60000
Best mean reward: -7.54 - Last mean reward per episode: -8.17
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 43.9         |
|    ep_rew_mean          | -8.35        |
| time/                   |              |
|    fps                  | 188          |
|    iterations           | 5            |
|    time_elapsed         | 325          |
|    total_timesteps      | 61440        |
| train/                  |              |
|    approx_kl            | 0.0035060004 |
|    clip_fraction        | 0.337        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.63        |
|    explained_variance   | 0.851        |
|    learning_rate        | 0.000512     |
|    loss                 | 0.126        |
|    n_updates            | 48           |
|    policy_gradient_loss | -0.0113      |
|    std                  | 0.988        |
|    value_loss           | 0.221        |
------------------------------------------
Num timesteps: 63000
Best mean reward: -7.54 - Last mean reward per episode: -7.37

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_003512_numTimesteps_63000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
Num timesteps: 66000
Best mean reward: -7.37 - Last mean reward per episode: -7.01

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_003526_numTimesteps_66000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
Num timesteps: 69000
Best mean reward: -7.01 - Last mean reward per episode: -7.96
Num timesteps: 72000
Best mean reward: -7.01 - Last mean reward per episode: -6.33

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_003601_numTimesteps_72000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 41.7        |
|    ep_rew_mean          | -6.46       |
| time/                   |             |
|    fps                  | 188         |
|    iterations           | 6           |
|    time_elapsed         | 390         |
|    total_timesteps      | 73728       |
| train/                  |             |
|    approx_kl            | 0.002959567 |
|    clip_fraction        | 0.303       |
|    clip_range           | 0.075       |
|    entropy_loss         | -5.61       |
|    explained_variance   | 0.866       |
|    learning_rate        | 0.000512    |
|    loss                 | 0.174       |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0085     |
|    std                  | 0.983       |
|    value_loss           | 0.233       |
-----------------------------------------
Num timesteps: 75000
Best mean reward: -6.33 - Last mean reward per episode: -6.99
Num timesteps: 78000
Best mean reward: -6.33 - Last mean reward per episode: -7.89
Num timesteps: 81000
Best mean reward: -6.33 - Last mean reward per episode: -7.40
Num timesteps: 84000
Best mean reward: -6.33 - Last mean reward per episode: -7.70
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 41.5         |
|    ep_rew_mean          | -7.39        |
| time/                   |              |
|    fps                  | 187          |
|    iterations           | 7            |
|    time_elapsed         | 459          |
|    total_timesteps      | 86016        |
| train/                  |              |
|    approx_kl            | 0.0034904834 |
|    clip_fraction        | 0.324        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.58        |
|    explained_variance   | 0.835        |
|    learning_rate        | 0.000512     |
|    loss                 | 0.145        |
|    n_updates            | 72           |
|    policy_gradient_loss | -0.00784     |
|    std                  | 0.977        |
|    value_loss           | 0.207        |
------------------------------------------
Num timesteps: 87000
Best mean reward: -6.33 - Last mean reward per episode: -7.74
Num timesteps: 90000
Best mean reward: -6.33 - Last mean reward per episode: -6.75
Num timesteps: 93000
Best mean reward: -6.33 - Last mean reward per episode: -6.93
Num timesteps: 96000
Best mean reward: -6.33 - Last mean reward per episode: -7.69
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 37.9         |
|    ep_rew_mean          | -6.53        |
| time/                   |              |
|    fps                  | 185          |
|    iterations           | 8            |
|    time_elapsed         | 530          |
|    total_timesteps      | 98304        |
| train/                  |              |
|    approx_kl            | 0.0031755741 |
|    clip_fraction        | 0.307        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.885        |
|    learning_rate        | 0.000512     |
|    loss                 | 0.156        |
|    n_updates            | 84           |
|    policy_gradient_loss | -0.00615     |
|    std                  | 0.972        |
|    value_loss           | 0.237        |
------------------------------------------
Num timesteps: 99000
Best mean reward: -6.33 - Last mean reward per episode: -6.74
Num timesteps: 102000
Best mean reward: -6.33 - Last mean reward per episode: -6.09

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_003855_numTimesteps_102000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
Num timesteps: 105000
Best mean reward: -6.09 - Last mean reward per episode: -6.73
Num timesteps: 108000
Best mean reward: -6.09 - Last mean reward per episode: -5.88

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_003935_numTimesteps_108000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 36.5         |
|    ep_rew_mean          | -5.82        |
| time/                   |              |
|    fps                  | 180          |
|    iterations           | 9            |
|    time_elapsed         | 613          |
|    total_timesteps      | 110592       |
| train/                  |              |
|    approx_kl            | 0.0039151986 |
|    clip_fraction        | 0.311        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.53        |
|    explained_variance   | 0.899        |
|    learning_rate        | 0.000512     |
|    loss                 | 0.159        |
|    n_updates            | 96           |
|    policy_gradient_loss | -0.00521     |
|    std                  | 0.964        |
|    value_loss           | 0.281        |
------------------------------------------
Num timesteps: 111000
Best mean reward: -5.88 - Last mean reward per episode: -6.05
Num timesteps: 114000
Best mean reward: -5.88 - Last mean reward per episode: -6.56
Num timesteps: 117000
Best mean reward: -5.88 - Last mean reward per episode: -5.79

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_004039_numTimesteps_117000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
Num timesteps: 120000
Best mean reward: -5.79 - Last mean reward per episode: -5.78

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_004056_numTimesteps_120000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 35.6         |
|    ep_rew_mean          | -6.11        |
| time/                   |              |
|    fps                  | 175          |
|    iterations           | 10           |
|    time_elapsed         | 699          |
|    total_timesteps      | 122880       |
| train/                  |              |
|    approx_kl            | 0.0036302644 |
|    clip_fraction        | 0.3          |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.5         |
|    explained_variance   | 0.907        |
|    learning_rate        | 0.000512     |
|    loss                 | 0.123        |
|    n_updates            | 108          |
|    policy_gradient_loss | -0.00593     |
|    std                  | 0.958        |
|    value_loss           | 0.229        |
------------------------------------------
Num timesteps: 123000
Best mean reward: -5.78 - Last mean reward per episode: -6.05
Num timesteps: 126000
Best mean reward: -5.78 - Last mean reward per episode: -5.85
Num timesteps: 129000
Best mean reward: -5.78 - Last mean reward per episode: -5.58

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_004158_numTimesteps_129000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
Num timesteps: 132000
Best mean reward: -5.58 - Last mean reward per episode: -5.30

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_004218_numTimesteps_132000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
Num timesteps: 135000
Best mean reward: -5.30 - Last mean reward per episode: -6.93
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 37.5        |
|    ep_rew_mean          | -6.96       |
| time/                   |             |
|    fps                  | 173         |
|    iterations           | 11          |
|    time_elapsed         | 779         |
|    total_timesteps      | 135168      |
| train/                  |             |
|    approx_kl            | 0.003756351 |
|    clip_fraction        | 0.309       |
|    clip_range           | 0.075       |
|    entropy_loss         | -5.48       |
|    explained_variance   | 0.907       |
|    learning_rate        | 0.000512    |
|    loss                 | 0.198       |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.00375    |
|    std                  | 0.952       |
|    value_loss           | 0.259       |
-----------------------------------------
Num timesteps: 138000
Best mean reward: -5.30 - Last mean reward per episode: -5.80
Num timesteps: 141000
Best mean reward: -5.30 - Last mean reward per episode: -5.41
Num timesteps: 144000
Best mean reward: -5.30 - Last mean reward per episode: -6.16
Num timesteps: 147000
Best mean reward: -5.30 - Last mean reward per episode: -5.43
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 34.5        |
|    ep_rew_mean          | -5.45       |
| time/                   |             |
|    fps                  | 179         |
|    iterations           | 12          |
|    time_elapsed         | 819         |
|    total_timesteps      | 147456      |
| train/                  |             |
|    approx_kl            | 0.004339415 |
|    clip_fraction        | 0.319       |
|    clip_range           | 0.075       |
|    entropy_loss         | -5.45       |
|    explained_variance   | 0.922       |
|    learning_rate        | 0.000512    |
|    loss                 | 0.143       |
|    n_updates            | 132         |
|    policy_gradient_loss | -0.00448    |
|    std                  | 0.947       |
|    value_loss           | 0.277       |
-----------------------------------------
Num timesteps: 150000
Best mean reward: -5.30 - Last mean reward per episode: -4.92

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_004325_numTimesteps_150000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
Num timesteps: 153000
Best mean reward: -4.92 - Last mean reward per episode: -5.37
Num timesteps: 156000
Best mean reward: -4.92 - Last mean reward per episode: -5.31
Num timesteps: 159000
Best mean reward: -4.92 - Last mean reward per episode: -5.20
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 30.9         |
|    ep_rew_mean          | -4.74        |
| time/                   |              |
|    fps                  | 182          |
|    iterations           | 13           |
|    time_elapsed         | 877          |
|    total_timesteps      | 159744       |
| train/                  |              |
|    approx_kl            | 0.0037563464 |
|    clip_fraction        | 0.317        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.43        |
|    explained_variance   | 0.921        |
|    learning_rate        | 0.000512     |
|    loss                 | 0.188        |
|    n_updates            | 144          |
|    policy_gradient_loss | -0.00456     |
|    std                  | 0.94         |
|    value_loss           | 0.236        |
------------------------------------------
Num timesteps: 162000
Best mean reward: -4.92 - Last mean reward per episode: -4.80

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_004427_numTimesteps_162000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
Num timesteps: 165000
Best mean reward: -4.80 - Last mean reward per episode: -4.95
Num timesteps: 168000
Best mean reward: -4.80 - Last mean reward per episode: -4.31

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_004456_numTimesteps_168000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
Num timesteps: 171000
Best mean reward: -4.31 - Last mean reward per episode: -4.33
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 29.7         |
|    ep_rew_mean          | -5.14        |
| time/                   |              |
|    fps                  | 183          |
|    iterations           | 14           |
|    time_elapsed         | 937          |
|    total_timesteps      | 172032       |
| train/                  |              |
|    approx_kl            | 0.0039571156 |
|    clip_fraction        | 0.33         |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.39        |
|    explained_variance   | 0.921        |
|    learning_rate        | 0.000512     |
|    loss                 | 0.121        |
|    n_updates            | 156          |
|    policy_gradient_loss | -0.00471     |
|    std                  | 0.933        |
|    value_loss           | 0.232        |
------------------------------------------
Num timesteps: 174000
Best mean reward: -4.31 - Last mean reward per episode: -5.33
Num timesteps: 177000
Best mean reward: -4.31 - Last mean reward per episode: -4.89
Num timesteps: 180000
Best mean reward: -4.31 - Last mean reward per episode: -4.44
Num timesteps: 183000
Best mean reward: -4.31 - Last mean reward per episode: -3.48

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_004555_numTimesteps_183000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 25.7         |
|    ep_rew_mean          | -4.07        |
| time/                   |              |
|    fps                  | 187          |
|    iterations           | 15           |
|    time_elapsed         | 985          |
|    total_timesteps      | 184320       |
| train/                  |              |
|    approx_kl            | 0.0037738876 |
|    clip_fraction        | 0.334        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.37        |
|    explained_variance   | 0.914        |
|    learning_rate        | 0.000512     |
|    loss                 | 0.135        |
|    n_updates            | 168          |
|    policy_gradient_loss | -0.00321     |
|    std                  | 0.928        |
|    value_loss           | 0.264        |
------------------------------------------
Num timesteps: 186000
Best mean reward: -3.48 - Last mean reward per episode: -3.70
Num timesteps: 189000
Best mean reward: -3.48 - Last mean reward per episode: -4.64
Num timesteps: 192000
Best mean reward: -3.48 - Last mean reward per episode: -5.47
Num timesteps: 195000
Best mean reward: -3.48 - Last mean reward per episode: -4.65
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 25.9        |
|    ep_rew_mean          | -4.31       |
| time/                   |             |
|    fps                  | 187         |
|    iterations           | 16          |
|    time_elapsed         | 1048        |
|    total_timesteps      | 196608      |
| train/                  |             |
|    approx_kl            | 0.004624328 |
|    clip_fraction        | 0.346       |
|    clip_range           | 0.075       |
|    entropy_loss         | -5.35       |
|    explained_variance   | 0.917       |
|    learning_rate        | 0.000512    |
|    loss                 | 0.193       |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.00366    |
|    std                  | 0.923       |
|    value_loss           | 0.238       |
-----------------------------------------
Num timesteps: 198000
Best mean reward: -3.48 - Last mean reward per episode: -4.43
Num timesteps: 201000
Best mean reward: -3.48 - Last mean reward per episode: -4.26
Num timesteps: 204000
Best mean reward: -3.48 - Last mean reward per episode: -4.43
Num timesteps: 207000
Best mean reward: -3.48 - Last mean reward per episode: -4.99
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 24.8         |
|    ep_rew_mean          | -4.76        |
| time/                   |              |
|    fps                  | 191          |
|    iterations           | 17           |
|    time_elapsed         | 1088         |
|    total_timesteps      | 208896       |
| train/                  |              |
|    approx_kl            | 0.0046487623 |
|    clip_fraction        | 0.337        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.32        |
|    explained_variance   | 0.927        |
|    learning_rate        | 0.000512     |
|    loss                 | 0.14         |
|    n_updates            | 192          |
|    policy_gradient_loss | -0.00334     |
|    std                  | 0.917        |
|    value_loss           | 0.242        |
------------------------------------------
Num timesteps: 210000
Best mean reward: -3.48 - Last mean reward per episode: -4.85
Num timesteps: 213000
Best mean reward: -3.48 - Last mean reward per episode: -4.03
Num timesteps: 216000
Best mean reward: -3.48 - Last mean reward per episode: -4.38
Num timesteps: 219000
Best mean reward: -3.48 - Last mean reward per episode: -3.63
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 22         |
|    ep_rew_mean          | -3.59      |
| time/                   |            |
|    fps                  | 191        |
|    iterations           | 18         |
|    time_elapsed         | 1155       |
|    total_timesteps      | 221184     |
| train/                  |            |
|    approx_kl            | 0.00495329 |
|    clip_fraction        | 0.354      |
|    clip_range           | 0.075      |
|    entropy_loss         | -5.3       |
|    explained_variance   | 0.929      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.156      |
|    n_updates            | 204        |
|    policy_gradient_loss | -0.00311   |
|    std                  | 0.912      |
|    value_loss           | 0.248      |
----------------------------------------
Num timesteps: 222000
Best mean reward: -3.48 - Last mean reward per episode: -3.99
Num timesteps: 225000
Best mean reward: -3.48 - Last mean reward per episode: -3.17

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_004909_numTimesteps_225000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
Num timesteps: 228000
Best mean reward: -3.17 - Last mean reward per episode: -2.59

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_004918_numTimesteps_228000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
Num timesteps: 231000
Best mean reward: -2.59 - Last mean reward per episode: -2.83
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 24.1         |
|    ep_rew_mean          | -3.85        |
| time/                   |              |
|    fps                  | 193          |
|    iterations           | 19           |
|    time_elapsed         | 1207         |
|    total_timesteps      | 233472       |
| train/                  |              |
|    approx_kl            | 0.0054422147 |
|    clip_fraction        | 0.387        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.27        |
|    explained_variance   | 0.938        |
|    learning_rate        | 0.000512     |
|    loss                 | 0.106        |
|    n_updates            | 216          |
|    policy_gradient_loss | -0.00258     |
|    std                  | 0.904        |
|    value_loss           | 0.238        |
------------------------------------------
Num timesteps: 234000
Best mean reward: -2.59 - Last mean reward per episode: -3.13
Num timesteps: 237000
Best mean reward: -2.59 - Last mean reward per episode: -3.55
Num timesteps: 240000
Best mean reward: -2.59 - Last mean reward per episode: -3.92
Num timesteps: 243000
Best mean reward: -2.59 - Last mean reward per episode: -3.59
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.5        |
|    ep_rew_mean          | -3.67       |
| time/                   |             |
|    fps                  | 192         |
|    iterations           | 20          |
|    time_elapsed         | 1277        |
|    total_timesteps      | 245760      |
| train/                  |             |
|    approx_kl            | 0.006389515 |
|    clip_fraction        | 0.409       |
|    clip_range           | 0.075       |
|    entropy_loss         | -5.23       |
|    explained_variance   | 0.922       |
|    learning_rate        | 0.000512    |
|    loss                 | 0.113       |
|    n_updates            | 228         |
|    policy_gradient_loss | -0.00284    |
|    std                  | 0.896       |
|    value_loss           | 0.186       |
-----------------------------------------
Num timesteps: 246000
Best mean reward: -2.59 - Last mean reward per episode: -3.58
Num timesteps: 249000
Best mean reward: -2.59 - Last mean reward per episode: -2.60
Num timesteps: 252000
Best mean reward: -2.59 - Last mean reward per episode: -3.22
Num timesteps: 255000
Best mean reward: -2.59 - Last mean reward per episode: -2.55

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_005149_numTimesteps_255000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
Num timesteps: 258000
Best mean reward: -2.55 - Last mean reward per episode: -3.57
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 21           |
|    ep_rew_mean          | -3.67        |
| time/                   |              |
|    fps                  | 191          |
|    iterations           | 21           |
|    time_elapsed         | 1350         |
|    total_timesteps      | 258048       |
| train/                  |              |
|    approx_kl            | 0.0060916296 |
|    clip_fraction        | 0.411        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.2         |
|    explained_variance   | 0.95         |
|    learning_rate        | 0.000512     |
|    loss                 | 0.173        |
|    n_updates            | 240          |
|    policy_gradient_loss | -0.00235     |
|    std                  | 0.889        |
|    value_loss           | 0.218        |
------------------------------------------
Num timesteps: 261000
Best mean reward: -2.55 - Last mean reward per episode: -2.66
Num timesteps: 264000
Best mean reward: -2.55 - Last mean reward per episode: -2.81
Num timesteps: 267000
Best mean reward: -2.55 - Last mean reward per episode: -2.87
Num timesteps: 270000
Best mean reward: -2.55 - Last mean reward per episode: -2.68
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 17.7         |
|    ep_rew_mean          | -2.69        |
| time/                   |              |
|    fps                  | 190          |
|    iterations           | 22           |
|    time_elapsed         | 1421         |
|    total_timesteps      | 270336       |
| train/                  |              |
|    approx_kl            | 0.0053326115 |
|    clip_fraction        | 0.406        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.17        |
|    explained_variance   | 0.932        |
|    learning_rate        | 0.000512     |
|    loss                 | 0.134        |
|    n_updates            | 252          |
|    policy_gradient_loss | -0.00191     |
|    std                  | 0.883        |
|    value_loss           | 0.201        |
------------------------------------------
Num timesteps: 273000
Best mean reward: -2.55 - Last mean reward per episode: -2.29

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_005339_numTimesteps_273000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
Num timesteps: 276000
Best mean reward: -2.29 - Last mean reward per episode: -3.13
Num timesteps: 279000
Best mean reward: -2.29 - Last mean reward per episode: -2.94
Num timesteps: 282000
Best mean reward: -2.29 - Last mean reward per episode: -2.30
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 16           |
|    ep_rew_mean          | -2.37        |
| time/                   |              |
|    fps                  | 189          |
|    iterations           | 23           |
|    time_elapsed         | 1494         |
|    total_timesteps      | 282624       |
| train/                  |              |
|    approx_kl            | 0.0071859225 |
|    clip_fraction        | 0.415        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.14        |
|    explained_variance   | 0.937        |
|    learning_rate        | 0.000512     |
|    loss                 | 0.112        |
|    n_updates            | 264          |
|    policy_gradient_loss | -0.00261     |
|    std                  | 0.876        |
|    value_loss           | 0.174        |
------------------------------------------
Num timesteps: 285000
Best mean reward: -2.29 - Last mean reward per episode: -3.25
Num timesteps: 288000
Best mean reward: -2.29 - Last mean reward per episode: -2.17

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_005505_numTimesteps_288000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
Num timesteps: 291000
Best mean reward: -2.17 - Last mean reward per episode: -2.29
Num timesteps: 294000
Best mean reward: -2.17 - Last mean reward per episode: -2.19
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 16.4        |
|    ep_rew_mean          | -2.57       |
| time/                   |             |
|    fps                  | 188         |
|    iterations           | 24          |
|    time_elapsed         | 1562        |
|    total_timesteps      | 294912      |
| train/                  |             |
|    approx_kl            | 0.006285282 |
|    clip_fraction        | 0.417       |
|    clip_range           | 0.075       |
|    entropy_loss         | -5.1        |
|    explained_variance   | 0.938       |
|    learning_rate        | 0.000512    |
|    loss                 | 0.124       |
|    n_updates            | 276         |
|    policy_gradient_loss | -0.00239    |
|    std                  | 0.869       |
|    value_loss           | 0.182       |
-----------------------------------------
Num timesteps: 297000
Best mean reward: -2.17 - Last mean reward per episode: -1.79

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_005555_numTimesteps_297000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
Num timesteps: 300000
Best mean reward: -1.79 - Last mean reward per episode: -1.70

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_005614_numTimesteps_300000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
Num timesteps: 303000
Best mean reward: -1.70 - Last mean reward per episode: -1.67

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_005633_numTimesteps_303000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
Num timesteps: 306000
Best mean reward: -1.67 - Last mean reward per episode: -1.48

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_005650_numTimesteps_306000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 11.4         |
|    ep_rew_mean          | -1.42        |
| time/                   |              |
|    fps                  | 187          |
|    iterations           | 25           |
|    time_elapsed         | 1638         |
|    total_timesteps      | 307200       |
| train/                  |              |
|    approx_kl            | 0.0070264596 |
|    clip_fraction        | 0.454        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.06        |
|    explained_variance   | 0.946        |
|    learning_rate        | 0.000512     |
|    loss                 | 0.119        |
|    n_updates            | 288          |
|    policy_gradient_loss | -0.00264     |
|    std                  | 0.861        |
|    value_loss           | 0.161        |
------------------------------------------
Num timesteps: 309000
Best mean reward: -1.48 - Last mean reward per episode: -2.22
Num timesteps: 312000
Best mean reward: -1.48 - Last mean reward per episode: -1.40

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_005722_numTimesteps_312000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
Num timesteps: 315000
Best mean reward: -1.40 - Last mean reward per episode: -1.76
Num timesteps: 318000
Best mean reward: -1.40 - Last mean reward per episode: -1.50
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 11.7        |
|    ep_rew_mean          | -1.54       |
| time/                   |             |
|    fps                  | 188         |
|    iterations           | 26          |
|    time_elapsed         | 1697        |
|    total_timesteps      | 319488      |
| train/                  |             |
|    approx_kl            | 0.008398448 |
|    clip_fraction        | 0.467       |
|    clip_range           | 0.075       |
|    entropy_loss         | -5.02       |
|    explained_variance   | 0.923       |
|    learning_rate        | 0.000512    |
|    loss                 | 0.0493      |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.00332    |
|    std                  | 0.853       |
|    value_loss           | 0.105       |
-----------------------------------------
Num timesteps: 321000
Best mean reward: -1.40 - Last mean reward per episode: -1.28

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_005805_numTimesteps_321000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
Num timesteps: 324000
Best mean reward: -1.28 - Last mean reward per episode: -1.63
Num timesteps: 327000
Best mean reward: -1.28 - Last mean reward per episode: -1.02

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_005837_numTimesteps_327000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
Num timesteps: 330000
Best mean reward: -1.02 - Last mean reward per episode: -2.06
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 12          |
|    ep_rew_mean          | -1.67       |
| time/                   |             |
|    fps                  | 188         |
|    iterations           | 27          |
|    time_elapsed         | 1758        |
|    total_timesteps      | 331776      |
| train/                  |             |
|    approx_kl            | 0.008273447 |
|    clip_fraction        | 0.468       |
|    clip_range           | 0.075       |
|    entropy_loss         | -4.98       |
|    explained_variance   | 0.942       |
|    learning_rate        | 0.000512    |
|    loss                 | 0.0579      |
|    n_updates            | 312         |
|    policy_gradient_loss | -0.00177    |
|    std                  | 0.844       |
|    value_loss           | 0.134       |
-----------------------------------------
Num timesteps: 333000
Best mean reward: -1.02 - Last mean reward per episode: -0.96

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_005901_numTimesteps_333000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
Num timesteps: 336000
Best mean reward: -0.96 - Last mean reward per episode: -1.52
Num timesteps: 339000
Best mean reward: -0.96 - Last mean reward per episode: -0.94

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_005931_numTimesteps_339000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
Num timesteps: 342000
Best mean reward: -0.94 - Last mean reward per episode: -1.70
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 8.14         |
|    ep_rew_mean          | -0.965       |
| time/                   |              |
|    fps                  | 189          |
|    iterations           | 28           |
|    time_elapsed         | 1814         |
|    total_timesteps      | 344064       |
| train/                  |              |
|    approx_kl            | 0.0105759865 |
|    clip_fraction        | 0.489        |
|    clip_range           | 0.075        |
|    entropy_loss         | -4.93        |
|    explained_variance   | 0.946        |
|    learning_rate        | 0.000512     |
|    loss                 | 0.0549       |
|    n_updates            | 324          |
|    policy_gradient_loss | -0.000839    |
|    std                  | 0.833        |
|    value_loss           | 0.114        |
------------------------------------------
Num timesteps: 345000
Best mean reward: -0.94 - Last mean reward per episode: -0.93

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_005956_numTimesteps_345000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
Num timesteps: 348000
Best mean reward: -0.93 - Last mean reward per episode: -1.47
Num timesteps: 351000
Best mean reward: -0.93 - Last mean reward per episode: -1.63
Num timesteps: 354000
Best mean reward: -0.93 - Last mean reward per episode: -1.39
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 9.8         |
|    ep_rew_mean          | -1.31       |
| time/                   |             |
|    fps                  | 190         |
|    iterations           | 29          |
|    time_elapsed         | 1875        |
|    total_timesteps      | 356352      |
| train/                  |             |
|    approx_kl            | 0.012752983 |
|    clip_fraction        | 0.512       |
|    clip_range           | 0.075       |
|    entropy_loss         | -4.88       |
|    explained_variance   | 0.947       |
|    learning_rate        | 0.000512    |
|    loss                 | 0.0459      |
|    n_updates            | 336         |
|    policy_gradient_loss | -0.000897   |
|    std                  | 0.823       |
|    value_loss           | 0.0933      |
-----------------------------------------
Num timesteps: 357000
Best mean reward: -0.93 - Last mean reward per episode: -1.72
Num timesteps: 360000
Best mean reward: -0.93 - Last mean reward per episode: -1.65
Num timesteps: 363000
Best mean reward: -0.93 - Last mean reward per episode: -0.88

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_010130_numTimesteps_363000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
Num timesteps: 366000
Best mean reward: -0.88 - Last mean reward per episode: -1.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 8.83        |
|    ep_rew_mean          | -1.44       |
| time/                   |             |
|    fps                  | 190         |
|    iterations           | 30          |
|    time_elapsed         | 1938        |
|    total_timesteps      | 368640      |
| train/                  |             |
|    approx_kl            | 0.011001813 |
|    clip_fraction        | 0.526       |
|    clip_range           | 0.075       |
|    entropy_loss         | -4.83       |
|    explained_variance   | 0.949       |
|    learning_rate        | 0.000512    |
|    loss                 | 0.0571      |
|    n_updates            | 348         |
|    policy_gradient_loss | 0.000321    |
|    std                  | 0.814       |
|    value_loss           | 0.104       |
-----------------------------------------
Num timesteps: 369000
Best mean reward: -0.88 - Last mean reward per episode: -1.61
Num timesteps: 372000
Best mean reward: -0.88 - Last mean reward per episode: -1.48
Num timesteps: 375000
Best mean reward: -0.88 - Last mean reward per episode: -0.66

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_010232_numTimesteps_375000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
Num timesteps: 378000
Best mean reward: -0.66 - Last mean reward per episode: -0.99
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 7.9         |
|    ep_rew_mean          | -1.12       |
| time/                   |             |
|    fps                  | 190         |
|    iterations           | 31          |
|    time_elapsed         | 2004        |
|    total_timesteps      | 380928      |
| train/                  |             |
|    approx_kl            | 0.010030274 |
|    clip_fraction        | 0.514       |
|    clip_range           | 0.075       |
|    entropy_loss         | -4.79       |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.000512    |
|    loss                 | 0.0725      |
|    n_updates            | 360         |
|    policy_gradient_loss | -0.000984   |
|    std                  | 0.806       |
|    value_loss           | 0.109       |
-----------------------------------------
Num timesteps: 381000
Best mean reward: -0.66 - Last mean reward per episode: -1.00
Num timesteps: 384000
Best mean reward: -0.66 - Last mean reward per episode: -0.76
Num timesteps: 387000
Best mean reward: -0.66 - Last mean reward per episode: -1.15
Num timesteps: 390000
Best mean reward: -0.66 - Last mean reward per episode: -1.42
Num timesteps: 393000
Best mean reward: -0.66 - Last mean reward per episode: -0.85
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 7.78        |
|    ep_rew_mean          | -0.882      |
| time/                   |             |
|    fps                  | 189         |
|    iterations           | 32          |
|    time_elapsed         | 2072        |
|    total_timesteps      | 393216      |
| train/                  |             |
|    approx_kl            | 0.012936045 |
|    clip_fraction        | 0.552       |
|    clip_range           | 0.075       |
|    entropy_loss         | -4.74       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.000512    |
|    loss                 | 0.0775      |
|    n_updates            | 372         |
|    policy_gradient_loss | -0.00167    |
|    std                  | 0.796       |
|    value_loss           | 0.0804      |
-----------------------------------------
Num timesteps: 396000
Best mean reward: -0.66 - Last mean reward per episode: -0.61

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_010425_numTimesteps_396000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
Num timesteps: 399000
Best mean reward: -0.61 - Last mean reward per episode: -0.62
Num timesteps: 402000
Best mean reward: -0.61 - Last mean reward per episode: -0.99
Num timesteps: 405000
Best mean reward: -0.61 - Last mean reward per episode: -0.95
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 5.81        |
|    ep_rew_mean          | -0.573      |
| time/                   |             |
|    fps                  | 190         |
|    iterations           | 33          |
|    time_elapsed         | 2133        |
|    total_timesteps      | 405504      |
| train/                  |             |
|    approx_kl            | 0.011309561 |
|    clip_fraction        | 0.554       |
|    clip_range           | 0.075       |
|    entropy_loss         | -4.68       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.000512    |
|    loss                 | 0.0421      |
|    n_updates            | 384         |
|    policy_gradient_loss | -0.00037    |
|    std                  | 0.785       |
|    value_loss           | 0.0639      |
-----------------------------------------
Num timesteps: 408000
Best mean reward: -0.61 - Last mean reward per episode: -0.73
Num timesteps: 411000
Best mean reward: -0.61 - Last mean reward per episode: -0.86
Num timesteps: 414000
Best mean reward: -0.61 - Last mean reward per episode: -0.60

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_010546_numTimesteps_414000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
Num timesteps: 417000
Best mean reward: -0.60 - Last mean reward per episode: -0.59

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_010601_numTimesteps_417000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 6.98       |
|    ep_rew_mean          | -0.83      |
| time/                   |            |
|    fps                  | 191        |
|    iterations           | 34         |
|    time_elapsed         | 2186       |
|    total_timesteps      | 417792     |
| train/                  |            |
|    approx_kl            | 0.01527626 |
|    clip_fraction        | 0.578      |
|    clip_range           | 0.075      |
|    entropy_loss         | -4.63      |
|    explained_variance   | 0.965      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0462     |
|    n_updates            | 396        |
|    policy_gradient_loss | 0.00117    |
|    std                  | 0.775      |
|    value_loss           | 0.0628     |
----------------------------------------
Num timesteps: 420000
Best mean reward: -0.59 - Last mean reward per episode: -0.56

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_010618_numTimesteps_420000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
Num timesteps: 423000
Best mean reward: -0.56 - Last mean reward per episode: -1.27
Num timesteps: 426000
Best mean reward: -0.56 - Last mean reward per episode: -0.77
Num timesteps: 429000
Best mean reward: -0.56 - Last mean reward per episode: -0.65
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 5.62        |
|    ep_rew_mean          | -0.56       |
| time/                   |             |
|    fps                  | 191         |
|    iterations           | 35          |
|    time_elapsed         | 2246        |
|    total_timesteps      | 430080      |
| train/                  |             |
|    approx_kl            | 0.014321479 |
|    clip_fraction        | 0.577       |
|    clip_range           | 0.075       |
|    entropy_loss         | -4.57       |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.000512    |
|    loss                 | 0.0233      |
|    n_updates            | 408         |
|    policy_gradient_loss | -0.000484   |
|    std                  | 0.763       |
|    value_loss           | 0.0598      |
-----------------------------------------
Num timesteps: 432000
Best mean reward: -0.56 - Last mean reward per episode: -0.56
Num timesteps: 435000
Best mean reward: -0.56 - Last mean reward per episode: -0.63
Num timesteps: 438000
Best mean reward: -0.56 - Last mean reward per episode: -0.69
Num timesteps: 441000
Best mean reward: -0.56 - Last mean reward per episode: -0.55

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_010756_numTimesteps_441000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 5.39        |
|    ep_rew_mean          | -0.519      |
| time/                   |             |
|    fps                  | 191         |
|    iterations           | 36          |
|    time_elapsed         | 2304        |
|    total_timesteps      | 442368      |
| train/                  |             |
|    approx_kl            | 0.016845644 |
|    clip_fraction        | 0.584       |
|    clip_range           | 0.075       |
|    entropy_loss         | -4.51       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.000512    |
|    loss                 | 0.0374      |
|    n_updates            | 420         |
|    policy_gradient_loss | 0.00175     |
|    std                  | 0.753       |
|    value_loss           | 0.0582      |
-----------------------------------------
Num timesteps: 444000
Best mean reward: -0.55 - Last mean reward per episode: -0.65
Num timesteps: 447000
Best mean reward: -0.55 - Last mean reward per episode: -0.71
Num timesteps: 450000
Best mean reward: -0.55 - Last mean reward per episode: -0.70
Num timesteps: 453000
Best mean reward: -0.55 - Last mean reward per episode: -0.51

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_010905_numTimesteps_453000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 6.31        |
|    ep_rew_mean          | -0.836      |
| time/                   |             |
|    fps                  | 191         |
|    iterations           | 37          |
|    time_elapsed         | 2376        |
|    total_timesteps      | 454656      |
| train/                  |             |
|    approx_kl            | 0.018704558 |
|    clip_fraction        | 0.602       |
|    clip_range           | 0.075       |
|    entropy_loss         | -4.45       |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.000512    |
|    loss                 | 0.05        |
|    n_updates            | 432         |
|    policy_gradient_loss | 0.00255     |
|    std                  | 0.742       |
|    value_loss           | 0.0448      |
-----------------------------------------
Num timesteps: 456000
Best mean reward: -0.51 - Last mean reward per episode: -0.62
Num timesteps: 459000
Best mean reward: -0.51 - Last mean reward per episode: -0.50

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_010941_numTimesteps_459000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
Num timesteps: 462000
Best mean reward: -0.50 - Last mean reward per episode: -0.55
Num timesteps: 465000
Best mean reward: -0.50 - Last mean reward per episode: -0.55
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 5.43        |
|    ep_rew_mean          | -0.683      |
| time/                   |             |
|    fps                  | 191         |
|    iterations           | 38          |
|    time_elapsed         | 2437        |
|    total_timesteps      | 466944      |
| train/                  |             |
|    approx_kl            | 0.019877404 |
|    clip_fraction        | 0.622       |
|    clip_range           | 0.075       |
|    entropy_loss         | -4.38       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.000512    |
|    loss                 | 0.012       |
|    n_updates            | 444         |
|    policy_gradient_loss | 0.000148    |
|    std                  | 0.729       |
|    value_loss           | 0.0332      |
-----------------------------------------
Num timesteps: 468000
Best mean reward: -0.50 - Last mean reward per episode: -0.76
Num timesteps: 471000
Best mean reward: -0.50 - Last mean reward per episode: -0.50

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_011036_numTimesteps_471000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
Num timesteps: 474000
Best mean reward: -0.50 - Last mean reward per episode: -0.73
Num timesteps: 477000
Best mean reward: -0.50 - Last mean reward per episode: -0.48

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_011059_numTimesteps_477000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 5.85        |
|    ep_rew_mean          | -0.938      |
| time/                   |             |
|    fps                  | 192         |
|    iterations           | 39          |
|    time_elapsed         | 2491        |
|    total_timesteps      | 479232      |
| train/                  |             |
|    approx_kl            | 0.028950289 |
|    clip_fraction        | 0.667       |
|    clip_range           | 0.075       |
|    entropy_loss         | -4.31       |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.000512    |
|    loss                 | 0.022       |
|    n_updates            | 456         |
|    policy_gradient_loss | 0.00438     |
|    std                  | 0.717       |
|    value_loss           | 0.0349      |
-----------------------------------------
Num timesteps: 480000
Best mean reward: -0.48 - Last mean reward per episode: -0.52
Num timesteps: 483000
Best mean reward: -0.48 - Last mean reward per episode: -0.48

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_011129_numTimesteps_483000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
Num timesteps: 486000
Best mean reward: -0.48 - Last mean reward per episode: -0.48

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_011143_numTimesteps_486000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
Num timesteps: 489000
Best mean reward: -0.48 - Last mean reward per episode: -0.46

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_011154_numTimesteps_489000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 5.06        |
|    ep_rew_mean          | -0.822      |
| time/                   |             |
|    fps                  | 193         |
|    iterations           | 40          |
|    time_elapsed         | 2542        |
|    total_timesteps      | 491520      |
| train/                  |             |
|    approx_kl            | 0.019196844 |
|    clip_fraction        | 0.635       |
|    clip_range           | 0.075       |
|    entropy_loss         | -4.24       |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.000512    |
|    loss                 | 0.0186      |
|    n_updates            | 468         |
|    policy_gradient_loss | 0.00328     |
|    std                  | 0.705       |
|    value_loss           | 0.0288      |
-----------------------------------------
Num timesteps: 492000
Best mean reward: -0.46 - Last mean reward per episode: -0.47
Num timesteps: 495000
Best mean reward: -0.46 - Last mean reward per episode: -0.46
Num timesteps: 498000
Best mean reward: -0.46 - Last mean reward per episode: -1.09
Num timesteps: 501000
Best mean reward: -0.46 - Last mean reward per episode: -0.64
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4.67        |
|    ep_rew_mean          | -0.461      |
| time/                   |             |
|    fps                  | 195         |
|    iterations           | 41          |
|    time_elapsed         | 2577        |
|    total_timesteps      | 503808      |
| train/                  |             |
|    approx_kl            | 0.021112442 |
|    clip_fraction        | 0.65        |
|    clip_range           | 0.075       |
|    entropy_loss         | -4.16       |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.000512    |
|    loss                 | 0.012       |
|    n_updates            | 480         |
|    policy_gradient_loss | 0.00107     |
|    std                  | 0.69        |
|    value_loss           | 0.0169      |
-----------------------------------------
Num timesteps: 504000
Best mean reward: -0.46 - Last mean reward per episode: -0.46

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_011239_numTimesteps_504000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
Num timesteps: 507000
Best mean reward: -0.46 - Last mean reward per episode: -0.44

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_011252_numTimesteps_507000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
Num timesteps: 510000
Best mean reward: -0.44 - Last mean reward per episode: -0.45
Num timesteps: 513000
Best mean reward: -0.44 - Last mean reward per episode: -0.44

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_011315_numTimesteps_513000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
Num timesteps: 516000
Best mean reward: -0.44 - Last mean reward per episode: -0.44
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4.54        |
|    ep_rew_mean          | -0.446      |
| time/                   |             |
|    fps                  | 196         |
|    iterations           | 42          |
|    time_elapsed         | 2625        |
|    total_timesteps      | 516096      |
| train/                  |             |
|    approx_kl            | 0.023940736 |
|    clip_fraction        | 0.648       |
|    clip_range           | 0.075       |
|    entropy_loss         | -4.09       |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.000512    |
|    loss                 | 0.000416    |
|    n_updates            | 492         |
|    policy_gradient_loss | 0.00235     |
|    std                  | 0.679       |
|    value_loss           | 0.0294      |
-----------------------------------------
Num timesteps: 519000
Best mean reward: -0.44 - Last mean reward per episode: -0.44
Num timesteps: 522000
Best mean reward: -0.44 - Last mean reward per episode: -0.46
Num timesteps: 525000
Best mean reward: -0.44 - Last mean reward per episode: -0.48
Num timesteps: 528000
Best mean reward: -0.44 - Last mean reward per episode: -0.46
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4.54        |
|    ep_rew_mean          | -0.449      |
| time/                   |             |
|    fps                  | 198         |
|    iterations           | 43          |
|    time_elapsed         | 2668        |
|    total_timesteps      | 528384      |
| train/                  |             |
|    approx_kl            | 0.022156661 |
|    clip_fraction        | 0.685       |
|    clip_range           | 0.075       |
|    entropy_loss         | -4          |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.000512    |
|    loss                 | 0.00684     |
|    n_updates            | 504         |
|    policy_gradient_loss | 0.00699     |
|    std                  | 0.663       |
|    value_loss           | 0.0171      |
-----------------------------------------
Num timesteps: 531000
Best mean reward: -0.44 - Last mean reward per episode: -0.43

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_011413_numTimesteps_531000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
Num timesteps: 534000
Best mean reward: -0.43 - Last mean reward per episode: -0.57
Num timesteps: 537000
Best mean reward: -0.43 - Last mean reward per episode: -0.43

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_011430_numTimesteps_537000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
Num timesteps: 540000
Best mean reward: -0.43 - Last mean reward per episode: -0.42

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_011447_numTimesteps_540000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4.86        |
|    ep_rew_mean          | -0.553      |
| time/                   |             |
|    fps                  | 199         |
|    iterations           | 44          |
|    time_elapsed         | 2711        |
|    total_timesteps      | 540672      |
| train/                  |             |
|    approx_kl            | 0.027282588 |
|    clip_fraction        | 0.672       |
|    clip_range           | 0.075       |
|    entropy_loss         | -3.91       |
|    explained_variance   | 0.949       |
|    learning_rate        | 0.000512    |
|    loss                 | 0.0095      |
|    n_updates            | 516         |
|    policy_gradient_loss | 0.00559     |
|    std                  | 0.65        |
|    value_loss           | 0.0103      |
-----------------------------------------
Num timesteps: 543000
Best mean reward: -0.42 - Last mean reward per episode: -0.42

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_011455_numTimesteps_543000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
Num timesteps: 546000
Best mean reward: -0.42 - Last mean reward per episode: -0.43
Num timesteps: 549000
Best mean reward: -0.42 - Last mean reward per episode: -0.44
Num timesteps: 552000
Best mean reward: -0.42 - Last mean reward per episode: -0.43
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 5.11       |
|    ep_rew_mean          | -0.672     |
| time/                   |            |
|    fps                  | 199        |
|    iterations           | 45         |
|    time_elapsed         | 2768       |
|    total_timesteps      | 552960     |
| train/                  |            |
|    approx_kl            | 0.02560276 |
|    clip_fraction        | 0.666      |
|    clip_range           | 0.075      |
|    entropy_loss         | -3.81      |
|    explained_variance   | 0.972      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.00489    |
|    n_updates            | 528        |
|    policy_gradient_loss | 0.000289   |
|    std                  | 0.635      |
|    value_loss           | 0.016      |
----------------------------------------
Num timesteps: 555000
Best mean reward: -0.42 - Last mean reward per episode: -0.41

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_011554_numTimesteps_555000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
Num timesteps: 558000
Best mean reward: -0.41 - Last mean reward per episode: -0.42
Num timesteps: 561000
Best mean reward: -0.41 - Last mean reward per episode: -0.41
Num timesteps: 564000
Best mean reward: -0.41 - Last mean reward per episode: -0.42
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4.22        |
|    ep_rew_mean          | -0.417      |
| time/                   |             |
|    fps                  | 199         |
|    iterations           | 46          |
|    time_elapsed         | 2832        |
|    total_timesteps      | 565248      |
| train/                  |             |
|    approx_kl            | 0.028609147 |
|    clip_fraction        | 0.693       |
|    clip_range           | 0.075       |
|    entropy_loss         | -3.72       |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.000512    |
|    loss                 | 0.00571     |
|    n_updates            | 540         |
|    policy_gradient_loss | 0.00729     |
|    std                  | 0.623       |
|    value_loss           | 0.00695     |
-----------------------------------------
Num timesteps: 567000
Best mean reward: -0.41 - Last mean reward per episode: -0.79
Num timesteps: 570000
Best mean reward: -0.41 - Last mean reward per episode: -0.42
Num timesteps: 573000
Best mean reward: -0.41 - Last mean reward per episode: -0.40

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_011731_numTimesteps_573000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
Num timesteps: 576000
Best mean reward: -0.40 - Last mean reward per episode: -0.41
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4.12        |
|    ep_rew_mean          | -0.402      |
| time/                   |             |
|    fps                  | 199         |
|    iterations           | 47          |
|    time_elapsed         | 2900        |
|    total_timesteps      | 577536      |
| train/                  |             |
|    approx_kl            | 0.023400143 |
|    clip_fraction        | 0.655       |
|    clip_range           | 0.075       |
|    entropy_loss         | -3.64       |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.000512    |
|    loss                 | -0.00262    |
|    n_updates            | 552         |
|    policy_gradient_loss | 0.00219     |
|    std                  | 0.61        |
|    value_loss           | 0.00961     |
-----------------------------------------
Num timesteps: 579000
Best mean reward: -0.40 - Last mean reward per episode: -0.59
Num timesteps: 582000
Best mean reward: -0.40 - Last mean reward per episode: -0.40

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_011825_numTimesteps_582000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
Num timesteps: 585000
Best mean reward: -0.40 - Last mean reward per episode: -0.40
Num timesteps: 588000
Best mean reward: -0.40 - Last mean reward per episode: -0.45
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4.13        |
|    ep_rew_mean          | -0.402      |
| time/                   |             |
|    fps                  | 198         |
|    iterations           | 48          |
|    time_elapsed         | 2971        |
|    total_timesteps      | 589824      |
| train/                  |             |
|    approx_kl            | 0.028251747 |
|    clip_fraction        | 0.696       |
|    clip_range           | 0.075       |
|    entropy_loss         | -3.56       |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.000512    |
|    loss                 | 0.0059      |
|    n_updates            | 564         |
|    policy_gradient_loss | 0.0125      |
|    std                  | 0.599       |
|    value_loss           | 0.00916     |
-----------------------------------------
Num timesteps: 591000
Best mean reward: -0.40 - Last mean reward per episode: -0.40
Num timesteps: 594000
Best mean reward: -0.40 - Last mean reward per episode: -0.41
Num timesteps: 597000
Best mean reward: -0.40 - Last mean reward per episode: -0.40
Num timesteps: 600000
Best mean reward: -0.40 - Last mean reward per episode: -0.58
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4.14       |
|    ep_rew_mean          | -0.399     |
| time/                   |            |
|    fps                  | 197        |
|    iterations           | 49         |
|    time_elapsed         | 3047       |
|    total_timesteps      | 602112     |
| train/                  |            |
|    approx_kl            | 0.04017662 |
|    clip_fraction        | 0.696      |
|    clip_range           | 0.075      |
|    entropy_loss         | -3.48      |
|    explained_variance   | 0.97       |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0145     |
|    n_updates            | 576        |
|    policy_gradient_loss | 0.00587    |
|    std                  | 0.587      |
|    value_loss           | 0.0144     |
----------------------------------------
Num timesteps: 603000
Best mean reward: -0.40 - Last mean reward per episode: -0.40
Num timesteps: 606000
Best mean reward: -0.40 - Last mean reward per episode: -0.39

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_012043_numTimesteps_606000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
Num timesteps: 609000
Best mean reward: -0.39 - Last mean reward per episode: -0.39
Num timesteps: 612000
Best mean reward: -0.39 - Last mean reward per episode: -0.39

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_012108_numTimesteps_612000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4.25        |
|    ep_rew_mean          | -0.404      |
| time/                   |             |
|    fps                  | 197         |
|    iterations           | 50          |
|    time_elapsed         | 3105        |
|    total_timesteps      | 614400      |
| train/                  |             |
|    approx_kl            | 0.046709698 |
|    clip_fraction        | 0.692       |
|    clip_range           | 0.075       |
|    entropy_loss         | -3.41       |
|    explained_variance   | 0.951       |
|    learning_rate        | 0.000512    |
|    loss                 | 0.0143      |
|    n_updates            | 588         |
|    policy_gradient_loss | 0.00839     |
|    std                  | 0.579       |
|    value_loss           | 0.0113      |
-----------------------------------------
Num timesteps: 615000
Best mean reward: -0.39 - Last mean reward per episode: -0.39

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_012128_numTimesteps_615000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
Num timesteps: 618000
Best mean reward: -0.39 - Last mean reward per episode: -0.41
Num timesteps: 621000
Best mean reward: -0.39 - Last mean reward per episode: -0.38

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_012152_numTimesteps_621000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
Num timesteps: 624000
Best mean reward: -0.38 - Last mean reward per episode: -0.40
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4.54        |
|    ep_rew_mean          | -0.542      |
| time/                   |             |
|    fps                  | 198         |
|    iterations           | 51          |
|    time_elapsed         | 3162        |
|    total_timesteps      | 626688      |
| train/                  |             |
|    approx_kl            | 0.033795476 |
|    clip_fraction        | 0.676       |
|    clip_range           | 0.075       |
|    entropy_loss         | -3.32       |
|    explained_variance   | 0.936       |
|    learning_rate        | 0.000512    |
|    loss                 | -0.00278    |
|    n_updates            | 600         |
|    policy_gradient_loss | 0.0133      |
|    std                  | 0.569       |
|    value_loss           | 0.00193     |
-----------------------------------------
Num timesteps: 627000
Best mean reward: -0.38 - Last mean reward per episode: -0.39
Num timesteps: 630000
Best mean reward: -0.38 - Last mean reward per episode: -0.40
Num timesteps: 633000
Best mean reward: -0.38 - Last mean reward per episode: -0.41
Num timesteps: 636000
Best mean reward: -0.38 - Last mean reward per episode: -0.39
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4.06       |
|    ep_rew_mean          | -0.402     |
| time/                   |            |
|    fps                  | 198        |
|    iterations           | 52         |
|    time_elapsed         | 3220       |
|    total_timesteps      | 638976     |
| train/                  |            |
|    approx_kl            | 0.02280944 |
|    clip_fraction        | 0.668      |
|    clip_range           | 0.075      |
|    entropy_loss         | -3.23      |
|    explained_variance   | 0.94       |
|    learning_rate        | 0.000512   |
|    loss                 | 0.00599    |
|    n_updates            | 612        |
|    policy_gradient_loss | 0.00421    |
|    std                  | 0.556      |
|    value_loss           | 0.00228    |
----------------------------------------
Num timesteps: 639000
Best mean reward: -0.38 - Last mean reward per episode: -0.40
Num timesteps: 642000
Best mean reward: -0.38 - Last mean reward per episode: -0.39
Num timesteps: 645000
Best mean reward: -0.38 - Last mean reward per episode: -0.39
Num timesteps: 648000
Best mean reward: -0.38 - Last mean reward per episode: -0.39
Num timesteps: 651000
Best mean reward: -0.38 - Last mean reward per episode: -0.39
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4.08        |
|    ep_rew_mean          | -0.388      |
| time/                   |             |
|    fps                  | 199         |
|    iterations           | 53          |
|    time_elapsed         | 3270        |
|    total_timesteps      | 651264      |
| train/                  |             |
|    approx_kl            | 0.015965754 |
|    clip_fraction        | 0.615       |
|    clip_range           | 0.075       |
|    entropy_loss         | -3.13       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.000512    |
|    loss                 | -0.00544    |
|    n_updates            | 624         |
|    policy_gradient_loss | 0.00229     |
|    std                  | 0.546       |
|    value_loss           | 0.000348    |
-----------------------------------------
Num timesteps: 654000
Best mean reward: -0.38 - Last mean reward per episode: -0.39
Num timesteps: 657000
Best mean reward: -0.38 - Last mean reward per episode: -0.39
Num timesteps: 660000
Best mean reward: -0.38 - Last mean reward per episode: -0.40
Num timesteps: 663000
Best mean reward: -0.38 - Last mean reward per episode: -0.39
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4.06        |
|    ep_rew_mean          | -0.39       |
| time/                   |             |
|    fps                  | 200         |
|    iterations           | 54          |
|    time_elapsed         | 3313        |
|    total_timesteps      | 663552      |
| train/                  |             |
|    approx_kl            | 0.042222917 |
|    clip_fraction        | 0.658       |
|    clip_range           | 0.075       |
|    entropy_loss         | -3.05       |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.000512    |
|    loss                 | 0.0156      |
|    n_updates            | 636         |
|    policy_gradient_loss | 0.00873     |
|    std                  | 0.534       |
|    value_loss           | 0.00222     |
-----------------------------------------
Num timesteps: 666000
Best mean reward: -0.38 - Last mean reward per episode: -0.39
Num timesteps: 669000
Best mean reward: -0.38 - Last mean reward per episode: -0.39
Num timesteps: 672000
Best mean reward: -0.38 - Last mean reward per episode: -0.62
Num timesteps: 675000
Best mean reward: -0.38 - Last mean reward per episode: -0.40
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4.03       |
|    ep_rew_mean          | -0.385     |
| time/                   |            |
|    fps                  | 200        |
|    iterations           | 55         |
|    time_elapsed         | 3371       |
|    total_timesteps      | 675840     |
| train/                  |            |
|    approx_kl            | 0.02728256 |
|    clip_fraction        | 0.659      |
|    clip_range           | 0.075      |
|    entropy_loss         | -2.95      |
|    explained_variance   | 0.952      |
|    learning_rate        | 0.000512   |
|    loss                 | -0.00235   |
|    n_updates            | 648        |
|    policy_gradient_loss | 0.00411    |
|    std                  | 0.523      |
|    value_loss           | 0.00133    |
----------------------------------------
Num timesteps: 678000
Best mean reward: -0.38 - Last mean reward per episode: -0.38

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_012602_numTimesteps_678000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
Num timesteps: 681000
Best mean reward: -0.38 - Last mean reward per episode: -0.39
Num timesteps: 684000
Best mean reward: -0.38 - Last mean reward per episode: -0.39
Num timesteps: 687000
Best mean reward: -0.38 - Last mean reward per episode: -0.39
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4.03        |
|    ep_rew_mean          | -0.393      |
| time/                   |             |
|    fps                  | 200         |
|    iterations           | 56          |
|    time_elapsed         | 3431        |
|    total_timesteps      | 688128      |
| train/                  |             |
|    approx_kl            | 0.076188065 |
|    clip_fraction        | 0.708       |
|    clip_range           | 0.075       |
|    entropy_loss         | -2.86       |
|    explained_variance   | 0.89        |
|    learning_rate        | 0.000512    |
|    loss                 | -0.00427    |
|    n_updates            | 660         |
|    policy_gradient_loss | 0.0195      |
|    std                  | 0.513       |
|    value_loss           | 0.00187     |
-----------------------------------------
Num timesteps: 690000
Best mean reward: -0.38 - Last mean reward per episode: -0.38

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_012705_numTimesteps_690000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
Num timesteps: 693000
Best mean reward: -0.38 - Last mean reward per episode: -0.39
Num timesteps: 696000
Best mean reward: -0.38 - Last mean reward per episode: -0.38

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_012723_numTimesteps_696000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
Num timesteps: 699000
Best mean reward: -0.38 - Last mean reward per episode: -0.39
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4.07        |
|    ep_rew_mean          | -0.396      |
| time/                   |             |
|    fps                  | 201         |
|    iterations           | 57          |
|    time_elapsed         | 3478        |
|    total_timesteps      | 700416      |
| train/                  |             |
|    approx_kl            | 0.028850516 |
|    clip_fraction        | 0.661       |
|    clip_range           | 0.075       |
|    entropy_loss         | -2.76       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.000512    |
|    loss                 | 0.0135      |
|    n_updates            | 672         |
|    policy_gradient_loss | 0.0108      |
|    std                  | 0.503       |
|    value_loss           | 0.000275    |
-----------------------------------------
Num timesteps: 702000
Best mean reward: -0.38 - Last mean reward per episode: -0.38

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_012748_numTimesteps_702000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
Num timesteps: 705000
Best mean reward: -0.38 - Last mean reward per episode: -0.39
Num timesteps: 708000
Best mean reward: -0.38 - Last mean reward per episode: -0.46
Num timesteps: 711000
Best mean reward: -0.38 - Last mean reward per episode: -0.40
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4.08        |
|    ep_rew_mean          | -0.384      |
| time/                   |             |
|    fps                  | 201         |
|    iterations           | 58          |
|    time_elapsed         | 3537        |
|    total_timesteps      | 712704      |
| train/                  |             |
|    approx_kl            | 0.037071135 |
|    clip_fraction        | 0.682       |
|    clip_range           | 0.075       |
|    entropy_loss         | -2.68       |
|    explained_variance   | 0.891       |
|    learning_rate        | 0.000512    |
|    loss                 | 0.00445     |
|    n_updates            | 684         |
|    policy_gradient_loss | 0.0136      |
|    std                  | 0.495       |
|    value_loss           | 0.00138     |
-----------------------------------------
Num timesteps: 714000
Best mean reward: -0.38 - Last mean reward per episode: -0.38
Num timesteps: 717000
Best mean reward: -0.38 - Last mean reward per episode: -0.39
Num timesteps: 720000
Best mean reward: -0.38 - Last mean reward per episode: -0.38
Num timesteps: 723000
Best mean reward: -0.38 - Last mean reward per episode: -0.39
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4           |
|    ep_rew_mean          | -0.376      |
| time/                   |             |
|    fps                  | 201         |
|    iterations           | 59          |
|    time_elapsed         | 3604        |
|    total_timesteps      | 724992      |
| train/                  |             |
|    approx_kl            | 0.051952053 |
|    clip_fraction        | 0.692       |
|    clip_range           | 0.075       |
|    entropy_loss         | -2.59       |
|    explained_variance   | 0.869       |
|    learning_rate        | 0.000512    |
|    loss                 | 0.0173      |
|    n_updates            | 696         |
|    policy_gradient_loss | 0.0112      |
|    std                  | 0.485       |
|    value_loss           | 0.000939    |
-----------------------------------------
Num timesteps: 726000
Best mean reward: -0.38 - Last mean reward per episode: -0.39
Num timesteps: 729000
Best mean reward: -0.38 - Last mean reward per episode: -0.38
Num timesteps: 732000
Best mean reward: -0.38 - Last mean reward per episode: -0.38
Num timesteps: 735000
Best mean reward: -0.38 - Last mean reward per episode: -0.39
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4.01       |
|    ep_rew_mean          | -0.386     |
| time/                   |            |
|    fps                  | 201        |
|    iterations           | 60         |
|    time_elapsed         | 3662       |
|    total_timesteps      | 737280     |
| train/                  |            |
|    approx_kl            | 0.02753327 |
|    clip_fraction        | 0.615      |
|    clip_range           | 0.075      |
|    entropy_loss         | -2.48      |
|    explained_variance   | 0.991      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0145     |
|    n_updates            | 708        |
|    policy_gradient_loss | 0.00831    |
|    std                  | 0.475      |
|    value_loss           | 0.000127   |
----------------------------------------
Num timesteps: 738000
Best mean reward: -0.38 - Last mean reward per episode: -0.38
Num timesteps: 741000
Best mean reward: -0.38 - Last mean reward per episode: -0.38
Num timesteps: 744000
Best mean reward: -0.38 - Last mean reward per episode: -0.39
Num timesteps: 747000
Best mean reward: -0.38 - Last mean reward per episode: -0.39
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4.01       |
|    ep_rew_mean          | -0.379     |
| time/                   |            |
|    fps                  | 202        |
|    iterations           | 61         |
|    time_elapsed         | 3706       |
|    total_timesteps      | 749568     |
| train/                  |            |
|    approx_kl            | 0.03557309 |
|    clip_fraction        | 0.645      |
|    clip_range           | 0.075      |
|    entropy_loss         | -2.39      |
|    explained_variance   | 0.991      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0318     |
|    n_updates            | 720        |
|    policy_gradient_loss | 0.0108     |
|    std                  | 0.466      |
|    value_loss           | 0.00016    |
----------------------------------------
Num timesteps: 750000
Best mean reward: -0.38 - Last mean reward per episode: -0.38
Num timesteps: 753000
Best mean reward: -0.38 - Last mean reward per episode: -0.40
Num timesteps: 756000
Best mean reward: -0.38 - Last mean reward per episode: -0.38
Num timesteps: 759000
Best mean reward: -0.38 - Last mean reward per episode: -0.38
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4           |
|    ep_rew_mean          | -0.388      |
| time/                   |             |
|    fps                  | 202         |
|    iterations           | 62          |
|    time_elapsed         | 3764        |
|    total_timesteps      | 761856      |
| train/                  |             |
|    approx_kl            | 0.021713795 |
|    clip_fraction        | 0.624       |
|    clip_range           | 0.075       |
|    entropy_loss         | -2.29       |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.000512    |
|    loss                 | 0.00548     |
|    n_updates            | 732         |
|    policy_gradient_loss | 0.0105      |
|    std                  | 0.457       |
|    value_loss           | 9.39e-05    |
-----------------------------------------
Num timesteps: 762000
Best mean reward: -0.38 - Last mean reward per episode: -0.38
Num timesteps: 765000
Best mean reward: -0.38 - Last mean reward per episode: -0.38
Num timesteps: 768000
Best mean reward: -0.38 - Last mean reward per episode: -0.38

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_013255_numTimesteps_768000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
Num timesteps: 771000
Best mean reward: -0.38 - Last mean reward per episode: -0.38
Num timesteps: 774000
Best mean reward: -0.38 - Last mean reward per episode: -0.38
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4           |
|    ep_rew_mean          | -0.378      |
| time/                   |             |
|    fps                  | 201         |
|    iterations           | 63          |
|    time_elapsed         | 3837        |
|    total_timesteps      | 774144      |
| train/                  |             |
|    approx_kl            | 0.028297046 |
|    clip_fraction        | 0.646       |
|    clip_range           | 0.075       |
|    entropy_loss         | -2.21       |
|    explained_variance   | 0.993       |
|    learning_rate        | 0.000512    |
|    loss                 | 0.00887     |
|    n_updates            | 744         |
|    policy_gradient_loss | 0.0165      |
|    std                  | 0.45        |
|    value_loss           | 0.000123    |
-----------------------------------------
Num timesteps: 777000
Best mean reward: -0.38 - Last mean reward per episode: -0.37

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_013354_numTimesteps_777000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
Num timesteps: 780000
Best mean reward: -0.37 - Last mean reward per episode: -0.38
Num timesteps: 783000
Best mean reward: -0.37 - Last mean reward per episode: -0.38
Num timesteps: 786000
Best mean reward: -0.37 - Last mean reward per episode: -0.37

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_013440_numTimesteps_786000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4           |
|    ep_rew_mean          | -0.377      |
| time/                   |             |
|    fps                  | 201         |
|    iterations           | 64          |
|    time_elapsed         | 3904        |
|    total_timesteps      | 786432      |
| train/                  |             |
|    approx_kl            | 0.022625536 |
|    clip_fraction        | 0.632       |
|    clip_range           | 0.075       |
|    entropy_loss         | -2.13       |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.000512    |
|    loss                 | 0.00482     |
|    n_updates            | 756         |
|    policy_gradient_loss | 0.0144      |
|    std                  | 0.443       |
|    value_loss           | 8.38e-05    |
-----------------------------------------
Num timesteps: 789000
Best mean reward: -0.37 - Last mean reward per episode: -0.38
Num timesteps: 792000
Best mean reward: -0.37 - Last mean reward per episode: -0.39
Num timesteps: 795000
Best mean reward: -0.37 - Last mean reward per episode: -0.39
Num timesteps: 798000
Best mean reward: -0.37 - Last mean reward per episode: -0.38
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4           |
|    ep_rew_mean          | -0.389      |
| time/                   |             |
|    fps                  | 201         |
|    iterations           | 65          |
|    time_elapsed         | 3969        |
|    total_timesteps      | 798720      |
| train/                  |             |
|    approx_kl            | 0.019777326 |
|    clip_fraction        | 0.608       |
|    clip_range           | 0.075       |
|    entropy_loss         | -2.05       |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.000512    |
|    loss                 | 0.0227      |
|    n_updates            | 768         |
|    policy_gradient_loss | 0.0173      |
|    std                  | 0.437       |
|    value_loss           | 4.77e-05    |
-----------------------------------------
Num timesteps: 801000
Best mean reward: -0.37 - Last mean reward per episode: -0.38
Num timesteps: 804000
Best mean reward: -0.37 - Last mean reward per episode: -0.38
Num timesteps: 807000
Best mean reward: -0.37 - Last mean reward per episode: -0.38
Num timesteps: 810000
Best mean reward: -0.37 - Last mean reward per episode: -0.38
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4.01        |
|    ep_rew_mean          | -0.387      |
| time/                   |             |
|    fps                  | 200         |
|    iterations           | 66          |
|    time_elapsed         | 4035        |
|    total_timesteps      | 811008      |
| train/                  |             |
|    approx_kl            | 0.020260883 |
|    clip_fraction        | 0.622       |
|    clip_range           | 0.075       |
|    entropy_loss         | -1.97       |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.000512    |
|    loss                 | 0.013       |
|    n_updates            | 780         |
|    policy_gradient_loss | 0.0145      |
|    std                  | 0.43        |
|    value_loss           | 4.55e-05    |
-----------------------------------------
Num timesteps: 813000
Best mean reward: -0.37 - Last mean reward per episode: -0.38
Num timesteps: 816000
Best mean reward: -0.37 - Last mean reward per episode: -0.37
Num timesteps: 819000
Best mean reward: -0.37 - Last mean reward per episode: -0.38
Num timesteps: 822000
Best mean reward: -0.37 - Last mean reward per episode: -0.38
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4           |
|    ep_rew_mean          | -0.389      |
| time/                   |             |
|    fps                  | 200         |
|    iterations           | 67          |
|    time_elapsed         | 4101        |
|    total_timesteps      | 823296      |
| train/                  |             |
|    approx_kl            | 0.026673883 |
|    clip_fraction        | 0.649       |
|    clip_range           | 0.075       |
|    entropy_loss         | -1.89       |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.000512    |
|    loss                 | 0.00592     |
|    n_updates            | 792         |
|    policy_gradient_loss | 0.0181      |
|    std                  | 0.424       |
|    value_loss           | 8.6e-05     |
-----------------------------------------
Num timesteps: 825000
Best mean reward: -0.37 - Last mean reward per episode: -0.38
Num timesteps: 828000
Best mean reward: -0.37 - Last mean reward per episode: -0.37
Num timesteps: 831000
Best mean reward: -0.37 - Last mean reward per episode: -0.37
Num timesteps: 834000
Best mean reward: -0.37 - Last mean reward per episode: -0.38
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4           |
|    ep_rew_mean          | -0.383      |
| time/                   |             |
|    fps                  | 200         |
|    iterations           | 68          |
|    time_elapsed         | 4165        |
|    total_timesteps      | 835584      |
| train/                  |             |
|    approx_kl            | 0.037352897 |
|    clip_fraction        | 0.653       |
|    clip_range           | 0.075       |
|    entropy_loss         | -1.82       |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.000512    |
|    loss                 | 0.0211      |
|    n_updates            | 804         |
|    policy_gradient_loss | 0.0202      |
|    std                  | 0.418       |
|    value_loss           | 8.75e-05    |
-----------------------------------------
Num timesteps: 837000
Best mean reward: -0.37 - Last mean reward per episode: -0.39
Num timesteps: 840000
Best mean reward: -0.37 - Last mean reward per episode: -0.38
Num timesteps: 843000
Best mean reward: -0.37 - Last mean reward per episode: -0.37
Num timesteps: 846000
Best mean reward: -0.37 - Last mean reward per episode: -0.38
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4.03       |
|    ep_rew_mean          | -0.385     |
| time/                   |            |
|    fps                  | 200        |
|    iterations           | 69         |
|    time_elapsed         | 4239       |
|    total_timesteps      | 847872     |
| train/                  |            |
|    approx_kl            | 0.03481214 |
|    clip_fraction        | 0.671      |
|    clip_range           | 0.075      |
|    entropy_loss         | -1.73      |
|    explained_variance   | 0.997      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0372     |
|    n_updates            | 816        |
|    policy_gradient_loss | 0.0243     |
|    std                  | 0.412      |
|    value_loss           | 0.000137   |
----------------------------------------
Num timesteps: 849000
Best mean reward: -0.37 - Last mean reward per episode: -0.38
Num timesteps: 852000
Best mean reward: -0.37 - Last mean reward per episode: -0.38
Num timesteps: 855000
Best mean reward: -0.37 - Last mean reward per episode: -0.36

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_014103_numTimesteps_855000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
Num timesteps: 858000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4.03        |
|    ep_rew_mean          | -0.374      |
| time/                   |             |
|    fps                  | 199         |
|    iterations           | 70          |
|    time_elapsed         | 4316        |
|    total_timesteps      | 860160      |
| train/                  |             |
|    approx_kl            | 0.044036165 |
|    clip_fraction        | 0.646       |
|    clip_range           | 0.075       |
|    entropy_loss         | -1.66       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.000512    |
|    loss                 | 0.00407     |
|    n_updates            | 828         |
|    policy_gradient_loss | 0.0198      |
|    std                  | 0.408       |
|    value_loss           | 9.51e-05    |
-----------------------------------------
Num timesteps: 861000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 864000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 867000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 870000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4.01        |
|    ep_rew_mean          | -0.387      |
| time/                   |             |
|    fps                  | 199         |
|    iterations           | 71          |
|    time_elapsed         | 4382        |
|    total_timesteps      | 872448      |
| train/                  |             |
|    approx_kl            | 0.030275524 |
|    clip_fraction        | 0.671       |
|    clip_range           | 0.075       |
|    entropy_loss         | -1.59       |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.000512    |
|    loss                 | 0.0113      |
|    n_updates            | 840         |
|    policy_gradient_loss | 0.019       |
|    std                  | 0.403       |
|    value_loss           | 0.00104     |
-----------------------------------------
Num timesteps: 873000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 876000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 879000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 882000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.379     |
| time/                   |            |
|    fps                  | 198        |
|    iterations           | 72         |
|    time_elapsed         | 4455       |
|    total_timesteps      | 884736     |
| train/                  |            |
|    approx_kl            | 0.06907297 |
|    clip_fraction        | 0.632      |
|    clip_range           | 0.075      |
|    entropy_loss         | -1.52      |
|    explained_variance   | 0.939      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.000492   |
|    n_updates            | 852        |
|    policy_gradient_loss | 0.00421    |
|    std                  | 0.398      |
|    value_loss           | 0.00339    |
----------------------------------------
Num timesteps: 885000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 888000
Best mean reward: -0.36 - Last mean reward per episode: -0.39
Num timesteps: 891000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 894000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 897000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4.03       |
|    ep_rew_mean          | -0.372     |
| time/                   |            |
|    fps                  | 198        |
|    iterations           | 73         |
|    time_elapsed         | 4516       |
|    total_timesteps      | 897024     |
| train/                  |            |
|    approx_kl            | 0.20550811 |
|    clip_fraction        | 0.694      |
|    clip_range           | 0.075      |
|    entropy_loss         | -1.46      |
|    explained_variance   | 0.993      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0362     |
|    n_updates            | 864        |
|    policy_gradient_loss | 0.0324     |
|    std                  | 0.395      |
|    value_loss           | 7.23e-05   |
----------------------------------------
Num timesteps: 900000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 903000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 906000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 909000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.373     |
| time/                   |            |
|    fps                  | 199        |
|    iterations           | 74         |
|    time_elapsed         | 4564       |
|    total_timesteps      | 909312     |
| train/                  |            |
|    approx_kl            | 0.30227408 |
|    clip_fraction        | 0.706      |
|    clip_range           | 0.075      |
|    entropy_loss         | -1.42      |
|    explained_variance   | 0.975      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.00243    |
|    n_updates            | 876        |
|    policy_gradient_loss | 0.00832    |
|    std                  | 0.391      |
|    value_loss           | 0.0035     |
----------------------------------------
Num timesteps: 912000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 915000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 918000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 921000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4.01       |
|    ep_rew_mean          | -0.373     |
| time/                   |            |
|    fps                  | 200        |
|    iterations           | 75         |
|    time_elapsed         | 4602       |
|    total_timesteps      | 921600     |
| train/                  |            |
|    approx_kl            | 0.03957209 |
|    clip_fraction        | 0.646      |
|    clip_range           | 0.075      |
|    entropy_loss         | -1.36      |
|    explained_variance   | 0.993      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0468     |
|    n_updates            | 888        |
|    policy_gradient_loss | 0.0345     |
|    std                  | 0.388      |
|    value_loss           | 8.51e-05   |
----------------------------------------
Num timesteps: 924000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 927000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 930000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 933000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4           |
|    ep_rew_mean          | -0.375      |
| time/                   |             |
|    fps                  | 200         |
|    iterations           | 76          |
|    time_elapsed         | 4667        |
|    total_timesteps      | 933888      |
| train/                  |             |
|    approx_kl            | 0.052180156 |
|    clip_fraction        | 0.68        |
|    clip_range           | 0.075       |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.000512    |
|    loss                 | 0.024       |
|    n_updates            | 900         |
|    policy_gradient_loss | 0.03        |
|    std                  | 0.384       |
|    value_loss           | 0.00044     |
-----------------------------------------
Num timesteps: 936000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 939000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 942000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 945000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4           |
|    ep_rew_mean          | -0.372      |
| time/                   |             |
|    fps                  | 199         |
|    iterations           | 77          |
|    time_elapsed         | 4737        |
|    total_timesteps      | 946176      |
| train/                  |             |
|    approx_kl            | 0.045019347 |
|    clip_fraction        | 0.667       |
|    clip_range           | 0.075       |
|    entropy_loss         | -1.25       |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.000512    |
|    loss                 | 0.0365      |
|    n_updates            | 912         |
|    policy_gradient_loss | 0.0338      |
|    std                  | 0.381       |
|    value_loss           | 4.24e-05    |
-----------------------------------------
Num timesteps: 948000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 951000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 954000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 957000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4.01        |
|    ep_rew_mean          | -0.379      |
| time/                   |             |
|    fps                  | 199         |
|    iterations           | 78          |
|    time_elapsed         | 4792        |
|    total_timesteps      | 958464      |
| train/                  |             |
|    approx_kl            | 0.025872717 |
|    clip_fraction        | 0.677       |
|    clip_range           | 0.075       |
|    entropy_loss         | -1.2        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.000512    |
|    loss                 | 0.0273      |
|    n_updates            | 924         |
|    policy_gradient_loss | 0.0344      |
|    std                  | 0.378       |
|    value_loss           | 3.74e-05    |
-----------------------------------------
Num timesteps: 960000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 963000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 966000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 969000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.375     |
| time/                   |            |
|    fps                  | 199        |
|    iterations           | 79         |
|    time_elapsed         | 4858       |
|    total_timesteps      | 970752     |
| train/                  |            |
|    approx_kl            | 0.06589115 |
|    clip_fraction        | 0.684      |
|    clip_range           | 0.075      |
|    entropy_loss         | -1.15      |
|    explained_variance   | 0.998      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.019      |
|    n_updates            | 936        |
|    policy_gradient_loss | 0.0396     |
|    std                  | 0.376      |
|    value_loss           | 3.43e-05   |
----------------------------------------
Num timesteps: 972000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 975000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 978000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 981000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.377     |
| time/                   |            |
|    fps                  | 199        |
|    iterations           | 80         |
|    time_elapsed         | 4923       |
|    total_timesteps      | 983040     |
| train/                  |            |
|    approx_kl            | 0.06709761 |
|    clip_fraction        | 0.642      |
|    clip_range           | 0.075      |
|    entropy_loss         | -1.09      |
|    explained_variance   | 0.584      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.000435   |
|    n_updates            | 948        |
|    policy_gradient_loss | 0.00388    |
|    std                  | 0.372      |
|    value_loss           | 0.00338    |
----------------------------------------
Num timesteps: 984000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 987000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 990000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 993000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4.01        |
|    ep_rew_mean          | -0.386      |
| time/                   |             |
|    fps                  | 199         |
|    iterations           | 81          |
|    time_elapsed         | 4985        |
|    total_timesteps      | 995328      |
| train/                  |             |
|    approx_kl            | 0.056588035 |
|    clip_fraction        | 0.703       |
|    clip_range           | 0.075       |
|    entropy_loss         | -1.04       |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.000512    |
|    loss                 | 0.0384      |
|    n_updates            | 960         |
|    policy_gradient_loss | 0.0417      |
|    std                  | 0.369       |
|    value_loss           | 0.000375    |
-----------------------------------------
Num timesteps: 996000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 999000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1002000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1005000
Best mean reward: -0.36 - Last mean reward per episode: -0.39
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4.01       |
|    ep_rew_mean          | -0.381     |
| time/                   |            |
|    fps                  | 199        |
|    iterations           | 82         |
|    time_elapsed         | 5054       |
|    total_timesteps      | 1007616    |
| train/                  |            |
|    approx_kl            | 0.22688048 |
|    clip_fraction        | 0.722      |
|    clip_range           | 0.075      |
|    entropy_loss         | -0.985     |
|    explained_variance   | 0.996      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0607     |
|    n_updates            | 972        |
|    policy_gradient_loss | 0.0464     |
|    std                  | 0.367      |
|    value_loss           | 5.51e-05   |
----------------------------------------
Num timesteps: 1008000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1011000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1014000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1017000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.376    |
| time/                   |           |
|    fps                  | 199       |
|    iterations           | 83        |
|    time_elapsed         | 5113      |
|    total_timesteps      | 1019904   |
| train/                  |           |
|    approx_kl            | 0.3632042 |
|    clip_fraction        | 0.727     |
|    clip_range           | 0.075     |
|    entropy_loss         | -0.934    |
|    explained_variance   | 0.997     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.00963   |
|    n_updates            | 984       |
|    policy_gradient_loss | 0.0415    |
|    std                  | 0.362     |
|    value_loss           | 4.08e-05  |
---------------------------------------
Num timesteps: 1020000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1023000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1026000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1029000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1032000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.372     |
| time/                   |            |
|    fps                  | 198        |
|    iterations           | 84         |
|    time_elapsed         | 5192       |
|    total_timesteps      | 1032192    |
| train/                  |            |
|    approx_kl            | 0.06439039 |
|    clip_fraction        | 0.694      |
|    clip_range           | 0.075      |
|    entropy_loss         | -0.875     |
|    explained_variance   | 0.998      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0221     |
|    n_updates            | 996        |
|    policy_gradient_loss | 0.0358     |
|    std                  | 0.359      |
|    value_loss           | 3.04e-05   |
----------------------------------------
Num timesteps: 1035000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1038000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1041000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1044000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4           |
|    ep_rew_mean          | -0.384      |
| time/                   |             |
|    fps                  | 198         |
|    iterations           | 85          |
|    time_elapsed         | 5265        |
|    total_timesteps      | 1044480     |
| train/                  |             |
|    approx_kl            | 0.069083564 |
|    clip_fraction        | 0.705       |
|    clip_range           | 0.075       |
|    entropy_loss         | -0.81       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.000512    |
|    loss                 | 0.0038      |
|    n_updates            | 1008        |
|    policy_gradient_loss | 0.0397      |
|    std                  | 0.356       |
|    value_loss           | 2.86e-05    |
-----------------------------------------
Num timesteps: 1047000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1050000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1053000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1056000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.377    |
| time/                   |           |
|    fps                  | 198       |
|    iterations           | 86        |
|    time_elapsed         | 5336      |
|    total_timesteps      | 1056768   |
| train/                  |           |
|    approx_kl            | 0.2057605 |
|    clip_fraction        | 0.698     |
|    clip_range           | 0.075     |
|    entropy_loss         | -0.751    |
|    explained_variance   | 0.998     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0281    |
|    n_updates            | 1020      |
|    policy_gradient_loss | 0.0356    |
|    std                  | 0.353     |
|    value_loss           | 2.75e-05  |
---------------------------------------
Num timesteps: 1059000
Best mean reward: -0.36 - Last mean reward per episode: -0.36

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_015844_numTimesteps_1059000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
Num timesteps: 1062000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1065000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1068000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.384     |
| time/                   |            |
|    fps                  | 197        |
|    iterations           | 87         |
|    time_elapsed         | 5400       |
|    total_timesteps      | 1069056    |
| train/                  |            |
|    approx_kl            | 0.09079788 |
|    clip_fraction        | 0.684      |
|    clip_range           | 0.075      |
|    entropy_loss         | -0.702     |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0468     |
|    n_updates            | 1032       |
|    policy_gradient_loss | 0.0352     |
|    std                  | 0.35       |
|    value_loss           | 2.35e-05   |
----------------------------------------
Num timesteps: 1071000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1074000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1077000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1080000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.37      |
| time/                   |            |
|    fps                  | 197        |
|    iterations           | 88         |
|    time_elapsed         | 5472       |
|    total_timesteps      | 1081344    |
| train/                  |            |
|    approx_kl            | 0.09288319 |
|    clip_fraction        | 0.71       |
|    clip_range           | 0.075      |
|    entropy_loss         | -0.656     |
|    explained_variance   | 0.929      |
|    learning_rate        | 0.000512   |
|    loss                 | -0.000165  |
|    n_updates            | 1044       |
|    policy_gradient_loss | 0.011      |
|    std                  | 0.348      |
|    value_loss           | 0.000908   |
----------------------------------------
Num timesteps: 1083000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1086000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1089000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1092000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4.01       |
|    ep_rew_mean          | -0.373     |
| time/                   |            |
|    fps                  | 197        |
|    iterations           | 89         |
|    time_elapsed         | 5542       |
|    total_timesteps      | 1093632    |
| train/                  |            |
|    approx_kl            | 0.10687014 |
|    clip_fraction        | 0.715      |
|    clip_range           | 0.075      |
|    entropy_loss         | -0.618     |
|    explained_variance   | 0.991      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0384     |
|    n_updates            | 1056       |
|    policy_gradient_loss | 0.0434     |
|    std                  | 0.346      |
|    value_loss           | 8.15e-05   |
----------------------------------------
Num timesteps: 1095000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1098000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1101000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1104000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.378     |
| time/                   |            |
|    fps                  | 197        |
|    iterations           | 90         |
|    time_elapsed         | 5611       |
|    total_timesteps      | 1105920    |
| train/                  |            |
|    approx_kl            | 0.07568158 |
|    clip_fraction        | 0.717      |
|    clip_range           | 0.075      |
|    entropy_loss         | -0.579     |
|    explained_variance   | 0.996      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0301     |
|    n_updates            | 1068       |
|    policy_gradient_loss | 0.0416     |
|    std                  | 0.344      |
|    value_loss           | 3.4e-05    |
----------------------------------------
Num timesteps: 1107000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1110000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1113000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1116000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.377    |
| time/                   |           |
|    fps                  | 196       |
|    iterations           | 91        |
|    time_elapsed         | 5683      |
|    total_timesteps      | 1118208   |
| train/                  |           |
|    approx_kl            | 0.1541955 |
|    clip_fraction        | 0.71      |
|    clip_range           | 0.075     |
|    entropy_loss         | -0.542    |
|    explained_variance   | 0.997     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0297    |
|    n_updates            | 1080      |
|    policy_gradient_loss | 0.0362    |
|    std                  | 0.342     |
|    value_loss           | 2.94e-05  |
---------------------------------------
Num timesteps: 1119000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1122000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1125000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1128000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.367    |
| time/                   |           |
|    fps                  | 196       |
|    iterations           | 92        |
|    time_elapsed         | 5746      |
|    total_timesteps      | 1130496   |
| train/                  |           |
|    approx_kl            | 0.2509886 |
|    clip_fraction        | 0.698     |
|    clip_range           | 0.075     |
|    entropy_loss         | -0.492    |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0431    |
|    n_updates            | 1092      |
|    policy_gradient_loss | 0.0358    |
|    std                  | 0.339     |
|    value_loss           | 2.57e-05  |
---------------------------------------
Num timesteps: 1131000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1134000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1137000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1140000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4.01       |
|    ep_rew_mean          | -0.381     |
| time/                   |            |
|    fps                  | 196        |
|    iterations           | 93         |
|    time_elapsed         | 5810       |
|    total_timesteps      | 1142784    |
| train/                  |            |
|    approx_kl            | 0.17734905 |
|    clip_fraction        | 0.744      |
|    clip_range           | 0.075      |
|    entropy_loss         | -0.447     |
|    explained_variance   | 0.996      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0388     |
|    n_updates            | 1104       |
|    policy_gradient_loss | 0.0493     |
|    std                  | 0.336      |
|    value_loss           | 6.97e-05   |
----------------------------------------
Num timesteps: 1143000
Best mean reward: -0.36 - Last mean reward per episode: -0.39
Num timesteps: 1146000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1149000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1152000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1155000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.377     |
| time/                   |            |
|    fps                  | 196        |
|    iterations           | 94         |
|    time_elapsed         | 5881       |
|    total_timesteps      | 1155072    |
| train/                  |            |
|    approx_kl            | 0.28988615 |
|    clip_fraction        | 0.734      |
|    clip_range           | 0.075      |
|    entropy_loss         | -0.398     |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.051      |
|    n_updates            | 1116       |
|    policy_gradient_loss | 0.0497     |
|    std                  | 0.334      |
|    value_loss           | 2.51e-05   |
----------------------------------------
Num timesteps: 1158000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1161000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1164000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1167000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.376     |
| time/                   |            |
|    fps                  | 196        |
|    iterations           | 95         |
|    time_elapsed         | 5945       |
|    total_timesteps      | 1167360    |
| train/                  |            |
|    approx_kl            | 0.09886228 |
|    clip_fraction        | 0.701      |
|    clip_range           | 0.075      |
|    entropy_loss         | -0.353     |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0185     |
|    n_updates            | 1128       |
|    policy_gradient_loss | 0.0406     |
|    std                  | 0.333      |
|    value_loss           | 2.29e-05   |
----------------------------------------
Num timesteps: 1170000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1173000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1176000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1179000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.01      |
|    ep_rew_mean          | -0.372    |
| time/                   |           |
|    fps                  | 197       |
|    iterations           | 96        |
|    time_elapsed         | 5987      |
|    total_timesteps      | 1179648   |
| train/                  |           |
|    approx_kl            | 0.2319436 |
|    clip_fraction        | 0.731     |
|    clip_range           | 0.075     |
|    entropy_loss         | -0.295    |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0685    |
|    n_updates            | 1140      |
|    policy_gradient_loss | 0.0486    |
|    std                  | 0.33      |
|    value_loss           | 2.15e-05  |
---------------------------------------
Num timesteps: 1182000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1185000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1188000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1191000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.01      |
|    ep_rew_mean          | -0.373    |
| time/                   |           |
|    fps                  | 197       |
|    iterations           | 97        |
|    time_elapsed         | 6047      |
|    total_timesteps      | 1191936   |
| train/                  |           |
|    approx_kl            | 0.2424354 |
|    clip_fraction        | 0.737     |
|    clip_range           | 0.075     |
|    entropy_loss         | -0.257    |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0327    |
|    n_updates            | 1152      |
|    policy_gradient_loss | 0.121     |
|    std                  | 0.33      |
|    value_loss           | 2.74e-05  |
---------------------------------------
Num timesteps: 1194000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1197000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1200000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1203000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.371     |
| time/                   |            |
|    fps                  | 196        |
|    iterations           | 98         |
|    time_elapsed         | 6117       |
|    total_timesteps      | 1204224    |
| train/                  |            |
|    approx_kl            | 0.17515753 |
|    clip_fraction        | 0.739      |
|    clip_range           | 0.075      |
|    entropy_loss         | -0.217     |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0264     |
|    n_updates            | 1164       |
|    policy_gradient_loss | 0.0615     |
|    std                  | 0.327      |
|    value_loss           | 2.05e-05   |
----------------------------------------
Num timesteps: 1206000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1209000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1212000
Best mean reward: -0.36 - Last mean reward per episode: -0.36
Num timesteps: 1215000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 3.99       |
|    ep_rew_mean          | -0.365     |
| time/                   |            |
|    fps                  | 196        |
|    iterations           | 99         |
|    time_elapsed         | 6177       |
|    total_timesteps      | 1216512    |
| train/                  |            |
|    approx_kl            | 0.17250152 |
|    clip_fraction        | 0.735      |
|    clip_range           | 0.075      |
|    entropy_loss         | -0.175     |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0448     |
|    n_updates            | 1176       |
|    policy_gradient_loss | 0.054      |
|    std                  | 0.326      |
|    value_loss           | 3.05e-05   |
----------------------------------------
Num timesteps: 1218000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1221000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1224000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1227000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 3.99       |
|    ep_rew_mean          | -0.37      |
| time/                   |            |
|    fps                  | 196        |
|    iterations           | 100        |
|    time_elapsed         | 6247       |
|    total_timesteps      | 1228800    |
| train/                  |            |
|    approx_kl            | 0.26717708 |
|    clip_fraction        | 0.769      |
|    clip_range           | 0.075      |
|    entropy_loss         | -0.152     |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0702     |
|    n_updates            | 1188       |
|    policy_gradient_loss | 0.07       |
|    std                  | 0.326      |
|    value_loss           | 2.63e-05   |
----------------------------------------
Num timesteps: 1230000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1233000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1236000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1239000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.363    |
| time/                   |           |
|    fps                  | 196       |
|    iterations           | 101       |
|    time_elapsed         | 6308      |
|    total_timesteps      | 1241088   |
| train/                  |           |
|    approx_kl            | 0.2929596 |
|    clip_fraction        | 0.736     |
|    clip_range           | 0.075     |
|    entropy_loss         | -0.0997   |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0499    |
|    n_updates            | 1200      |
|    policy_gradient_loss | 0.0442    |
|    std                  | 0.324     |
|    value_loss           | 2.12e-05  |
---------------------------------------
Num timesteps: 1242000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1245000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1248000
Best mean reward: -0.36 - Last mean reward per episode: -0.66
Num timesteps: 1251000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.37      |
| time/                   |            |
|    fps                  | 197        |
|    iterations           | 102        |
|    time_elapsed         | 6360       |
|    total_timesteps      | 1253376    |
| train/                  |            |
|    approx_kl            | 0.09734956 |
|    clip_fraction        | 0.716      |
|    clip_range           | 0.075      |
|    entropy_loss         | -0.0522    |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0584     |
|    n_updates            | 1212       |
|    policy_gradient_loss | 0.0433     |
|    std                  | 0.322      |
|    value_loss           | 2.47e-05   |
----------------------------------------
Num timesteps: 1254000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1257000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1260000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1263000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.37     |
| time/                   |           |
|    fps                  | 197       |
|    iterations           | 103       |
|    time_elapsed         | 6418      |
|    total_timesteps      | 1265664   |
| train/                  |           |
|    approx_kl            | 0.2520253 |
|    clip_fraction        | 0.685     |
|    clip_range           | 0.075     |
|    entropy_loss         | -0.00405  |
|    explained_variance   | 0.965     |
|    learning_rate        | 0.000512  |
|    loss                 | -0.00102  |
|    n_updates            | 1224      |
|    policy_gradient_loss | 0.00532   |
|    std                  | 0.319     |
|    value_loss           | 0.00402   |
---------------------------------------
Num timesteps: 1266000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1269000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1272000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1275000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.367     |
| time/                   |            |
|    fps                  | 196        |
|    iterations           | 104        |
|    time_elapsed         | 6495       |
|    total_timesteps      | 1277952    |
| train/                  |            |
|    approx_kl            | 0.10992285 |
|    clip_fraction        | 0.719      |
|    clip_range           | 0.075      |
|    entropy_loss         | 0.0287     |
|    explained_variance   | 0.998      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0403     |
|    n_updates            | 1236       |
|    policy_gradient_loss | 0.0473     |
|    std                  | 0.318      |
|    value_loss           | 0.000112   |
----------------------------------------
Num timesteps: 1278000
Best mean reward: -0.36 - Last mean reward per episode: -0.36
Num timesteps: 1281000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1284000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1287000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1290000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4.01       |
|    ep_rew_mean          | -0.381     |
| time/                   |            |
|    fps                  | 196        |
|    iterations           | 105        |
|    time_elapsed         | 6575       |
|    total_timesteps      | 1290240    |
| train/                  |            |
|    approx_kl            | 0.30760825 |
|    clip_fraction        | 0.758      |
|    clip_range           | 0.075      |
|    entropy_loss         | 0.0385     |
|    explained_variance   | 0.998      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0631     |
|    n_updates            | 1248       |
|    policy_gradient_loss | 0.0566     |
|    std                  | 0.318      |
|    value_loss           | 2.85e-05   |
----------------------------------------
Num timesteps: 1293000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1296000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1299000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1302000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4.05       |
|    ep_rew_mean          | -0.375     |
| time/                   |            |
|    fps                  | 196        |
|    iterations           | 106        |
|    time_elapsed         | 6634       |
|    total_timesteps      | 1302528    |
| train/                  |            |
|    approx_kl            | 0.20684548 |
|    clip_fraction        | 0.715      |
|    clip_range           | 0.075      |
|    entropy_loss         | 0.0718     |
|    explained_variance   | 0.967      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0115     |
|    n_updates            | 1260       |
|    policy_gradient_loss | 0.00951    |
|    std                  | 0.317      |
|    value_loss           | 0.00353    |
----------------------------------------
Num timesteps: 1305000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1308000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1311000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1314000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.372     |
| time/                   |            |
|    fps                  | 196        |
|    iterations           | 107        |
|    time_elapsed         | 6700       |
|    total_timesteps      | 1314816    |
| train/                  |            |
|    approx_kl            | 0.63057566 |
|    clip_fraction        | 0.77       |
|    clip_range           | 0.075      |
|    entropy_loss         | 0.0833     |
|    explained_variance   | 0.99       |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0285     |
|    n_updates            | 1272       |
|    policy_gradient_loss | 0.0547     |
|    std                  | 0.318      |
|    value_loss           | 7.1e-05    |
----------------------------------------
Num timesteps: 1317000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1320000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1323000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1326000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.379     |
| time/                   |            |
|    fps                  | 196        |
|    iterations           | 108        |
|    time_elapsed         | 6769       |
|    total_timesteps      | 1327104    |
| train/                  |            |
|    approx_kl            | 0.18965547 |
|    clip_fraction        | 0.755      |
|    clip_range           | 0.075      |
|    entropy_loss         | 0.105      |
|    explained_variance   | 0.998      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0647     |
|    n_updates            | 1284       |
|    policy_gradient_loss | 0.0572     |
|    std                  | 0.316      |
|    value_loss           | 2.47e-05   |
----------------------------------------
Num timesteps: 1329000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1332000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1335000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1338000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.367    |
| time/                   |           |
|    fps                  | 196       |
|    iterations           | 109       |
|    time_elapsed         | 6811      |
|    total_timesteps      | 1339392   |
| train/                  |           |
|    approx_kl            | 0.1397814 |
|    clip_fraction        | 0.771     |
|    clip_range           | 0.075     |
|    entropy_loss         | 0.124     |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0725    |
|    n_updates            | 1296      |
|    policy_gradient_loss | 0.0693    |
|    std                  | 0.315     |
|    value_loss           | 2.4e-05   |
---------------------------------------
Num timesteps: 1341000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1344000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1347000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1350000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.377    |
| time/                   |           |
|    fps                  | 196       |
|    iterations           | 110       |
|    time_elapsed         | 6870      |
|    total_timesteps      | 1351680   |
| train/                  |           |
|    approx_kl            | 0.1929264 |
|    clip_fraction        | 0.747     |
|    clip_range           | 0.075     |
|    entropy_loss         | 0.154     |
|    explained_variance   | 0.996     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0296    |
|    n_updates            | 1308      |
|    policy_gradient_loss | 0.0511    |
|    std                  | 0.313     |
|    value_loss           | 3.15e-05  |
---------------------------------------
Num timesteps: 1353000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1356000
Best mean reward: -0.36 - Last mean reward per episode: -0.36
Num timesteps: 1359000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1362000
Best mean reward: -0.36 - Last mean reward per episode: -0.36

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_022449_numTimesteps_1362000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.371     |
| time/                   |            |
|    fps                  | 197        |
|    iterations           | 111        |
|    time_elapsed         | 6916       |
|    total_timesteps      | 1363968    |
| train/                  |            |
|    approx_kl            | 0.08768151 |
|    clip_fraction        | 0.75       |
|    clip_range           | 0.075      |
|    entropy_loss         | 0.19       |
|    explained_variance   | 0.998      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0665     |
|    n_updates            | 1320       |
|    policy_gradient_loss | 0.0581     |
|    std                  | 0.312      |
|    value_loss           | 2.24e-05   |
----------------------------------------
Num timesteps: 1365000
Best mean reward: -0.36 - Last mean reward per episode: -0.39
Num timesteps: 1368000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1371000
Best mean reward: -0.36 - Last mean reward per episode: -0.36

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_022534_numTimesteps_1371000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
Num timesteps: 1374000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.375     |
| time/                   |            |
|    fps                  | 197        |
|    iterations           | 112        |
|    time_elapsed         | 6982       |
|    total_timesteps      | 1376256    |
| train/                  |            |
|    approx_kl            | 0.27420992 |
|    clip_fraction        | 0.77       |
|    clip_range           | 0.075      |
|    entropy_loss         | 0.221      |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0654     |
|    n_updates            | 1332       |
|    policy_gradient_loss | 0.0634     |
|    std                  | 0.311      |
|    value_loss           | 2.06e-05   |
----------------------------------------
Num timesteps: 1377000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1380000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1383000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1386000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.376     |
| time/                   |            |
|    fps                  | 197        |
|    iterations           | 113        |
|    time_elapsed         | 7047       |
|    total_timesteps      | 1388544    |
| train/                  |            |
|    approx_kl            | 0.16884144 |
|    clip_fraction        | 0.738      |
|    clip_range           | 0.075      |
|    entropy_loss         | 0.243      |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0434     |
|    n_updates            | 1344       |
|    policy_gradient_loss | 0.0471     |
|    std                  | 0.311      |
|    value_loss           | 2.11e-05   |
----------------------------------------
Num timesteps: 1389000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1392000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1395000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1398000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.376     |
| time/                   |            |
|    fps                  | 197        |
|    iterations           | 114        |
|    time_elapsed         | 7090       |
|    total_timesteps      | 1400832    |
| train/                  |            |
|    approx_kl            | 0.21247776 |
|    clip_fraction        | 0.753      |
|    clip_range           | 0.075      |
|    entropy_loss         | 0.271      |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0397     |
|    n_updates            | 1356       |
|    policy_gradient_loss | 0.0543     |
|    std                  | 0.308      |
|    value_loss           | 1.95e-05   |
----------------------------------------
Num timesteps: 1401000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1404000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1407000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1410000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1413000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.373     |
| time/                   |            |
|    fps                  | 197        |
|    iterations           | 115        |
|    time_elapsed         | 7140       |
|    total_timesteps      | 1413120    |
| train/                  |            |
|    approx_kl            | 0.25450033 |
|    clip_fraction        | 0.741      |
|    clip_range           | 0.075      |
|    entropy_loss         | 0.314      |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0365     |
|    n_updates            | 1368       |
|    policy_gradient_loss | 0.0472     |
|    std                  | 0.306      |
|    value_loss           | 2.12e-05   |
----------------------------------------
Num timesteps: 1416000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1419000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1422000
Best mean reward: -0.36 - Last mean reward per episode: -0.36

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_022918_numTimesteps_1422000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
Num timesteps: 1425000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.374     |
| time/                   |            |
|    fps                  | 198        |
|    iterations           | 116        |
|    time_elapsed         | 7194       |
|    total_timesteps      | 1425408    |
| train/                  |            |
|    approx_kl            | 0.49231628 |
|    clip_fraction        | 0.756      |
|    clip_range           | 0.075      |
|    entropy_loss         | 0.355      |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0444     |
|    n_updates            | 1380       |
|    policy_gradient_loss | 0.0561     |
|    std                  | 0.304      |
|    value_loss           | 1.87e-05   |
----------------------------------------
Num timesteps: 1428000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1431000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1434000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1437000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.371     |
| time/                   |            |
|    fps                  | 198        |
|    iterations           | 117        |
|    time_elapsed         | 7253       |
|    total_timesteps      | 1437696    |
| train/                  |            |
|    approx_kl            | 0.41356933 |
|    clip_fraction        | 0.772      |
|    clip_range           | 0.075      |
|    entropy_loss         | 0.389      |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0659     |
|    n_updates            | 1392       |
|    policy_gradient_loss | 0.0633     |
|    std                  | 0.302      |
|    value_loss           | 2.06e-05   |
----------------------------------------
Num timesteps: 1440000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1443000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1446000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1449000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 4        |
|    ep_rew_mean          | -0.372   |
| time/                   |          |
|    fps                  | 198      |
|    iterations           | 118      |
|    time_elapsed         | 7306     |
|    total_timesteps      | 1449984  |
| train/                  |          |
|    approx_kl            | 0.286621 |
|    clip_fraction        | 0.784    |
|    clip_range           | 0.075    |
|    entropy_loss         | 0.408    |
|    explained_variance   | 0.999    |
|    learning_rate        | 0.000512 |
|    loss                 | 0.0423   |
|    n_updates            | 1404     |
|    policy_gradient_loss | 0.0712   |
|    std                  | 0.301    |
|    value_loss           | 1.89e-05 |
--------------------------------------
Num timesteps: 1452000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1455000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1458000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1461000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.372     |
| time/                   |            |
|    fps                  | 198        |
|    iterations           | 119        |
|    time_elapsed         | 7362       |
|    total_timesteps      | 1462272    |
| train/                  |            |
|    approx_kl            | 0.28079388 |
|    clip_fraction        | 0.752      |
|    clip_range           | 0.075      |
|    entropy_loss         | 0.448      |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0418     |
|    n_updates            | 1416       |
|    policy_gradient_loss | 0.0506     |
|    std                  | 0.299      |
|    value_loss           | 1.84e-05   |
----------------------------------------
Num timesteps: 1464000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1467000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1470000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1473000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.364    |
| time/                   |           |
|    fps                  | 199       |
|    iterations           | 120       |
|    time_elapsed         | 7408      |
|    total_timesteps      | 1474560   |
| train/                  |           |
|    approx_kl            | 0.6702147 |
|    clip_fraction        | 0.791     |
|    clip_range           | 0.075     |
|    entropy_loss         | 0.482     |
|    explained_variance   | 0.975     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0298    |
|    n_updates            | 1428      |
|    policy_gradient_loss | 0.051     |
|    std                  | 0.297     |
|    value_loss           | 0.00367   |
---------------------------------------
Num timesteps: 1476000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1479000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1482000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1485000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.366    |
| time/                   |           |
|    fps                  | 199       |
|    iterations           | 121       |
|    time_elapsed         | 7467      |
|    total_timesteps      | 1486848   |
| train/                  |           |
|    approx_kl            | 0.5785799 |
|    clip_fraction        | 0.819     |
|    clip_range           | 0.075     |
|    entropy_loss         | 0.482     |
|    explained_variance   | 0.994     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0621    |
|    n_updates            | 1440      |
|    policy_gradient_loss | 0.0773    |
|    std                  | 0.298     |
|    value_loss           | 4.44e-05  |
---------------------------------------
Num timesteps: 1488000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1491000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1494000
Best mean reward: -0.36 - Last mean reward per episode: -0.36
Num timesteps: 1497000
Best mean reward: -0.36 - Last mean reward per episode: -0.36
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4.01       |
|    ep_rew_mean          | -0.378     |
| time/                   |            |
|    fps                  | 199        |
|    iterations           | 122        |
|    time_elapsed         | 7526       |
|    total_timesteps      | 1499136    |
| train/                  |            |
|    approx_kl            | 0.19070911 |
|    clip_fraction        | 0.763      |
|    clip_range           | 0.075      |
|    entropy_loss         | 0.482      |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0702     |
|    n_updates            | 1452       |
|    policy_gradient_loss | 0.0573     |
|    std                  | 0.297      |
|    value_loss           | 2.08e-05   |
----------------------------------------
Num timesteps: 1500000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1503000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1506000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1509000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.365     |
| time/                   |            |
|    fps                  | 198        |
|    iterations           | 123        |
|    time_elapsed         | 7601       |
|    total_timesteps      | 1511424    |
| train/                  |            |
|    approx_kl            | 0.34748483 |
|    clip_fraction        | 0.769      |
|    clip_range           | 0.075      |
|    entropy_loss         | 0.523      |
|    explained_variance   | 0.995      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0619     |
|    n_updates            | 1464       |
|    policy_gradient_loss | 0.0621     |
|    std                  | 0.294      |
|    value_loss           | 3.86e-05   |
----------------------------------------
Num timesteps: 1512000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1515000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1518000
Best mean reward: -0.36 - Last mean reward per episode: -0.36
Num timesteps: 1521000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.368     |
| time/                   |            |
|    fps                  | 198        |
|    iterations           | 124        |
|    time_elapsed         | 7666       |
|    total_timesteps      | 1523712    |
| train/                  |            |
|    approx_kl            | 0.18258874 |
|    clip_fraction        | 0.745      |
|    clip_range           | 0.075      |
|    entropy_loss         | 0.558      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.000407   |
|    n_updates            | 1476       |
|    policy_gradient_loss | 0.00969    |
|    std                  | 0.292      |
|    value_loss           | 0.00551    |
----------------------------------------
Num timesteps: 1524000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1527000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1530000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1533000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1536000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.379    |
| time/                   |           |
|    fps                  | 199       |
|    iterations           | 125       |
|    time_elapsed         | 7714      |
|    total_timesteps      | 1536000   |
| train/                  |           |
|    approx_kl            | 1.7863818 |
|    clip_fraction        | 0.814     |
|    clip_range           | 0.075     |
|    entropy_loss         | 0.581     |
|    explained_variance   | 0.996     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0497    |
|    n_updates            | 1488      |
|    policy_gradient_loss | 0.0757    |
|    std                  | 0.29      |
|    value_loss           | 0.000666  |
---------------------------------------
Num timesteps: 1539000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1542000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1545000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1548000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.385     |
| time/                   |            |
|    fps                  | 199        |
|    iterations           | 126        |
|    time_elapsed         | 7779       |
|    total_timesteps      | 1548288    |
| train/                  |            |
|    approx_kl            | 0.35372582 |
|    clip_fraction        | 0.822      |
|    clip_range           | 0.075      |
|    entropy_loss         | 0.592      |
|    explained_variance   | 0.997      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0764     |
|    n_updates            | 1500       |
|    policy_gradient_loss | 0.0917     |
|    std                  | 0.289      |
|    value_loss           | 3.97e-05   |
----------------------------------------
Num timesteps: 1551000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1554000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1557000
Best mean reward: -0.36 - Last mean reward per episode: -0.36
Num timesteps: 1560000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.362     |
| time/                   |            |
|    fps                  | 199        |
|    iterations           | 127        |
|    time_elapsed         | 7834       |
|    total_timesteps      | 1560576    |
| train/                  |            |
|    approx_kl            | 0.57632905 |
|    clip_fraction        | 0.787      |
|    clip_range           | 0.075      |
|    entropy_loss         | 0.576      |
|    explained_variance   | 0.998      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0955     |
|    n_updates            | 1512       |
|    policy_gradient_loss | 0.0644     |
|    std                  | 0.289      |
|    value_loss           | 3.36e-05   |
----------------------------------------
Num timesteps: 1563000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1566000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1569000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1572000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 4        |
|    ep_rew_mean          | -0.37    |
| time/                   |          |
|    fps                  | 198      |
|    iterations           | 128      |
|    time_elapsed         | 7907     |
|    total_timesteps      | 1572864  |
| train/                  |          |
|    approx_kl            | 0.715836 |
|    clip_fraction        | 0.781    |
|    clip_range           | 0.075    |
|    entropy_loss         | 0.605    |
|    explained_variance   | 0.999    |
|    learning_rate        | 0.000512 |
|    loss                 | 0.061    |
|    n_updates            | 1524     |
|    policy_gradient_loss | 0.0619   |
|    std                  | 0.286    |
|    value_loss           | 2.29e-05 |
--------------------------------------
Num timesteps: 1575000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1578000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1581000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1584000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.377     |
| time/                   |            |
|    fps                  | 199        |
|    iterations           | 129        |
|    time_elapsed         | 7959       |
|    total_timesteps      | 1585152    |
| train/                  |            |
|    approx_kl            | 0.49429014 |
|    clip_fraction        | 0.79       |
|    clip_range           | 0.075      |
|    entropy_loss         | 0.624      |
|    explained_variance   | 0.998      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0522     |
|    n_updates            | 1536       |
|    policy_gradient_loss | 0.064      |
|    std                  | 0.285      |
|    value_loss           | 2.4e-05    |
----------------------------------------
Num timesteps: 1587000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1590000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1593000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1596000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.378    |
| time/                   |           |
|    fps                  | 199       |
|    iterations           | 130       |
|    time_elapsed         | 8024      |
|    total_timesteps      | 1597440   |
| train/                  |           |
|    approx_kl            | 0.7690191 |
|    clip_fraction        | 0.788     |
|    clip_range           | 0.075     |
|    entropy_loss         | 0.652     |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0434    |
|    n_updates            | 1548      |
|    policy_gradient_loss | 0.0643    |
|    std                  | 0.284     |
|    value_loss           | 2.12e-05  |
---------------------------------------
Num timesteps: 1599000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1602000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1605000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1608000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.365    |
| time/                   |           |
|    fps                  | 199       |
|    iterations           | 131       |
|    time_elapsed         | 8088      |
|    total_timesteps      | 1609728   |
| train/                  |           |
|    approx_kl            | 0.3781351 |
|    clip_fraction        | 0.775     |
|    clip_range           | 0.075     |
|    entropy_loss         | 0.675     |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0548    |
|    n_updates            | 1560      |
|    policy_gradient_loss | 0.0644    |
|    std                  | 0.283     |
|    value_loss           | 2.04e-05  |
---------------------------------------
Num timesteps: 1611000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1614000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1617000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1620000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.376    |
| time/                   |           |
|    fps                  | 198       |
|    iterations           | 132       |
|    time_elapsed         | 8158      |
|    total_timesteps      | 1622016   |
| train/                  |           |
|    approx_kl            | 0.8280247 |
|    clip_fraction        | 0.788     |
|    clip_range           | 0.075     |
|    entropy_loss         | 0.701     |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.035     |
|    n_updates            | 1572      |
|    policy_gradient_loss | 0.0655    |
|    std                  | 0.282     |
|    value_loss           | 2.01e-05  |
---------------------------------------
Num timesteps: 1623000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1626000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1629000
Best mean reward: -0.36 - Last mean reward per episode: -0.36
Num timesteps: 1632000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 4        |
|    ep_rew_mean          | -0.366   |
| time/                   |          |
|    fps                  | 198      |
|    iterations           | 133      |
|    time_elapsed         | 8235     |
|    total_timesteps      | 1634304  |
| train/                  |          |
|    approx_kl            | 0.703732 |
|    clip_fraction        | 0.781    |
|    clip_range           | 0.075    |
|    entropy_loss         | 0.726    |
|    explained_variance   | 0.999    |
|    learning_rate        | 0.000512 |
|    loss                 | 0.0529   |
|    n_updates            | 1584     |
|    policy_gradient_loss | 0.0606   |
|    std                  | 0.281    |
|    value_loss           | 2e-05    |
--------------------------------------
Num timesteps: 1635000
Best mean reward: -0.36 - Last mean reward per episode: -0.36
Num timesteps: 1638000
Best mean reward: -0.36 - Last mean reward per episode: -0.36
Num timesteps: 1641000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1644000
Best mean reward: -0.36 - Last mean reward per episode: -0.36
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.01      |
|    ep_rew_mean          | -0.373    |
| time/                   |           |
|    fps                  | 198       |
|    iterations           | 134       |
|    time_elapsed         | 8308      |
|    total_timesteps      | 1646592   |
| train/                  |           |
|    approx_kl            | 0.4275327 |
|    clip_fraction        | 0.809     |
|    clip_range           | 0.075     |
|    entropy_loss         | 0.741     |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0556    |
|    n_updates            | 1596      |
|    policy_gradient_loss | 0.0826    |
|    std                  | 0.28      |
|    value_loss           | 1.96e-05  |
---------------------------------------
Num timesteps: 1647000
Best mean reward: -0.36 - Last mean reward per episode: -0.36
Num timesteps: 1650000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1653000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1656000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.368    |
| time/                   |           |
|    fps                  | 198       |
|    iterations           | 135       |
|    time_elapsed         | 8370      |
|    total_timesteps      | 1658880   |
| train/                  |           |
|    approx_kl            | 0.6733716 |
|    clip_fraction        | 0.803     |
|    clip_range           | 0.075     |
|    entropy_loss         | 0.777     |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0321    |
|    n_updates            | 1608      |
|    policy_gradient_loss | 0.082     |
|    std                  | 0.277     |
|    value_loss           | 2.28e-05  |
---------------------------------------
Num timesteps: 1659000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1662000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1665000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1668000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1671000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.01      |
|    ep_rew_mean          | -0.368    |
| time/                   |           |
|    fps                  | 197       |
|    iterations           | 136       |
|    time_elapsed         | 8441      |
|    total_timesteps      | 1671168   |
| train/                  |           |
|    approx_kl            | 0.7454433 |
|    clip_fraction        | 0.774     |
|    clip_range           | 0.075     |
|    entropy_loss         | 0.798     |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0603    |
|    n_updates            | 1620      |
|    policy_gradient_loss | 0.065     |
|    std                  | 0.276     |
|    value_loss           | 1.83e-05  |
---------------------------------------
Num timesteps: 1674000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1677000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1680000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1683000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.375    |
| time/                   |           |
|    fps                  | 198       |
|    iterations           | 137       |
|    time_elapsed         | 8497      |
|    total_timesteps      | 1683456   |
| train/                  |           |
|    approx_kl            | 0.8466781 |
|    clip_fraction        | 0.807     |
|    clip_range           | 0.075     |
|    entropy_loss         | 0.815     |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.402     |
|    n_updates            | 1632      |
|    policy_gradient_loss | 0.145     |
|    std                  | 0.275     |
|    value_loss           | 2.03e-05  |
---------------------------------------
Num timesteps: 1686000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1689000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1692000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1695000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.374     |
| time/                   |            |
|    fps                  | 198        |
|    iterations           | 138        |
|    time_elapsed         | 8550       |
|    total_timesteps      | 1695744    |
| train/                  |            |
|    approx_kl            | 0.85136193 |
|    clip_fraction        | 0.813      |
|    clip_range           | 0.075      |
|    entropy_loss         | 0.823      |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0641     |
|    n_updates            | 1644       |
|    policy_gradient_loss | 0.0785     |
|    std                  | 0.275      |
|    value_loss           | 2.11e-05   |
----------------------------------------
Num timesteps: 1698000
Best mean reward: -0.36 - Last mean reward per episode: -0.36
Num timesteps: 1701000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1704000
Best mean reward: -0.36 - Last mean reward per episode: -0.36
Num timesteps: 1707000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.367    |
| time/                   |           |
|    fps                  | 198       |
|    iterations           | 139       |
|    time_elapsed         | 8593      |
|    total_timesteps      | 1708032   |
| train/                  |           |
|    approx_kl            | 0.4371238 |
|    clip_fraction        | 0.789     |
|    clip_range           | 0.075     |
|    entropy_loss         | 0.843     |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.076     |
|    n_updates            | 1656      |
|    policy_gradient_loss | 0.0659    |
|    std                  | 0.274     |
|    value_loss           | 1.92e-05  |
---------------------------------------
Num timesteps: 1710000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1713000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1716000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1719000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.373     |
| time/                   |            |
|    fps                  | 199        |
|    iterations           | 140        |
|    time_elapsed         | 8637       |
|    total_timesteps      | 1720320    |
| train/                  |            |
|    approx_kl            | 0.63172877 |
|    clip_fraction        | 0.766      |
|    clip_range           | 0.075      |
|    entropy_loss         | 0.881      |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0549     |
|    n_updates            | 1668       |
|    policy_gradient_loss | 0.0631     |
|    std                  | 0.272      |
|    value_loss           | 1.9e-05    |
----------------------------------------
Num timesteps: 1722000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1725000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1728000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1731000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.377    |
| time/                   |           |
|    fps                  | 199       |
|    iterations           | 141       |
|    time_elapsed         | 8681      |
|    total_timesteps      | 1732608   |
| train/                  |           |
|    approx_kl            | 0.5575928 |
|    clip_fraction        | 0.796     |
|    clip_range           | 0.075     |
|    entropy_loss         | 0.916     |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.055     |
|    n_updates            | 1680      |
|    policy_gradient_loss | 0.0746    |
|    std                  | 0.27      |
|    value_loss           | 1.95e-05  |
---------------------------------------
Num timesteps: 1734000
Best mean reward: -0.36 - Last mean reward per episode: -0.36
Num timesteps: 1737000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1740000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1743000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.367    |
| time/                   |           |
|    fps                  | 199       |
|    iterations           | 142       |
|    time_elapsed         | 8731      |
|    total_timesteps      | 1744896   |
| train/                  |           |
|    approx_kl            | 0.2921358 |
|    clip_fraction        | 0.784     |
|    clip_range           | 0.075     |
|    entropy_loss         | 0.934     |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0433    |
|    n_updates            | 1692      |
|    policy_gradient_loss | 0.0655    |
|    std                  | 0.27      |
|    value_loss           | 2.17e-05  |
---------------------------------------
Num timesteps: 1746000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1749000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1752000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1755000
Best mean reward: -0.36 - Last mean reward per episode: -0.36
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.38      |
| time/                   |            |
|    fps                  | 199        |
|    iterations           | 143        |
|    time_elapsed         | 8797       |
|    total_timesteps      | 1757184    |
| train/                  |            |
|    approx_kl            | 0.27675393 |
|    clip_fraction        | 0.81       |
|    clip_range           | 0.075      |
|    entropy_loss         | 0.941      |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0678     |
|    n_updates            | 1704       |
|    policy_gradient_loss | 0.0809     |
|    std                  | 0.268      |
|    value_loss           | 3.26e-05   |
----------------------------------------
Num timesteps: 1758000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1761000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1764000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1767000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.376    |
| time/                   |           |
|    fps                  | 199       |
|    iterations           | 144       |
|    time_elapsed         | 8870      |
|    total_timesteps      | 1769472   |
| train/                  |           |
|    approx_kl            | 0.6337021 |
|    clip_fraction        | 0.795     |
|    clip_range           | 0.075     |
|    entropy_loss         | 0.955     |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0574    |
|    n_updates            | 1716      |
|    policy_gradient_loss | 0.0682    |
|    std                  | 0.266     |
|    value_loss           | 9.19e-05  |
---------------------------------------
Num timesteps: 1770000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1773000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1776000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1779000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.373    |
| time/                   |           |
|    fps                  | 199       |
|    iterations           | 145       |
|    time_elapsed         | 8940      |
|    total_timesteps      | 1781760   |
| train/                  |           |
|    approx_kl            | 1.0147809 |
|    clip_fraction        | 0.79      |
|    clip_range           | 0.075     |
|    entropy_loss         | 0.967     |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0482    |
|    n_updates            | 1728      |
|    policy_gradient_loss | 0.0702    |
|    std                  | 0.266     |
|    value_loss           | 2.96e-05  |
---------------------------------------
Num timesteps: 1782000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1785000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1788000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1791000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1794000
Best mean reward: -0.36 - Last mean reward per episode: -0.36
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.369    |
| time/                   |           |
|    fps                  | 199       |
|    iterations           | 146       |
|    time_elapsed         | 9007      |
|    total_timesteps      | 1794048   |
| train/                  |           |
|    approx_kl            | 0.6009609 |
|    clip_fraction        | 0.771     |
|    clip_range           | 0.075     |
|    entropy_loss         | 0.991     |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0652    |
|    n_updates            | 1740      |
|    policy_gradient_loss | 0.0628    |
|    std                  | 0.264     |
|    value_loss           | 0.000151  |
---------------------------------------
Num timesteps: 1797000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1800000
Best mean reward: -0.36 - Last mean reward per episode: -0.36
Num timesteps: 1803000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1806000
Best mean reward: -0.36 - Last mean reward per episode: -0.36
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.368    |
| time/                   |           |
|    fps                  | 199       |
|    iterations           | 147       |
|    time_elapsed         | 9068      |
|    total_timesteps      | 1806336   |
| train/                  |           |
|    approx_kl            | 0.7941664 |
|    clip_fraction        | 0.795     |
|    clip_range           | 0.075     |
|    entropy_loss         | 1.01      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0336    |
|    n_updates            | 1752      |
|    policy_gradient_loss | 0.0737    |
|    std                  | 0.264     |
|    value_loss           | 1.88e-05  |
---------------------------------------
Num timesteps: 1809000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1812000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1815000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1818000
Best mean reward: -0.36 - Last mean reward per episode: -0.36
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.374    |
| time/                   |           |
|    fps                  | 199       |
|    iterations           | 148       |
|    time_elapsed         | 9137      |
|    total_timesteps      | 1818624   |
| train/                  |           |
|    approx_kl            | 0.6259864 |
|    clip_fraction        | 0.765     |
|    clip_range           | 0.075     |
|    entropy_loss         | 1.03      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0815    |
|    n_updates            | 1764      |
|    policy_gradient_loss | 0.0533    |
|    std                  | 0.265     |
|    value_loss           | 2.49e-05  |
---------------------------------------
Num timesteps: 1821000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1824000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1827000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1830000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 3.99       |
|    ep_rew_mean          | -0.385     |
| time/                   |            |
|    fps                  | 199        |
|    iterations           | 149        |
|    time_elapsed         | 9186       |
|    total_timesteps      | 1830912    |
| train/                  |            |
|    approx_kl            | 0.45569777 |
|    clip_fraction        | 0.79       |
|    clip_range           | 0.075      |
|    entropy_loss         | 1.04       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.119      |
|    n_updates            | 1776       |
|    policy_gradient_loss | 0.0667     |
|    std                  | 0.265      |
|    value_loss           | 0.000174   |
----------------------------------------
Num timesteps: 1833000
Best mean reward: -0.36 - Last mean reward per episode: -0.36
Num timesteps: 1836000
Best mean reward: -0.36 - Last mean reward per episode: -0.36
Num timesteps: 1839000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1842000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 4.01     |
|    ep_rew_mean          | -0.365   |
| time/                   |          |
|    fps                  | 199      |
|    iterations           | 150      |
|    time_elapsed         | 9259     |
|    total_timesteps      | 1843200  |
| train/                  |          |
|    approx_kl            | 1.533079 |
|    clip_fraction        | 0.843    |
|    clip_range           | 0.075    |
|    entropy_loss         | 1.05     |
|    explained_variance   | 0.959    |
|    learning_rate        | 0.000512 |
|    loss                 | 0.0747   |
|    n_updates            | 1788     |
|    policy_gradient_loss | 0.065    |
|    std                  | 0.266    |
|    value_loss           | 0.00485  |
--------------------------------------
Num timesteps: 1845000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1848000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1851000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1854000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4.02       |
|    ep_rew_mean          | -0.37      |
| time/                   |            |
|    fps                  | 198        |
|    iterations           | 151        |
|    time_elapsed         | 9331       |
|    total_timesteps      | 1855488    |
| train/                  |            |
|    approx_kl            | 0.69722986 |
|    clip_fraction        | 0.792      |
|    clip_range           | 0.075      |
|    entropy_loss         | 1.03       |
|    explained_variance   | 0.993      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0455     |
|    n_updates            | 1800       |
|    policy_gradient_loss | 0.0676     |
|    std                  | 0.264      |
|    value_loss           | 6.52e-05   |
----------------------------------------
Num timesteps: 1857000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1860000
Best mean reward: -0.36 - Last mean reward per episode: -0.36
Num timesteps: 1863000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1866000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.376    |
| time/                   |           |
|    fps                  | 198       |
|    iterations           | 152       |
|    time_elapsed         | 9399      |
|    total_timesteps      | 1867776   |
| train/                  |           |
|    approx_kl            | 0.6180318 |
|    clip_fraction        | 0.795     |
|    clip_range           | 0.075     |
|    entropy_loss         | 1.04      |
|    explained_variance   | 0.998     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0216    |
|    n_updates            | 1812      |
|    policy_gradient_loss | 0.058     |
|    std                  | 0.264     |
|    value_loss           | 3.36e-05  |
---------------------------------------
Num timesteps: 1869000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1872000
Best mean reward: -0.36 - Last mean reward per episode: -0.36

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_030640_numTimesteps_1872000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
Num timesteps: 1875000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1878000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.358     |
| time/                   |            |
|    fps                  | 198        |
|    iterations           | 153        |
|    time_elapsed         | 9470       |
|    total_timesteps      | 1880064    |
| train/                  |            |
|    approx_kl            | 0.49578604 |
|    clip_fraction        | 0.804      |
|    clip_range           | 0.075      |
|    entropy_loss         | 1.06       |
|    explained_variance   | 0.998      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0621     |
|    n_updates            | 1824       |
|    policy_gradient_loss | 0.08       |
|    std                  | 0.262      |
|    value_loss           | 2.73e-05   |
----------------------------------------
Num timesteps: 1881000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1884000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1887000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1890000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.362     |
| time/                   |            |
|    fps                  | 198        |
|    iterations           | 154        |
|    time_elapsed         | 9541       |
|    total_timesteps      | 1892352    |
| train/                  |            |
|    approx_kl            | 0.39769924 |
|    clip_fraction        | 0.793      |
|    clip_range           | 0.075      |
|    entropy_loss         | 1.07       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0966     |
|    n_updates            | 1836       |
|    policy_gradient_loss | 0.0744     |
|    std                  | 0.261      |
|    value_loss           | 2.16e-05   |
----------------------------------------
Num timesteps: 1893000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1896000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1899000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1902000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.371     |
| time/                   |            |
|    fps                  | 198        |
|    iterations           | 155        |
|    time_elapsed         | 9596       |
|    total_timesteps      | 1904640    |
| train/                  |            |
|    approx_kl            | 0.48584357 |
|    clip_fraction        | 0.795      |
|    clip_range           | 0.075      |
|    entropy_loss         | 1.09       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0598     |
|    n_updates            | 1848       |
|    policy_gradient_loss | 0.0681     |
|    std                  | 0.259      |
|    value_loss           | 2.13e-05   |
----------------------------------------
Num timesteps: 1905000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1908000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1911000
Best mean reward: -0.36 - Last mean reward per episode: -0.36
Num timesteps: 1914000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.375     |
| time/                   |            |
|    fps                  | 198        |
|    iterations           | 156        |
|    time_elapsed         | 9642       |
|    total_timesteps      | 1916928    |
| train/                  |            |
|    approx_kl            | 0.65014684 |
|    clip_fraction        | 0.789      |
|    clip_range           | 0.075      |
|    entropy_loss         | 1.12       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0678     |
|    n_updates            | 1860       |
|    policy_gradient_loss | 0.0645     |
|    std                  | 0.258      |
|    value_loss           | 1.97e-05   |
----------------------------------------
Num timesteps: 1917000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1920000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1923000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1926000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1929000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.369    |
| time/                   |           |
|    fps                  | 198       |
|    iterations           | 157       |
|    time_elapsed         | 9695      |
|    total_timesteps      | 1929216   |
| train/                  |           |
|    approx_kl            | 1.1290817 |
|    clip_fraction        | 0.784     |
|    clip_range           | 0.075     |
|    entropy_loss         | 1.14      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0746    |
|    n_updates            | 1872      |
|    policy_gradient_loss | 0.0693    |
|    std                  | 0.257     |
|    value_loss           | 1.95e-05  |
---------------------------------------
Num timesteps: 1932000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1935000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1938000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1941000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4.07       |
|    ep_rew_mean          | -0.377     |
| time/                   |            |
|    fps                  | 198        |
|    iterations           | 158        |
|    time_elapsed         | 9761       |
|    total_timesteps      | 1941504    |
| train/                  |            |
|    approx_kl            | 0.46531162 |
|    clip_fraction        | 0.805      |
|    clip_range           | 0.075      |
|    entropy_loss         | 1.15       |
|    explained_variance   | 0.998      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0501     |
|    n_updates            | 1884       |
|    policy_gradient_loss | 0.0772     |
|    std                  | 0.255      |
|    value_loss           | 2.69e-05   |
----------------------------------------
Num timesteps: 1944000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1947000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1950000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1953000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.377    |
| time/                   |           |
|    fps                  | 198       |
|    iterations           | 159       |
|    time_elapsed         | 9818      |
|    total_timesteps      | 1953792   |
| train/                  |           |
|    approx_kl            | 1.1486334 |
|    clip_fraction        | 0.775     |
|    clip_range           | 0.075     |
|    entropy_loss         | 1.17      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0423    |
|    n_updates            | 1896      |
|    policy_gradient_loss | 0.0505    |
|    std                  | 0.256     |
|    value_loss           | 2.35e-05  |
---------------------------------------
Num timesteps: 1956000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1959000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1962000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1965000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.371    |
| time/                   |           |
|    fps                  | 199       |
|    iterations           | 160       |
|    time_elapsed         | 9862      |
|    total_timesteps      | 1966080   |
| train/                  |           |
|    approx_kl            | 1.1792121 |
|    clip_fraction        | 0.82      |
|    clip_range           | 0.075     |
|    entropy_loss         | 1.18      |
|    explained_variance   | 0.998     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0542    |
|    n_updates            | 1908      |
|    policy_gradient_loss | 0.08      |
|    std                  | 0.255     |
|    value_loss           | 2.24e-05  |
---------------------------------------
Num timesteps: 1968000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1971000
Best mean reward: -0.36 - Last mean reward per episode: -0.36
Num timesteps: 1974000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1977000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.371     |
| time/                   |            |
|    fps                  | 199        |
|    iterations           | 161        |
|    time_elapsed         | 9913       |
|    total_timesteps      | 1978368    |
| train/                  |            |
|    approx_kl            | 0.51689583 |
|    clip_fraction        | 0.777      |
|    clip_range           | 0.075      |
|    entropy_loss         | 1.19       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0537     |
|    n_updates            | 1920       |
|    policy_gradient_loss | 0.0568     |
|    std                  | 0.255      |
|    value_loss           | 1.94e-05   |
----------------------------------------
Num timesteps: 1980000
Best mean reward: -0.36 - Last mean reward per episode: -0.38
Num timesteps: 1983000
Best mean reward: -0.36 - Last mean reward per episode: -0.36
Num timesteps: 1986000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1989000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.368     |
| time/                   |            |
|    fps                  | 199        |
|    iterations           | 162        |
|    time_elapsed         | 9973       |
|    total_timesteps      | 1990656    |
| train/                  |            |
|    approx_kl            | 0.95369196 |
|    clip_fraction        | 0.807      |
|    clip_range           | 0.075      |
|    entropy_loss         | 1.2        |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0687     |
|    n_updates            | 1932       |
|    policy_gradient_loss | 0.0868     |
|    std                  | 0.254      |
|    value_loss           | 1.71e-05   |
----------------------------------------
Num timesteps: 1992000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1995000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 1998000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 2001000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.368    |
| time/                   |           |
|    fps                  | 199       |
|    iterations           | 163       |
|    time_elapsed         | 10054     |
|    total_timesteps      | 2002944   |
| train/                  |           |
|    approx_kl            | 0.4758097 |
|    clip_fraction        | 0.781     |
|    clip_range           | 0.075     |
|    entropy_loss         | 1.22      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0342    |
|    n_updates            | 1944      |
|    policy_gradient_loss | 0.0576    |
|    std                  | 0.253     |
|    value_loss           | 1.84e-05  |
---------------------------------------
Num timesteps: 2004000
Best mean reward: -0.36 - Last mean reward per episode: -0.37
Num timesteps: 2007000
Best mean reward: -0.36 - Last mean reward per episode: -0.35

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_031739_numTimesteps_2007000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
Num timesteps: 2010000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2013000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 4        |
|    ep_rew_mean          | -0.373   |
| time/                   |          |
|    fps                  | 198      |
|    iterations           | 164      |
|    time_elapsed         | 10127    |
|    total_timesteps      | 2015232  |
| train/                  |          |
|    approx_kl            | 0.615377 |
|    clip_fraction        | 0.778    |
|    clip_range           | 0.075    |
|    entropy_loss         | 1.25     |
|    explained_variance   | 0.999    |
|    learning_rate        | 0.000512 |
|    loss                 | 0.0864   |
|    n_updates            | 1956     |
|    policy_gradient_loss | 0.0619   |
|    std                  | 0.253    |
|    value_loss           | 1.81e-05 |
--------------------------------------
Num timesteps: 2016000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2019000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2022000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 2025000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.375     |
| time/                   |            |
|    fps                  | 198        |
|    iterations           | 165        |
|    time_elapsed         | 10205      |
|    total_timesteps      | 2027520    |
| train/                  |            |
|    approx_kl            | 0.34647432 |
|    clip_fraction        | 0.796      |
|    clip_range           | 0.075      |
|    entropy_loss         | 1.26       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0401     |
|    n_updates            | 1968       |
|    policy_gradient_loss | 0.0707     |
|    std                  | 0.253      |
|    value_loss           | 1.72e-05   |
----------------------------------------
Num timesteps: 2028000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2031000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 2034000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2037000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.364    |
| time/                   |           |
|    fps                  | 198       |
|    iterations           | 166       |
|    time_elapsed         | 10271     |
|    total_timesteps      | 2039808   |
| train/                  |           |
|    approx_kl            | 0.4188497 |
|    clip_fraction        | 0.796     |
|    clip_range           | 0.075     |
|    entropy_loss         | 1.29      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0378    |
|    n_updates            | 1980      |
|    policy_gradient_loss | 0.0738    |
|    std                  | 0.252     |
|    value_loss           | 1.71e-05  |
---------------------------------------
Num timesteps: 2040000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 2043000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2046000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2049000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2052000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.374    |
| time/                   |           |
|    fps                  | 198       |
|    iterations           | 167       |
|    time_elapsed         | 10343     |
|    total_timesteps      | 2052096   |
| train/                  |           |
|    approx_kl            | 0.7622948 |
|    clip_fraction        | 0.797     |
|    clip_range           | 0.075     |
|    entropy_loss         | 1.32      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0889    |
|    n_updates            | 1992      |
|    policy_gradient_loss | 0.0684    |
|    std                  | 0.251     |
|    value_loss           | 2.19e-05  |
---------------------------------------
Num timesteps: 2055000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 2058000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2061000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2064000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.368     |
| time/                   |            |
|    fps                  | 198        |
|    iterations           | 168        |
|    time_elapsed         | 10391      |
|    total_timesteps      | 2064384    |
| train/                  |            |
|    approx_kl            | 0.79160625 |
|    clip_fraction        | 0.808      |
|    clip_range           | 0.075      |
|    entropy_loss         | 1.35       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0875     |
|    n_updates            | 2004       |
|    policy_gradient_loss | 0.0778     |
|    std                  | 0.251      |
|    value_loss           | 2.2e-05    |
----------------------------------------
Num timesteps: 2067000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2070000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 2073000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2076000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.376     |
| time/                   |            |
|    fps                  | 198        |
|    iterations           | 169        |
|    time_elapsed         | 10460      |
|    total_timesteps      | 2076672    |
| train/                  |            |
|    approx_kl            | 0.99377173 |
|    clip_fraction        | 0.807      |
|    clip_range           | 0.075      |
|    entropy_loss         | 1.38       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0467     |
|    n_updates            | 2016       |
|    policy_gradient_loss | 0.0673     |
|    std                  | 0.248      |
|    value_loss           | 0.000528   |
----------------------------------------
Num timesteps: 2079000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2082000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2085000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2088000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.372     |
| time/                   |            |
|    fps                  | 198        |
|    iterations           | 170        |
|    time_elapsed         | 10511      |
|    total_timesteps      | 2088960    |
| train/                  |            |
|    approx_kl            | 0.45174685 |
|    clip_fraction        | 0.82       |
|    clip_range           | 0.075      |
|    entropy_loss         | 1.4        |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0544     |
|    n_updates            | 2028       |
|    policy_gradient_loss | 0.0903     |
|    std                  | 0.248      |
|    value_loss           | 1.54e-05   |
----------------------------------------
Num timesteps: 2091000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2094000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 2097000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2100000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 3.99       |
|    ep_rew_mean          | -0.364     |
| time/                   |            |
|    fps                  | 198        |
|    iterations           | 171        |
|    time_elapsed         | 10584      |
|    total_timesteps      | 2101248    |
| train/                  |            |
|    approx_kl            | 0.48143783 |
|    clip_fraction        | 0.79       |
|    clip_range           | 0.075      |
|    entropy_loss         | 1.43       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0748     |
|    n_updates            | 2040       |
|    policy_gradient_loss | 0.0604     |
|    std                  | 0.246      |
|    value_loss           | 1.63e-05   |
----------------------------------------
Num timesteps: 2103000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 2106000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 2109000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2112000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 3.99      |
|    ep_rew_mean          | -0.364    |
| time/                   |           |
|    fps                  | 198       |
|    iterations           | 172       |
|    time_elapsed         | 10644     |
|    total_timesteps      | 2113536   |
| train/                  |           |
|    approx_kl            | 1.0283471 |
|    clip_fraction        | 0.797     |
|    clip_range           | 0.075     |
|    entropy_loss         | 1.45      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0563    |
|    n_updates            | 2052      |
|    policy_gradient_loss | 0.062     |
|    std                  | 0.247     |
|    value_loss           | 1.68e-05  |
---------------------------------------
Num timesteps: 2115000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 2118000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2121000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2124000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.372    |
| time/                   |           |
|    fps                  | 198       |
|    iterations           | 173       |
|    time_elapsed         | 10715     |
|    total_timesteps      | 2125824   |
| train/                  |           |
|    approx_kl            | 0.8499097 |
|    clip_fraction        | 0.804     |
|    clip_range           | 0.075     |
|    entropy_loss         | 1.47      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.049     |
|    n_updates            | 2064      |
|    policy_gradient_loss | 0.0684    |
|    std                  | 0.247     |
|    value_loss           | 1.55e-05  |
---------------------------------------
Num timesteps: 2127000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2130000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2133000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 2136000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.371     |
| time/                   |            |
|    fps                  | 198        |
|    iterations           | 174        |
|    time_elapsed         | 10795      |
|    total_timesteps      | 2138112    |
| train/                  |            |
|    approx_kl            | 0.74055624 |
|    clip_fraction        | 0.843      |
|    clip_range           | 0.075      |
|    entropy_loss         | 1.49       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0638     |
|    n_updates            | 2076       |
|    policy_gradient_loss | 0.0948     |
|    std                  | 0.246      |
|    value_loss           | 3.5e-05    |
----------------------------------------
Num timesteps: 2139000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 2142000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2145000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 2148000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.368     |
| time/                   |            |
|    fps                  | 198        |
|    iterations           | 175        |
|    time_elapsed         | 10847      |
|    total_timesteps      | 2150400    |
| train/                  |            |
|    approx_kl            | 0.62369806 |
|    clip_fraction        | 0.829      |
|    clip_range           | 0.075      |
|    entropy_loss         | 1.48       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0716     |
|    n_updates            | 2088       |
|    policy_gradient_loss | 0.0813     |
|    std                  | 0.246      |
|    value_loss           | 1.69e-05   |
----------------------------------------
Num timesteps: 2151000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2154000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 2157000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2160000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.378     |
| time/                   |            |
|    fps                  | 198        |
|    iterations           | 176        |
|    time_elapsed         | 10889      |
|    total_timesteps      | 2162688    |
| train/                  |            |
|    approx_kl            | 0.88257766 |
|    clip_fraction        | 0.812      |
|    clip_range           | 0.075      |
|    entropy_loss         | 1.52       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0396     |
|    n_updates            | 2100       |
|    policy_gradient_loss | 0.0703     |
|    std                  | 0.244      |
|    value_loss           | 0.000279   |
----------------------------------------
Num timesteps: 2163000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2166000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2169000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 2172000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.368    |
| time/                   |           |
|    fps                  | 198       |
|    iterations           | 177       |
|    time_elapsed         | 10938     |
|    total_timesteps      | 2174976   |
| train/                  |           |
|    approx_kl            | 1.5453844 |
|    clip_fraction        | 0.838     |
|    clip_range           | 0.075     |
|    entropy_loss         | 1.53      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0487    |
|    n_updates            | 2112      |
|    policy_gradient_loss | 0.0856    |
|    std                  | 0.243     |
|    value_loss           | 2.41e-05  |
---------------------------------------
Num timesteps: 2175000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2178000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 2181000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2184000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 2187000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.368     |
| time/                   |            |
|    fps                  | 198        |
|    iterations           | 178        |
|    time_elapsed         | 10993      |
|    total_timesteps      | 2187264    |
| train/                  |            |
|    approx_kl            | 0.87907344 |
|    clip_fraction        | 0.801      |
|    clip_range           | 0.075      |
|    entropy_loss         | 1.56       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0462     |
|    n_updates            | 2124       |
|    policy_gradient_loss | 0.0654     |
|    std                  | 0.242      |
|    value_loss           | 1.67e-05   |
----------------------------------------
Num timesteps: 2190000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 2193000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 2196000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2199000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4.03       |
|    ep_rew_mean          | -0.372     |
| time/                   |            |
|    fps                  | 198        |
|    iterations           | 179        |
|    time_elapsed         | 11053      |
|    total_timesteps      | 2199552    |
| train/                  |            |
|    approx_kl            | 0.98847264 |
|    clip_fraction        | 0.817      |
|    clip_range           | 0.075      |
|    entropy_loss         | 1.57       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0677     |
|    n_updates            | 2136       |
|    policy_gradient_loss | 0.0792     |
|    std                  | 0.241      |
|    value_loss           | 2.2e-05    |
----------------------------------------
Num timesteps: 2202000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2205000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 2208000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 2211000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.371    |
| time/                   |           |
|    fps                  | 199       |
|    iterations           | 180       |
|    time_elapsed         | 11104     |
|    total_timesteps      | 2211840   |
| train/                  |           |
|    approx_kl            | 1.0577585 |
|    clip_fraction        | 0.849     |
|    clip_range           | 0.075     |
|    entropy_loss         | 1.59      |
|    explained_variance   | 0.996     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0226    |
|    n_updates            | 2148      |
|    policy_gradient_loss | 0.064     |
|    std                  | 0.238     |
|    value_loss           | 0.00149   |
---------------------------------------
Num timesteps: 2214000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 2217000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2220000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2223000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.381    |
| time/                   |           |
|    fps                  | 198       |
|    iterations           | 181       |
|    time_elapsed         | 11185     |
|    total_timesteps      | 2224128   |
| train/                  |           |
|    approx_kl            | 0.8023586 |
|    clip_fraction        | 0.827     |
|    clip_range           | 0.075     |
|    entropy_loss         | 1.62      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0755    |
|    n_updates            | 2160      |
|    policy_gradient_loss | 0.0891    |
|    std                  | 0.239     |
|    value_loss           | 1.64e-05  |
---------------------------------------
Num timesteps: 2226000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 2229000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 2232000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2235000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.362     |
| time/                   |            |
|    fps                  | 198        |
|    iterations           | 182        |
|    time_elapsed         | 11250      |
|    total_timesteps      | 2236416    |
| train/                  |            |
|    approx_kl            | 0.81541616 |
|    clip_fraction        | 0.829      |
|    clip_range           | 0.075      |
|    entropy_loss         | 1.62       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0465     |
|    n_updates            | 2172       |
|    policy_gradient_loss | 0.0855     |
|    std                  | 0.239      |
|    value_loss           | 1.65e-05   |
----------------------------------------
Num timesteps: 2238000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2241000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2244000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2247000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.363    |
| time/                   |           |
|    fps                  | 198       |
|    iterations           | 183       |
|    time_elapsed         | 11305     |
|    total_timesteps      | 2248704   |
| train/                  |           |
|    approx_kl            | 1.0987895 |
|    clip_fraction        | 0.849     |
|    clip_range           | 0.075     |
|    entropy_loss         | 1.64      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0538    |
|    n_updates            | 2184      |
|    policy_gradient_loss | 0.0981    |
|    std                  | 0.237     |
|    value_loss           | 1.53e-05  |
---------------------------------------
Num timesteps: 2250000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2253000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2256000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 2259000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.379    |
| time/                   |           |
|    fps                  | 198       |
|    iterations           | 184       |
|    time_elapsed         | 11383     |
|    total_timesteps      | 2260992   |
| train/                  |           |
|    approx_kl            | 1.8068643 |
|    clip_fraction        | 0.858     |
|    clip_range           | 0.075     |
|    entropy_loss         | 1.66      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0878    |
|    n_updates            | 2196      |
|    policy_gradient_loss | 0.11      |
|    std                  | 0.237     |
|    value_loss           | 1.69e-05  |
---------------------------------------
Num timesteps: 2262000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2265000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2268000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2271000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 3.99      |
|    ep_rew_mean          | -0.385    |
| time/                   |           |
|    fps                  | 198       |
|    iterations           | 185       |
|    time_elapsed         | 11449     |
|    total_timesteps      | 2273280   |
| train/                  |           |
|    approx_kl            | 1.8694035 |
|    clip_fraction        | 0.833     |
|    clip_range           | 0.075     |
|    entropy_loss         | 1.66      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0921    |
|    n_updates            | 2208      |
|    policy_gradient_loss | 0.0797    |
|    std                  | 0.236     |
|    value_loss           | 1.98e-05  |
---------------------------------------
Num timesteps: 2274000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2277000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 2280000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 2283000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.374     |
| time/                   |            |
|    fps                  | 198        |
|    iterations           | 186        |
|    time_elapsed         | 11509      |
|    total_timesteps      | 2285568    |
| train/                  |            |
|    approx_kl            | 0.70069194 |
|    clip_fraction        | 0.824      |
|    clip_range           | 0.075      |
|    entropy_loss         | 1.68       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0621     |
|    n_updates            | 2220       |
|    policy_gradient_loss | 0.0831     |
|    std                  | 0.236      |
|    value_loss           | 1.43e-05   |
----------------------------------------
Num timesteps: 2286000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2289000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2292000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2295000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 3.99      |
|    ep_rew_mean          | -0.373    |
| time/                   |           |
|    fps                  | 198       |
|    iterations           | 187       |
|    time_elapsed         | 11564     |
|    total_timesteps      | 2297856   |
| train/                  |           |
|    approx_kl            | 0.6872675 |
|    clip_fraction        | 0.811     |
|    clip_range           | 0.075     |
|    entropy_loss         | 1.68      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.049     |
|    n_updates            | 2232      |
|    policy_gradient_loss | 0.0726    |
|    std                  | 0.236     |
|    value_loss           | 1.4e-05   |
---------------------------------------
Num timesteps: 2298000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2301000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 2304000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 2307000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2310000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.371    |
| time/                   |           |
|    fps                  | 198       |
|    iterations           | 188       |
|    time_elapsed         | 11639     |
|    total_timesteps      | 2310144   |
| train/                  |           |
|    approx_kl            | 1.2273391 |
|    clip_fraction        | 0.816     |
|    clip_range           | 0.075     |
|    entropy_loss         | 1.68      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0335    |
|    n_updates            | 2244      |
|    policy_gradient_loss | 0.0726    |
|    std                  | 0.236     |
|    value_loss           | 1.66e-05  |
---------------------------------------
Num timesteps: 2313000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2316000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2319000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2322000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.377    |
| time/                   |           |
|    fps                  | 198       |
|    iterations           | 189       |
|    time_elapsed         | 11704     |
|    total_timesteps      | 2322432   |
| train/                  |           |
|    approx_kl            | 1.0771267 |
|    clip_fraction        | 0.828     |
|    clip_range           | 0.075     |
|    entropy_loss         | 1.7       |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0597    |
|    n_updates            | 2256      |
|    policy_gradient_loss | 0.0853    |
|    std                  | 0.235     |
|    value_loss           | 1.36e-05  |
---------------------------------------
Num timesteps: 2325000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 2328000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 2331000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2334000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.371    |
| time/                   |           |
|    fps                  | 198       |
|    iterations           | 190       |
|    time_elapsed         | 11778     |
|    total_timesteps      | 2334720   |
| train/                  |           |
|    approx_kl            | 0.8887543 |
|    clip_fraction        | 0.82      |
|    clip_range           | 0.075     |
|    entropy_loss         | 1.74      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0443    |
|    n_updates            | 2268      |
|    policy_gradient_loss | 0.0744    |
|    std                  | 0.233     |
|    value_loss           | 1.46e-05  |
---------------------------------------
Num timesteps: 2337000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2340000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 2343000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2346000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.365    |
| time/                   |           |
|    fps                  | 198       |
|    iterations           | 191       |
|    time_elapsed         | 11840     |
|    total_timesteps      | 2347008   |
| train/                  |           |
|    approx_kl            | 1.3802508 |
|    clip_fraction        | 0.842     |
|    clip_range           | 0.075     |
|    entropy_loss         | 1.74      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0618    |
|    n_updates            | 2280      |
|    policy_gradient_loss | 0.0956    |
|    std                  | 0.233     |
|    value_loss           | 0.000205  |
---------------------------------------
Num timesteps: 2349000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2352000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 2355000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2358000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.01      |
|    ep_rew_mean          | -0.374    |
| time/                   |           |
|    fps                  | 198       |
|    iterations           | 192       |
|    time_elapsed         | 11891     |
|    total_timesteps      | 2359296   |
| train/                  |           |
|    approx_kl            | 0.9887696 |
|    clip_fraction        | 0.842     |
|    clip_range           | 0.075     |
|    entropy_loss         | 1.74      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0764    |
|    n_updates            | 2292      |
|    policy_gradient_loss | 0.0918    |
|    std                  | 0.234     |
|    value_loss           | 4.58e-05  |
---------------------------------------
Num timesteps: 2361000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 2364000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 2367000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 2370000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 4        |
|    ep_rew_mean          | -0.369   |
| time/                   |          |
|    fps                  | 198      |
|    iterations           | 193      |
|    time_elapsed         | 11951    |
|    total_timesteps      | 2371584  |
| train/                  |          |
|    approx_kl            | 0.49678  |
|    clip_fraction        | 0.842    |
|    clip_range           | 0.075    |
|    entropy_loss         | 1.73     |
|    explained_variance   | 0.999    |
|    learning_rate        | 0.000512 |
|    loss                 | 0.0825   |
|    n_updates            | 2304     |
|    policy_gradient_loss | 0.0872   |
|    std                  | 0.233    |
|    value_loss           | 2.01e-05 |
--------------------------------------
Num timesteps: 2373000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2376000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2379000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 2382000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.375    |
| time/                   |           |
|    fps                  | 198       |
|    iterations           | 194       |
|    time_elapsed         | 12007     |
|    total_timesteps      | 2383872   |
| train/                  |           |
|    approx_kl            | 1.8040525 |
|    clip_fraction        | 0.824     |
|    clip_range           | 0.075     |
|    entropy_loss         | 1.74      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0647    |
|    n_updates            | 2316      |
|    policy_gradient_loss | 0.0772    |
|    std                  | 0.233     |
|    value_loss           | 3.46e-05  |
---------------------------------------
Num timesteps: 2385000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2388000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 2391000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 2394000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.378    |
| time/                   |           |
|    fps                  | 198       |
|    iterations           | 195       |
|    time_elapsed         | 12050     |
|    total_timesteps      | 2396160   |
| train/                  |           |
|    approx_kl            | 0.8301136 |
|    clip_fraction        | 0.834     |
|    clip_range           | 0.075     |
|    entropy_loss         | 1.76      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0711    |
|    n_updates            | 2328      |
|    policy_gradient_loss | 0.0953    |
|    std                  | 0.231     |
|    value_loss           | 1.78e-05  |
---------------------------------------
Num timesteps: 2397000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2400000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2403000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2406000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.374    |
| time/                   |           |
|    fps                  | 199       |
|    iterations           | 196       |
|    time_elapsed         | 12088     |
|    total_timesteps      | 2408448   |
| train/                  |           |
|    approx_kl            | 1.0076095 |
|    clip_fraction        | 0.825     |
|    clip_range           | 0.075     |
|    entropy_loss         | 1.77      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0778    |
|    n_updates            | 2340      |
|    policy_gradient_loss | 0.0755    |
|    std                  | 0.232     |
|    value_loss           | 0.000212  |
---------------------------------------
Num timesteps: 2409000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 2412000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2415000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2418000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.369     |
| time/                   |            |
|    fps                  | 199        |
|    iterations           | 197        |
|    time_elapsed         | 12134      |
|    total_timesteps      | 2420736    |
| train/                  |            |
|    approx_kl            | 0.41997966 |
|    clip_fraction        | 0.815      |
|    clip_range           | 0.075      |
|    entropy_loss         | 1.79       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0559     |
|    n_updates            | 2352       |
|    policy_gradient_loss | 0.0719     |
|    std                  | 0.232      |
|    value_loss           | 1.56e-05   |
----------------------------------------
Num timesteps: 2421000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2424000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2427000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2430000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2433000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.371    |
| time/                   |           |
|    fps                  | 199       |
|    iterations           | 198       |
|    time_elapsed         | 12204     |
|    total_timesteps      | 2433024   |
| train/                  |           |
|    approx_kl            | 1.0841011 |
|    clip_fraction        | 0.814     |
|    clip_range           | 0.075     |
|    entropy_loss         | 1.82      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0227    |
|    n_updates            | 2364      |
|    policy_gradient_loss | 0.0685    |
|    std                  | 0.231     |
|    value_loss           | 1.42e-05  |
---------------------------------------
Num timesteps: 2436000
Best mean reward: -0.35 - Last mean reward per episode: -0.39
Num timesteps: 2439000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2442000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 2445000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 3.99       |
|    ep_rew_mean          | -0.377     |
| time/                   |            |
|    fps                  | 199        |
|    iterations           | 199        |
|    time_elapsed         | 12260      |
|    total_timesteps      | 2445312    |
| train/                  |            |
|    approx_kl            | 0.79882926 |
|    clip_fraction        | 0.834      |
|    clip_range           | 0.075      |
|    entropy_loss         | 1.86       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0695     |
|    n_updates            | 2376       |
|    policy_gradient_loss | 0.0866     |
|    std                  | 0.229      |
|    value_loss           | 2.03e-05   |
----------------------------------------
Num timesteps: 2448000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2451000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2454000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2457000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 3.99      |
|    ep_rew_mean          | -0.371    |
| time/                   |           |
|    fps                  | 199       |
|    iterations           | 200       |
|    time_elapsed         | 12315     |
|    total_timesteps      | 2457600   |
| train/                  |           |
|    approx_kl            | 1.1498826 |
|    clip_fraction        | 0.817     |
|    clip_range           | 0.075     |
|    entropy_loss         | 1.89      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0536    |
|    n_updates            | 2388      |
|    policy_gradient_loss | 0.0659    |
|    std                  | 0.226     |
|    value_loss           | 6.75e-05  |
---------------------------------------
Num timesteps: 2460000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2463000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2466000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2469000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.383     |
| time/                   |            |
|    fps                  | 199        |
|    iterations           | 201        |
|    time_elapsed         | 12366      |
|    total_timesteps      | 2469888    |
| train/                  |            |
|    approx_kl            | 0.84529227 |
|    clip_fraction        | 0.821      |
|    clip_range           | 0.075      |
|    entropy_loss         | 1.92       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0926     |
|    n_updates            | 2400       |
|    policy_gradient_loss | 0.0779     |
|    std                  | 0.226      |
|    value_loss           | 1.49e-05   |
----------------------------------------
Num timesteps: 2472000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2475000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2478000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 2481000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.365    |
| time/                   |           |
|    fps                  | 199       |
|    iterations           | 202       |
|    time_elapsed         | 12426     |
|    total_timesteps      | 2482176   |
| train/                  |           |
|    approx_kl            | 1.6586691 |
|    clip_fraction        | 0.83      |
|    clip_range           | 0.075     |
|    entropy_loss         | 1.94      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.058     |
|    n_updates            | 2412      |
|    policy_gradient_loss | 0.0936    |
|    std                  | 0.224     |
|    value_loss           | 8.69e-05  |
---------------------------------------
Num timesteps: 2484000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 2487000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 2490000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2493000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.373    |
| time/                   |           |
|    fps                  | 199       |
|    iterations           | 203       |
|    time_elapsed         | 12484     |
|    total_timesteps      | 2494464   |
| train/                  |           |
|    approx_kl            | 1.0889035 |
|    clip_fraction        | 0.833     |
|    clip_range           | 0.075     |
|    entropy_loss         | 1.94      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0755    |
|    n_updates            | 2424      |
|    policy_gradient_loss | 0.0819    |
|    std                  | 0.224     |
|    value_loss           | 2.02e-05  |
---------------------------------------
Num timesteps: 2496000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2499000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 2502000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2505000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.367    |
| time/                   |           |
|    fps                  | 199       |
|    iterations           | 204       |
|    time_elapsed         | 12559     |
|    total_timesteps      | 2506752   |
| train/                  |           |
|    approx_kl            | 0.8357257 |
|    clip_fraction        | 0.847     |
|    clip_range           | 0.075     |
|    entropy_loss         | 1.95      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0702    |
|    n_updates            | 2436      |
|    policy_gradient_loss | 0.0822    |
|    std                  | 0.225     |
|    value_loss           | 0.000323  |
---------------------------------------
Num timesteps: 2508000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2511000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 2514000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 2517000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.366    |
| time/                   |           |
|    fps                  | 199       |
|    iterations           | 205       |
|    time_elapsed         | 12632     |
|    total_timesteps      | 2519040   |
| train/                  |           |
|    approx_kl            | 1.5361546 |
|    clip_fraction        | 0.862     |
|    clip_range           | 0.075     |
|    entropy_loss         | 1.95      |
|    explained_variance   | 0.996     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0618    |
|    n_updates            | 2448      |
|    policy_gradient_loss | 0.0653    |
|    std                  | 0.225     |
|    value_loss           | 0.000719  |
---------------------------------------
Num timesteps: 2520000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2523000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 2526000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 2529000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.369    |
| time/                   |           |
|    fps                  | 199       |
|    iterations           | 206       |
|    time_elapsed         | 12701     |
|    total_timesteps      | 2531328   |
| train/                  |           |
|    approx_kl            | 2.3704722 |
|    clip_fraction        | 0.884     |
|    clip_range           | 0.075     |
|    entropy_loss         | 1.95      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0863    |
|    n_updates            | 2460      |
|    policy_gradient_loss | 0.117     |
|    std                  | 0.223     |
|    value_loss           | 1.72e-05  |
---------------------------------------
Num timesteps: 2532000
Best mean reward: -0.35 - Last mean reward per episode: -0.39
Num timesteps: 2535000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 2538000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2541000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.05      |
|    ep_rew_mean          | -0.378    |
| time/                   |           |
|    fps                  | 199       |
|    iterations           | 207       |
|    time_elapsed         | 12753     |
|    total_timesteps      | 2543616   |
| train/                  |           |
|    approx_kl            | 2.1796749 |
|    clip_fraction        | 0.896     |
|    clip_range           | 0.075     |
|    entropy_loss         | 1.92      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0853    |
|    n_updates            | 2472      |
|    policy_gradient_loss | 0.139     |
|    std                  | 0.222     |
|    value_loss           | 1.51e-05  |
---------------------------------------
Num timesteps: 2544000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2547000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2550000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2553000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.367    |
| time/                   |           |
|    fps                  | 199       |
|    iterations           | 208       |
|    time_elapsed         | 12824     |
|    total_timesteps      | 2555904   |
| train/                  |           |
|    approx_kl            | 2.5873873 |
|    clip_fraction        | 0.847     |
|    clip_range           | 0.075     |
|    entropy_loss         | 1.91      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0586    |
|    n_updates            | 2484      |
|    policy_gradient_loss | 0.0847    |
|    std                  | 0.223     |
|    value_loss           | 1.79e-05  |
---------------------------------------
Num timesteps: 2556000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2559000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2562000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 2565000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2568000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.363     |
| time/                   |            |
|    fps                  | 199        |
|    iterations           | 209        |
|    time_elapsed         | 12896      |
|    total_timesteps      | 2568192    |
| train/                  |            |
|    approx_kl            | 0.99608874 |
|    clip_fraction        | 0.821      |
|    clip_range           | 0.075      |
|    entropy_loss         | 1.92       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0499     |
|    n_updates            | 2496       |
|    policy_gradient_loss | 0.0738     |
|    std                  | 0.221      |
|    value_loss           | 1.08e-05   |
----------------------------------------
Num timesteps: 2571000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2574000
Best mean reward: -0.35 - Last mean reward per episode: -0.39
Num timesteps: 2577000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 2580000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.368    |
| time/                   |           |
|    fps                  | 199       |
|    iterations           | 210       |
|    time_elapsed         | 12955     |
|    total_timesteps      | 2580480   |
| train/                  |           |
|    approx_kl            | 1.5790281 |
|    clip_fraction        | 0.818     |
|    clip_range           | 0.075     |
|    entropy_loss         | 1.96      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0468    |
|    n_updates            | 2508      |
|    policy_gradient_loss | 0.0708    |
|    std                  | 0.22      |
|    value_loss           | 1.06e-05  |
---------------------------------------
Num timesteps: 2583000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2586000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2589000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2592000
Best mean reward: -0.35 - Last mean reward per episode: -0.39
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.373    |
| time/                   |           |
|    fps                  | 199       |
|    iterations           | 211       |
|    time_elapsed         | 12999     |
|    total_timesteps      | 2592768   |
| train/                  |           |
|    approx_kl            | 1.2132454 |
|    clip_fraction        | 0.838     |
|    clip_range           | 0.075     |
|    entropy_loss         | 1.98      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0764    |
|    n_updates            | 2520      |
|    policy_gradient_loss | 0.0904    |
|    std                  | 0.22      |
|    value_loss           | 1.17e-05  |
---------------------------------------
Num timesteps: 2595000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2598000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2601000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2604000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.356    |
| time/                   |           |
|    fps                  | 199       |
|    iterations           | 212       |
|    time_elapsed         | 13074     |
|    total_timesteps      | 2605056   |
| train/                  |           |
|    approx_kl            | 1.2610356 |
|    clip_fraction        | 0.853     |
|    clip_range           | 0.075     |
|    entropy_loss         | 1.99      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0806    |
|    n_updates            | 2532      |
|    policy_gradient_loss | 0.0949    |
|    std                  | 0.22      |
|    value_loss           | 1.42e-05  |
---------------------------------------
Num timesteps: 2607000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 2610000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2613000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2616000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.367    |
| time/                   |           |
|    fps                  | 199       |
|    iterations           | 213       |
|    time_elapsed         | 13146     |
|    total_timesteps      | 2617344   |
| train/                  |           |
|    approx_kl            | 2.3638442 |
|    clip_fraction        | 0.84      |
|    clip_range           | 0.075     |
|    entropy_loss         | 2         |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0768    |
|    n_updates            | 2544      |
|    policy_gradient_loss | 0.0846    |
|    std                  | 0.22      |
|    value_loss           | 1.11e-05  |
---------------------------------------
Num timesteps: 2619000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 2622000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2625000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2628000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.369    |
| time/                   |           |
|    fps                  | 198       |
|    iterations           | 214       |
|    time_elapsed         | 13215     |
|    total_timesteps      | 2629632   |
| train/                  |           |
|    approx_kl            | 1.1675829 |
|    clip_fraction        | 0.855     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.01      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0709    |
|    n_updates            | 2556      |
|    policy_gradient_loss | 0.123     |
|    std                  | 0.221     |
|    value_loss           | 1.28e-05  |
---------------------------------------
Num timesteps: 2631000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2634000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2637000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2640000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.371     |
| time/                   |            |
|    fps                  | 198        |
|    iterations           | 215        |
|    time_elapsed         | 13280      |
|    total_timesteps      | 2641920    |
| train/                  |            |
|    approx_kl            | 0.96446306 |
|    clip_fraction        | 0.857      |
|    clip_range           | 0.075      |
|    entropy_loss         | 2.01       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0625     |
|    n_updates            | 2568       |
|    policy_gradient_loss | 0.109      |
|    std                  | 0.22       |
|    value_loss           | 1.18e-05   |
----------------------------------------
Num timesteps: 2643000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2646000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2649000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2652000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.365    |
| time/                   |           |
|    fps                  | 198       |
|    iterations           | 216       |
|    time_elapsed         | 13356     |
|    total_timesteps      | 2654208   |
| train/                  |           |
|    approx_kl            | 2.3578184 |
|    clip_fraction        | 0.823     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.02      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0587    |
|    n_updates            | 2580      |
|    policy_gradient_loss | 0.0674    |
|    std                  | 0.219     |
|    value_loss           | 9.08e-05  |
---------------------------------------
Num timesteps: 2655000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 2658000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2661000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2664000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.372    |
| time/                   |           |
|    fps                  | 198       |
|    iterations           | 217       |
|    time_elapsed         | 13419     |
|    total_timesteps      | 2666496   |
| train/                  |           |
|    approx_kl            | 1.3226689 |
|    clip_fraction        | 0.839     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.03      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.037     |
|    n_updates            | 2592      |
|    policy_gradient_loss | 0.0713    |
|    std                  | 0.219     |
|    value_loss           | 0.000838  |
---------------------------------------
Num timesteps: 2667000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2670000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 2673000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2676000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.383    |
| time/                   |           |
|    fps                  | 198       |
|    iterations           | 218       |
|    time_elapsed         | 13480     |
|    total_timesteps      | 2678784   |
| train/                  |           |
|    approx_kl            | 1.4184226 |
|    clip_fraction        | 0.841     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.05      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0803    |
|    n_updates            | 2604      |
|    policy_gradient_loss | 0.0861    |
|    std                  | 0.218     |
|    value_loss           | 1.19e-05  |
---------------------------------------
Num timesteps: 2679000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2682000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2685000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2688000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 2691000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.374    |
| time/                   |           |
|    fps                  | 198       |
|    iterations           | 219       |
|    time_elapsed         | 13545     |
|    total_timesteps      | 2691072   |
| train/                  |           |
|    approx_kl            | 0.9385746 |
|    clip_fraction        | 0.825     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.07      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0541    |
|    n_updates            | 2616      |
|    policy_gradient_loss | 0.0725    |
|    std                  | 0.217     |
|    value_loss           | 1.41e-05  |
---------------------------------------
Num timesteps: 2694000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 2697000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 2700000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2703000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.368    |
| time/                   |           |
|    fps                  | 198       |
|    iterations           | 220       |
|    time_elapsed         | 13610     |
|    total_timesteps      | 2703360   |
| train/                  |           |
|    approx_kl            | 1.6083325 |
|    clip_fraction        | 0.84      |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.09      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0532    |
|    n_updates            | 2628      |
|    policy_gradient_loss | 0.0914    |
|    std                  | 0.216     |
|    value_loss           | 1.27e-05  |
---------------------------------------
Num timesteps: 2706000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2709000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2712000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2715000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 4        |
|    ep_rew_mean          | -0.374   |
| time/                   |          |
|    fps                  | 198      |
|    iterations           | 221      |
|    time_elapsed         | 13658    |
|    total_timesteps      | 2715648  |
| train/                  |          |
|    approx_kl            | 2.574035 |
|    clip_fraction        | 0.846    |
|    clip_range           | 0.075    |
|    entropy_loss         | 2.11     |
|    explained_variance   | 0.999    |
|    learning_rate        | 0.000512 |
|    loss                 | 0.0702   |
|    n_updates            | 2640     |
|    policy_gradient_loss | 0.109    |
|    std                  | 0.214    |
|    value_loss           | 1.16e-05 |
--------------------------------------
Num timesteps: 2718000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 2721000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 2724000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2727000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 4        |
|    ep_rew_mean          | -0.366   |
| time/                   |          |
|    fps                  | 198      |
|    iterations           | 222      |
|    time_elapsed         | 13713    |
|    total_timesteps      | 2727936  |
| train/                  |          |
|    approx_kl            | 2.006224 |
|    clip_fraction        | 0.849    |
|    clip_range           | 0.075    |
|    entropy_loss         | 2.13     |
|    explained_variance   | 0.999    |
|    learning_rate        | 0.000512 |
|    loss                 | 0.0514   |
|    n_updates            | 2652     |
|    policy_gradient_loss | 0.0995   |
|    std                  | 0.214    |
|    value_loss           | 1.23e-05 |
--------------------------------------
Num timesteps: 2730000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 2733000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 2736000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2739000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.38     |
| time/                   |           |
|    fps                  | 199       |
|    iterations           | 223       |
|    time_elapsed         | 13765     |
|    total_timesteps      | 2740224   |
| train/                  |           |
|    approx_kl            | 0.7555987 |
|    clip_fraction        | 0.808     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.17      |
|    explained_variance   | 0.985     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.00586   |
|    n_updates            | 2664      |
|    policy_gradient_loss | 0.0199    |
|    std                  | 0.213     |
|    value_loss           | 0.01      |
---------------------------------------
Num timesteps: 2742000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2745000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2748000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 2751000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.378    |
| time/                   |           |
|    fps                  | 198       |
|    iterations           | 224       |
|    time_elapsed         | 13831     |
|    total_timesteps      | 2752512   |
| train/                  |           |
|    approx_kl            | 7.6386046 |
|    clip_fraction        | 0.872     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.16      |
|    explained_variance   | 0.995     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0502    |
|    n_updates            | 2676      |
|    policy_gradient_loss | 0.0908    |
|    std                  | 0.212     |
|    value_loss           | 4.16e-05  |
---------------------------------------
Num timesteps: 2754000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2757000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2760000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 2763000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.364    |
| time/                   |           |
|    fps                  | 199       |
|    iterations           | 225       |
|    time_elapsed         | 13885     |
|    total_timesteps      | 2764800   |
| train/                  |           |
|    approx_kl            | 2.5880864 |
|    clip_fraction        | 0.846     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.14      |
|    explained_variance   | 0.998     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0431    |
|    n_updates            | 2688      |
|    policy_gradient_loss | 0.0702    |
|    std                  | 0.212     |
|    value_loss           | 3.02e-05  |
---------------------------------------
Num timesteps: 2766000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 2769000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2772000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2775000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.374    |
| time/                   |           |
|    fps                  | 199       |
|    iterations           | 226       |
|    time_elapsed         | 13947     |
|    total_timesteps      | 2777088   |
| train/                  |           |
|    approx_kl            | 1.6386638 |
|    clip_fraction        | 0.854     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.14      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0526    |
|    n_updates            | 2700      |
|    policy_gradient_loss | 0.0869    |
|    std                  | 0.213     |
|    value_loss           | 1.79e-05  |
---------------------------------------
Num timesteps: 2778000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 2781000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 2784000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 2787000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 4.15     |
|    ep_rew_mean          | -0.379   |
| time/                   |          |
|    fps                  | 199      |
|    iterations           | 227      |
|    time_elapsed         | 14001    |
|    total_timesteps      | 2789376  |
| train/                  |          |
|    approx_kl            | 7.347763 |
|    clip_fraction        | 0.954    |
|    clip_range           | 0.075    |
|    entropy_loss         | 2.05     |
|    explained_variance   | 0.984    |
|    learning_rate        | 0.000512 |
|    loss                 | 0.0756   |
|    n_updates            | 2712     |
|    policy_gradient_loss | 0.0932   |
|    std                  | 0.216    |
|    value_loss           | 0.00559  |
--------------------------------------
Num timesteps: 2790000
Best mean reward: -0.35 - Last mean reward per episode: -0.41
Num timesteps: 2793000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 2796000
Best mean reward: -0.35 - Last mean reward per episode: -0.39
Num timesteps: 2799000
Best mean reward: -0.35 - Last mean reward per episode: -0.39
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 4.11     |
|    ep_rew_mean          | -0.382   |
| time/                   |          |
|    fps                  | 199      |
|    iterations           | 228      |
|    time_elapsed         | 14045    |
|    total_timesteps      | 2801664  |
| train/                  |          |
|    approx_kl            | 1.976482 |
|    clip_fraction        | 0.874    |
|    clip_range           | 0.075    |
|    entropy_loss         | 1.95     |
|    explained_variance   | 0.98     |
|    learning_rate        | 0.000512 |
|    loss                 | 0.0381   |
|    n_updates            | 2724     |
|    policy_gradient_loss | 0.0362   |
|    std                  | 0.217    |
|    value_loss           | 0.00154  |
--------------------------------------
Num timesteps: 2802000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2805000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 2808000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 2811000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.01      |
|    ep_rew_mean          | -0.381    |
| time/                   |           |
|    fps                  | 199       |
|    iterations           | 229       |
|    time_elapsed         | 14101     |
|    total_timesteps      | 2813952   |
| train/                  |           |
|    approx_kl            | 2.9448912 |
|    clip_fraction        | 0.88      |
|    clip_range           | 0.075     |
|    entropy_loss         | 1.94      |
|    explained_variance   | 0.955     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0516    |
|    n_updates            | 2736      |
|    policy_gradient_loss | 0.0583    |
|    std                  | 0.216     |
|    value_loss           | 0.00316   |
---------------------------------------
Num timesteps: 2814000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 2817000
Best mean reward: -0.35 - Last mean reward per episode: -0.39
Num timesteps: 2820000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 2823000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2826000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.02      |
|    ep_rew_mean          | -0.375    |
| time/                   |           |
|    fps                  | 199       |
|    iterations           | 230       |
|    time_elapsed         | 14149     |
|    total_timesteps      | 2826240   |
| train/                  |           |
|    approx_kl            | 15.467354 |
|    clip_fraction        | 0.952     |
|    clip_range           | 0.075     |
|    entropy_loss         | 1.84      |
|    explained_variance   | 0.949     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0848    |
|    n_updates            | 2748      |
|    policy_gradient_loss | 0.0888    |
|    std                  | 0.219     |
|    value_loss           | 0.00343   |
---------------------------------------
Num timesteps: 2829000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 2832000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 2835000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2838000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 4.04     |
|    ep_rew_mean          | -0.379   |
| time/                   |          |
|    fps                  | 200      |
|    iterations           | 231      |
|    time_elapsed         | 14191    |
|    total_timesteps      | 2838528  |
| train/                  |          |
|    approx_kl            | 9.283807 |
|    clip_fraction        | 0.921    |
|    clip_range           | 0.075    |
|    entropy_loss         | 1.71     |
|    explained_variance   | 0.942    |
|    learning_rate        | 0.000512 |
|    loss                 | 0.109    |
|    n_updates            | 2760     |
|    policy_gradient_loss | 0.1      |
|    std                  | 0.22     |
|    value_loss           | 0.000244 |
--------------------------------------
Num timesteps: 2841000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2844000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2847000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 2850000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.378    |
| time/                   |           |
|    fps                  | 199       |
|    iterations           | 232       |
|    time_elapsed         | 14258     |
|    total_timesteps      | 2850816   |
| train/                  |           |
|    approx_kl            | 5.4698997 |
|    clip_fraction        | 0.91      |
|    clip_range           | 0.075     |
|    entropy_loss         | 1.62      |
|    explained_variance   | 0.944     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0394    |
|    n_updates            | 2772      |
|    policy_gradient_loss | 0.0793    |
|    std                  | 0.222     |
|    value_loss           | 0.000158  |
---------------------------------------
Num timesteps: 2853000
Best mean reward: -0.35 - Last mean reward per episode: -0.41
Num timesteps: 2856000
Best mean reward: -0.35 - Last mean reward per episode: -0.42
Num timesteps: 2859000
Best mean reward: -0.35 - Last mean reward per episode: -0.43
Num timesteps: 2862000
Best mean reward: -0.35 - Last mean reward per episode: -0.42
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.22      |
|    ep_rew_mean          | -0.411    |
| time/                   |           |
|    fps                  | 199       |
|    iterations           | 233       |
|    time_elapsed         | 14319     |
|    total_timesteps      | 2863104   |
| train/                  |           |
|    approx_kl            | 46.705532 |
|    clip_fraction        | 0.929     |
|    clip_range           | 0.075     |
|    entropy_loss         | 1.55      |
|    explained_variance   | 0.431     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0183    |
|    n_updates            | 2784      |
|    policy_gradient_loss | 0.0471    |
|    std                  | 0.223     |
|    value_loss           | 0.0027    |
---------------------------------------
Num timesteps: 2865000
Best mean reward: -0.35 - Last mean reward per episode: -0.40
Num timesteps: 2868000
Best mean reward: -0.35 - Last mean reward per episode: -0.40
Num timesteps: 2871000
Best mean reward: -0.35 - Last mean reward per episode: -0.41
Num timesteps: 2874000
Best mean reward: -0.35 - Last mean reward per episode: -0.41
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.12      |
|    ep_rew_mean          | -0.403    |
| time/                   |           |
|    fps                  | 199       |
|    iterations           | 234       |
|    time_elapsed         | 14385     |
|    total_timesteps      | 2875392   |
| train/                  |           |
|    approx_kl            | 2.9199018 |
|    clip_fraction        | 0.891     |
|    clip_range           | 0.075     |
|    entropy_loss         | 1.44      |
|    explained_variance   | 0.859     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0558    |
|    n_updates            | 2796      |
|    policy_gradient_loss | 0.0869    |
|    std                  | 0.225     |
|    value_loss           | 0.00116   |
---------------------------------------
Num timesteps: 2877000
Best mean reward: -0.35 - Last mean reward per episode: -0.40
Num timesteps: 2880000
Best mean reward: -0.35 - Last mean reward per episode: -0.41
Num timesteps: 2883000
Best mean reward: -0.35 - Last mean reward per episode: -0.41
Num timesteps: 2886000
Best mean reward: -0.35 - Last mean reward per episode: -0.41
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4.12       |
|    ep_rew_mean          | -0.403     |
| time/                   |            |
|    fps                  | 199        |
|    iterations           | 235        |
|    time_elapsed         | 14457      |
|    total_timesteps      | 2887680    |
| train/                  |            |
|    approx_kl            | 0.27177334 |
|    clip_fraction        | 0.816      |
|    clip_range           | 0.075      |
|    entropy_loss         | 1.39       |
|    explained_variance   | 0.988      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0603     |
|    n_updates            | 2808       |
|    policy_gradient_loss | 0.0877     |
|    std                  | 0.228      |
|    value_loss           | 0.000119   |
----------------------------------------
Num timesteps: 2889000
Best mean reward: -0.35 - Last mean reward per episode: -0.41
Num timesteps: 2892000
Best mean reward: -0.35 - Last mean reward per episode: -0.41
Num timesteps: 2895000
Best mean reward: -0.35 - Last mean reward per episode: -0.40
Num timesteps: 2898000
Best mean reward: -0.35 - Last mean reward per episode: -0.41
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.04      |
|    ep_rew_mean          | -0.396    |
| time/                   |           |
|    fps                  | 199       |
|    iterations           | 236       |
|    time_elapsed         | 14523     |
|    total_timesteps      | 2899968   |
| train/                  |           |
|    approx_kl            | 0.7051962 |
|    clip_fraction        | 0.835     |
|    clip_range           | 0.075     |
|    entropy_loss         | 1.35      |
|    explained_variance   | 0.995     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0618    |
|    n_updates            | 2820      |
|    policy_gradient_loss | 0.0646    |
|    std                  | 0.228     |
|    value_loss           | 9.08e-05  |
---------------------------------------
Num timesteps: 2901000
Best mean reward: -0.35 - Last mean reward per episode: -0.40
Num timesteps: 2904000
Best mean reward: -0.35 - Last mean reward per episode: -0.39
Num timesteps: 2907000
Best mean reward: -0.35 - Last mean reward per episode: -0.40
Num timesteps: 2910000
Best mean reward: -0.35 - Last mean reward per episode: -0.41
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4.05       |
|    ep_rew_mean          | -0.402     |
| time/                   |            |
|    fps                  | 199        |
|    iterations           | 237        |
|    time_elapsed         | 14596      |
|    total_timesteps      | 2912256    |
| train/                  |            |
|    approx_kl            | 0.50993687 |
|    clip_fraction        | 0.828      |
|    clip_range           | 0.075      |
|    entropy_loss         | 1.31       |
|    explained_variance   | 0.996      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0424     |
|    n_updates            | 2832       |
|    policy_gradient_loss | 0.07       |
|    std                  | 0.229      |
|    value_loss           | 6.3e-05    |
----------------------------------------
Num timesteps: 2913000
Best mean reward: -0.35 - Last mean reward per episode: -0.40
Num timesteps: 2916000
Best mean reward: -0.35 - Last mean reward per episode: -0.39
Num timesteps: 2919000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 2922000
Best mean reward: -0.35 - Last mean reward per episode: -0.39
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 4.01     |
|    ep_rew_mean          | -0.394   |
| time/                   |          |
|    fps                  | 199      |
|    iterations           | 238      |
|    time_elapsed         | 14640    |
|    total_timesteps      | 2924544  |
| train/                  |          |
|    approx_kl            | 0.566098 |
|    clip_fraction        | 0.836    |
|    clip_range           | 0.075    |
|    entropy_loss         | 1.28     |
|    explained_variance   | 0.995    |
|    learning_rate        | 0.000512 |
|    loss                 | 0.0552   |
|    n_updates            | 2844     |
|    policy_gradient_loss | 0.0595   |
|    std                  | 0.229    |
|    value_loss           | 8.77e-05 |
--------------------------------------
Num timesteps: 2925000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 2928000
Best mean reward: -0.35 - Last mean reward per episode: -0.39
Num timesteps: 2931000
Best mean reward: -0.35 - Last mean reward per episode: -0.39
Num timesteps: 2934000
Best mean reward: -0.35 - Last mean reward per episode: -0.39
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.01      |
|    ep_rew_mean          | -0.394    |
| time/                   |           |
|    fps                  | 199       |
|    iterations           | 239       |
|    time_elapsed         | 14688     |
|    total_timesteps      | 2936832   |
| train/                  |           |
|    approx_kl            | 1.0451663 |
|    clip_fraction        | 0.843     |
|    clip_range           | 0.075     |
|    entropy_loss         | 1.26      |
|    explained_variance   | 0.997     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.037     |
|    n_updates            | 2856      |
|    policy_gradient_loss | 0.0616    |
|    std                  | 0.23      |
|    value_loss           | 4.98e-05  |
---------------------------------------
Num timesteps: 2937000
Best mean reward: -0.35 - Last mean reward per episode: -0.39
Num timesteps: 2940000
Best mean reward: -0.35 - Last mean reward per episode: -0.39
Num timesteps: 2943000
Best mean reward: -0.35 - Last mean reward per episode: -0.39
Num timesteps: 2946000
Best mean reward: -0.35 - Last mean reward per episode: -0.40
Num timesteps: 2949000
Best mean reward: -0.35 - Last mean reward per episode: -0.39
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4.02       |
|    ep_rew_mean          | -0.385     |
| time/                   |            |
|    fps                  | 199        |
|    iterations           | 240        |
|    time_elapsed         | 14750      |
|    total_timesteps      | 2949120    |
| train/                  |            |
|    approx_kl            | 0.49062547 |
|    clip_fraction        | 0.832      |
|    clip_range           | 0.075      |
|    entropy_loss         | 1.24       |
|    explained_variance   | 0.996      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0459     |
|    n_updates            | 2868       |
|    policy_gradient_loss | 0.048      |
|    std                  | 0.23       |
|    value_loss           | 5.38e-05   |
----------------------------------------
Num timesteps: 2952000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 2955000
Best mean reward: -0.35 - Last mean reward per episode: -0.39
Num timesteps: 2958000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 2961000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4.01       |
|    ep_rew_mean          | -0.391     |
| time/                   |            |
|    fps                  | 199        |
|    iterations           | 241        |
|    time_elapsed         | 14823      |
|    total_timesteps      | 2961408    |
| train/                  |            |
|    approx_kl            | 0.46874616 |
|    clip_fraction        | 0.829      |
|    clip_range           | 0.075      |
|    entropy_loss         | 1.26       |
|    explained_variance   | 0.998      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.041      |
|    n_updates            | 2880       |
|    policy_gradient_loss | 0.0466     |
|    std                  | 0.229      |
|    value_loss           | 3.99e-05   |
----------------------------------------
Num timesteps: 2964000
Best mean reward: -0.35 - Last mean reward per episode: -0.39
Num timesteps: 2967000
Best mean reward: -0.35 - Last mean reward per episode: -0.39
Num timesteps: 2970000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 2973000
Best mean reward: -0.35 - Last mean reward per episode: -0.39
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4.01       |
|    ep_rew_mean          | -0.39      |
| time/                   |            |
|    fps                  | 199        |
|    iterations           | 242        |
|    time_elapsed         | 14890      |
|    total_timesteps      | 2973696    |
| train/                  |            |
|    approx_kl            | 0.65815765 |
|    clip_fraction        | 0.825      |
|    clip_range           | 0.075      |
|    entropy_loss         | 1.28       |
|    explained_variance   | 0.998      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0438     |
|    n_updates            | 2892       |
|    policy_gradient_loss | 0.046      |
|    std                  | 0.229      |
|    value_loss           | 3.15e-05   |
----------------------------------------
Num timesteps: 2976000
Best mean reward: -0.35 - Last mean reward per episode: -0.39
Num timesteps: 2979000
Best mean reward: -0.35 - Last mean reward per episode: -0.39
Num timesteps: 2982000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 2985000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.01      |
|    ep_rew_mean          | -0.388    |
| time/                   |           |
|    fps                  | 199       |
|    iterations           | 243       |
|    time_elapsed         | 14956     |
|    total_timesteps      | 2985984   |
| train/                  |           |
|    approx_kl            | 0.4242438 |
|    clip_fraction        | 0.819     |
|    clip_range           | 0.075     |
|    entropy_loss         | 1.29      |
|    explained_variance   | 0.998     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0221    |
|    n_updates            | 2904      |
|    policy_gradient_loss | 0.0435    |
|    std                  | 0.227     |
|    value_loss           | 3.23e-05  |
---------------------------------------
Num timesteps: 2988000
Best mean reward: -0.35 - Last mean reward per episode: -0.39
Num timesteps: 2991000
Best mean reward: -0.35 - Last mean reward per episode: -0.39
Num timesteps: 2994000
Best mean reward: -0.35 - Last mean reward per episode: -0.39
Num timesteps: 2997000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.383     |
| time/                   |            |
|    fps                  | 199        |
|    iterations           | 244        |
|    time_elapsed         | 14995      |
|    total_timesteps      | 2998272    |
| train/                  |            |
|    approx_kl            | 0.41217127 |
|    clip_fraction        | 0.818      |
|    clip_range           | 0.075      |
|    entropy_loss         | 1.31       |
|    explained_variance   | 0.998      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0552     |
|    n_updates            | 2916       |
|    policy_gradient_loss | 0.0449     |
|    std                  | 0.226      |
|    value_loss           | 2.85e-05   |
----------------------------------------
Num timesteps: 3000000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 3003000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3006000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 3009000
Best mean reward: -0.35 - Last mean reward per episode: -0.39
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.375     |
| time/                   |            |
|    fps                  | 200        |
|    iterations           | 245        |
|    time_elapsed         | 15037      |
|    total_timesteps      | 3010560    |
| train/                  |            |
|    approx_kl            | 0.37527514 |
|    clip_fraction        | 0.82       |
|    clip_range           | 0.075      |
|    entropy_loss         | 1.33       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0324     |
|    n_updates            | 2928       |
|    policy_gradient_loss | 0.0481     |
|    std                  | 0.225      |
|    value_loss           | 2.48e-05   |
----------------------------------------
Num timesteps: 3012000
Best mean reward: -0.35 - Last mean reward per episode: -0.39
Num timesteps: 3015000
Best mean reward: -0.35 - Last mean reward per episode: -0.39
Num timesteps: 3018000
Best mean reward: -0.35 - Last mean reward per episode: -0.39
Num timesteps: 3021000
Best mean reward: -0.35 - Last mean reward per episode: -0.39
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.391     |
| time/                   |            |
|    fps                  | 200        |
|    iterations           | 246        |
|    time_elapsed         | 15104      |
|    total_timesteps      | 3022848    |
| train/                  |            |
|    approx_kl            | 0.74220854 |
|    clip_fraction        | 0.83       |
|    clip_range           | 0.075      |
|    entropy_loss         | 1.35       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0117     |
|    n_updates            | 2940       |
|    policy_gradient_loss | 0.0504     |
|    std                  | 0.224      |
|    value_loss           | 2.2e-05    |
----------------------------------------
Num timesteps: 3024000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 3027000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 3030000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 3033000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.381    |
| time/                   |           |
|    fps                  | 200       |
|    iterations           | 247       |
|    time_elapsed         | 15158     |
|    total_timesteps      | 3035136   |
| train/                  |           |
|    approx_kl            | 0.5600782 |
|    clip_fraction        | 0.82      |
|    clip_range           | 0.075     |
|    entropy_loss         | 1.38      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0254    |
|    n_updates            | 2952      |
|    policy_gradient_loss | 0.056     |
|    std                  | 0.223     |
|    value_loss           | 2.06e-05  |
---------------------------------------
Num timesteps: 3036000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 3039000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3042000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3045000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.384    |
| time/                   |           |
|    fps                  | 200       |
|    iterations           | 248       |
|    time_elapsed         | 15219     |
|    total_timesteps      | 3047424   |
| train/                  |           |
|    approx_kl            | 0.6217172 |
|    clip_fraction        | 0.806     |
|    clip_range           | 0.075     |
|    entropy_loss         | 1.41      |
|    explained_variance   | 0.998     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0219    |
|    n_updates            | 2964      |
|    policy_gradient_loss | 0.0434    |
|    std                  | 0.222     |
|    value_loss           | 3.26e-05  |
---------------------------------------
Num timesteps: 3048000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 3051000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 3054000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3057000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.373    |
| time/                   |           |
|    fps                  | 200       |
|    iterations           | 249       |
|    time_elapsed         | 15269     |
|    total_timesteps      | 3059712   |
| train/                  |           |
|    approx_kl            | 0.6720136 |
|    clip_fraction        | 0.813     |
|    clip_range           | 0.075     |
|    entropy_loss         | 1.43      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0203    |
|    n_updates            | 2976      |
|    policy_gradient_loss | 0.0512    |
|    std                  | 0.222     |
|    value_loss           | 2.34e-05  |
---------------------------------------
Num timesteps: 3060000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 3063000
Best mean reward: -0.35 - Last mean reward per episode: -0.39
Num timesteps: 3066000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 3069000
Best mean reward: -0.35 - Last mean reward per episode: -0.39
Num timesteps: 3072000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.374     |
| time/                   |            |
|    fps                  | 200        |
|    iterations           | 250        |
|    time_elapsed         | 15315      |
|    total_timesteps      | 3072000    |
| train/                  |            |
|    approx_kl            | 0.72111607 |
|    clip_fraction        | 0.843      |
|    clip_range           | 0.075      |
|    entropy_loss         | 1.44       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0993     |
|    n_updates            | 2988       |
|    policy_gradient_loss | 0.0806     |
|    std                  | 0.222      |
|    value_loss           | 0.000444   |
----------------------------------------
Num timesteps: 3075000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3078000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3081000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3084000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.375    |
| time/                   |           |
|    fps                  | 200       |
|    iterations           | 251       |
|    time_elapsed         | 15385     |
|    total_timesteps      | 3084288   |
| train/                  |           |
|    approx_kl            | 1.2582645 |
|    clip_fraction        | 0.792     |
|    clip_range           | 0.075     |
|    entropy_loss         | 1.45      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0372    |
|    n_updates            | 3000      |
|    policy_gradient_loss | 0.0472    |
|    std                  | 0.222     |
|    value_loss           | 2.09e-05  |
---------------------------------------
Num timesteps: 3087000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3090000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 3093000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3096000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.379     |
| time/                   |            |
|    fps                  | 200        |
|    iterations           | 252        |
|    time_elapsed         | 15460      |
|    total_timesteps      | 3096576    |
| train/                  |            |
|    approx_kl            | 0.55844086 |
|    clip_fraction        | 0.825      |
|    clip_range           | 0.075      |
|    entropy_loss         | 1.47       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0485     |
|    n_updates            | 3012       |
|    policy_gradient_loss | 0.0637     |
|    std                  | 0.22       |
|    value_loss           | 2.63e-05   |
----------------------------------------
Num timesteps: 3099000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3102000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 3105000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 3108000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.37     |
| time/                   |           |
|    fps                  | 200       |
|    iterations           | 253       |
|    time_elapsed         | 15521     |
|    total_timesteps      | 3108864   |
| train/                  |           |
|    approx_kl            | 1.0483427 |
|    clip_fraction        | 0.816     |
|    clip_range           | 0.075     |
|    entropy_loss         | 1.51      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0425    |
|    n_updates            | 3024      |
|    policy_gradient_loss | 0.0582    |
|    std                  | 0.221     |
|    value_loss           | 2.4e-05   |
---------------------------------------
Num timesteps: 3111000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 3114000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3117000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 3120000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.376    |
| time/                   |           |
|    fps                  | 200       |
|    iterations           | 254       |
|    time_elapsed         | 15585     |
|    total_timesteps      | 3121152   |
| train/                  |           |
|    approx_kl            | 0.8231333 |
|    clip_fraction        | 0.832     |
|    clip_range           | 0.075     |
|    entropy_loss         | 1.52      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0306    |
|    n_updates            | 3036      |
|    policy_gradient_loss | 0.0669    |
|    std                  | 0.22      |
|    value_loss           | 6.04e-05  |
---------------------------------------
Num timesteps: 3123000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3126000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 3129000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3132000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.372    |
| time/                   |           |
|    fps                  | 200       |
|    iterations           | 255       |
|    time_elapsed         | 15647     |
|    total_timesteps      | 3133440   |
| train/                  |           |
|    approx_kl            | 1.5672413 |
|    clip_fraction        | 0.826     |
|    clip_range           | 0.075     |
|    entropy_loss         | 1.56      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0422    |
|    n_updates            | 3048      |
|    policy_gradient_loss | 0.0633    |
|    std                  | 0.219     |
|    value_loss           | 1.81e-05  |
---------------------------------------
Num timesteps: 3135000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 3138000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3141000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3144000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.378     |
| time/                   |            |
|    fps                  | 200        |
|    iterations           | 256        |
|    time_elapsed         | 15710      |
|    total_timesteps      | 3145728    |
| train/                  |            |
|    approx_kl            | 0.56920934 |
|    clip_fraction        | 0.814      |
|    clip_range           | 0.075      |
|    entropy_loss         | 1.59       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0503     |
|    n_updates            | 3060       |
|    policy_gradient_loss | 0.0599     |
|    std                  | 0.219      |
|    value_loss           | 1.51e-05   |
----------------------------------------
Num timesteps: 3147000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 3150000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 3153000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 3156000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.371     |
| time/                   |            |
|    fps                  | 200        |
|    iterations           | 257        |
|    time_elapsed         | 15776      |
|    total_timesteps      | 3158016    |
| train/                  |            |
|    approx_kl            | 0.63910264 |
|    clip_fraction        | 0.838      |
|    clip_range           | 0.075      |
|    entropy_loss         | 1.62       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0591     |
|    n_updates            | 3072       |
|    policy_gradient_loss | 0.0708     |
|    std                  | 0.218      |
|    value_loss           | 0.000142   |
----------------------------------------
Num timesteps: 3159000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3162000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 3165000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 3168000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.387    |
| time/                   |           |
|    fps                  | 200       |
|    iterations           | 258       |
|    time_elapsed         | 15848     |
|    total_timesteps      | 3170304   |
| train/                  |           |
|    approx_kl            | 0.7373199 |
|    clip_fraction        | 0.81      |
|    clip_range           | 0.075     |
|    entropy_loss         | 1.65      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0393    |
|    n_updates            | 3084      |
|    policy_gradient_loss | 0.0492    |
|    std                  | 0.217     |
|    value_loss           | 1.69e-05  |
---------------------------------------
Num timesteps: 3171000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3174000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3177000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3180000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.37      |
| time/                   |            |
|    fps                  | 200        |
|    iterations           | 259        |
|    time_elapsed         | 15912      |
|    total_timesteps      | 3182592    |
| train/                  |            |
|    approx_kl            | 0.92721796 |
|    clip_fraction        | 0.822      |
|    clip_range           | 0.075      |
|    entropy_loss         | 1.69       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0697     |
|    n_updates            | 3096       |
|    policy_gradient_loss | 0.0619     |
|    std                  | 0.215      |
|    value_loss           | 4.55e-05   |
----------------------------------------
Num timesteps: 3183000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 3186000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 3189000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3192000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 3.99      |
|    ep_rew_mean          | -0.373    |
| time/                   |           |
|    fps                  | 200       |
|    iterations           | 260       |
|    time_elapsed         | 15954     |
|    total_timesteps      | 3194880   |
| train/                  |           |
|    approx_kl            | 0.8184735 |
|    clip_fraction        | 0.835     |
|    clip_range           | 0.075     |
|    entropy_loss         | 1.72      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0681    |
|    n_updates            | 3108      |
|    policy_gradient_loss | 0.0838    |
|    std                  | 0.214     |
|    value_loss           | 4.96e-05  |
---------------------------------------
Num timesteps: 3195000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3198000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3201000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3204000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 3207000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.362     |
| time/                   |            |
|    fps                  | 200        |
|    iterations           | 261        |
|    time_elapsed         | 16002      |
|    total_timesteps      | 3207168    |
| train/                  |            |
|    approx_kl            | 0.79711777 |
|    clip_fraction        | 0.849      |
|    clip_range           | 0.075      |
|    entropy_loss         | 1.74       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.058      |
|    n_updates            | 3120       |
|    policy_gradient_loss | 0.0934     |
|    std                  | 0.213      |
|    value_loss           | 0.000512   |
----------------------------------------
Num timesteps: 3210000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3213000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3216000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3219000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.367    |
| time/                   |           |
|    fps                  | 200       |
|    iterations           | 262       |
|    time_elapsed         | 16068     |
|    total_timesteps      | 3219456   |
| train/                  |           |
|    approx_kl            | 0.6891406 |
|    clip_fraction        | 0.832     |
|    clip_range           | 0.075     |
|    entropy_loss         | 1.79      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0641    |
|    n_updates            | 3132      |
|    policy_gradient_loss | 0.0739    |
|    std                  | 0.211     |
|    value_loss           | 1.42e-05  |
---------------------------------------
Num timesteps: 3222000
Best mean reward: -0.35 - Last mean reward per episode: -0.39
Num timesteps: 3225000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3228000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 3231000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.01      |
|    ep_rew_mean          | -0.367    |
| time/                   |           |
|    fps                  | 200       |
|    iterations           | 263       |
|    time_elapsed         | 16117     |
|    total_timesteps      | 3231744   |
| train/                  |           |
|    approx_kl            | 4.2858467 |
|    clip_fraction        | 0.865     |
|    clip_range           | 0.075     |
|    entropy_loss         | 1.81      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0847    |
|    n_updates            | 3144      |
|    policy_gradient_loss | 0.111     |
|    std                  | 0.211     |
|    value_loss           | 1.32e-05  |
---------------------------------------
Num timesteps: 3234000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3237000
Best mean reward: -0.35 - Last mean reward per episode: -0.39
Num timesteps: 3240000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3243000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.01      |
|    ep_rew_mean          | -0.375    |
| time/                   |           |
|    fps                  | 200       |
|    iterations           | 264       |
|    time_elapsed         | 16182     |
|    total_timesteps      | 3244032   |
| train/                  |           |
|    approx_kl            | 4.2086034 |
|    clip_fraction        | 0.883     |
|    clip_range           | 0.075     |
|    entropy_loss         | 1.82      |
|    explained_variance   | 0.976     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0398    |
|    n_updates            | 3156      |
|    policy_gradient_loss | 0.0567    |
|    std                  | 0.21      |
|    value_loss           | 0.00193   |
---------------------------------------
Num timesteps: 3246000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3249000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 3252000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3255000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.01      |
|    ep_rew_mean          | -0.372    |
| time/                   |           |
|    fps                  | 200       |
|    iterations           | 265       |
|    time_elapsed         | 16245     |
|    total_timesteps      | 3256320   |
| train/                  |           |
|    approx_kl            | 1.6254363 |
|    clip_fraction        | 0.876     |
|    clip_range           | 0.075     |
|    entropy_loss         | 1.8       |
|    explained_variance   | 0.996     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0557    |
|    n_updates            | 3168      |
|    policy_gradient_loss | 0.0975    |
|    std                  | 0.21      |
|    value_loss           | 5e-05     |
---------------------------------------
Num timesteps: 3258000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3261000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 3264000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3267000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 3.99      |
|    ep_rew_mean          | -0.365    |
| time/                   |           |
|    fps                  | 200       |
|    iterations           | 266       |
|    time_elapsed         | 16295     |
|    total_timesteps      | 3268608   |
| train/                  |           |
|    approx_kl            | 1.0583088 |
|    clip_fraction        | 0.864     |
|    clip_range           | 0.075     |
|    entropy_loss         | 1.76      |
|    explained_variance   | 0.998     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0627    |
|    n_updates            | 3180      |
|    policy_gradient_loss | 0.0957    |
|    std                  | 0.21      |
|    value_loss           | 2.36e-05  |
---------------------------------------
Num timesteps: 3270000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3273000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3276000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3279000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.01      |
|    ep_rew_mean          | -0.371    |
| time/                   |           |
|    fps                  | 200       |
|    iterations           | 267       |
|    time_elapsed         | 16358     |
|    total_timesteps      | 3280896   |
| train/                  |           |
|    approx_kl            | 1.2869917 |
|    clip_fraction        | 0.844     |
|    clip_range           | 0.075     |
|    entropy_loss         | 1.76      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0771    |
|    n_updates            | 3192      |
|    policy_gradient_loss | 0.0816    |
|    std                  | 0.21      |
|    value_loss           | 1.64e-05  |
---------------------------------------
Num timesteps: 3282000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3285000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3288000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 3291000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.01      |
|    ep_rew_mean          | -0.369    |
| time/                   |           |
|    fps                  | 200       |
|    iterations           | 268       |
|    time_elapsed         | 16428     |
|    total_timesteps      | 3293184   |
| train/                  |           |
|    approx_kl            | 0.5010592 |
|    clip_fraction        | 0.825     |
|    clip_range           | 0.075     |
|    entropy_loss         | 1.79      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0302    |
|    n_updates            | 3204      |
|    policy_gradient_loss | 0.0646    |
|    std                  | 0.209     |
|    value_loss           | 1.97e-05  |
---------------------------------------
Num timesteps: 3294000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 3297000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 3300000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 3303000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.02      |
|    ep_rew_mean          | -0.378    |
| time/                   |           |
|    fps                  | 200       |
|    iterations           | 269       |
|    time_elapsed         | 16495     |
|    total_timesteps      | 3305472   |
| train/                  |           |
|    approx_kl            | 0.5922821 |
|    clip_fraction        | 0.835     |
|    clip_range           | 0.075     |
|    entropy_loss         | 1.84      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0646    |
|    n_updates            | 3216      |
|    policy_gradient_loss | 0.0732    |
|    std                  | 0.207     |
|    value_loss           | 1.64e-05  |
---------------------------------------
Num timesteps: 3306000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3309000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 3312000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3315000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.37      |
| time/                   |            |
|    fps                  | 200        |
|    iterations           | 270        |
|    time_elapsed         | 16559      |
|    total_timesteps      | 3317760    |
| train/                  |            |
|    approx_kl            | 0.89474875 |
|    clip_fraction        | 0.839      |
|    clip_range           | 0.075      |
|    entropy_loss         | 1.87       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0649     |
|    n_updates            | 3228       |
|    policy_gradient_loss | 0.0867     |
|    std                  | 0.206      |
|    value_loss           | 1.63e-05   |
----------------------------------------
Num timesteps: 3318000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3321000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3324000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3327000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3330000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.372    |
| time/                   |           |
|    fps                  | 200       |
|    iterations           | 271       |
|    time_elapsed         | 16621     |
|    total_timesteps      | 3330048   |
| train/                  |           |
|    approx_kl            | 0.9597352 |
|    clip_fraction        | 0.83      |
|    clip_range           | 0.075     |
|    entropy_loss         | 1.9       |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0317    |
|    n_updates            | 3240      |
|    policy_gradient_loss | 0.0634    |
|    std                  | 0.204     |
|    value_loss           | 1.73e-05  |
---------------------------------------
Num timesteps: 3333000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3336000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3339000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3342000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.373    |
| time/                   |           |
|    fps                  | 200       |
|    iterations           | 272       |
|    time_elapsed         | 16670     |
|    total_timesteps      | 3342336   |
| train/                  |           |
|    approx_kl            | 0.7501533 |
|    clip_fraction        | 0.847     |
|    clip_range           | 0.075     |
|    entropy_loss         | 1.92      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0437    |
|    n_updates            | 3252      |
|    policy_gradient_loss | 0.0898    |
|    std                  | 0.203     |
|    value_loss           | 1.29e-05  |
---------------------------------------
Num timesteps: 3345000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3348000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3351000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3354000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 4.01     |
|    ep_rew_mean          | -0.389   |
| time/                   |          |
|    fps                  | 200      |
|    iterations           | 273      |
|    time_elapsed         | 16730    |
|    total_timesteps      | 3354624  |
| train/                  |          |
|    approx_kl            | 1.225168 |
|    clip_fraction        | 0.849    |
|    clip_range           | 0.075    |
|    entropy_loss         | 1.96     |
|    explained_variance   | 0.999    |
|    learning_rate        | 0.000512 |
|    loss                 | 0.0536   |
|    n_updates            | 3264     |
|    policy_gradient_loss | 0.081    |
|    std                  | 0.202    |
|    value_loss           | 1.37e-05 |
--------------------------------------
Num timesteps: 3357000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 3360000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3363000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 3366000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.369    |
| time/                   |           |
|    fps                  | 200       |
|    iterations           | 274       |
|    time_elapsed         | 16784     |
|    total_timesteps      | 3366912   |
| train/                  |           |
|    approx_kl            | 3.4358284 |
|    clip_fraction        | 0.848     |
|    clip_range           | 0.075     |
|    entropy_loss         | 1.99      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0514    |
|    n_updates            | 3276      |
|    policy_gradient_loss | 0.0892    |
|    std                  | 0.201     |
|    value_loss           | 1.48e-05  |
---------------------------------------
Num timesteps: 3369000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 3372000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3375000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 3378000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 4        |
|    ep_rew_mean          | -0.363   |
| time/                   |          |
|    fps                  | 200      |
|    iterations           | 275      |
|    time_elapsed         | 16840    |
|    total_timesteps      | 3379200  |
| train/                  |          |
|    approx_kl            | 1.071833 |
|    clip_fraction        | 0.841    |
|    clip_range           | 0.075    |
|    entropy_loss         | 2.02     |
|    explained_variance   | 0.999    |
|    learning_rate        | 0.000512 |
|    loss                 | 0.0656   |
|    n_updates            | 3288     |
|    policy_gradient_loss | 0.0749   |
|    std                  | 0.2      |
|    value_loss           | 1.68e-05 |
--------------------------------------
Num timesteps: 3381000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3384000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 3387000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 3390000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.369    |
| time/                   |           |
|    fps                  | 200       |
|    iterations           | 276       |
|    time_elapsed         | 16887     |
|    total_timesteps      | 3391488   |
| train/                  |           |
|    approx_kl            | 1.2965595 |
|    clip_fraction        | 0.845     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.05      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0914    |
|    n_updates            | 3300      |
|    policy_gradient_loss | 0.0758    |
|    std                  | 0.199     |
|    value_loss           | 1.23e-05  |
---------------------------------------
Num timesteps: 3393000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3396000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 3399000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 3402000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.37     |
| time/                   |           |
|    fps                  | 201       |
|    iterations           | 277       |
|    time_elapsed         | 16928     |
|    total_timesteps      | 3403776   |
| train/                  |           |
|    approx_kl            | 0.8114447 |
|    clip_fraction        | 0.836     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.08      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0553    |
|    n_updates            | 3312      |
|    policy_gradient_loss | 0.0663    |
|    std                  | 0.199     |
|    value_loss           | 6.07e-05  |
---------------------------------------
Num timesteps: 3405000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3408000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3411000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3414000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.367    |
| time/                   |           |
|    fps                  | 201       |
|    iterations           | 278       |
|    time_elapsed         | 16973     |
|    total_timesteps      | 3416064   |
| train/                  |           |
|    approx_kl            | 1.4273325 |
|    clip_fraction        | 0.853     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.1       |
|    explained_variance   | 0.995     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0408    |
|    n_updates            | 3324      |
|    policy_gradient_loss | 0.0604    |
|    std                  | 0.198     |
|    value_loss           | 0.00492   |
---------------------------------------
Num timesteps: 3417000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3420000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3423000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3426000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.366    |
| time/                   |           |
|    fps                  | 201       |
|    iterations           | 279       |
|    time_elapsed         | 17025     |
|    total_timesteps      | 3428352   |
| train/                  |           |
|    approx_kl            | 0.9822068 |
|    clip_fraction        | 0.85      |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.11      |
|    explained_variance   | 0.998     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0607    |
|    n_updates            | 3336      |
|    policy_gradient_loss | 0.0692    |
|    std                  | 0.197     |
|    value_loss           | 2.74e-05  |
---------------------------------------
Num timesteps: 3429000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 3432000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3435000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 3438000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.01      |
|    ep_rew_mean          | -0.374    |
| time/                   |           |
|    fps                  | 201       |
|    iterations           | 280       |
|    time_elapsed         | 17081     |
|    total_timesteps      | 3440640   |
| train/                  |           |
|    approx_kl            | 1.6042465 |
|    clip_fraction        | 0.845     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.1       |
|    explained_variance   | 0.994     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0692    |
|    n_updates            | 3348      |
|    policy_gradient_loss | 0.0643    |
|    std                  | 0.198     |
|    value_loss           | 0.00178   |
---------------------------------------
Num timesteps: 3441000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 3444000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 3447000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 3450000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 4.01     |
|    ep_rew_mean          | -0.373   |
| time/                   |          |
|    fps                  | 201      |
|    iterations           | 281      |
|    time_elapsed         | 17159    |
|    total_timesteps      | 3452928  |
| train/                  |          |
|    approx_kl            | 1.812603 |
|    clip_fraction        | 0.851    |
|    clip_range           | 0.075    |
|    entropy_loss         | 2.11     |
|    explained_variance   | 0.997    |
|    learning_rate        | 0.000512 |
|    loss                 | 0.0797   |
|    n_updates            | 3360     |
|    policy_gradient_loss | 0.0765   |
|    std                  | 0.197    |
|    value_loss           | 7.37e-05 |
--------------------------------------
Num timesteps: 3453000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3456000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3459000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3462000
Best mean reward: -0.35 - Last mean reward per episode: -0.39
Num timesteps: 3465000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.378    |
| time/                   |           |
|    fps                  | 201       |
|    iterations           | 282       |
|    time_elapsed         | 17230     |
|    total_timesteps      | 3465216   |
| train/                  |           |
|    approx_kl            | 6.0897865 |
|    clip_fraction        | 0.892     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.1       |
|    explained_variance   | 0.998     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0509    |
|    n_updates            | 3372      |
|    policy_gradient_loss | 0.0912    |
|    std                  | 0.196     |
|    value_loss           | 2.06e-05  |
---------------------------------------
Num timesteps: 3468000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 3471000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3474000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3477000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.386    |
| time/                   |           |
|    fps                  | 201       |
|    iterations           | 283       |
|    time_elapsed         | 17279     |
|    total_timesteps      | 3477504   |
| train/                  |           |
|    approx_kl            | 1.3499284 |
|    clip_fraction        | 0.854     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.09      |
|    explained_variance   | 0.998     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0503    |
|    n_updates            | 3384      |
|    policy_gradient_loss | 0.0755    |
|    std                  | 0.196     |
|    value_loss           | 2.49e-05  |
---------------------------------------
Num timesteps: 3480000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 3483000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3486000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3489000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.368    |
| time/                   |           |
|    fps                  | 201       |
|    iterations           | 284       |
|    time_elapsed         | 17348     |
|    total_timesteps      | 3489792   |
| train/                  |           |
|    approx_kl            | 1.4064107 |
|    clip_fraction        | 0.839     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.08      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.103     |
|    n_updates            | 3396      |
|    policy_gradient_loss | 0.0575    |
|    std                  | 0.196     |
|    value_loss           | 1.85e-05  |
---------------------------------------
Num timesteps: 3492000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3495000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 3498000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3501000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.01      |
|    ep_rew_mean          | -0.363    |
| time/                   |           |
|    fps                  | 201       |
|    iterations           | 285       |
|    time_elapsed         | 17409     |
|    total_timesteps      | 3502080   |
| train/                  |           |
|    approx_kl            | 1.1251621 |
|    clip_fraction        | 0.872     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.06      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0464    |
|    n_updates            | 3408      |
|    policy_gradient_loss | 0.0886    |
|    std                  | 0.197     |
|    value_loss           | 1.57e-05  |
---------------------------------------
Num timesteps: 3504000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 3507000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3510000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 3513000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.375    |
| time/                   |           |
|    fps                  | 201       |
|    iterations           | 286       |
|    time_elapsed         | 17461     |
|    total_timesteps      | 3514368   |
| train/                  |           |
|    approx_kl            | 1.7775187 |
|    clip_fraction        | 0.874     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.06      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0606    |
|    n_updates            | 3420      |
|    policy_gradient_loss | 0.0934    |
|    std                  | 0.197     |
|    value_loss           | 1.66e-05  |
---------------------------------------
Num timesteps: 3516000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3519000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 3522000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3525000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.374    |
| time/                   |           |
|    fps                  | 201       |
|    iterations           | 287       |
|    time_elapsed         | 17533     |
|    total_timesteps      | 3526656   |
| train/                  |           |
|    approx_kl            | 1.4867678 |
|    clip_fraction        | 0.851     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.06      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0478    |
|    n_updates            | 3432      |
|    policy_gradient_loss | 0.0762    |
|    std                  | 0.196     |
|    value_loss           | 1.55e-05  |
---------------------------------------
Num timesteps: 3528000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 3531000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 3534000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3537000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.02      |
|    ep_rew_mean          | -0.359    |
| time/                   |           |
|    fps                  | 201       |
|    iterations           | 288       |
|    time_elapsed         | 17599     |
|    total_timesteps      | 3538944   |
| train/                  |           |
|    approx_kl            | 0.7253892 |
|    clip_fraction        | 0.858     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.07      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0724    |
|    n_updates            | 3444      |
|    policy_gradient_loss | 0.0981    |
|    std                  | 0.195     |
|    value_loss           | 1.51e-05  |
---------------------------------------
Num timesteps: 3540000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3543000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3546000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 3549000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.01      |
|    ep_rew_mean          | -0.372    |
| time/                   |           |
|    fps                  | 201       |
|    iterations           | 289       |
|    time_elapsed         | 17650     |
|    total_timesteps      | 3551232   |
| train/                  |           |
|    approx_kl            | 0.7533679 |
|    clip_fraction        | 0.85      |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.07      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.178     |
|    n_updates            | 3456      |
|    policy_gradient_loss | 0.0773    |
|    std                  | 0.195     |
|    value_loss           | 1.62e-05  |
---------------------------------------
Num timesteps: 3552000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3555000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3558000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3561000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.366     |
| time/                   |            |
|    fps                  | 201        |
|    iterations           | 290        |
|    time_elapsed         | 17712      |
|    total_timesteps      | 3563520    |
| train/                  |            |
|    approx_kl            | 0.74428105 |
|    clip_fraction        | 0.842      |
|    clip_range           | 0.075      |
|    entropy_loss         | 2.08       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0589     |
|    n_updates            | 3468       |
|    policy_gradient_loss | 0.0985     |
|    std                  | 0.194      |
|    value_loss           | 1.65e-05   |
----------------------------------------
Num timesteps: 3564000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3567000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 3570000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 3573000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.378    |
| time/                   |           |
|    fps                  | 201       |
|    iterations           | 291       |
|    time_elapsed         | 17785     |
|    total_timesteps      | 3575808   |
| train/                  |           |
|    approx_kl            | 1.5310045 |
|    clip_fraction        | 0.854     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.09      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0775    |
|    n_updates            | 3480      |
|    policy_gradient_loss | 0.087     |
|    std                  | 0.195     |
|    value_loss           | 1.74e-05  |
---------------------------------------
Num timesteps: 3576000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 3579000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 3582000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 3585000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3588000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.375    |
| time/                   |           |
|    fps                  | 200       |
|    iterations           | 292       |
|    time_elapsed         | 17855     |
|    total_timesteps      | 3588096   |
| train/                  |           |
|    approx_kl            | 0.5657132 |
|    clip_fraction        | 0.832     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.09      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0831    |
|    n_updates            | 3492      |
|    policy_gradient_loss | 0.0685    |
|    std                  | 0.194     |
|    value_loss           | 1.22e-05  |
---------------------------------------
Num timesteps: 3591000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3594000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 3597000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3600000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.373    |
| time/                   |           |
|    fps                  | 200       |
|    iterations           | 293       |
|    time_elapsed         | 17928     |
|    total_timesteps      | 3600384   |
| train/                  |           |
|    approx_kl            | 1.1856457 |
|    clip_fraction        | 0.844     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.14      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0697    |
|    n_updates            | 3504      |
|    policy_gradient_loss | 0.0791    |
|    std                  | 0.193     |
|    value_loss           | 1.21e-05  |
---------------------------------------
Num timesteps: 3603000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 3606000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3609000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3612000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.381     |
| time/                   |            |
|    fps                  | 200        |
|    iterations           | 294        |
|    time_elapsed         | 17993      |
|    total_timesteps      | 3612672    |
| train/                  |            |
|    approx_kl            | 0.72520703 |
|    clip_fraction        | 0.846      |
|    clip_range           | 0.075      |
|    entropy_loss         | 2.16       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0506     |
|    n_updates            | 3516       |
|    policy_gradient_loss | 0.0762     |
|    std                  | 0.193      |
|    value_loss           | 2.32e-05   |
----------------------------------------
Num timesteps: 3615000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 3618000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 3621000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 3624000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 3.99      |
|    ep_rew_mean          | -0.377    |
| time/                   |           |
|    fps                  | 200       |
|    iterations           | 295       |
|    time_elapsed         | 18064     |
|    total_timesteps      | 3624960   |
| train/                  |           |
|    approx_kl            | 0.6126907 |
|    clip_fraction        | 0.831     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.19      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0365    |
|    n_updates            | 3528      |
|    policy_gradient_loss | 0.0727    |
|    std                  | 0.192     |
|    value_loss           | 1.89e-05  |
---------------------------------------
Num timesteps: 3627000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3630000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 3633000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3636000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.359    |
| time/                   |           |
|    fps                  | 200       |
|    iterations           | 296       |
|    time_elapsed         | 18119     |
|    total_timesteps      | 3637248   |
| train/                  |           |
|    approx_kl            | 1.3605455 |
|    clip_fraction        | 0.858     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.2       |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0594    |
|    n_updates            | 3540      |
|    policy_gradient_loss | 0.0987    |
|    std                  | 0.193     |
|    value_loss           | 0.000114  |
---------------------------------------
Num timesteps: 3639000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 3642000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 3645000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 3648000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.368    |
| time/                   |           |
|    fps                  | 200       |
|    iterations           | 297       |
|    time_elapsed         | 18182     |
|    total_timesteps      | 3649536   |
| train/                  |           |
|    approx_kl            | 1.3333815 |
|    clip_fraction        | 0.831     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.22      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0458    |
|    n_updates            | 3552      |
|    policy_gradient_loss | 0.0588    |
|    std                  | 0.192     |
|    value_loss           | 0.00166   |
---------------------------------------
Num timesteps: 3651000
Best mean reward: -0.35 - Last mean reward per episode: -0.39
Num timesteps: 3654000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 3657000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 3660000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.35     |
| time/                   |           |
|    fps                  | 200       |
|    iterations           | 298       |
|    time_elapsed         | 18247     |
|    total_timesteps      | 3661824   |
| train/                  |           |
|    approx_kl            | 1.0533758 |
|    clip_fraction        | 0.849     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.23      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.068     |
|    n_updates            | 3564      |
|    policy_gradient_loss | 0.0688    |
|    std                  | 0.192     |
|    value_loss           | 1.44e-05  |
---------------------------------------
Num timesteps: 3663000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 3666000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 3669000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3672000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.374    |
| time/                   |           |
|    fps                  | 200       |
|    iterations           | 299       |
|    time_elapsed         | 18303     |
|    total_timesteps      | 3674112   |
| train/                  |           |
|    approx_kl            | 2.2747269 |
|    clip_fraction        | 0.851     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.26      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0556    |
|    n_updates            | 3576      |
|    policy_gradient_loss | 0.0814    |
|    std                  | 0.192     |
|    value_loss           | 1.16e-05  |
---------------------------------------
Num timesteps: 3675000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3678000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 3681000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3684000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 3.98      |
|    ep_rew_mean          | -0.37     |
| time/                   |           |
|    fps                  | 200       |
|    iterations           | 300       |
|    time_elapsed         | 18369     |
|    total_timesteps      | 3686400   |
| train/                  |           |
|    approx_kl            | 1.3341414 |
|    clip_fraction        | 0.863     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.27      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.043     |
|    n_updates            | 3588      |
|    policy_gradient_loss | 0.12      |
|    std                  | 0.193     |
|    value_loss           | 1.17e-05  |
---------------------------------------
Num timesteps: 3687000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3690000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 3693000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3696000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 3.99      |
|    ep_rew_mean          | -0.383    |
| time/                   |           |
|    fps                  | 200       |
|    iterations           | 301       |
|    time_elapsed         | 18420     |
|    total_timesteps      | 3698688   |
| train/                  |           |
|    approx_kl            | 1.0681247 |
|    clip_fraction        | 0.852     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.28      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0742    |
|    n_updates            | 3600      |
|    policy_gradient_loss | 0.0904    |
|    std                  | 0.193     |
|    value_loss           | 1.12e-05  |
---------------------------------------
Num timesteps: 3699000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 3702000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 3705000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 3708000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.371    |
| time/                   |           |
|    fps                  | 200       |
|    iterations           | 302       |
|    time_elapsed         | 18466     |
|    total_timesteps      | 3710976   |
| train/                  |           |
|    approx_kl            | 1.1586065 |
|    clip_fraction        | 0.86      |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.27      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0556    |
|    n_updates            | 3612      |
|    policy_gradient_loss | 0.0867    |
|    std                  | 0.193     |
|    value_loss           | 1.33e-05  |
---------------------------------------
Num timesteps: 3711000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3714000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3717000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3720000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3723000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 4        |
|    ep_rew_mean          | -0.37    |
| time/                   |          |
|    fps                  | 201      |
|    iterations           | 303      |
|    time_elapsed         | 18512    |
|    total_timesteps      | 3723264  |
| train/                  |          |
|    approx_kl            | 1.665402 |
|    clip_fraction        | 0.863    |
|    clip_range           | 0.075    |
|    entropy_loss         | 2.28     |
|    explained_variance   | 0.999    |
|    learning_rate        | 0.000512 |
|    loss                 | 0.0674   |
|    n_updates            | 3624     |
|    policy_gradient_loss | 0.0859   |
|    std                  | 0.192    |
|    value_loss           | 1.74e-05 |
--------------------------------------
Num timesteps: 3726000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3729000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3732000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 3735000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 3.99      |
|    ep_rew_mean          | -0.376    |
| time/                   |           |
|    fps                  | 201       |
|    iterations           | 304       |
|    time_elapsed         | 18561     |
|    total_timesteps      | 3735552   |
| train/                  |           |
|    approx_kl            | 3.4214146 |
|    clip_fraction        | 0.859     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.31      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0818    |
|    n_updates            | 3636      |
|    policy_gradient_loss | 0.0929    |
|    std                  | 0.192     |
|    value_loss           | 1.05e-05  |
---------------------------------------
Num timesteps: 3738000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 3741000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3744000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 3747000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.373    |
| time/                   |           |
|    fps                  | 201       |
|    iterations           | 305       |
|    time_elapsed         | 18633     |
|    total_timesteps      | 3747840   |
| train/                  |           |
|    approx_kl            | 0.9416743 |
|    clip_fraction        | 0.842     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.34      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.028     |
|    n_updates            | 3648      |
|    policy_gradient_loss | 0.062     |
|    std                  | 0.191     |
|    value_loss           | 0.00118   |
---------------------------------------
Num timesteps: 3750000
Best mean reward: -0.35 - Last mean reward per episode: -0.35
Num timesteps: 3753000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3756000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3759000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 3.99      |
|    ep_rew_mean          | -0.363    |
| time/                   |           |
|    fps                  | 201       |
|    iterations           | 306       |
|    time_elapsed         | 18693     |
|    total_timesteps      | 3760128   |
| train/                  |           |
|    approx_kl            | 1.3326921 |
|    clip_fraction        | 0.856     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.35      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0439    |
|    n_updates            | 3660      |
|    policy_gradient_loss | 0.0831    |
|    std                  | 0.191     |
|    value_loss           | 5.48e-05  |
---------------------------------------
Num timesteps: 3762000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3765000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3768000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3771000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4.02       |
|    ep_rew_mean          | -0.365     |
| time/                   |            |
|    fps                  | 201        |
|    iterations           | 307        |
|    time_elapsed         | 18760      |
|    total_timesteps      | 3772416    |
| train/                  |            |
|    approx_kl            | 0.79147416 |
|    clip_fraction        | 0.856      |
|    clip_range           | 0.075      |
|    entropy_loss         | 2.36       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0443     |
|    n_updates            | 3672       |
|    policy_gradient_loss | 0.0764     |
|    std                  | 0.192      |
|    value_loss           | 1.94e-05   |
----------------------------------------
Num timesteps: 3774000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3777000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3780000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 3783000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.364     |
| time/                   |            |
|    fps                  | 201        |
|    iterations           | 308        |
|    time_elapsed         | 18823      |
|    total_timesteps      | 3784704    |
| train/                  |            |
|    approx_kl            | 0.85095215 |
|    clip_fraction        | 0.855      |
|    clip_range           | 0.075      |
|    entropy_loss         | 2.38       |
|    explained_variance   | 0.995      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0443     |
|    n_updates            | 3684       |
|    policy_gradient_loss | 0.0621     |
|    std                  | 0.191      |
|    value_loss           | 0.00207    |
----------------------------------------
Num timesteps: 3786000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3789000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3792000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3795000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.01      |
|    ep_rew_mean          | -0.381    |
| time/                   |           |
|    fps                  | 201       |
|    iterations           | 309       |
|    time_elapsed         | 18888     |
|    total_timesteps      | 3796992   |
| train/                  |           |
|    approx_kl            | 3.4750888 |
|    clip_fraction        | 0.888     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.36      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0587    |
|    n_updates            | 3696      |
|    policy_gradient_loss | 0.108     |
|    std                  | 0.192     |
|    value_loss           | 2.56e-05  |
---------------------------------------
Num timesteps: 3798000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3801000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 3804000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3807000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 3.99      |
|    ep_rew_mean          | -0.378    |
| time/                   |           |
|    fps                  | 201       |
|    iterations           | 310       |
|    time_elapsed         | 18936     |
|    total_timesteps      | 3809280   |
| train/                  |           |
|    approx_kl            | 2.5927298 |
|    clip_fraction        | 0.848     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.38      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0733    |
|    n_updates            | 3708      |
|    policy_gradient_loss | 0.0707    |
|    std                  | 0.192     |
|    value_loss           | 1.42e-05  |
---------------------------------------
Num timesteps: 3810000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 3813000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3816000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3819000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.371    |
| time/                   |           |
|    fps                  | 201       |
|    iterations           | 311       |
|    time_elapsed         | 19001     |
|    total_timesteps      | 3821568   |
| train/                  |           |
|    approx_kl            | 2.4779966 |
|    clip_fraction        | 0.855     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.38      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0761    |
|    n_updates            | 3720      |
|    policy_gradient_loss | 0.0927    |
|    std                  | 0.192     |
|    value_loss           | 1.28e-05  |
---------------------------------------
Num timesteps: 3822000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3825000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3828000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 3831000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.361    |
| time/                   |           |
|    fps                  | 201       |
|    iterations           | 312       |
|    time_elapsed         | 19059     |
|    total_timesteps      | 3833856   |
| train/                  |           |
|    approx_kl            | 1.2406396 |
|    clip_fraction        | 0.866     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.41      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0667    |
|    n_updates            | 3732      |
|    policy_gradient_loss | 0.087     |
|    std                  | 0.191     |
|    value_loss           | 1.37e-05  |
---------------------------------------
Num timesteps: 3834000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3837000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3840000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3843000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3846000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.376     |
| time/                   |            |
|    fps                  | 201        |
|    iterations           | 313        |
|    time_elapsed         | 19127      |
|    total_timesteps      | 3846144    |
| train/                  |            |
|    approx_kl            | 0.89022475 |
|    clip_fraction        | 0.854      |
|    clip_range           | 0.075      |
|    entropy_loss         | 2.44       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0826     |
|    n_updates            | 3744       |
|    policy_gradient_loss | 0.0757     |
|    std                  | 0.19       |
|    value_loss           | 1.22e-05   |
----------------------------------------
Num timesteps: 3849000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3852000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3855000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 3858000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.369    |
| time/                   |           |
|    fps                  | 201       |
|    iterations           | 314       |
|    time_elapsed         | 19178     |
|    total_timesteps      | 3858432   |
| train/                  |           |
|    approx_kl            | 1.1709772 |
|    clip_fraction        | 0.854     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.45      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0951    |
|    n_updates            | 3756      |
|    policy_gradient_loss | 0.0883    |
|    std                  | 0.19      |
|    value_loss           | 1.18e-05  |
---------------------------------------
Num timesteps: 3861000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 3864000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 3867000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3870000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.368    |
| time/                   |           |
|    fps                  | 201       |
|    iterations           | 315       |
|    time_elapsed         | 19243     |
|    total_timesteps      | 3870720   |
| train/                  |           |
|    approx_kl            | 0.8461914 |
|    clip_fraction        | 0.864     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.45      |
|    explained_variance   | 0.996     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0622    |
|    n_updates            | 3768      |
|    policy_gradient_loss | 0.078     |
|    std                  | 0.189     |
|    value_loss           | 0.0016    |
---------------------------------------
Num timesteps: 3873000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 3876000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 3879000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3882000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 3.99      |
|    ep_rew_mean          | -0.375    |
| time/                   |           |
|    fps                  | 201       |
|    iterations           | 316       |
|    time_elapsed         | 19299     |
|    total_timesteps      | 3883008   |
| train/                  |           |
|    approx_kl            | 2.2301686 |
|    clip_fraction        | 0.863     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.43      |
|    explained_variance   | 0.998     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0419    |
|    n_updates            | 3780      |
|    policy_gradient_loss | 0.0837    |
|    std                  | 0.189     |
|    value_loss           | 0.00012   |
---------------------------------------
Num timesteps: 3885000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 3888000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 3891000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3894000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.368    |
| time/                   |           |
|    fps                  | 201       |
|    iterations           | 317       |
|    time_elapsed         | 19348     |
|    total_timesteps      | 3895296   |
| train/                  |           |
|    approx_kl            | 3.0655515 |
|    clip_fraction        | 0.865     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.43      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.102     |
|    n_updates            | 3792      |
|    policy_gradient_loss | 0.0954    |
|    std                  | 0.19      |
|    value_loss           | 1.02e-05  |
---------------------------------------
Num timesteps: 3897000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3900000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3903000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 3906000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.374    |
| time/                   |           |
|    fps                  | 201       |
|    iterations           | 318       |
|    time_elapsed         | 19417     |
|    total_timesteps      | 3907584   |
| train/                  |           |
|    approx_kl            | 2.2760458 |
|    clip_fraction        | 0.852     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.42      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.035     |
|    n_updates            | 3804      |
|    policy_gradient_loss | 0.0673    |
|    std                  | 0.189     |
|    value_loss           | 1.28e-05  |
---------------------------------------
Num timesteps: 3909000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3912000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 3915000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3918000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.37      |
| time/                   |            |
|    fps                  | 201        |
|    iterations           | 319        |
|    time_elapsed         | 19468      |
|    total_timesteps      | 3919872    |
| train/                  |            |
|    approx_kl            | 0.74848765 |
|    clip_fraction        | 0.86       |
|    clip_range           | 0.075      |
|    entropy_loss         | 2.44       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0414     |
|    n_updates            | 3816       |
|    policy_gradient_loss | 0.0836     |
|    std                  | 0.19       |
|    value_loss           | 1.36e-05   |
----------------------------------------
Num timesteps: 3921000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 3924000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3927000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3930000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 3.99      |
|    ep_rew_mean          | -0.368    |
| time/                   |           |
|    fps                  | 201       |
|    iterations           | 320       |
|    time_elapsed         | 19543     |
|    total_timesteps      | 3932160   |
| train/                  |           |
|    approx_kl            | 2.0695608 |
|    clip_fraction        | 0.869     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.43      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0594    |
|    n_updates            | 3828      |
|    policy_gradient_loss | 0.0866    |
|    std                  | 0.19      |
|    value_loss           | 1.61e-05  |
---------------------------------------
Num timesteps: 3933000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 3936000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3939000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 3942000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 3.98      |
|    ep_rew_mean          | -0.363    |
| time/                   |           |
|    fps                  | 201       |
|    iterations           | 321       |
|    time_elapsed         | 19616     |
|    total_timesteps      | 3944448   |
| train/                  |           |
|    approx_kl            | 1.5052959 |
|    clip_fraction        | 0.875     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.42      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0801    |
|    n_updates            | 3840      |
|    policy_gradient_loss | 0.0907    |
|    std                  | 0.192     |
|    value_loss           | 1.87e-05  |
---------------------------------------
Num timesteps: 3945000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3948000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 3951000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 3954000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.358    |
| time/                   |           |
|    fps                  | 201       |
|    iterations           | 322       |
|    time_elapsed         | 19682     |
|    total_timesteps      | 3956736   |
| train/                  |           |
|    approx_kl            | 1.3163136 |
|    clip_fraction        | 0.867     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.41      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0894    |
|    n_updates            | 3852      |
|    policy_gradient_loss | 0.0902    |
|    std                  | 0.193     |
|    value_loss           | 1.38e-05  |
---------------------------------------
Num timesteps: 3957000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3960000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 3963000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3966000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3969000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.367    |
| time/                   |           |
|    fps                  | 201       |
|    iterations           | 323       |
|    time_elapsed         | 19741     |
|    total_timesteps      | 3969024   |
| train/                  |           |
|    approx_kl            | 1.0849255 |
|    clip_fraction        | 0.864     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.4       |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0652    |
|    n_updates            | 3864      |
|    policy_gradient_loss | 0.0778    |
|    std                  | 0.192     |
|    value_loss           | 1.94e-05  |
---------------------------------------
Num timesteps: 3972000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 3975000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3978000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3981000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.379     |
| time/                   |            |
|    fps                  | 201        |
|    iterations           | 324        |
|    time_elapsed         | 19785      |
|    total_timesteps      | 3981312    |
| train/                  |            |
|    approx_kl            | 0.95440656 |
|    clip_fraction        | 0.855      |
|    clip_range           | 0.075      |
|    entropy_loss         | 2.41       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0768     |
|    n_updates            | 3876       |
|    policy_gradient_loss | 0.086      |
|    std                  | 0.192      |
|    value_loss           | 9.96e-06   |
----------------------------------------
Num timesteps: 3984000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3987000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3990000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 3993000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.365     |
| time/                   |            |
|    fps                  | 201        |
|    iterations           | 325        |
|    time_elapsed         | 19833      |
|    total_timesteps      | 3993600    |
| train/                  |            |
|    approx_kl            | 0.83603525 |
|    clip_fraction        | 0.831      |
|    clip_range           | 0.075      |
|    entropy_loss         | 2.42       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0529     |
|    n_updates            | 3888       |
|    policy_gradient_loss | 0.064      |
|    std                  | 0.191      |
|    value_loss           | 1.31e-05   |
----------------------------------------
Num timesteps: 3996000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 3999000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 4002000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4005000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.363    |
| time/                   |           |
|    fps                  | 201       |
|    iterations           | 326       |
|    time_elapsed         | 19893     |
|    total_timesteps      | 4005888   |
| train/                  |           |
|    approx_kl            | 1.2549785 |
|    clip_fraction        | 0.862     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.42      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0743    |
|    n_updates            | 3900      |
|    policy_gradient_loss | 0.0923    |
|    std                  | 0.191     |
|    value_loss           | 1.22e-05  |
---------------------------------------
Num timesteps: 4008000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4011000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 4014000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4017000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.368    |
| time/                   |           |
|    fps                  | 201       |
|    iterations           | 327       |
|    time_elapsed         | 19949     |
|    total_timesteps      | 4018176   |
| train/                  |           |
|    approx_kl            | 1.0450573 |
|    clip_fraction        | 0.859     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.42      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0878    |
|    n_updates            | 3912      |
|    policy_gradient_loss | 0.0956    |
|    std                  | 0.191     |
|    value_loss           | 1.32e-05  |
---------------------------------------
Num timesteps: 4020000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4023000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 4026000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 4029000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.02      |
|    ep_rew_mean          | -0.378    |
| time/                   |           |
|    fps                  | 201       |
|    iterations           | 328       |
|    time_elapsed         | 19985     |
|    total_timesteps      | 4030464   |
| train/                  |           |
|    approx_kl            | 0.5553399 |
|    clip_fraction        | 0.838     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.44      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0477    |
|    n_updates            | 3924      |
|    policy_gradient_loss | 0.067     |
|    std                  | 0.191     |
|    value_loss           | 1.22e-05  |
---------------------------------------
Num timesteps: 4032000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 4035000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4038000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 4041000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 3.99      |
|    ep_rew_mean          | -0.376    |
| time/                   |           |
|    fps                  | 201       |
|    iterations           | 329       |
|    time_elapsed         | 20040     |
|    total_timesteps      | 4042752   |
| train/                  |           |
|    approx_kl            | 2.0585144 |
|    clip_fraction        | 0.854     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.46      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0915    |
|    n_updates            | 3936      |
|    policy_gradient_loss | 0.076     |
|    std                  | 0.19      |
|    value_loss           | 0.000876  |
---------------------------------------
Num timesteps: 4044000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4047000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 4050000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 4053000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.365    |
| time/                   |           |
|    fps                  | 201       |
|    iterations           | 330       |
|    time_elapsed         | 20105     |
|    total_timesteps      | 4055040   |
| train/                  |           |
|    approx_kl            | 2.3173907 |
|    clip_fraction        | 0.876     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.47      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0651    |
|    n_updates            | 3948      |
|    policy_gradient_loss | 0.0879    |
|    std                  | 0.19      |
|    value_loss           | 1.85e-05  |
---------------------------------------
Num timesteps: 4056000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 4059000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4062000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4065000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.374    |
| time/                   |           |
|    fps                  | 201       |
|    iterations           | 331       |
|    time_elapsed         | 20176     |
|    total_timesteps      | 4067328   |
| train/                  |           |
|    approx_kl            | 2.2402995 |
|    clip_fraction        | 0.867     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.5       |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0703    |
|    n_updates            | 3960      |
|    policy_gradient_loss | 0.0908    |
|    std                  | 0.188     |
|    value_loss           | 1.02e-05  |
---------------------------------------
Num timesteps: 4068000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4071000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 4074000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4077000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.37     |
| time/                   |           |
|    fps                  | 201       |
|    iterations           | 332       |
|    time_elapsed         | 20241     |
|    total_timesteps      | 4079616   |
| train/                  |           |
|    approx_kl            | 1.8662748 |
|    clip_fraction        | 0.86      |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.52      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0459    |
|    n_updates            | 3972      |
|    policy_gradient_loss | 0.0829    |
|    std                  | 0.187     |
|    value_loss           | 9.1e-06   |
---------------------------------------
Num timesteps: 4080000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 4083000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 4086000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4089000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 3.99      |
|    ep_rew_mean          | -0.385    |
| time/                   |           |
|    fps                  | 201       |
|    iterations           | 333       |
|    time_elapsed         | 20307     |
|    total_timesteps      | 4091904   |
| train/                  |           |
|    approx_kl            | 1.4588971 |
|    clip_fraction        | 0.862     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.54      |
|    explained_variance   | 1         |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0625    |
|    n_updates            | 3984      |
|    policy_gradient_loss | 0.0952    |
|    std                  | 0.187     |
|    value_loss           | 9.28e-06  |
---------------------------------------
Num timesteps: 4092000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 4095000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4098000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4101000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4104000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.364    |
| time/                   |           |
|    fps                  | 201       |
|    iterations           | 334       |
|    time_elapsed         | 20375     |
|    total_timesteps      | 4104192   |
| train/                  |           |
|    approx_kl            | 1.8908113 |
|    clip_fraction        | 0.861     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.55      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0611    |
|    n_updates            | 3996      |
|    policy_gradient_loss | 0.082     |
|    std                  | 0.186     |
|    value_loss           | 2.02e-05  |
---------------------------------------
Num timesteps: 4107000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4110000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4113000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 4116000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.36     |
| time/                   |           |
|    fps                  | 201       |
|    iterations           | 335       |
|    time_elapsed         | 20439     |
|    total_timesteps      | 4116480   |
| train/                  |           |
|    approx_kl            | 1.7994547 |
|    clip_fraction        | 0.859     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.6       |
|    explained_variance   | 1         |
|    learning_rate        | 0.000512  |
|    loss                 | 0.112     |
|    n_updates            | 4008      |
|    policy_gradient_loss | 0.0847    |
|    std                  | 0.185     |
|    value_loss           | 2.44e-05  |
---------------------------------------
Num timesteps: 4119000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 4122000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4125000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4128000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.365    |
| time/                   |           |
|    fps                  | 201       |
|    iterations           | 336       |
|    time_elapsed         | 20500     |
|    total_timesteps      | 4128768   |
| train/                  |           |
|    approx_kl            | 1.6676751 |
|    clip_fraction        | 0.858     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.61      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0959    |
|    n_updates            | 4020      |
|    policy_gradient_loss | 0.0777    |
|    std                  | 0.186     |
|    value_loss           | 0.000139  |
---------------------------------------
Num timesteps: 4131000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4134000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4137000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4140000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.01      |
|    ep_rew_mean          | -0.364    |
| time/                   |           |
|    fps                  | 201       |
|    iterations           | 337       |
|    time_elapsed         | 20574     |
|    total_timesteps      | 4141056   |
| train/                  |           |
|    approx_kl            | 1.9030323 |
|    clip_fraction        | 0.886     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.59      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0709    |
|    n_updates            | 4032      |
|    policy_gradient_loss | 0.1       |
|    std                  | 0.186     |
|    value_loss           | 8.48e-05  |
---------------------------------------
Num timesteps: 4143000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 4146000
Best mean reward: -0.35 - Last mean reward per episode: -0.35

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/model_20220627_061259_numTimesteps_4146000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-27/PPOGrasp/callback/best_model
Num timesteps: 4149000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 4152000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.366    |
| time/                   |           |
|    fps                  | 201       |
|    iterations           | 338       |
|    time_elapsed         | 20639     |
|    total_timesteps      | 4153344   |
| train/                  |           |
|    approx_kl            | 1.2967832 |
|    clip_fraction        | 0.85      |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.61      |
|    explained_variance   | 0.98      |
|    learning_rate        | 0.000512  |
|    loss                 | 0.00743   |
|    n_updates            | 4044      |
|    policy_gradient_loss | 0.0343    |
|    std                  | 0.185     |
|    value_loss           | 0.0042    |
---------------------------------------
Num timesteps: 4155000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4158000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4161000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4164000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.02      |
|    ep_rew_mean          | -0.377    |
| time/                   |           |
|    fps                  | 201       |
|    iterations           | 339       |
|    time_elapsed         | 20679     |
|    total_timesteps      | 4165632   |
| train/                  |           |
|    approx_kl            | 3.7798297 |
|    clip_fraction        | 0.902     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.59      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0624    |
|    n_updates            | 4056      |
|    policy_gradient_loss | 0.116     |
|    std                  | 0.187     |
|    value_loss           | 2.43e-05  |
---------------------------------------
Num timesteps: 4167000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4170000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 4173000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 4176000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.01      |
|    ep_rew_mean          | -0.375    |
| time/                   |           |
|    fps                  | 201       |
|    iterations           | 340       |
|    time_elapsed         | 20749     |
|    total_timesteps      | 4177920   |
| train/                  |           |
|    approx_kl            | 2.9813755 |
|    clip_fraction        | 0.894     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.56      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0658    |
|    n_updates            | 4068      |
|    policy_gradient_loss | 0.0958    |
|    std                  | 0.187     |
|    value_loss           | 1.98e-05  |
---------------------------------------
Num timesteps: 4179000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 4182000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4185000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 4188000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 3.99      |
|    ep_rew_mean          | -0.375    |
| time/                   |           |
|    fps                  | 201       |
|    iterations           | 341       |
|    time_elapsed         | 20820     |
|    total_timesteps      | 4190208   |
| train/                  |           |
|    approx_kl            | 1.3558564 |
|    clip_fraction        | 0.859     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.57      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.049     |
|    n_updates            | 4080      |
|    policy_gradient_loss | 0.075     |
|    std                  | 0.186     |
|    value_loss           | 1.78e-05  |
---------------------------------------
Num timesteps: 4191000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4194000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4197000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4200000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 3.99       |
|    ep_rew_mean          | -0.377     |
| time/                   |            |
|    fps                  | 201        |
|    iterations           | 342        |
|    time_elapsed         | 20873      |
|    total_timesteps      | 4202496    |
| train/                  |            |
|    approx_kl            | 0.81471086 |
|    clip_fraction        | 0.851      |
|    clip_range           | 0.075      |
|    entropy_loss         | 2.6        |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0578     |
|    n_updates            | 4092       |
|    policy_gradient_loss | 0.0784     |
|    std                  | 0.186      |
|    value_loss           | 1.21e-05   |
----------------------------------------
Num timesteps: 4203000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 4206000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 4209000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4212000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 3.99       |
|    ep_rew_mean          | -0.377     |
| time/                   |            |
|    fps                  | 201        |
|    iterations           | 343        |
|    time_elapsed         | 20925      |
|    total_timesteps      | 4214784    |
| train/                  |            |
|    approx_kl            | 0.88955516 |
|    clip_fraction        | 0.866      |
|    clip_range           | 0.075      |
|    entropy_loss         | 2.6        |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0661     |
|    n_updates            | 4104       |
|    policy_gradient_loss | 0.0839     |
|    std                  | 0.186      |
|    value_loss           | 1.47e-05   |
----------------------------------------
Num timesteps: 4215000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4218000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 4221000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4224000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4227000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.372    |
| time/                   |           |
|    fps                  | 201       |
|    iterations           | 344       |
|    time_elapsed         | 20986     |
|    total_timesteps      | 4227072   |
| train/                  |           |
|    approx_kl            | 2.0404725 |
|    clip_fraction        | 0.872     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.61      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0374    |
|    n_updates            | 4116      |
|    policy_gradient_loss | 0.0918    |
|    std                  | 0.185     |
|    value_loss           | 1.29e-05  |
---------------------------------------
Num timesteps: 4230000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4233000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4236000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 4239000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.364    |
| time/                   |           |
|    fps                  | 201       |
|    iterations           | 345       |
|    time_elapsed         | 21044     |
|    total_timesteps      | 4239360   |
| train/                  |           |
|    approx_kl            | 0.9576354 |
|    clip_fraction        | 0.87      |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.61      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0866    |
|    n_updates            | 4128      |
|    policy_gradient_loss | 0.0906    |
|    std                  | 0.185     |
|    value_loss           | 1.12e-05  |
---------------------------------------
Num timesteps: 4242000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 4245000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4248000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 4251000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.01      |
|    ep_rew_mean          | -0.383    |
| time/                   |           |
|    fps                  | 201       |
|    iterations           | 346       |
|    time_elapsed         | 21106     |
|    total_timesteps      | 4251648   |
| train/                  |           |
|    approx_kl            | 1.0493693 |
|    clip_fraction        | 0.868     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.61      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0719    |
|    n_updates            | 4140      |
|    policy_gradient_loss | 0.0863    |
|    std                  | 0.185     |
|    value_loss           | 8.98e-06  |
---------------------------------------
Num timesteps: 4254000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 4257000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 4260000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4263000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.369    |
| time/                   |           |
|    fps                  | 201       |
|    iterations           | 347       |
|    time_elapsed         | 21153     |
|    total_timesteps      | 4263936   |
| train/                  |           |
|    approx_kl            | 1.8589993 |
|    clip_fraction        | 0.855     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.63      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0693    |
|    n_updates            | 4152      |
|    policy_gradient_loss | 0.0718    |
|    std                  | 0.182     |
|    value_loss           | 1.27e-05  |
---------------------------------------
Num timesteps: 4266000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4269000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4272000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 4275000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.371    |
| time/                   |           |
|    fps                  | 201       |
|    iterations           | 348       |
|    time_elapsed         | 21216     |
|    total_timesteps      | 4276224   |
| train/                  |           |
|    approx_kl            | 1.2604505 |
|    clip_fraction        | 0.853     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.67      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0761    |
|    n_updates            | 4164      |
|    policy_gradient_loss | 0.076     |
|    std                  | 0.182     |
|    value_loss           | 9.15e-06  |
---------------------------------------
Num timesteps: 4278000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4281000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4284000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4287000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.366    |
| time/                   |           |
|    fps                  | 201       |
|    iterations           | 349       |
|    time_elapsed         | 21276     |
|    total_timesteps      | 4288512   |
| train/                  |           |
|    approx_kl            | 1.1014249 |
|    clip_fraction        | 0.86      |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.68      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0593    |
|    n_updates            | 4176      |
|    policy_gradient_loss | 0.087     |
|    std                  | 0.181     |
|    value_loss           | 1.12e-05  |
---------------------------------------
Num timesteps: 4290000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4293000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4296000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4299000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.371    |
| time/                   |           |
|    fps                  | 201       |
|    iterations           | 350       |
|    time_elapsed         | 21331     |
|    total_timesteps      | 4300800   |
| train/                  |           |
|    approx_kl            | 1.6081777 |
|    clip_fraction        | 0.863     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.69      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.11      |
|    n_updates            | 4188      |
|    policy_gradient_loss | 0.0859    |
|    std                  | 0.18      |
|    value_loss           | 1.04e-05  |
---------------------------------------
Num timesteps: 4302000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4305000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 4308000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 4311000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.369    |
| time/                   |           |
|    fps                  | 201       |
|    iterations           | 351       |
|    time_elapsed         | 21396     |
|    total_timesteps      | 4313088   |
| train/                  |           |
|    approx_kl            | 1.7828459 |
|    clip_fraction        | 0.845     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.71      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.026     |
|    n_updates            | 4200      |
|    policy_gradient_loss | 0.0686    |
|    std                  | 0.18      |
|    value_loss           | 1.56e-05  |
---------------------------------------
Num timesteps: 4314000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 4317000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4320000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 4323000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.372    |
| time/                   |           |
|    fps                  | 201       |
|    iterations           | 352       |
|    time_elapsed         | 21452     |
|    total_timesteps      | 4325376   |
| train/                  |           |
|    approx_kl            | 1.7004257 |
|    clip_fraction        | 0.861     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.74      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0829    |
|    n_updates            | 4212      |
|    policy_gradient_loss | 0.0948    |
|    std                  | 0.179     |
|    value_loss           | 1.34e-05  |
---------------------------------------
Num timesteps: 4326000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 4329000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4332000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4335000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.376    |
| time/                   |           |
|    fps                  | 201       |
|    iterations           | 353       |
|    time_elapsed         | 21526     |
|    total_timesteps      | 4337664   |
| train/                  |           |
|    approx_kl            | 5.9989944 |
|    clip_fraction        | 0.928     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.68      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0923    |
|    n_updates            | 4224      |
|    policy_gradient_loss | 0.15      |
|    std                  | 0.181     |
|    value_loss           | 0.00418   |
---------------------------------------
Num timesteps: 4338000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4341000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 4344000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4347000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 3.99      |
|    ep_rew_mean          | -0.373    |
| time/                   |           |
|    fps                  | 201       |
|    iterations           | 354       |
|    time_elapsed         | 21592     |
|    total_timesteps      | 4349952   |
| train/                  |           |
|    approx_kl            | 1.9679441 |
|    clip_fraction        | 0.872     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.63      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0446    |
|    n_updates            | 4236      |
|    policy_gradient_loss | 0.0802    |
|    std                  | 0.183     |
|    value_loss           | 1.74e-05  |
---------------------------------------
Num timesteps: 4350000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4353000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4356000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4359000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 4362000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.37     |
| time/                   |           |
|    fps                  | 201       |
|    iterations           | 355       |
|    time_elapsed         | 21659     |
|    total_timesteps      | 4362240   |
| train/                  |           |
|    approx_kl            | 2.1334515 |
|    clip_fraction        | 0.857     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.6       |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0498    |
|    n_updates            | 4248      |
|    policy_gradient_loss | 0.073     |
|    std                  | 0.184     |
|    value_loss           | 1.11e-05  |
---------------------------------------
Num timesteps: 4365000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4368000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4371000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4374000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 4        |
|    ep_rew_mean          | -0.384   |
| time/                   |          |
|    fps                  | 201      |
|    iterations           | 356      |
|    time_elapsed         | 21734    |
|    total_timesteps      | 4374528  |
| train/                  |          |
|    approx_kl            | 2.048417 |
|    clip_fraction        | 0.869    |
|    clip_range           | 0.075    |
|    entropy_loss         | 2.59     |
|    explained_variance   | 0.999    |
|    learning_rate        | 0.000512 |
|    loss                 | 0.0618   |
|    n_updates            | 4260     |
|    policy_gradient_loss | 0.0795   |
|    std                  | 0.183    |
|    value_loss           | 1.62e-05 |
--------------------------------------
Num timesteps: 4377000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4380000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4383000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4386000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.381    |
| time/                   |           |
|    fps                  | 201       |
|    iterations           | 357       |
|    time_elapsed         | 21805     |
|    total_timesteps      | 4386816   |
| train/                  |           |
|    approx_kl            | 1.7056433 |
|    clip_fraction        | 0.873     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.58      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0589    |
|    n_updates            | 4272      |
|    policy_gradient_loss | 0.083     |
|    std                  | 0.184     |
|    value_loss           | 1.29e-05  |
---------------------------------------
Num timesteps: 4389000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 4392000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 4395000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4398000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.374    |
| time/                   |           |
|    fps                  | 201       |
|    iterations           | 358       |
|    time_elapsed         | 21878     |
|    total_timesteps      | 4399104   |
| train/                  |           |
|    approx_kl            | 1.7876878 |
|    clip_fraction        | 0.859     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.58      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0534    |
|    n_updates            | 4284      |
|    policy_gradient_loss | 0.0708    |
|    std                  | 0.183     |
|    value_loss           | 1.24e-05  |
---------------------------------------
Num timesteps: 4401000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 4404000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 4407000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4410000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 4.01     |
|    ep_rew_mean          | -0.374   |
| time/                   |          |
|    fps                  | 200      |
|    iterations           | 359      |
|    time_elapsed         | 21961    |
|    total_timesteps      | 4411392  |
| train/                  |          |
|    approx_kl            | 1.70115  |
|    clip_fraction        | 0.872    |
|    clip_range           | 0.075    |
|    entropy_loss         | 2.6      |
|    explained_variance   | 0.999    |
|    learning_rate        | 0.000512 |
|    loss                 | 0.0725   |
|    n_updates            | 4296     |
|    policy_gradient_loss | 0.0907   |
|    std                  | 0.181    |
|    value_loss           | 9.49e-06 |
--------------------------------------
Num timesteps: 4413000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4416000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 4419000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4422000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.01      |
|    ep_rew_mean          | -0.362    |
| time/                   |           |
|    fps                  | 200       |
|    iterations           | 360       |
|    time_elapsed         | 22009     |
|    total_timesteps      | 4423680   |
| train/                  |           |
|    approx_kl            | 1.3735472 |
|    clip_fraction        | 0.855     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.63      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0693    |
|    n_updates            | 4308      |
|    policy_gradient_loss | 0.074     |
|    std                  | 0.181     |
|    value_loss           | 1.01e-05  |
---------------------------------------
Num timesteps: 4425000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 4428000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4431000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4434000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.36     |
| time/                   |           |
|    fps                  | 200       |
|    iterations           | 361       |
|    time_elapsed         | 22071     |
|    total_timesteps      | 4435968   |
| train/                  |           |
|    approx_kl            | 1.5445486 |
|    clip_fraction        | 0.878     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.64      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0445    |
|    n_updates            | 4320      |
|    policy_gradient_loss | 0.0955    |
|    std                  | 0.181     |
|    value_loss           | 1.23e-05  |
---------------------------------------
Num timesteps: 4437000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 4440000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4443000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 4446000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.02      |
|    ep_rew_mean          | -0.371    |
| time/                   |           |
|    fps                  | 200       |
|    iterations           | 362       |
|    time_elapsed         | 22143     |
|    total_timesteps      | 4448256   |
| train/                  |           |
|    approx_kl            | 1.6620655 |
|    clip_fraction        | 0.868     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.66      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0588    |
|    n_updates            | 4332      |
|    policy_gradient_loss | 0.0822    |
|    std                  | 0.18      |
|    value_loss           | 1.09e-05  |
---------------------------------------
Num timesteps: 4449000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 4452000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 4455000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4458000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.365    |
| time/                   |           |
|    fps                  | 200       |
|    iterations           | 363       |
|    time_elapsed         | 22225     |
|    total_timesteps      | 4460544   |
| train/                  |           |
|    approx_kl            | 1.0953919 |
|    clip_fraction        | 0.873     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.66      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0539    |
|    n_updates            | 4344      |
|    policy_gradient_loss | 0.1       |
|    std                  | 0.18      |
|    value_loss           | 1.41e-05  |
---------------------------------------
Num timesteps: 4461000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4464000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4467000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 4470000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.371     |
| time/                   |            |
|    fps                  | 200        |
|    iterations           | 364        |
|    time_elapsed         | 22289      |
|    total_timesteps      | 4472832    |
| train/                  |            |
|    approx_kl            | 0.92597437 |
|    clip_fraction        | 0.849      |
|    clip_range           | 0.075      |
|    entropy_loss         | 2.66       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0647     |
|    n_updates            | 4356       |
|    policy_gradient_loss | 0.0703     |
|    std                  | 0.18       |
|    value_loss           | 1.42e-05   |
----------------------------------------
Num timesteps: 4473000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4476000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4479000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 4482000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4485000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 3.99      |
|    ep_rew_mean          | -0.362    |
| time/                   |           |
|    fps                  | 200       |
|    iterations           | 365       |
|    time_elapsed         | 22355     |
|    total_timesteps      | 4485120   |
| train/                  |           |
|    approx_kl            | 1.7747809 |
|    clip_fraction        | 0.873     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.7       |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0582    |
|    n_updates            | 4368      |
|    policy_gradient_loss | 0.0924    |
|    std                  | 0.179     |
|    value_loss           | 1.73e-05  |
---------------------------------------
Num timesteps: 4488000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4491000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4494000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4497000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.374    |
| time/                   |           |
|    fps                  | 200       |
|    iterations           | 366       |
|    time_elapsed         | 22416     |
|    total_timesteps      | 4497408   |
| train/                  |           |
|    approx_kl            | 1.4713517 |
|    clip_fraction        | 0.885     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.7       |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0825    |
|    n_updates            | 4380      |
|    policy_gradient_loss | 0.114     |
|    std                  | 0.178     |
|    value_loss           | 1.38e-05  |
---------------------------------------
Num timesteps: 4500000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4503000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4506000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4509000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.367    |
| time/                   |           |
|    fps                  | 200       |
|    iterations           | 367       |
|    time_elapsed         | 22476     |
|    total_timesteps      | 4509696   |
| train/                  |           |
|    approx_kl            | 1.6338692 |
|    clip_fraction        | 0.877     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.68      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0571    |
|    n_updates            | 4392      |
|    policy_gradient_loss | 0.0987    |
|    std                  | 0.18      |
|    value_loss           | 0.000193  |
---------------------------------------
Num timesteps: 4512000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 4515000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 4518000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4521000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.03      |
|    ep_rew_mean          | -0.368    |
| time/                   |           |
|    fps                  | 200       |
|    iterations           | 368       |
|    time_elapsed         | 22544     |
|    total_timesteps      | 4521984   |
| train/                  |           |
|    approx_kl            | 1.1647571 |
|    clip_fraction        | 0.851     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.68      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0461    |
|    n_updates            | 4404      |
|    policy_gradient_loss | 0.077     |
|    std                  | 0.179     |
|    value_loss           | 9.41e-06  |
---------------------------------------
Num timesteps: 4524000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4527000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 4530000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4533000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.368    |
| time/                   |           |
|    fps                  | 200       |
|    iterations           | 369       |
|    time_elapsed         | 22590     |
|    total_timesteps      | 4534272   |
| train/                  |           |
|    approx_kl            | 1.4433489 |
|    clip_fraction        | 0.853     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.69      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0438    |
|    n_updates            | 4416      |
|    policy_gradient_loss | 0.0708    |
|    std                  | 0.181     |
|    value_loss           | 1.54e-05  |
---------------------------------------
Num timesteps: 4536000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 4539000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4542000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 4545000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.36     |
| time/                   |           |
|    fps                  | 200       |
|    iterations           | 370       |
|    time_elapsed         | 22666     |
|    total_timesteps      | 4546560   |
| train/                  |           |
|    approx_kl            | 1.5609745 |
|    clip_fraction        | 0.86      |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.68      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0783    |
|    n_updates            | 4428      |
|    policy_gradient_loss | 0.0771    |
|    std                  | 0.179     |
|    value_loss           | 0.000109  |
---------------------------------------
Num timesteps: 4548000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4551000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 4554000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 4557000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.369    |
| time/                   |           |
|    fps                  | 200       |
|    iterations           | 371       |
|    time_elapsed         | 22738     |
|    total_timesteps      | 4558848   |
| train/                  |           |
|    approx_kl            | 1.8133417 |
|    clip_fraction        | 0.865     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.69      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0574    |
|    n_updates            | 4440      |
|    policy_gradient_loss | 0.0856    |
|    std                  | 0.18      |
|    value_loss           | 1.2e-05   |
---------------------------------------
Num timesteps: 4560000
Best mean reward: -0.35 - Last mean reward per episode: -0.35
Num timesteps: 4563000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4566000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 4569000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 3.99     |
|    ep_rew_mean          | -0.369   |
| time/                   |          |
|    fps                  | 200      |
|    iterations           | 372      |
|    time_elapsed         | 22800    |
|    total_timesteps      | 4571136  |
| train/                  |          |
|    approx_kl            | 1.732637 |
|    clip_fraction        | 0.86     |
|    clip_range           | 0.075    |
|    entropy_loss         | 2.72     |
|    explained_variance   | 0.999    |
|    learning_rate        | 0.000512 |
|    loss                 | 0.0349   |
|    n_updates            | 4452     |
|    policy_gradient_loss | 0.0598   |
|    std                  | 0.179    |
|    value_loss           | 0.00176  |
--------------------------------------
Num timesteps: 4572000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4575000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4578000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4581000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.37     |
| time/                   |           |
|    fps                  | 200       |
|    iterations           | 373       |
|    time_elapsed         | 22865     |
|    total_timesteps      | 4583424   |
| train/                  |           |
|    approx_kl            | 1.9949389 |
|    clip_fraction        | 0.878     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.73      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0527    |
|    n_updates            | 4464      |
|    policy_gradient_loss | 0.0922    |
|    std                  | 0.179     |
|    value_loss           | 1.45e-05  |
---------------------------------------
Num timesteps: 4584000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 4587000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4590000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4593000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.368     |
| time/                   |            |
|    fps                  | 200        |
|    iterations           | 374        |
|    time_elapsed         | 22940      |
|    total_timesteps      | 4595712    |
| train/                  |            |
|    approx_kl            | 0.95285064 |
|    clip_fraction        | 0.866      |
|    clip_range           | 0.075      |
|    entropy_loss         | 2.72       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0364     |
|    n_updates            | 4476       |
|    policy_gradient_loss | 0.0739     |
|    std                  | 0.179      |
|    value_loss           | 1.91e-05   |
----------------------------------------
Num timesteps: 4596000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 4599000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 4602000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4605000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4608000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.02      |
|    ep_rew_mean          | -0.378    |
| time/                   |           |
|    fps                  | 200       |
|    iterations           | 375       |
|    time_elapsed         | 23005     |
|    total_timesteps      | 4608000   |
| train/                  |           |
|    approx_kl            | 1.7834533 |
|    clip_fraction        | 0.863     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.74      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0306    |
|    n_updates            | 4488      |
|    policy_gradient_loss | 0.0842    |
|    std                  | 0.179     |
|    value_loss           | 1.17e-05  |
---------------------------------------
Num timesteps: 4611000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4614000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 4617000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4620000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.38     |
| time/                   |           |
|    fps                  | 200       |
|    iterations           | 376       |
|    time_elapsed         | 23076     |
|    total_timesteps      | 4620288   |
| train/                  |           |
|    approx_kl            | 2.3580925 |
|    clip_fraction        | 0.866     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.75      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0352    |
|    n_updates            | 4500      |
|    policy_gradient_loss | 0.0716    |
|    std                  | 0.179     |
|    value_loss           | 1.79e-05  |
---------------------------------------
Num timesteps: 4623000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4626000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4629000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 4632000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.36     |
| time/                   |           |
|    fps                  | 200       |
|    iterations           | 377       |
|    time_elapsed         | 23137     |
|    total_timesteps      | 4632576   |
| train/                  |           |
|    approx_kl            | 1.5200815 |
|    clip_fraction        | 0.863     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.76      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0511    |
|    n_updates            | 4512      |
|    policy_gradient_loss | 0.0807    |
|    std                  | 0.178     |
|    value_loss           | 1.09e-05  |
---------------------------------------
Num timesteps: 4635000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4638000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4641000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 4644000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.367    |
| time/                   |           |
|    fps                  | 200       |
|    iterations           | 378       |
|    time_elapsed         | 23208     |
|    total_timesteps      | 4644864   |
| train/                  |           |
|    approx_kl            | 1.3457721 |
|    clip_fraction        | 0.857     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.78      |
|    explained_variance   | 0.998     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0372    |
|    n_updates            | 4524      |
|    policy_gradient_loss | 0.0587    |
|    std                  | 0.178     |
|    value_loss           | 0.000622  |
---------------------------------------
Num timesteps: 4647000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4650000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4653000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4656000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 4        |
|    ep_rew_mean          | -0.363   |
| time/                   |          |
|    fps                  | 200      |
|    iterations           | 379      |
|    time_elapsed         | 23276    |
|    total_timesteps      | 4657152  |
| train/                  |          |
|    approx_kl            | 5.242845 |
|    clip_fraction        | 0.877    |
|    clip_range           | 0.075    |
|    entropy_loss         | 2.77     |
|    explained_variance   | 0.998    |
|    learning_rate        | 0.000512 |
|    loss                 | 0.0524   |
|    n_updates            | 4536     |
|    policy_gradient_loss | 0.0894   |
|    std                  | 0.178    |
|    value_loss           | 2.91e-05 |
--------------------------------------
Num timesteps: 4659000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4662000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 4665000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 4668000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.368    |
| time/                   |           |
|    fps                  | 199       |
|    iterations           | 380       |
|    time_elapsed         | 23349     |
|    total_timesteps      | 4669440   |
| train/                  |           |
|    approx_kl            | 1.2538095 |
|    clip_fraction        | 0.864     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.76      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0698    |
|    n_updates            | 4548      |
|    policy_gradient_loss | 0.0791    |
|    std                  | 0.178     |
|    value_loss           | 1.08e-05  |
---------------------------------------
Num timesteps: 4671000
Best mean reward: -0.35 - Last mean reward per episode: -0.39
Num timesteps: 4674000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 4677000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4680000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.377    |
| time/                   |           |
|    fps                  | 199       |
|    iterations           | 381       |
|    time_elapsed         | 23415     |
|    total_timesteps      | 4681728   |
| train/                  |           |
|    approx_kl            | 2.0351691 |
|    clip_fraction        | 0.888     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.75      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.114     |
|    n_updates            | 4560      |
|    policy_gradient_loss | 0.111     |
|    std                  | 0.178     |
|    value_loss           | 9.55e-06  |
---------------------------------------
Num timesteps: 4683000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 4686000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 4689000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 4692000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.01      |
|    ep_rew_mean          | -0.376    |
| time/                   |           |
|    fps                  | 199       |
|    iterations           | 382       |
|    time_elapsed         | 23486     |
|    total_timesteps      | 4694016   |
| train/                  |           |
|    approx_kl            | 1.8660237 |
|    clip_fraction        | 0.866     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.74      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0598    |
|    n_updates            | 4572      |
|    policy_gradient_loss | 0.084     |
|    std                  | 0.178     |
|    value_loss           | 5.09e-05  |
---------------------------------------
Num timesteps: 4695000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4698000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 4701000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 4704000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.01      |
|    ep_rew_mean          | -0.373    |
| time/                   |           |
|    fps                  | 199       |
|    iterations           | 383       |
|    time_elapsed         | 23557     |
|    total_timesteps      | 4706304   |
| train/                  |           |
|    approx_kl            | 3.1258678 |
|    clip_fraction        | 0.851     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.76      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0505    |
|    n_updates            | 4584      |
|    policy_gradient_loss | 0.0745    |
|    std                  | 0.178     |
|    value_loss           | 1.05e-05  |
---------------------------------------
Num timesteps: 4707000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4710000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 4713000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 4716000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.373    |
| time/                   |           |
|    fps                  | 199       |
|    iterations           | 384       |
|    time_elapsed         | 23622     |
|    total_timesteps      | 4718592   |
| train/                  |           |
|    approx_kl            | 2.1734447 |
|    clip_fraction        | 0.859     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.79      |
|    explained_variance   | 0.995     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0261    |
|    n_updates            | 4596      |
|    policy_gradient_loss | 0.0378    |
|    std                  | 0.178     |
|    value_loss           | 0.000813  |
---------------------------------------
Num timesteps: 4719000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 4722000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4725000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 4728000
Best mean reward: -0.35 - Last mean reward per episode: -0.39
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 4        |
|    ep_rew_mean          | -0.373   |
| time/                   |          |
|    fps                  | 199      |
|    iterations           | 385      |
|    time_elapsed         | 23689    |
|    total_timesteps      | 4730880  |
| train/                  |          |
|    approx_kl            | 7.676775 |
|    clip_fraction        | 0.917    |
|    clip_range           | 0.075    |
|    entropy_loss         | 2.75     |
|    explained_variance   | 0.995    |
|    learning_rate        | 0.000512 |
|    loss                 | 0.0506   |
|    n_updates            | 4608     |
|    policy_gradient_loss | 0.0803   |
|    std                  | 0.182    |
|    value_loss           | 1.64e-05 |
--------------------------------------
Num timesteps: 4731000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4734000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4737000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4740000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4743000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 4        |
|    ep_rew_mean          | -0.364   |
| time/                   |          |
|    fps                  | 199      |
|    iterations           | 386      |
|    time_elapsed         | 23758    |
|    total_timesteps      | 4743168  |
| train/                  |          |
|    approx_kl            | 4.115503 |
|    clip_fraction        | 0.861    |
|    clip_range           | 0.075    |
|    entropy_loss         | 2.71     |
|    explained_variance   | 0.999    |
|    learning_rate        | 0.000512 |
|    loss                 | 0.0664   |
|    n_updates            | 4620     |
|    policy_gradient_loss | 0.0727   |
|    std                  | 0.181    |
|    value_loss           | 1.31e-05 |
--------------------------------------
Num timesteps: 4746000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 4749000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4752000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4755000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.374    |
| time/                   |           |
|    fps                  | 199       |
|    iterations           | 387       |
|    time_elapsed         | 23833     |
|    total_timesteps      | 4755456   |
| train/                  |           |
|    approx_kl            | 2.4238741 |
|    clip_fraction        | 0.878     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.71      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0599    |
|    n_updates            | 4632      |
|    policy_gradient_loss | 0.0969    |
|    std                  | 0.182     |
|    value_loss           | 8.87e-06  |
---------------------------------------
Num timesteps: 4758000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4761000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4764000
Best mean reward: -0.35 - Last mean reward per episode: -0.39
Num timesteps: 4767000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.371    |
| time/                   |           |
|    fps                  | 199       |
|    iterations           | 388       |
|    time_elapsed         | 23903     |
|    total_timesteps      | 4767744   |
| train/                  |           |
|    approx_kl            | 1.5459887 |
|    clip_fraction        | 0.88      |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.7       |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0532    |
|    n_updates            | 4644      |
|    policy_gradient_loss | 0.1       |
|    std                  | 0.182     |
|    value_loss           | 1.05e-05  |
---------------------------------------
Num timesteps: 4770000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4773000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 4776000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4779000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.02      |
|    ep_rew_mean          | -0.376    |
| time/                   |           |
|    fps                  | 199       |
|    iterations           | 389       |
|    time_elapsed         | 23967     |
|    total_timesteps      | 4780032   |
| train/                  |           |
|    approx_kl            | 1.7789928 |
|    clip_fraction        | 0.859     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.71      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0445    |
|    n_updates            | 4656      |
|    policy_gradient_loss | 0.0827    |
|    std                  | 0.181     |
|    value_loss           | 1.21e-05  |
---------------------------------------
Num timesteps: 4782000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4785000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4788000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4791000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4         |
|    ep_rew_mean          | -0.371    |
| time/                   |           |
|    fps                  | 199       |
|    iterations           | 390       |
|    time_elapsed         | 24029     |
|    total_timesteps      | 4792320   |
| train/                  |           |
|    approx_kl            | 1.4560636 |
|    clip_fraction        | 0.87      |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.72      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0819    |
|    n_updates            | 4668      |
|    policy_gradient_loss | 0.089     |
|    std                  | 0.181     |
|    value_loss           | 1.72e-05  |
---------------------------------------
Num timesteps: 4794000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4797000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4800000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 4803000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.01      |
|    ep_rew_mean          | -0.369    |
| time/                   |           |
|    fps                  | 199       |
|    iterations           | 391       |
|    time_elapsed         | 24102     |
|    total_timesteps      | 4804608   |
| train/                  |           |
|    approx_kl            | 2.7629852 |
|    clip_fraction        | 0.882     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.7       |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0535    |
|    n_updates            | 4680      |
|    policy_gradient_loss | 0.0863    |
|    std                  | 0.182     |
|    value_loss           | 5.55e-05  |
---------------------------------------
Num timesteps: 4806000
Best mean reward: -0.35 - Last mean reward per episode: -0.39
Num timesteps: 4809000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4812000
Best mean reward: -0.35 - Last mean reward per episode: -0.35
Num timesteps: 4815000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 4        |
|    ep_rew_mean          | -0.368   |
| time/                   |          |
|    fps                  | 199      |
|    iterations           | 392      |
|    time_elapsed         | 24165    |
|    total_timesteps      | 4816896  |
| train/                  |          |
|    approx_kl            | 1.925096 |
|    clip_fraction        | 0.853    |
|    clip_range           | 0.075    |
|    entropy_loss         | 2.71     |
|    explained_variance   | 0.998    |
|    learning_rate        | 0.000512 |
|    loss                 | 0.0423   |
|    n_updates            | 4692     |
|    policy_gradient_loss | 0.0687   |
|    std                  | 0.182    |
|    value_loss           | 0.000418 |
--------------------------------------
Num timesteps: 4818000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 4821000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 4824000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 4827000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.02      |
|    ep_rew_mean          | -0.378    |
| time/                   |           |
|    fps                  | 199       |
|    iterations           | 393       |
|    time_elapsed         | 24238     |
|    total_timesteps      | 4829184   |
| train/                  |           |
|    approx_kl            | 19.133383 |
|    clip_fraction        | 0.892     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.7       |
|    explained_variance   | 0.998     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0218    |
|    n_updates            | 4704      |
|    policy_gradient_loss | 0.078     |
|    std                  | 0.182     |
|    value_loss           | 0.000141  |
---------------------------------------
Num timesteps: 4830000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 4833000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4836000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 4839000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.02      |
|    ep_rew_mean          | -0.367    |
| time/                   |           |
|    fps                  | 199       |
|    iterations           | 394       |
|    time_elapsed         | 24295     |
|    total_timesteps      | 4841472   |
| train/                  |           |
|    approx_kl            | 2.3991578 |
|    clip_fraction        | 0.879     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.67      |
|    explained_variance   | 0.996     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0586    |
|    n_updates            | 4716      |
|    policy_gradient_loss | 0.0741    |
|    std                  | 0.183     |
|    value_loss           | 4.09e-05  |
---------------------------------------
Num timesteps: 4842000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 4845000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4848000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 4851000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.07      |
|    ep_rew_mean          | -0.392    |
| time/                   |           |
|    fps                  | 199       |
|    iterations           | 395       |
|    time_elapsed         | 24361     |
|    total_timesteps      | 4853760   |
| train/                  |           |
|    approx_kl            | 0.9670498 |
|    clip_fraction        | 0.88      |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.63      |
|    explained_variance   | 0.998     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0351    |
|    n_updates            | 4728      |
|    policy_gradient_loss | 0.0717    |
|    std                  | 0.183     |
|    value_loss           | 3.34e-05  |
---------------------------------------
Num timesteps: 4854000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 4857000
Best mean reward: -0.35 - Last mean reward per episode: -0.39
Num timesteps: 4860000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 4863000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 4866000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.03      |
|    ep_rew_mean          | -0.378    |
| time/                   |           |
|    fps                  | 199       |
|    iterations           | 396       |
|    time_elapsed         | 24432     |
|    total_timesteps      | 4866048   |
| train/                  |           |
|    approx_kl            | 1.4742007 |
|    clip_fraction        | 0.884     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.57      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0544    |
|    n_updates            | 4740      |
|    policy_gradient_loss | 0.0827    |
|    std                  | 0.185     |
|    value_loss           | 2.16e-05  |
---------------------------------------
Num timesteps: 4869000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4872000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 4875000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 4878000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.02      |
|    ep_rew_mean          | -0.372    |
| time/                   |           |
|    fps                  | 199       |
|    iterations           | 397       |
|    time_elapsed         | 24497     |
|    total_timesteps      | 4878336   |
| train/                  |           |
|    approx_kl            | 1.4618512 |
|    clip_fraction        | 0.876     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.52      |
|    explained_variance   | 0.998     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0452    |
|    n_updates            | 4752      |
|    policy_gradient_loss | 0.0788    |
|    std                  | 0.187     |
|    value_loss           | 0.000149  |
---------------------------------------
Num timesteps: 4881000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4884000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4887000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4890000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.06      |
|    ep_rew_mean          | -0.378    |
| time/                   |           |
|    fps                  | 199       |
|    iterations           | 398       |
|    time_elapsed         | 24564     |
|    total_timesteps      | 4890624   |
| train/                  |           |
|    approx_kl            | 1.3160807 |
|    clip_fraction        | 0.874     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.5       |
|    explained_variance   | 0.998     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0438    |
|    n_updates            | 4764      |
|    policy_gradient_loss | 0.0784    |
|    std                  | 0.187     |
|    value_loss           | 3.03e-05  |
---------------------------------------
Num timesteps: 4893000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 4896000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4899000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 4902000
Best mean reward: -0.35 - Last mean reward per episode: -0.35
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.08      |
|    ep_rew_mean          | -0.375    |
| time/                   |           |
|    fps                  | 199       |
|    iterations           | 399       |
|    time_elapsed         | 24629     |
|    total_timesteps      | 4902912   |
| train/                  |           |
|    approx_kl            | 0.9666191 |
|    clip_fraction        | 0.858     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.49      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0453    |
|    n_updates            | 4776      |
|    policy_gradient_loss | 0.0734    |
|    std                  | 0.188     |
|    value_loss           | 2.48e-05  |
---------------------------------------
Num timesteps: 4905000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 4908000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 4911000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 4914000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.03      |
|    ep_rew_mean          | -0.366    |
| time/                   |           |
|    fps                  | 199       |
|    iterations           | 400       |
|    time_elapsed         | 24694     |
|    total_timesteps      | 4915200   |
| train/                  |           |
|    approx_kl            | 0.8833526 |
|    clip_fraction        | 0.854     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.46      |
|    explained_variance   | 0.996     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0464    |
|    n_updates            | 4788      |
|    policy_gradient_loss | 0.0638    |
|    std                  | 0.188     |
|    value_loss           | 0.00343   |
---------------------------------------
Num timesteps: 4917000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 4920000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4923000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 4926000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.11      |
|    ep_rew_mean          | -0.382    |
| time/                   |           |
|    fps                  | 198       |
|    iterations           | 401       |
|    time_elapsed         | 24763     |
|    total_timesteps      | 4927488   |
| train/                  |           |
|    approx_kl            | 1.9209467 |
|    clip_fraction        | 0.894     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.42      |
|    explained_variance   | 0.975     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.00838   |
|    n_updates            | 4800      |
|    policy_gradient_loss | 0.0442    |
|    std                  | 0.19      |
|    value_loss           | 0.00549   |
---------------------------------------
Num timesteps: 4929000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4932000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 4935000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 4938000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.03      |
|    ep_rew_mean          | -0.383    |
| time/                   |           |
|    fps                  | 198       |
|    iterations           | 402       |
|    time_elapsed         | 24831     |
|    total_timesteps      | 4939776   |
| train/                  |           |
|    approx_kl            | 1.3717631 |
|    clip_fraction        | 0.835     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.39      |
|    explained_variance   | 0.892     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0844    |
|    n_updates            | 4812      |
|    policy_gradient_loss | 0.0592    |
|    std                  | 0.19      |
|    value_loss           | 0.000258  |
---------------------------------------
Num timesteps: 4941000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4944000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4947000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4950000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.01      |
|    ep_rew_mean          | -0.373    |
| time/                   |           |
|    fps                  | 198       |
|    iterations           | 403       |
|    time_elapsed         | 24892     |
|    total_timesteps      | 4952064   |
| train/                  |           |
|    approx_kl            | 1.8208199 |
|    clip_fraction        | 0.854     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.37      |
|    explained_variance   | 0.996     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0579    |
|    n_updates            | 4824      |
|    policy_gradient_loss | 0.0889    |
|    std                  | 0.191     |
|    value_loss           | 3.83e-05  |
---------------------------------------
Num timesteps: 4953000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4956000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 4959000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4962000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.01      |
|    ep_rew_mean          | -0.38     |
| time/                   |           |
|    fps                  | 198       |
|    iterations           | 404       |
|    time_elapsed         | 24959     |
|    total_timesteps      | 4964352   |
| train/                  |           |
|    approx_kl            | 1.1567434 |
|    clip_fraction        | 0.859     |
|    clip_range           | 0.075     |
|    entropy_loss         | 2.37      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.000512  |
|    loss                 | 0.0251    |
|    n_updates            | 4836      |
|    policy_gradient_loss | 0.079     |
|    std                  | 0.19      |
|    value_loss           | 2.29e-05  |
---------------------------------------
Num timesteps: 4965000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 4968000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4971000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4974000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.371     |
| time/                   |            |
|    fps                  | 198        |
|    iterations           | 405        |
|    time_elapsed         | 25021      |
|    total_timesteps      | 4976640    |
| train/                  |            |
|    approx_kl            | 0.81160825 |
|    clip_fraction        | 0.849      |
|    clip_range           | 0.075      |
|    entropy_loss         | 2.37       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0605     |
|    n_updates            | 4848       |
|    policy_gradient_loss | 0.0821     |
|    std                  | 0.19       |
|    value_loss           | 1.68e-05   |
----------------------------------------
Num timesteps: 4977000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 4980000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 4983000
Best mean reward: -0.35 - Last mean reward per episode: -0.36
Num timesteps: 4986000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4          |
|    ep_rew_mean          | -0.378     |
| time/                   |            |
|    fps                  | 198        |
|    iterations           | 406        |
|    time_elapsed         | 25088      |
|    total_timesteps      | 4988928    |
| train/                  |            |
|    approx_kl            | 0.86176974 |
|    clip_fraction        | 0.841      |
|    clip_range           | 0.075      |
|    entropy_loss         | 2.35       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.0776     |
|    n_updates            | 4860       |
|    policy_gradient_loss | 0.0766     |
|    std                  | 0.191      |
|    value_loss           | 1.6e-05    |
----------------------------------------
Num timesteps: 4989000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 4992000
Best mean reward: -0.35 - Last mean reward per episode: -0.38
Num timesteps: 4995000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 4998000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
Num timesteps: 5001000
Best mean reward: -0.35 - Last mean reward per episode: -0.37
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4.02       |
|    ep_rew_mean          | -0.372     |
| time/                   |            |
|    fps                  | 198        |
|    iterations           | 407        |
|    time_elapsed         | 25158      |
|    total_timesteps      | 5001216    |
| train/                  |            |
|    approx_kl            | 0.61496013 |
|    clip_fraction        | 0.829      |
|    clip_range           | 0.075      |
|    entropy_loss         | 2.37       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.000512   |
|    loss                 | 0.033      |
|    n_updates            | 4872       |
|    policy_gradient_loss | 0.0669     |
|    std                  | 0.191      |
|    value_loss           | 1.44e-05   |
----------------------------------------
Traceback (most recent call last):
  File "/home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/mainPPOVect.py", line 220, in <module>
    plot_results(log_folder=callbackDir, window_size=100, title="PPO Tuned Training Curve Window=100", img_folder=imgDir,\
TypeError: plot_results() missing 1 required positional argument: 'maxlengthpercentage'
(rl_env) ]0;hjkwon@HJK-AI-Robotics: ~/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining[01;32mhjkwon@HJK-AI-Robotics[00m:[01;34m~/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining[00m$ python3 recoc[K[K[K[K[K[K[K[K[K[K[K[K[Kcd ..
(rl_env) ]0;hjkwon@HJK-AI-Robotics: ~/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor[01;32mhjkwon@HJK-AI-Robotics[00m:[01;34m~/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor[00m$ ^C
(rl_env) ]0;hjkwon@HJK-AI-Robotics: ~/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor[01;32mhjkwon@HJK-AI-Robotics[00m:[01;34m~/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor[00m$ python3 recordResults.py 
pybullet build time: Dec  1 2021 18:34:28
_XSERVTransSocketUNIXCreateListener: ...SocketCreateListener() failed
_XSERVTransMakeAllCOTSServerListeners: server already running
(EE) 
Fatal server error:
(EE) Cannot establish any listening sockets - Make sure an X server isn't already running(EE) 
starting video recorder: 
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
Traceback (most recent call last):
  File "/home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/recordResults.py", line 73, in <module>
    record_video_multiple(model=loaded_model, n_envs = N_ENVS, env_id=ENV_ID, video_length = 400, video_dir=vidDir, \
  File "/home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/SB3/recordVideo.py", line 58, in record_video_multiple
    obs = eval_env.reset()
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/stable_baselines3/common/vec_env/vec_video_recorder.py", line 67, in reset
    obs = self.venv.reset()
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/stable_baselines3/common/vec_env/vec_frame_stack.py", line 58, in reset
    observation = self.venv.reset()  # pytype:disable=annotation-type-mismatch
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 62, in reset
    self._save_obs(env_idx, obs)
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 92, in _save_obs
    self.buf_obs[key][env_idx] = obs
ValueError: could not broadcast input array from shape (7,) into shape (13,)
(rl_env) ]0;hjkwon@HJK-AI-Robotics: ~/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor[01;32mhjkwon@HJK-AI-Robotics[00m:[01;34m~/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor[00m$ python3 recordResults.py 
pybullet build time: Dec  1 2021 18:34:28
_XSERVTransSocketUNIXCreateListener: ...SocketCreateListener() failed
_XSERVTransMakeAllCOTSServerListeners: server already running
(EE) 
Fatal server error:
(EE) Cannot establish any listening sockets - Make sure an X server isn't already running(EE) 
starting video recorder: 
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/env/pybulletCust.py:94: UserWarning: The use of the render method is not recommended when the environment has not been created with render=True. The rendering will probably be weird. Prefer making the environment with option `render=True`. For example: `env = gym.make('PandaReach-v2', render=True)`.
  warnings.warn(
Traceback (most recent call last):
  File "/home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/recordResults.py", line 73, in <module>
    record_video_multiple(model=loaded_model, n_envs = N_ENVS, env_id=ENV_ID, video_length = 400, video_dir=vidDir, \
  File "/home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/SB3/recordVideo.py", line 60, in record_video_multiple
    action, _ = model.predict(obs)
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/stable_baselines3/common/base_class.py", line 562, in predict
    return self.policy.predict(observation, state, episode_start, deterministic)
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/stable_baselines3/common/policies.py", line 335, in predict
    observation, vectorized_env = self.obs_to_tensor(observation)
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/stable_baselines3/common/policies.py", line 250, in obs_to_tensor
    vectorized_env = is_vectorized_observation(observation, self.observation_space)
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/stable_baselines3/common/utils.py", line 375, in is_vectorized_observation
    return is_vec_obs_func(observation, observation_space)
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/stable_baselines3/common/utils.py", line 242, in is_vectorized_box_observation
    raise ValueError(
ValueError: Error: Unexpected observation shape (4, 28) for Box environment, please use (21,) or (n_env, 21) for the observation shape.
(rl_env) ]0;hjkwon@HJK-AI-Robotics: ~/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor[01;32mhjkwon@HJK-AI-Robotics[00m:[01;34m~/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor[00m$ python3 recordResults.py 
pybullet build time: Dec  1 2021 18:34:28
_XSERVTransSocketUNIXCreateListener: ...SocketCreateListener() failed
_XSERVTransMakeAllCOTSServerListeners: server already running
(EE) 
Fatal server error:
(EE) Cannot establish any listening sockets - Make sure an X server isn't already running(EE) 
starting video recorder: 
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/env/pybulletCust.py:94: UserWarning: The use of the render method is not recommended when the environment has not been created with render=True. The rendering will probably be weird. Prefer making the environment with option `render=True`. For example: `env = gym.make('PandaReach-v2', render=True)`.
  warnings.warn(
Traceback (most recent call last):
  File "/home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/recordResults.py", line 73, in <module>
    record_video_multiple(model=loaded_model, n_envs = N_ENVS, env_id=ENV_ID, video_length = 400, video_dir=vidDir, \
  File "/home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/SB3/recordVideo.py", line 60, in record_video_multiple
    action, _ = model.predict(obs)
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/stable_baselines3/common/base_class.py", line 562, in predict
    return self.policy.predict(observation, state, episode_start, deterministic)
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/stable_baselines3/common/policies.py", line 335, in predict
    observation, vectorized_env = self.obs_to_tensor(observation)
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/stable_baselines3/common/policies.py", line 250, in obs_to_tensor
    vectorized_env = is_vectorized_observation(observation, self.observation_space)
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/stable_baselines3/common/utils.py", line 375, in is_vectorized_observation
    return is_vec_obs_func(observation, observation_space)
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/stable_baselines3/common/utils.py", line 242, in is_vectorized_box_observation
    raise ValueError(
ValueError: Error: Unexpected observation shape (4, 28) for Box environment, please use (21,) or (n_env, 21) for the observation shape.
(rl_env) ]0;hjkwon@HJK-AI-Robotics: ~/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor[01;32mhjkwon@HJK-AI-Robotics[00m:[01;34m~/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor[00m$ python3 recordResults.py 
pybullet build time: Dec  1 2021 18:34:28
_XSERVTransSocketUNIXCreateListener: ...SocketCreateListener() failed
_XSERVTransMakeAllCOTSServerListeners: server already running
(EE) 
Fatal server error:
(EE) Cannot establish any listening sockets - Make sure an X server isn't already running(EE) 
starting video recorder: 
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/env/pybulletCust.py:94: UserWarning: The use of the render method is not recommended when the environment has not been created with render=True. The rendering will probably be weird. Prefer making the environment with option `render=True`. For example: `env = gym.make('PandaReach-v2', render=True)`.
  warnings.warn(
/home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/env/pybulletCust.py:94: UserWarning: The use of the render method is not recommended when the environment has not been created with render=True. The rendering will probably be weird. Prefer making the environment with option `render=True`. For example: `env = gym.make('PandaReach-v2', render=True)`.
  warnings.warn(
^CTraceback (most recent call last):
  File "/home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/recordResults.py", line 73, in <module>
    render=False, wrapper=PandaWrapper, prefix=POLICYNAME + "-" + ENV_ID)
  File "/home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/SB3/recordVideo.py", line 61, in record_video_multiple
    obs, _, _, _ = eval_env.step(action)
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/stable_baselines3/common/vec_env/base_vec_env.py", line 162, in step
    return self.step_wait()
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/stable_baselines3/common/vec_env/vec_video_recorder.py", line 92, in step_wait
    self.video_recorder.capture_frame()
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/gym/wrappers/monitoring/video_recorder.py", line 125, in capture_frame
    frame = self.env.render(mode=render_mode)
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/stable_baselines3/common/vec_env/base_vec_env.py", line 281, in render
    return self.venv.render(mode=mode)
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 87, in render
    return super().render(mode=mode)
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/stable_baselines3/common/vec_env/base_vec_env.py", line 177, in render
    imgs = self.get_images()
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 70, in get_images
    return [env.render(mode="rgb_array") for env in self.envs]
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 70, in <listcomp>
    return [env.render(mode="rgb_array") for env in self.envs]
  File "/home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/wrappers/pandaWrapperVect_Grasp_BW_D_0New.py", line 57, in render
    img, depth = self.env.render(mode = 'rgb_array', width = 1080, height= 720) 
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/gym/core.py", line 254, in render
    return self.env.render(mode, **kwargs)
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/gym/core.py", line 254, in render
    return self.env.render(mode, **kwargs)
  File "/home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/env/robottaskenv.py", line 104, in render
    return self.sim.render(
  File "/home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/env/pybulletCust.py", line 112, in render
    (_, _, px, depth, _) = self.physics_client.getCameraImage(
KeyboardInterrupt

(rl_env) ]0;hjkwon@HJK-AI-Robotics: ~/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor[01;32mhjkwon@HJK-AI-Robotics[00m:[01;34m~/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor[00m$ exit

Script done on 2022-06-27 13:54:59-04:00 [COMMAND_EXIT_CODE="0"]
