Script started on 2022-04-21 10:44:02-04:00 [TERM="xterm-256color" TTY="/dev/pts/3" COLUMNS="79" LINES="23"]
bash: devel/setup.bash: No such file or directory
]0;hjkwon@HJK-AI-Robotics: ~/scripts/stable_baselines/pandaAndrew/21April2022[01;32mhjkwon@HJK-AI-Robotics[00m:[01;34m~/scripts/stable_baselines/pandaAndrew/21April2022[00m$ ls[K[Kcd . .
]0;hjkwon@HJK-AI-Robotics: ~/scripts/stable_baselines/pandaAndrew[01;32mhjkwon@HJK-AI-Robotics[00m:[01;34m~/scripts/stable_baselines/pandaAndrew[00m$ conda activate r l)_[K[K_env
(rl_env) ]0;hjkwon@HJK-AI-Robotics: ~/scripts/stable_baselines/pandaAndrew[01;32mhjkwon@HJK-AI-Robotics[00m:[01;34m~/scripts/stable_baselines/pandaAndrew[00m$ l[Kls
[0m[01;34m11April2022[0m                       optunaCust_V01.py
[01;34m13April2022[0m                       optunaCust_V02.py
[01;34m14April2022[0m                       optunaCustVect_V00.py
[01;34m16April2022[0m                       pandaCust.py
[01;34m17April2022[0m                       pandagraspdepth.py
[01;34m18April2022[0m                       pandaGrasp.py
[01;34m19April2022[0m                       pandareachdepth.py
[01;34m20April2022[0m                       pandaReachTestPID.ipynb
[01;34m20April2022videos[0m                 pandaWrapperBW_D_0.py
[01;34m21April2022[0m                       pandaWrapperBW.py
[01;34m6April2022[0m                        pandaWrapper.py
check                             pandaWrapperVect_BW_D_0.py
customCnn_V01.py                  pandaWrapperVect_BW_D_0_Supervised.py
customCnn_V02.py                  pickPlaceVectOptV01.py
[01;34mimages[0m                            PPANDPUSH_VECT.py
imageVecRenderSupervised_V00.py   pybulletCust.py
linearSchedule.py                 [01;34m__pycache__[0m
loadDataSupervised.py             register.py
[01;34mlogs[0m                              render.py
mainFull_V00.py                   robottaskenv.py
mainFull_V01.py                   [01;34mruns[0m
mainFull_V02.py                   [01;34msupervisedLearning[0m
mainMulti.py                      supervisedTesting.py
mainOpt_V01.py                    supervisedTest.ipynb
mainOpt_V02.py                    timer.py
mainOptVect_V00.py                [01;34mtmp[0m
multiCompare.py                   [01;34mtmpMulti[0m
networkPretrainSupervised_V00.py  [01;34mtmpTestMulti[0m
optimize_V01.py                   [01;34mvideos[0m
optimize_V02.py                   Y_April20
optimizeVect_V00.py
(rl_env) ]0;hjkwon@HJK-AI-Robotics: ~/scripts/stable_baselines/pandaAndrew[01;32mhjkwon@HJK-AI-Robotics[00m:[01;34m~/scripts/stable_baselines/pandaAndrew[00m$ python3  [K[A(rl_env) ]0;hjkwon@HJK-AI-Robotics: ~/scripts/stable_baselines/pandaAndrew[01;32mhjkwon@HJK-AI-Robotics[00m:[01;34m~/scripts/stable_baselines/pandaAndrew[00m$ python3  [K[A(rl_env) ]0;hjkwon@HJK-AI-Robotics: ~/scripts/stable_baselines/pandaAndrew[01;32mhjkwon@HJK-AI-Robotics[00m:[01;34m~/scripts/stable_baselines/pandaAndrew[00m$ python3 [K(rl_env) ]0;hjkwon@HJK-AI-Robotics: ~/scripts/stable_baselines/pandaAndrew[01;32mhjkwon@HJK-AI-Robotics[00m:[01;34m~/scripts/stable_baselines/pandaAndrew[00m$ python3 [K(rl_env) ]0;hjkwon@HJK-AI-Robotics: ~/scripts/stable_baselines/pandaAndrew[01;32mhjkwon@HJK-AI-Robotics[00m:[01;34m~/scripts/stable_baselines/pandaAndrew[00m$ python3 [K(rl_env) ]0;hjkwon@HJK-AI-Robotics: ~/scripts/stable_baselines/pandaAndrew[01;32mhjkwon@HJK-AI-Robotics[00m:[01;34m~/scripts/stable_baselines/pandaAndrew[00m$ python3 [K(rl_env) ]0;hjkwon@HJK-AI-Robotics: ~/scripts/stable_baselines/pandaAndrew[01;32mhjkwon@HJK-AI-Robotics[00m:[01;34m~/scripts/stable_baselines/pandaAndrew[00m$ python3 [K(rl_env) ]0;hjkwon@HJK-AI-Robotics: ~/scripts/stable_baselines/pandaAndrew[01;32mhjkwon@HJK-AI-Robotics[00m:[01;34m~/scripts/stable_baselines/pandaAndrew[00m$ python3 [K(rl_env) ]0;hjkwon@HJK-AI-Robotics: ~/scripts/stable_baselines/pandaAndrew[01;32mhjkwon@HJK-AI-Robotics[00m:[01;34m~/scripts/stable_baselines/pandaAndrew[00m$ python3 [K(rl_env) ]0;hjkwon@HJK-AI-Robotics: ~/scripts/stable_baselines/pandaAndrew[01;32mhjkwon@HJK-AI-Robotics[00m:[01;34m~/scripts/stable_baselines/pandaAndrew[00m$ python3 [K(rl_env) ]0;hjkwon@HJK-AI-Robotics: ~/scripts/stable_baselines/pandaAndrew[01;32mhjkwon@HJK-AI-Robotics[00m:[01;34m~/scripts/stable_baselines/pandaAndrew[00m$ python3 [K(rl_env) ]0;hjkwon@HJK-AI-Robotics: ~/scripts/stable_baselines/pandaAndrew[01;32mhjkwon@HJK-AI-Robotics[00m:[01;34m~/scripts/stable_baselines/pandaAndrew[00m$ python3 [K(rl_env) ]0;hjkwon@HJK-AI-Robotics: ~/scripts/stable_baselines/pandaAndrew[01;32mhjkwon@HJK-AI-Robotics[00m:[01;34m~/scripts/stable_baselines/pandaAndrew[00m$ python3 [K(rl_env) ]0;hjkwon@HJK-AI-Robotics: ~/scripts/stable_baselines/pandaAndrew[01;32mhjkwon@HJK-AI-Robotics[00m:[01;34m~/scripts/stable_baselines/pandaAndrew[00m$ python3 [K(rl_env) ]0;hjkwon@HJK-AI-Robotics: ~/scripts/stable_baselines/pandaAndrew[01;32mhjkwon@HJK-AI-Robotics[00m:[01;34m~/scripts/stable_baselines/pandaAndrew[00m$ python3 [K(rl_env) ]0;hjkwon@HJK-AI-Robotics: ~/scripts/stable_baselines/pandaAndrew[01;32mhjkwon@HJK-AI-Robotics[00m:[01;34m~/scripts/stable_baselines/pandaAndrew[00m$ python3 [K(rl_env) ]0;hjkwon@HJK-AI-Robotics: ~/scripts/stable_baselines/pandaAndrew[01;32mhjkwon@HJK-AI-Robotics[00m:[01;34m~/scripts/stable_baselines/pandaAndrew[00m$ python3 [K(rl_env) ]0;hjkwon@HJK-AI-Robotics: ~/scripts/stable_baselines/pandaAndrew[01;32mhjkwon@HJK-AI-Robotics[00m:[01;34m~/scripts/stable_baselines/pandaAndrew[00m$ python3 [K(rl_env) ]0;hjkwon@HJK-AI-Robotics: ~/scripts/stable_baselines/pandaAndrew[01;32mhjkwon@HJK-AI-Robotics[00m:[01;34m~/scripts/stable_baselines/pandaAndrew[00m$ python3 [K(rl_env) ]0;hjkwon@HJK-AI-Robotics: ~/scripts/stable_baselines/pandaAndrew[01;32mhjkwon@HJK-AI-Robotics[00m:[01;34m~/scripts/stable_baselines/pandaAndrew[00m$ python3 [K(rl_env) ]0;hjkwon@HJK-AI-Robotics: ~/scripts/stable_baselines/pandaAndrew[01;32mhjkwon@HJK-AI-Robotics[00m:[01;34m~/scripts/stable_baselines/pandaAndrew[00m$ python3 mainOptVect_V00.py [K
pybullet build time: Dec  1 2021 18:34:28
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
plotly:  5.6.0
optuna:  2.10.0
[32m[I 2022-04-21 10:44:35,426][0m A new study created in memory with name: no-name-3b5a0e34-e928-4308-b060-04e8eea35a52[0m

 

{'policy': 'MlpPolicy', 'device': 'cuda', 'verbose': 1, 'n_steps': 16384, 'gamma': 0.9115869246832825, 'gae_lambda': 0.921739535496992, 'learning_rate': 0.003392275233457485, 'ent_coef': 1e-06, 'max_grad_norm': 0.5, 'vf_coef': 0.75, 'clip_range': 0.075, 'batch_size': 1024, 'n_epochs': 6, 'policy_kwargs': {'net_arch': [{'pi': [128, 128, 128, 128, 128], 'vf': [128, 128, 128, 128, 128]}]}}
n_envs:  6

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
_XSERVTransSocketUNIXCreateListener: ...SocketCreateListener() failed
_XSERVTransMakeAllCOTSServerListeners: server already running
(EE) 
Fatal server error:
(EE) Cannot establish any listening sockets - Make sure an X server isn't already running(EE) 
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
Using cuda device
Eval num_timesteps=60000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -50      |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 60000    |
---------------------------------
New best mean reward!
Elapsed time: 101.8300 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.1     |
|    ep_rew_mean     | -49.1    |
| time/              |          |
|    fps             | 595      |
|    iterations      | 1        |
|    time_elapsed    | 165      |
|    total_timesteps | 98304    |
---------------------------------
Eval num_timesteps=120000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | -50         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 120000      |
| train/                  |             |
|    approx_kl            | 0.002029502 |
|    clip_fraction        | 0.272       |
|    clip_range           | 0.075       |
|    entropy_loss         | -5.68       |
|    explained_variance   | -0.079      |
|    learning_rate        | 0.00339     |
|    loss                 | 0.799       |
|    n_updates            | 6           |
|    policy_gradient_loss | 0.0047      |
|    std                  | 0.999       |
|    value_loss           | 1.44        |
-----------------------------------------
Elapsed time: 103.5965 seconds
Eval num_timesteps=180000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -50      |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 180000   |
---------------------------------
Elapsed time: 99.9043 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.2     |
|    ep_rew_mean     | -49.2    |
| time/              |          |
|    fps             | 588      |
|    iterations      | 2        |
|    time_elapsed    | 333      |
|    total_timesteps | 196608   |
---------------------------------
^C[33m[W 2022-04-21 10:50:21,213][0m Your study does not have any completed trials.[0m
[33m[W 2022-04-21 10:50:21,256][0m Study instance does not contain trials.[0m
[33m[W 2022-04-21 10:50:21,259][0m Study instance does not contain completed trials.[0m
[33m[W 2022-04-21 10:50:21,260][0m Your study does not have any completed trials.[0m
[33m[W 2022-04-21 10:50:21,262][0m There are no complete trials.[0m
[33m[W 2022-04-21 10:50:21,274][0m Your study does not have any completed trials.[0m

 

Number of finished trials:  1
Best trial:
Traceback (most recent call last):
  File "/home/hjkwon/scripts/stable_baselines/pandaAndrew/mainOptVect_V00.py", line 73, in <module>
    trial = study.best_trial
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/optuna/study/study.py", line 97, in best_trial
    return copy.deepcopy(self._storage.get_best_trial(self._study_id))
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/optuna/storages/_in_memory.py", line 311, in get_best_trial
    raise ValueError("No trials are completed yet.")
ValueError: No trials are completed yet.
(rl_env) ]0;hjkwon@HJK-AI-Robotics: ~/scripts/stable_baselines/pandaAndrew[01;32mhjkwon@HJK-AI-Robotics[00m:[01;34m~/scripts/stable_baselines/pandaAndrew[00m$ python3 mainOptVect_V00.py
pybullet build time: Dec  1 2021 18:34:28
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
plotly:  5.6.0
optuna:  2.10.0
[32m[I 2022-04-21 10:50:31,527][0m A new study created in memory with name: no-name-4e61e90a-0e7f-41a2-943c-4c67f83eedbe[0m

 

{'policy': 'MlpPolicy', 'device': 'cuda', 'verbose': 1, 'n_steps': 16384, 'gamma': 0.9444511698133526, 'gae_lambda': 0.8783764641285153, 'learning_rate': 0.0008866517604970805, 'ent_coef': 1e-06, 'max_grad_norm': 0.5, 'vf_coef': 0.75, 'clip_range': 0.075, 'batch_size': 2048, 'n_epochs': 6, 'policy_kwargs': {'net_arch': [{'pi': [64, 64, 64, 64], 'vf': [64, 64, 64, 64]}]}}
n_envs:  4

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
_XSERVTransSocketUNIXCreateListener: ...SocketCreateListener() failed
_XSERVTransMakeAllCOTSServerListeners: server already running
(EE) 
Fatal server error:
(EE) Cannot establish any listening sockets - Make sure an X server isn't already running(EE) 
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
Using cuda device
Eval num_timesteps=40000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -50      |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
New best mean reward!
Elapsed time: 70.0850 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.4     |
|    ep_rew_mean     | -48.3    |
| time/              |          |
|    fps             | 575      |
|    iterations      | 1        |
|    time_elapsed    | 113      |
|    total_timesteps | 65536    |
---------------------------------
Eval num_timesteps=80000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 50            |
|    mean_reward          | -50           |
|    success_rate         | 0             |
| time/                   |               |
|    total_timesteps      | 80000         |
| train/                  |               |
|    approx_kl            | 0.00089602685 |
|    clip_fraction        | 0.0756        |
|    clip_range           | 0.075         |
|    entropy_loss         | -5.68         |
|    explained_variance   | -0.141        |
|    learning_rate        | 0.000887      |
|    loss                 | 0.894         |
|    n_updates            | 6             |
|    policy_gradient_loss | -0.000587     |
|    std                  | 1             |
|    value_loss           | 4.07          |
-------------------------------------------
Elapsed time: 69.8770 seconds
Eval num_timesteps=120000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -50      |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 120000   |
---------------------------------
Elapsed time: 67.7964 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.1     |
|    ep_rew_mean     | -49.1    |
| time/              |          |
|    fps             | 577      |
|    iterations      | 2        |
|    time_elapsed    | 226      |
|    total_timesteps | 131072   |
---------------------------------
Eval num_timesteps=160000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 50            |
|    mean_reward          | -50           |
|    success_rate         | 0             |
| time/                   |               |
|    total_timesteps      | 160000        |
| train/                  |               |
|    approx_kl            | 0.00091379264 |
|    clip_fraction        | 0.0642        |
|    clip_range           | 0.075         |
|    entropy_loss         | -5.68         |
|    explained_variance   | 0.27          |
|    learning_rate        | 0.000887      |
|    loss                 | 0.525         |
|    n_updates            | 12            |
|    policy_gradient_loss | -0.000591     |
|    std                  | 0.998         |
|    value_loss           | 1.64          |
-------------------------------------------
Elapsed time: 70.6083 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.1     |
|    ep_rew_mean     | -49      |
| time/              |          |
|    fps             | 575      |
|    iterations      | 3        |
|    time_elapsed    | 341      |
|    total_timesteps | 196608   |
---------------------------------
Eval num_timesteps=200000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | -50         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 200000      |
| train/                  |             |
|    approx_kl            | 0.001125821 |
|    clip_fraction        | 0.0958      |
|    clip_range           | 0.075       |
|    entropy_loss         | -5.67       |
|    explained_variance   | 0.327       |
|    learning_rate        | 0.000887    |
|    loss                 | 0.534       |
|    n_updates            | 18          |
|    policy_gradient_loss | -0.000903   |
|    std                  | 0.998       |
|    value_loss           | 0.99        |
-----------------------------------------
Elapsed time: 70.5952 seconds
Eval num_timesteps=240000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -50      |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 240000   |
---------------------------------
Elapsed time: 69.3563 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.4     |
|    ep_rew_mean     | -49.4    |
| time/              |          |
|    fps             | 574      |
|    iterations      | 4        |
|    time_elapsed    | 456      |
|    total_timesteps | 262144   |
---------------------------------
TRIAL Finished: Objective Trial Time
Elapsed time: 459.9835 seconds
[32m[I 2022-04-21 10:58:11,512][0m Trial 0 finished with value: -50.0 and parameters: {'gamma': 0.9444511698133526, 'gae_lambda': 0.8783764641285153, 'lr': 0.0008866517604970805, 'n_epochs': 6, 'batch_size': 11, 'net_arch_width_int': 6, 'net_arch_depth': 4, 'n_envs': 4}. Best is trial 0 with value: -50.0.[0m

 

{'policy': 'MlpPolicy', 'device': 'cuda', 'verbose': 1, 'n_steps': 16384, 'gamma': 0.9463063087190461, 'gae_lambda': 0.83759341585761, 'learning_rate': 0.0007045195795725124, 'ent_coef': 1e-06, 'max_grad_norm': 0.5, 'vf_coef': 0.75, 'clip_range': 0.075, 'batch_size': 8192, 'n_epochs': 7, 'policy_kwargs': {'net_arch': [{'pi': [128, 128, 128, 128, 128], 'vf': [128, 128, 128, 128, 128]}]}}
n_envs:  3

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
Using cuda device
Eval num_timesteps=30000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -50      |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
New best mean reward!
Elapsed time: 55.3747 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.5     |
|    ep_rew_mean     | -48.4    |
| time/              |          |
|    fps             | 531      |
|    iterations      | 1        |
|    time_elapsed    | 92       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 50            |
|    mean_reward          | -50           |
|    success_rate         | 0             |
| time/                   |               |
|    total_timesteps      | 60000         |
| train/                  |               |
|    approx_kl            | 0.00072676403 |
|    clip_fraction        | 0.0459        |
|    clip_range           | 0.075         |
|    entropy_loss         | -5.67         |
|    explained_variance   | -0.224        |
|    learning_rate        | 0.000705      |
|    loss                 | 0.535         |
|    n_updates            | 7             |
|    policy_gradient_loss | -0.00108      |
|    std                  | 0.998         |
|    value_loss           | 5.57          |
-------------------------------------------
Elapsed time: 57.5778 seconds
Eval num_timesteps=90000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -50      |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 90000    |
---------------------------------
Elapsed time: 57.0294 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.6     |
|    ep_rew_mean     | -49.6    |
| time/              |          |
|    fps             | 530      |
|    iterations      | 2        |
|    time_elapsed    | 185      |
|    total_timesteps | 98304    |
---------------------------------
Eval num_timesteps=120000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50           |
|    mean_reward          | -50          |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 120000       |
| train/                  |              |
|    approx_kl            | 0.0012593603 |
|    clip_fraction        | 0.104        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.67        |
|    explained_variance   | 0.102        |
|    learning_rate        | 0.000705     |
|    loss                 | 0.375        |
|    n_updates            | 14           |
|    policy_gradient_loss | -0.000343    |
|    std                  | 1            |
|    value_loss           | 2.4          |
------------------------------------------
Elapsed time: 54.4122 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.9     |
|    ep_rew_mean     | -48.9    |
| time/              |          |
|    fps             | 536      |
|    iterations      | 3        |
|    time_elapsed    | 274      |
|    total_timesteps | 147456   |
---------------------------------
Eval num_timesteps=150000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50           |
|    mean_reward          | -50          |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 150000       |
| train/                  |              |
|    approx_kl            | 0.0007583297 |
|    clip_fraction        | 0.0994       |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.69        |
|    explained_variance   | 0.0791       |
|    learning_rate        | 0.000705     |
|    loss                 | 0.342        |
|    n_updates            | 21           |
|    policy_gradient_loss | 2.93e-05     |
|    std                  | 1            |
|    value_loss           | 2.21         |
------------------------------------------
Elapsed time: 56.6073 seconds
Eval num_timesteps=180000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -50      |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 180000   |
---------------------------------
Elapsed time: 56.8654 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.6     |
|    ep_rew_mean     | -49.6    |
| time/              |          |
|    fps             | 533      |
|    iterations      | 4        |
|    time_elapsed    | 368      |
|    total_timesteps | 196608   |
---------------------------------
Eval num_timesteps=210000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50           |
|    mean_reward          | -50          |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 210000       |
| train/                  |              |
|    approx_kl            | 0.0010152004 |
|    clip_fraction        | 0.0837       |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.7         |
|    explained_variance   | 0.0535       |
|    learning_rate        | 0.000705     |
|    loss                 | 0.352        |
|    n_updates            | 28           |
|    policy_gradient_loss | 5.82e-05     |
|    std                  | 1.01         |
|    value_loss           | 1.53         |
------------------------------------------
Elapsed time: 56.6045 seconds
Eval num_timesteps=240000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -50      |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 240000   |
---------------------------------
Elapsed time: 57.2466 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | -50      |
| time/              |          |
|    fps             | 531      |
|    iterations      | 5        |
|    time_elapsed    | 462      |
|    total_timesteps | 245760   |
---------------------------------
TRIAL Finished: Objective Trial Time
Elapsed time: 462.6202 seconds
[32m[I 2022-04-21 11:05:54,229][0m Trial 1 finished with value: -50.0 and parameters: {'gamma': 0.9463063087190461, 'gae_lambda': 0.83759341585761, 'lr': 0.0007045195795725124, 'n_epochs': 7, 'batch_size': 13, 'net_arch_width_int': 7, 'net_arch_depth': 5, 'n_envs': 3}. Best is trial 0 with value: -50.0.[0m

 

{'policy': 'MlpPolicy', 'device': 'cuda', 'verbose': 1, 'n_steps': 16384, 'gamma': 0.9416151367859513, 'gae_lambda': 0.9383696813698079, 'learning_rate': 0.004031181808591559, 'ent_coef': 1e-06, 'max_grad_norm': 0.5, 'vf_coef': 0.75, 'clip_range': 0.075, 'batch_size': 512, 'n_epochs': 6, 'policy_kwargs': {'net_arch': [{'pi': [64, 64, 64, 64, 64], 'vf': [64, 64, 64, 64, 64]}]}}
n_envs:  3

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
Using cuda device
Eval num_timesteps=30000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -50      |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
New best mean reward!
Elapsed time: 56.0064 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.2     |
|    ep_rew_mean     | -49.2    |
| time/              |          |
|    fps             | 539      |
|    iterations      | 1        |
|    time_elapsed    | 91       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50           |
|    mean_reward          | -50          |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 60000        |
| train/                  |              |
|    approx_kl            | 0.0017506741 |
|    clip_fraction        | 0.215        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.68        |
|    explained_variance   | -0.0627      |
|    learning_rate        | 0.00403      |
|    loss                 | 1.96         |
|    n_updates            | 6            |
|    policy_gradient_loss | 0.0032       |
|    std                  | 0.998        |
|    value_loss           | 3.5          |
------------------------------------------
Elapsed time: 60.0758 seconds
Eval num_timesteps=90000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -50      |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 90000    |
---------------------------------
Elapsed time: 54.4276 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.4     |
|    ep_rew_mean     | -48.3    |
| time/              |          |
|    fps             | 530      |
|    iterations      | 2        |
|    time_elapsed    | 185      |
|    total_timesteps | 98304    |
---------------------------------
Eval num_timesteps=120000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50           |
|    mean_reward          | -50          |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 120000       |
| train/                  |              |
|    approx_kl            | 0.0018193783 |
|    clip_fraction        | 0.235        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.67        |
|    explained_variance   | 0.37         |
|    learning_rate        | 0.00403      |
|    loss                 | 1.07         |
|    n_updates            | 12           |
|    policy_gradient_loss | 0.00314      |
|    std                  | 1            |
|    value_loss           | 1.61         |
------------------------------------------
Elapsed time: 58.4989 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.9     |
|    ep_rew_mean     | -49.9    |
| time/              |          |
|    fps             | 531      |
|    iterations      | 3        |
|    time_elapsed    | 277      |
|    total_timesteps | 147456   |
---------------------------------
Eval num_timesteps=150000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | -50         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 150000      |
| train/                  |             |
|    approx_kl            | 0.002454665 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.075       |
|    entropy_loss         | -5.7        |
|    explained_variance   | 0.4         |
|    learning_rate        | 0.00403     |
|    loss                 | 0.7         |
|    n_updates            | 18          |
|    policy_gradient_loss | 0.00211     |
|    std                  | 1           |
|    value_loss           | 0.963       |
-----------------------------------------
Elapsed time: 58.2682 seconds
Eval num_timesteps=180000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -50      |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 180000   |
---------------------------------
Elapsed time: 54.1164 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | -50      |
| time/              |          |
|    fps             | 529      |
|    iterations      | 4        |
|    time_elapsed    | 371      |
|    total_timesteps | 196608   |
---------------------------------
Eval num_timesteps=210000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50           |
|    mean_reward          | -50          |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 210000       |
| train/                  |              |
|    approx_kl            | 0.0032275708 |
|    clip_fraction        | 0.326        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.69        |
|    explained_variance   | 0.381        |
|    learning_rate        | 0.00403      |
|    loss                 | 0.9          |
|    n_updates            | 24           |
|    policy_gradient_loss | 0.00252      |
|    std                  | 1            |
|    value_loss           | 0.708        |
------------------------------------------
Elapsed time: 64.8542 seconds
Eval num_timesteps=240000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -50      |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 240000   |
---------------------------------
Elapsed time: 54.6248 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49       |
|    ep_rew_mean     | -49      |
| time/              |          |
|    fps             | 521      |
|    iterations      | 5        |
|    time_elapsed    | 470      |
|    total_timesteps | 245760   |
---------------------------------
TRIAL Finished: Objective Trial Time
Elapsed time: 474.6949 seconds
[32m[I 2022-04-21 11:13:49,021][0m Trial 2 finished with value: -50.0 and parameters: {'gamma': 0.9416151367859513, 'gae_lambda': 0.9383696813698079, 'lr': 0.004031181808591559, 'n_epochs': 6, 'batch_size': 9, 'net_arch_width_int': 6, 'net_arch_depth': 5, 'n_envs': 3}. Best is trial 0 with value: -50.0.[0m

 

{'policy': 'MlpPolicy', 'device': 'cuda', 'verbose': 1, 'n_steps': 16384, 'gamma': 0.957950258954539, 'gae_lambda': 0.8879533741356276, 'learning_rate': 0.0012513386809314124, 'ent_coef': 1e-06, 'max_grad_norm': 0.5, 'vf_coef': 0.75, 'clip_range': 0.075, 'batch_size': 2048, 'n_epochs': 7, 'policy_kwargs': {'net_arch': [{'pi': [128, 128, 128, 128], 'vf': [128, 128, 128, 128]}]}}
n_envs:  5

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
Using cuda device
Eval num_timesteps=50000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -50      |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 50000    |
---------------------------------
New best mean reward!
Elapsed time: 83.8586 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49       |
|    ep_rew_mean     | -48.9    |
| time/              |          |
|    fps             | 598      |
|    iterations      | 1        |
|    time_elapsed    | 136      |
|    total_timesteps | 81920    |
---------------------------------
Eval num_timesteps=100000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50           |
|    mean_reward          | -50          |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 100000       |
| train/                  |              |
|    approx_kl            | 0.0008115964 |
|    clip_fraction        | 0.0836       |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.67        |
|    explained_variance   | -0.0719      |
|    learning_rate        | 0.00125      |
|    loss                 | 0.991        |
|    n_updates            | 7            |
|    policy_gradient_loss | 0.000398     |
|    std                  | 1            |
|    value_loss           | 2.73         |
------------------------------------------
Elapsed time: 83.9447 seconds
Eval num_timesteps=150000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -50      |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 150000   |
---------------------------------
Elapsed time: 85.3847 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | -50      |
| time/              |          |
|    fps             | 592      |
|    iterations      | 2        |
|    time_elapsed    | 276      |
|    total_timesteps | 163840   |
---------------------------------
Eval num_timesteps=200000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | -50         |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 200000      |
| train/                  |             |
|    approx_kl            | 0.001009626 |
|    clip_fraction        | 0.0888      |
|    clip_range           | 0.075       |
|    entropy_loss         | -5.67       |
|    explained_variance   | 0.325       |
|    learning_rate        | 0.00125     |
|    loss                 | 0.814       |
|    n_updates            | 14          |
|    policy_gradient_loss | -0.000281   |
|    std                  | 0.997       |
|    value_loss           | 1.73        |
-----------------------------------------
Elapsed time: 84.3465 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.2     |
|    ep_rew_mean     | -49.2    |
| time/              |          |
|    fps             | 595      |
|    iterations      | 3        |
|    time_elapsed    | 412      |
|    total_timesteps | 245760   |
---------------------------------
TRIAL Finished: Objective Trial Time
Elapsed time: 414.5827 seconds
[32m[I 2022-04-21 11:20:43,700][0m Trial 3 finished with value: -50.0 and parameters: {'gamma': 0.957950258954539, 'gae_lambda': 0.8879533741356276, 'lr': 0.0012513386809314124, 'n_epochs': 7, 'batch_size': 11, 'net_arch_width_int': 7, 'net_arch_depth': 4, 'n_envs': 5}. Best is trial 0 with value: -50.0.[0m

 

{'policy': 'MlpPolicy', 'device': 'cuda', 'verbose': 1, 'n_steps': 16384, 'gamma': 0.9778504780653963, 'gae_lambda': 0.9448504013260292, 'learning_rate': 0.0011179085442055154, 'ent_coef': 1e-06, 'max_grad_norm': 0.5, 'vf_coef': 0.75, 'clip_range': 0.075, 'batch_size': 1024, 'n_epochs': 6, 'policy_kwargs': {'net_arch': [{'pi': [64, 64, 64, 64, 64], 'vf': [64, 64, 64, 64, 64]}]}}
n_envs:  4

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
Using cuda device
^CTraceback (most recent call last):
  File "/home/hjkwon/scripts/stable_baselines/pandaAndrew/mainOptVect_V00.py", line 66, in <module>
    Plotter(study)
  File "/home/hjkwon/scripts/stable_baselines/pandaAndrew/optunaCustVect_V00.py", line 352, in Plotter
    fig3 = plot_param_importances(study)
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/optuna/visualization/_param_importances.py", line 112, in plot_param_importances
    importances = optuna.importance.get_param_importances(
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/optuna/importance/__init__.py", line 93, in get_param_importances
    return evaluator.evaluate(study, params=params, target=target)
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/optuna/importance/_fanova/_evaluator.py", line 122, in evaluate
    evaluator.fit(
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/optuna/importance/_fanova/_fanova.py", line 81, in fit
    raise RuntimeError("Encountered zero total variance in all trees.")
RuntimeError: Encountered zero total variance in all trees.
(rl_env) ]0;hjkwon@HJK-AI-Robotics: ~/scripts/stable_baselines/pandaAndrew[01;32mhjkwon@HJK-AI-Robotics[00m:[01;34m~/scripts/stable_baselines/pandaAndrew[00m$ python3 mainOptVect_V00.py
pybullet build time: Dec  1 2021 18:34:28
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
plotly:  5.6.0
optuna:  2.10.0
[32m[I 2022-04-21 11:47:10,455][0m A new study created in memory with name: no-name-caf50ceb-d812-46f1-863f-6fe8f7a750d0[0m

 

{'policy': 'MlpPolicy', 'device': 'cuda', 'verbose': 1, 'n_steps': 1024, 'gamma': 0.9936917197622922, 'gae_lambda': 0.8992549163207286, 'learning_rate': 0.0049264827998270234, 'ent_coef': 1e-06, 'max_grad_norm': 0.5, 'vf_coef': 0.75, 'clip_range': 0.075, 'batch_size': 1024, 'n_epochs': 13, 'policy_kwargs': {'net_arch': [{'pi': [128, 128, 128], 'vf': [128, 128, 128]}]}}
n_envs:  3

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
_XSERVTransSocketUNIXCreateListener: ...SocketCreateListener() failed
_XSERVTransMakeAllCOTSServerListeners: server already running
(EE) 
Fatal server error:
(EE) Cannot establish any listening sockets - Make sure an X server isn't already running(EE) 
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.4     |
|    ep_rew_mean     | -13.7    |
| time/              |          |
|    fps             | 527      |
|    iterations      | 1        |
|    time_elapsed    | 5        |
|    total_timesteps | 3072     |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 49.6         |
|    ep_rew_mean          | -12.2        |
| time/                   |              |
|    fps                  | 558          |
|    iterations           | 2            |
|    time_elapsed         | 10           |
|    total_timesteps      | 6144         |
| train/                  |              |
|    approx_kl            | 0.0033034666 |
|    clip_fraction        | 0.276        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.66        |
|    explained_variance   | 0.0154       |
|    learning_rate        | 0.00493      |
|    loss                 | 0.795        |
|    n_updates            | 13           |
|    policy_gradient_loss | -0.00536     |
|    std                  | 1            |
|    value_loss           | 1.64         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 49.6         |
|    ep_rew_mean          | -11.6        |
| time/                   |              |
|    fps                  | 568          |
|    iterations           | 3            |
|    time_elapsed         | 16           |
|    total_timesteps      | 9216         |
| train/                  |              |
|    approx_kl            | 0.0046892855 |
|    clip_fraction        | 0.294        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.67        |
|    explained_variance   | 0.33         |
|    learning_rate        | 0.00493      |
|    loss                 | 0.519        |
|    n_updates            | 26           |
|    policy_gradient_loss | -0.00457     |
|    std                  | 1            |
|    value_loss           | 1.33         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 50          |
|    ep_rew_mean          | -11.3       |
| time/                   |             |
|    fps                  | 536         |
|    iterations           | 4           |
|    time_elapsed         | 22          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.003932353 |
|    clip_fraction        | 0.319       |
|    clip_range           | 0.075       |
|    entropy_loss         | -5.67       |
|    explained_variance   | 0.377       |
|    learning_rate        | 0.00493     |
|    loss                 | 0.722       |
|    n_updates            | 39          |
|    policy_gradient_loss | -0.0041     |
|    std                  | 0.998       |
|    value_loss           | 1.39        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 49.6        |
|    ep_rew_mean          | -11.1       |
| time/                   |             |
|    fps                  | 537         |
|    iterations           | 5           |
|    time_elapsed         | 28          |
|    total_timesteps      | 15360       |
| train/                  |             |
|    approx_kl            | 0.005327253 |
|    clip_fraction        | 0.291       |
|    clip_range           | 0.075       |
|    entropy_loss         | -5.67       |
|    explained_variance   | 0.411       |
|    learning_rate        | 0.00493     |
|    loss                 | 0.613       |
|    n_updates            | 52          |
|    policy_gradient_loss | -0.00877    |
|    std                  | 0.998       |
|    value_loss           | 1.25        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 48.9        |
|    ep_rew_mean          | -10.1       |
| time/                   |             |
|    fps                  | 534         |
|    iterations           | 6           |
|    time_elapsed         | 34          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.007283076 |
|    clip_fraction        | 0.286       |
|    clip_range           | 0.075       |
|    entropy_loss         | -5.67       |
|    explained_variance   | 0.664       |
|    learning_rate        | 0.00493     |
|    loss                 | 0.897       |
|    n_updates            | 65          |
|    policy_gradient_loss | -0.00498    |
|    std                  | 0.995       |
|    value_loss           | 1.88        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 47.8        |
|    ep_rew_mean          | -9.76       |
| time/                   |             |
|    fps                  | 526         |
|    iterations           | 7           |
|    time_elapsed         | 40          |
|    total_timesteps      | 21504       |
| train/                  |             |
|    approx_kl            | 0.008262813 |
|    clip_fraction        | 0.404       |
|    clip_range           | 0.075       |
|    entropy_loss         | -5.64       |
|    explained_variance   | 0.653       |
|    learning_rate        | 0.00493     |
|    loss                 | 0.796       |
|    n_updates            | 78          |
|    policy_gradient_loss | -0.00447    |
|    std                  | 0.988       |
|    value_loss           | 1.36        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 47.5         |
|    ep_rew_mean          | -9.48        |
| time/                   |              |
|    fps                  | 522          |
|    iterations           | 8            |
|    time_elapsed         | 47           |
|    total_timesteps      | 24576        |
| train/                  |              |
|    approx_kl            | 0.0050010975 |
|    clip_fraction        | 0.313        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.63        |
|    explained_variance   | 0.679        |
|    learning_rate        | 0.00493      |
|    loss                 | 1.75         |
|    n_updates            | 91           |
|    policy_gradient_loss | -0.0087      |
|    std                  | 0.99         |
|    value_loss           | 2.83         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 47.5        |
|    ep_rew_mean          | -9.44       |
| time/                   |             |
|    fps                  | 524         |
|    iterations           | 9           |
|    time_elapsed         | 52          |
|    total_timesteps      | 27648       |
| train/                  |             |
|    approx_kl            | 0.007272378 |
|    clip_fraction        | 0.383       |
|    clip_range           | 0.075       |
|    entropy_loss         | -5.64       |
|    explained_variance   | 0.554       |
|    learning_rate        | 0.00493     |
|    loss                 | 2.1         |
|    n_updates            | 104         |
|    policy_gradient_loss | -0.00567    |
|    std                  | 0.987       |
|    value_loss           | 3.22        |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=-6.48 +/- 1.61
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 50         |
|    mean_reward          | -6.48      |
|    success_rate         | 0          |
| time/                   |            |
|    total_timesteps      | 30000      |
| train/                  |            |
|    approx_kl            | 0.00866774 |
|    clip_fraction        | 0.373      |
|    clip_range           | 0.075      |
|    entropy_loss         | -5.62      |
|    explained_variance   | 0.731      |
|    learning_rate        | 0.00493    |
|    loss                 | 2.48       |
|    n_updates            | 117        |
|    policy_gradient_loss | -0.00808   |
|    std                  | 0.986      |
|    value_loss           | 4.15       |
----------------------------------------
New best mean reward!
Elapsed time: 57.9677 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.5     |
|    ep_rew_mean     | -10.5    |
| time/              |          |
|    fps             | 518      |
|    iterations      | 10       |
|    time_elapsed    | 59       |
|    total_timesteps | 30720    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 48.3        |
|    ep_rew_mean          | -9.47       |
| time/                   |             |
|    fps                  | 517         |
|    iterations           | 11          |
|    time_elapsed         | 65          |
|    total_timesteps      | 33792       |
| train/                  |             |
|    approx_kl            | 0.008969044 |
|    clip_fraction        | 0.401       |
|    clip_range           | 0.075       |
|    entropy_loss         | -5.62       |
|    explained_variance   | 0.706       |
|    learning_rate        | 0.00493     |
|    loss                 | 2.85        |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.00985    |
|    std                  | 0.985       |
|    value_loss           | 4.5         |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 48.1        |
|    ep_rew_mean          | -9          |
| time/                   |             |
|    fps                  | 516         |
|    iterations           | 12          |
|    time_elapsed         | 71          |
|    total_timesteps      | 36864       |
| train/                  |             |
|    approx_kl            | 0.009270117 |
|    clip_fraction        | 0.383       |
|    clip_range           | 0.075       |
|    entropy_loss         | -5.62       |
|    explained_variance   | 0.767       |
|    learning_rate        | 0.00493     |
|    loss                 | 1.83        |
|    n_updates            | 143         |
|    policy_gradient_loss | -0.00923    |
|    std                  | 0.988       |
|    value_loss           | 3.43        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 48.1         |
|    ep_rew_mean          | -9.2         |
| time/                   |              |
|    fps                  | 519          |
|    iterations           | 13           |
|    time_elapsed         | 76           |
|    total_timesteps      | 39936        |
| train/                  |              |
|    approx_kl            | 0.0092559885 |
|    clip_fraction        | 0.377        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.63        |
|    explained_variance   | 0.722        |
|    learning_rate        | 0.00493      |
|    loss                 | 2.44         |
|    n_updates            | 156          |
|    policy_gradient_loss | -0.011       |
|    std                  | 0.988        |
|    value_loss           | 3.88         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 49          |
|    ep_rew_mean          | -9.07       |
| time/                   |             |
|    fps                  | 518         |
|    iterations           | 14          |
|    time_elapsed         | 82          |
|    total_timesteps      | 43008       |
| train/                  |             |
|    approx_kl            | 0.009482417 |
|    clip_fraction        | 0.407       |
|    clip_range           | 0.075       |
|    entropy_loss         | -5.62       |
|    explained_variance   | 0.813       |
|    learning_rate        | 0.00493     |
|    loss                 | 2.69        |
|    n_updates            | 169         |
|    policy_gradient_loss | -0.0106     |
|    std                  | 0.984       |
|    value_loss           | 5           |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 48.6       |
|    ep_rew_mean          | -9.29      |
| time/                   |            |
|    fps                  | 515        |
|    iterations           | 15         |
|    time_elapsed         | 89         |
|    total_timesteps      | 46080      |
| train/                  |            |
|    approx_kl            | 0.01590317 |
|    clip_fraction        | 0.491      |
|    clip_range           | 0.075      |
|    entropy_loss         | -5.6       |
|    explained_variance   | 0.801      |
|    learning_rate        | 0.00493    |
|    loss                 | 0.93       |
|    n_updates            | 182        |
|    policy_gradient_loss | -0.00896   |
|    std                  | 0.98       |
|    value_loss           | 2.63       |
----------------------------------------
10000----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 48.7       |
|    ep_rew_mean          | -9.89      |
| time/                   |            |
|    fps                  | 514        |
|    iterations           | 16         |
|    time_elapsed         | 95         |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.01647498 |
|    clip_fraction        | 0.524      |
|    clip_range           | 0.075      |
|    entropy_loss         | -5.59      |
|    explained_variance   | 0.743      |
|    learning_rate        | 0.00493    |
|    loss                 | 1.87       |
|    n_updates            | 195        |
|    policy_gradient_loss | -0.00794   |
|    std                  | 0.979      |
|    value_loss           | 3.99       |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 49          |
|    ep_rew_mean          | -8.49       |
| time/                   |             |
|    fps                  | 515         |
|    iterations           | 17          |
|    time_elapsed         | 101         |
|    total_timesteps      | 52224       |
| train/                  |             |
|    approx_kl            | 0.010380822 |
|    clip_fraction        | 0.455       |
|    clip_range           | 0.075       |
|    entropy_loss         | -5.57       |
|    explained_variance   | 0.852       |
|    learning_rate        | 0.00493     |
|    loss                 | 2.65        |
|    n_updates            | 208         |
|    policy_gradient_loss | -0.0123     |
|    std                  | 0.973       |
|    value_loss           | 4.62        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 48.9        |
|    ep_rew_mean          | -8.13       |
| time/                   |             |
|    fps                  | 511         |
|    iterations           | 18          |
|    time_elapsed         | 108         |
|    total_timesteps      | 55296       |
| train/                  |             |
|    approx_kl            | 0.014992633 |
|    clip_fraction        | 0.475       |
|    clip_range           | 0.075       |
|    entropy_loss         | -5.57       |
|    explained_variance   | 0.619       |
|    learning_rate        | 0.00493     |
|    loss                 | 3.03        |
|    n_updates            | 221         |
|    policy_gradient_loss | -0.0116     |
|    std                  | 0.973       |
|    value_loss           | 4.8         |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 48.8       |
|    ep_rew_mean          | -8.23      |
| time/                   |            |
|    fps                  | 510        |
|    iterations           | 19         |
|    time_elapsed         | 114        |
|    total_timesteps      | 58368      |
| train/                  |            |
|    approx_kl            | 0.01717783 |
|    clip_fraction        | 0.552      |
|    clip_range           | 0.075      |
|    entropy_loss         | -5.55      |
|    explained_variance   | 0.849      |
|    learning_rate        | 0.00493    |
|    loss                 | 1.37       |
|    n_updates            | 234        |
|    policy_gradient_loss | -0.0134    |
|    std                  | 0.969      |
|    value_loss           | 2.47       |
----------------------------------------
Eval num_timesteps=60000, episode_reward=-7.10 +/- 0.93
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | -7.1        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.020552387 |
|    clip_fraction        | 0.547       |
|    clip_range           | 0.075       |
|    entropy_loss         | -5.53       |
|    explained_variance   | 0.812       |
|    learning_rate        | 0.00493     |
|    loss                 | 1.43        |
|    n_updates            | 247         |
|    policy_gradient_loss | -0.0107     |
|    std                  | 0.962       |
|    value_loss           | 3.22        |
-----------------------------------------
Elapsed time: 60.3012 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.1     |
|    ep_rew_mean     | -8.95    |
| time/              |          |
|    fps             | 506      |
|    iterations      | 20       |
|    time_elapsed    | 121      |
|    total_timesteps | 61440    |
---------------------------------
^C[33m[W 2022-04-21 11:49:16,967][0m Your study does not have any completed trials.[0m
[33m[W 2022-04-21 11:49:16,973][0m Study instance does not contain trials.[0m
[33m[W 2022-04-21 11:49:16,975][0m Study instance does not contain completed trials.[0m
[33m[W 2022-04-21 11:49:16,976][0m Your study does not have any completed trials.[0m
[33m[W 2022-04-21 11:49:16,977][0m There are no complete trials.[0m
[33m[W 2022-04-21 11:49:16,980][0m Your study does not have any completed trials.[0m

 

Number of finished trials:  1
Best trial:
Traceback (most recent call last):
  File "/home/hjkwon/scripts/stable_baselines/pandaAndrew/mainOptVect_V00.py", line 73, in <module>
    trial = study.best_trial
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/optuna/study/study.py", line 97, in best_trial
    return copy.deepcopy(self._storage.get_best_trial(self._study_id))
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/optuna/storages/_in_memory.py", line 311, in get_best_trial
    raise ValueError("No trials are completed yet.")
ValueError: No trials are completed yet.
(rl_env) ]0;hjkwon@HJK-AI-Robotics: ~/scripts/stable_baselines/pandaAndrew[01;32mhjkwon@HJK-AI-Robotics[00m:[01;34m~/scripts/stable_baselines/pandaAndrew[00m$ python3 mainOptVect_V00.py
pybullet build time: Dec  1 2021 18:34:28
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
plotly:  5.6.0
optuna:  2.10.0
[32m[I 2022-04-21 11:49:29,478][0m A new study created in memory with name: no-name-ed161162-f88c-4e5e-9aac-84ad55ec9a56[0m

 

{'policy': 'MlpPolicy', 'device': 'cuda', 'verbose': 1, 'n_steps': 1024, 'gamma': 0.9732534662268582, 'gae_lambda': 0.8156591424011592, 'learning_rate': 0.0024448504762018255, 'ent_coef': 1e-06, 'max_grad_norm': 0.5, 'vf_coef': 0.75, 'clip_range': 0.075, 'batch_size': 1024, 'n_epochs': 14, 'policy_kwargs': {'net_arch': [{'pi': [128, 128, 128, 128, 128], 'vf': [128, 128, 128, 128, 128]}]}}
n_envs:  3

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
_XSERVTransSocketUNIXCreateListener: ...SocketCreateListener() failed
_XSERVTransMakeAllCOTSServerListeners: server already running
(EE) 
Fatal server error:
(EE) Cannot establish any listening sockets - Make sure an X server isn't already running(EE) 
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
Using cuda device
Eval num_timesteps=3000, episode_reward=-11.07 +/- 1.29
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -11.1    |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 3000     |
---------------------------------
New best mean reward!
Elapsed time: 6.7564 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.2     |
|    ep_rew_mean     | -12.6    |
| time/              |          |
|    fps             | 446      |
|    iterations      | 1        |
|    time_elapsed    | 6        |
|    total_timesteps | 3072     |
---------------------------------
Eval num_timesteps=6000, episode_reward=-9.42 +/- 1.85
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 50         |
|    mean_reward          | -9.42      |
|    success_rate         | 0          |
| time/                   |            |
|    total_timesteps      | 6000       |
| train/                  |            |
|    approx_kl            | 0.00486769 |
|    clip_fraction        | 0.314      |
|    clip_range           | 0.075      |
|    entropy_loss         | -5.67      |
|    explained_variance   | -0.0817    |
|    learning_rate        | 0.00244    |
|    loss                 | 0.115      |
|    n_updates            | 14         |
|    policy_gradient_loss | -0.0128    |
|    std                  | 0.999      |
|    value_loss           | 0.386      |
----------------------------------------
New best mean reward!
Elapsed time: 6.2424 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.5     |
|    ep_rew_mean     | -11.6    |
| time/              |          |
|    fps             | 463      |
|    iterations      | 2        |
|    time_elapsed    | 13       |
|    total_timesteps | 6144     |
---------------------------------
Eval num_timesteps=9000, episode_reward=-7.73 +/- 1.02
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | -7.73       |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 9000        |
| train/                  |             |
|    approx_kl            | 0.006696936 |
|    clip_fraction        | 0.327       |
|    clip_range           | 0.075       |
|    entropy_loss         | -5.67       |
|    explained_variance   | 0.254       |
|    learning_rate        | 0.00244     |
|    loss                 | 0.0904      |
|    n_updates            | 28          |
|    policy_gradient_loss | -0.0145     |
|    std                  | 0.994       |
|    value_loss           | 0.308       |
-----------------------------------------
New best mean reward!
Elapsed time: 6.7528 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49       |
|    ep_rew_mean     | -11.4    |
| time/              |          |
|    fps             | 458      |
|    iterations      | 3        |
|    time_elapsed    | 20       |
|    total_timesteps | 9216     |
---------------------------------
Eval num_timesteps=12000, episode_reward=-5.02 +/- 1.51
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | -5.02       |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 12000       |
| train/                  |             |
|    approx_kl            | 0.008907543 |
|    clip_fraction        | 0.396       |
|    clip_range           | 0.075       |
|    entropy_loss         | -5.65       |
|    explained_variance   | 0.534       |
|    learning_rate        | 0.00244     |
|    loss                 | 0.0711      |
|    n_updates            | 42          |
|    policy_gradient_loss | -0.0176     |
|    std                  | 0.99        |
|    value_loss           | 0.305       |
-----------------------------------------
New best mean reward!
Elapsed time: 7.1188 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.6     |
|    ep_rew_mean     | -10.8    |
| time/              |          |
|    fps             | 449      |
|    iterations      | 4        |
|    time_elapsed    | 27       |
|    total_timesteps | 12288    |
---------------------------------
TRIAL Finished: Objective Trial Time
Elapsed time: 30.0504 seconds
[32m[I 2022-04-21 11:49:59,529][0m Trial 0 finished with value: -5.0208183 and parameters: {'gamma': 0.9732534662268582, 'gae_lambda': 0.8156591424011592, 'lr': 0.0024448504762018255, 'n_epochs': 14, 'batch_size': 10, 'net_arch_width_int': 7, 'net_arch_depth': 5, 'n_envs': 3}. Best is trial 0 with value: -5.0208183.[0m

 

{'policy': 'MlpPolicy', 'device': 'cuda', 'verbose': 1, 'n_steps': 1024, 'gamma': 0.9475230211873257, 'gae_lambda': 0.9463770738423881, 'learning_rate': 0.0016934481795408097, 'ent_coef': 1e-06, 'max_grad_norm': 0.5, 'vf_coef': 0.75, 'clip_range': 0.075, 'batch_size': 256, 'n_epochs': 8, 'policy_kwargs': {'net_arch': [{'pi': [128, 128, 128, 128], 'vf': [128, 128, 128, 128]}]}}
n_envs:  6

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
Using cuda device
Eval num_timesteps=6000, episode_reward=-10.47 +/- 1.11
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -10.5    |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 6000     |
---------------------------------
New best mean reward!
Elapsed time: 10.7826 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.6     |
|    ep_rew_mean     | -11.6    |
| time/              |          |
|    fps             | 558      |
|    iterations      | 1        |
|    time_elapsed    | 10       |
|    total_timesteps | 6144     |
---------------------------------
Eval num_timesteps=12000, episode_reward=-8.09 +/- 0.94
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50           |
|    mean_reward          | -8.09        |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 12000        |
| train/                  |              |
|    approx_kl            | 0.0037294757 |
|    clip_fraction        | 0.299        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.67        |
|    explained_variance   | -0.139       |
|    learning_rate        | 0.00169      |
|    loss                 | 0.381        |
|    n_updates            | 8            |
|    policy_gradient_loss | -0.000939    |
|    std                  | 0.998        |
|    value_loss           | 0.831        |
------------------------------------------
New best mean reward!
Elapsed time: 12.6427 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.3     |
|    ep_rew_mean     | -11.4    |
| time/              |          |
|    fps             | 509      |
|    iterations      | 2        |
|    time_elapsed    | 24       |
|    total_timesteps | 12288    |
---------------------------------
TRIAL Finished: Objective Trial Time
Elapsed time: 25.4098 seconds
[32m[I 2022-04-21 11:50:25,034][0m Trial 1 finished with value: -8.088830300000001 and parameters: {'gamma': 0.9475230211873257, 'gae_lambda': 0.9463770738423881, 'lr': 0.0016934481795408097, 'n_epochs': 8, 'batch_size': 8, 'net_arch_width_int': 7, 'net_arch_depth': 4, 'n_envs': 6}. Best is trial 0 with value: -5.0208183.[0m
[33m[W 2022-04-21 11:50:26,376][0m Param net_arch_width_int unique value length is less than 2.[0m
[33m[W 2022-04-21 11:50:26,490][0m Param net_arch_width_int unique value length is less than 2.[0m
[33m[W 2022-04-21 11:50:26,602][0m Param net_arch_width_int unique value length is less than 2.[0m
[33m[W 2022-04-21 11:50:26,715][0m Param net_arch_width_int unique value length is less than 2.[0m
[33m[W 2022-04-21 11:50:26,804][0m Param net_arch_width_int unique value length is less than 2.[0m
[33m[W 2022-04-21 11:50:26,889][0m Param net_arch_width_int unique value length is less than 2.[0m
[33m[W 2022-04-21 11:50:26,961][0m Param net_arch_width_int unique value length is less than 2.[0m
[33m[W 2022-04-21 11:50:26,968][0m Param net_arch_width_int unique value length is less than 2.[0m
[33m[W 2022-04-21 11:50:26,973][0m Param net_arch_width_int unique value length is less than 2.[0m
[33m[W 2022-04-21 11:50:26,980][0m Param net_arch_width_int unique value length is less than 2.[0m
[33m[W 2022-04-21 11:50:26,986][0m Param net_arch_width_int unique value length is less than 2.[0m
[33m[W 2022-04-21 11:50:26,993][0m Param net_arch_width_int unique value length is less than 2.[0m
[33m[W 2022-04-21 11:50:26,998][0m Param net_arch_width_int unique value length is less than 2.[0m
[33m[W 2022-04-21 11:50:27,002][0m Param net_arch_width_int unique value length is less than 2.[0m

 

Number of finished trials:  2
Best trial:
  Value:  -5.0208183
  Params: 
    gamma: 0.9732534662268582
    gae_lambda: 0.8156591424011592
    lr: 0.0024448504762018255
    n_epochs: 14
    batch_size: 10
    net_arch_width_int: 7
    net_arch_depth: 5
    n_envs: 3
  User attrs:
    vf_coef: 0.75
    clip_range: 0.075
    max_grad_norm: 0.5
    batch_size: 1024
    n_steps: 1024
    ent_coef: 1e-06
    net_arch_width: 128

 
 
 

-----------------------------------------------------------------------------------
TRAINING OPTIMIZED STUDY

 
 
 

{'policy': 'MlpPolicy', 'device': 'cuda', 'verbose': 1, 'gamma': 0.9732534662268582, 'gae_lambda': 0.8156591424011592, 'learning_rate': 0.0024448504762018255, 'ent_coef': 1e-06, 'vf_coef': 0.75, 'clip_range': 0.075, 'max_grad_norm': 0.5, 'batch_size': 1024, 'n_steps': 1024, 'n_epochs': 14, 'policy_kwargs': {'net_arch': [{'pi': [128, 128, 128, 128, 128], 'vf': [128, 128, 128, 128, 128]}]}}
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
Using cuda device
Num timesteps: 3000
Best mean reward: -inf - Last mean reward per episode: -12.17
Saving new best model to 21April2022/optimizedVectStudy/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning:

positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.

---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.6     |
|    ep_rew_mean     | -12.2    |
| time/              |          |
|    fps             | 567      |
|    iterations      | 1        |
|    time_elapsed    | 5        |
|    total_timesteps | 3072     |
---------------------------------
Num timesteps: 6000
Best mean reward: -12.17 - Last mean reward per episode: -11.63
Saving new best model to 21April2022/optimizedVectStudy/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning:

positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.

------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 48.9         |
|    ep_rew_mean          | -11.6        |
| time/                   |              |
|    fps                  | 538          |
|    iterations           | 2            |
|    time_elapsed         | 11           |
|    total_timesteps      | 6144         |
| train/                  |              |
|    approx_kl            | 0.0027993445 |
|    clip_fraction        | 0.238        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.67        |
|    explained_variance   | -0.546       |
|    learning_rate        | 0.00244      |
|    loss                 | 0.132        |
|    n_updates            | 14           |
|    policy_gradient_loss | -0.00768     |
|    std                  | 0.998        |
|    value_loss           | 0.31         |
------------------------------------------
Num timesteps: 9000
Best mean reward: -11.63 - Last mean reward per episode: -11.31
Saving new best model to 21April2022/optimizedVectStudy/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning:

positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.

------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 48.9         |
|    ep_rew_mean          | -11.4        |
| time/                   |              |
|    fps                  | 530          |
|    iterations           | 3            |
|    time_elapsed         | 17           |
|    total_timesteps      | 9216         |
| train/                  |              |
|    approx_kl            | 0.0029452443 |
|    clip_fraction        | 0.217        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.67        |
|    explained_variance   | -0.275       |
|    learning_rate        | 0.00244      |
|    loss                 | 0.0746       |
|    n_updates            | 28           |
|    policy_gradient_loss | -0.00712     |
|    std                  | 0.999        |
|    value_loss           | 0.244        |
------------------------------------------
Num timesteps: 12000
Best mean reward: -11.31 - Last mean reward per episode: -10.89
Saving new best model to 21April2022/optimizedVectStudy/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning:

positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.

-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 48.4        |
|    ep_rew_mean          | -10.7       |
| time/                   |             |
|    fps                  | 518         |
|    iterations           | 4           |
|    time_elapsed         | 23          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.004466629 |
|    clip_fraction        | 0.328       |
|    clip_range           | 0.075       |
|    entropy_loss         | -5.67       |
|    explained_variance   | 0.286       |
|    learning_rate        | 0.00244     |
|    loss                 | 0.0725      |
|    n_updates            | 42          |
|    policy_gradient_loss | -0.00852    |
|    std                  | 0.998       |
|    value_loss           | 0.259       |
-----------------------------------------
Num timesteps: 15000
Best mean reward: -10.89 - Last mean reward per episode: -9.85
Saving new best model to 21April2022/optimizedVectStudy/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning:

positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.

------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 46.2         |
|    ep_rew_mean          | -9.54        |
| time/                   |              |
|    fps                  | 502          |
|    iterations           | 5            |
|    time_elapsed         | 30           |
|    total_timesteps      | 15360        |
| train/                  |              |
|    approx_kl            | 0.0067512193 |
|    clip_fraction        | 0.394        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.65        |
|    explained_variance   | 0.588        |
|    learning_rate        | 0.00244      |
|    loss                 | 0.111        |
|    n_updates            | 56           |
|    policy_gradient_loss | -0.0104      |
|    std                  | 0.992        |
|    value_loss           | 0.289        |
------------------------------------------
Num timesteps: 18000
Best mean reward: -9.85 - Last mean reward per episode: -9.15
Saving new best model to 21April2022/optimizedVectStudy/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning:

positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.

------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 46.8         |
|    ep_rew_mean          | -8.84        |
| time/                   |              |
|    fps                  | 510          |
|    iterations           | 6            |
|    time_elapsed         | 36           |
|    total_timesteps      | 18432        |
| train/                  |              |
|    approx_kl            | 0.0065034814 |
|    clip_fraction        | 0.461        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.64        |
|    explained_variance   | 0.602        |
|    learning_rate        | 0.00244      |
|    loss                 | 0.14         |
|    n_updates            | 70           |
|    policy_gradient_loss | -0.0104      |
|    std                  | 0.99         |
|    value_loss           | 0.369        |
------------------------------------------
Num timesteps: 21000
Best mean reward: -9.15 - Last mean reward per episode: -10.06
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 48.7        |
|    ep_rew_mean          | -9.96       |
| time/                   |             |
|    fps                  | 517         |
|    iterations           | 7           |
|    time_elapsed         | 41          |
|    total_timesteps      | 21504       |
| train/                  |             |
|    approx_kl            | 0.008442189 |
|    clip_fraction        | 0.424       |
|    clip_range           | 0.075       |
|    entropy_loss         | -5.62       |
|    explained_variance   | 0.723       |
|    learning_rate        | 0.00244     |
|    loss                 | 0.18        |
|    n_updates            | 84          |
|    policy_gradient_loss | -0.0103     |
|    std                  | 0.984       |
|    value_loss           | 0.367       |
-----------------------------------------

 
 
 

-----------------------------------------------------------------------------------

 
 
 

RECORDING OPTIMIZED STUDY
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/scripts/stable_baselines/pandaAndrew/pybulletCust.py:94: UserWarning:

The use of the render method is not recommended when the environment has not been created with render=True. The rendering will probably be weird. Prefer making the environment with option `render=True`. For example: `env = gym.make('PandaReach-v2', render=True)`.

/home/hjkwon/scripts/stable_baselines/pandaAndrew/pybulletCust.py:94: UserWarning:

The use of the render method is not recommended when the environment has not been created with render=True. The rendering will probably be weird. Prefer making the environment with option `render=True`. For example: `env = gym.make('PandaReach-v2', render=True)`.

Saving video to /home/hjkwon/scripts/stable_baselines/pandaAndrew/21April2022/videos/ppo-PandaReach-Vect-step-0-to-step-400.mp4
Exception ignored in: <function VecVideoRecorder.__del__ at 0x7f60b187c040>
Traceback (most recent call last):
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/stable_baselines3/common/vec_env/vec_video_recorder.py", line 113, in __del__
    self.close()
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/stable_baselines3/common/vec_env/vec_video_recorder.py", line 109, in close
    VecEnvWrapper.close(self)
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/stable_baselines3/common/vec_env/base_vec_env.py", line 278, in close
    return self.venv.close()
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 67, in close
    env.close()
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/gym/core.py", line 257, in close
    return self.env.close()
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/gym/core.py", line 257, in close
    return self.env.close()
  File "/home/hjkwon/scripts/stable_baselines/pandaAndrew/robottaskenv.py", line 73, in close
    self.sim.close()
  File "/home/hjkwon/scripts/stable_baselines/pandaAndrew/pybulletCust.py", line 59, in close
    self.physics_client.disconnect()
pybullet.error: Not connected to physics server.
Exception ignored in: <function BulletClient.__del__ at 0x7f60b29afe50>
Traceback (most recent call last):
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/pybullet_utils/bullet_client.py", line 39, in __del__
AttributeError: 'NoneType' object has no attribute 'getpid'
Exception ignored in: <function BulletClient.__del__ at 0x7f60b29afe50>
Traceback (most recent call last):
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/pybullet_utils/bullet_client.py", line 39, in __del__
AttributeError: 'NoneType' object has no attribute 'getpid'
Exception ignored in: <function BulletClient.__del__ at 0x7f60b29afe50>
Traceback (most recent call last):
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/pybullet_utils/bullet_client.py", line 39, in __del__
AttributeError: 'NoneType' object has no attribute 'getpid'
(rl_env) ]0;hjkwon@HJK-AI-Robotics: ~/scripts/stable_baselines/pandaAndrew[01;32mhjkwon@HJK-AI-Robotics[00m:[01;34m~/scripts/stable_baselines/pandaAndrew[00m$ python3 mainOptVect_V00.py
  File "/home/hjkwon/scripts/stable_baselines/pandaAndrew/mainOptVect_V00.py", line 71
    print("Best trial:")
IndentationError: unexpected indent
(rl_env) ]0;hjkwon@HJK-AI-Robotics: ~/scripts/stable_baselines/pandaAndrew[01;32mhjkwon@HJK-AI-Robotics[00m:[01;34m~/scripts/stable_baselines/pandaAndrew[00m$ python3 mainOptVect_V00.py
pybullet build time: Dec  1 2021 18:34:28
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
plotly:  5.6.0
optuna:  2.10.0
[32m[I 2022-04-21 12:36:54,511][0m A new study created in memory with name: no-name-f4dcc78c-15c2-406e-9fcd-27e91dda22af[0m

 

{'policy': 'MlpPolicy', 'device': 'cuda', 'verbose': 1, 'n_steps': 16384, 'gamma': 0.9256362119519798, 'gae_lambda': 0.8550404419491339, 'learning_rate': 0.003972318402931438, 'ent_coef': 1e-06, 'max_grad_norm': 0.5, 'vf_coef': 0.75, 'clip_range': 0.075, 'batch_size': 1024, 'n_epochs': 12, 'policy_kwargs': {'net_arch': [{'pi': [256, 256, 256], 'vf': [256, 256, 256]}]}}
n_envs:  4

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
_XSERVTransSocketUNIXCreateListener: ...SocketCreateListener() failed
_XSERVTransMakeAllCOTSServerListeners: server already running
(EE) 
Fatal server error:
(EE) Cannot establish any listening sockets - Make sure an X server isn't already running(EE) 
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
Using cuda device
Eval num_timesteps=40000, episode_reward=-11.25 +/- 1.31
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -11.2    |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
New best mean reward!
Elapsed time: 68.5332 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.2     |
|    ep_rew_mean     | -11.6    |
| time/              |          |
|    fps             | 598      |
|    iterations      | 1        |
|    time_elapsed    | 109      |
|    total_timesteps | 65536    |
---------------------------------
Eval num_timesteps=80000, episode_reward=-9.99 +/- 1.14
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50           |
|    mean_reward          | -9.99        |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 80000        |
| train/                  |              |
|    approx_kl            | 0.0019248934 |
|    clip_fraction        | 0.183        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.68        |
|    explained_variance   | -0.221       |
|    learning_rate        | 0.00397      |
|    loss                 | 0.0719       |
|    n_updates            | 12           |
|    policy_gradient_loss | 0.00191      |
|    std                  | 1            |
|    value_loss           | 0.143        |
------------------------------------------
New best mean reward!
Elapsed time: 70.0004 seconds
Eval num_timesteps=120000, episode_reward=-11.09 +/- 1.41
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -11.1    |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 120000   |
---------------------------------
Elapsed time: 64.8768 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.4     |
|    ep_rew_mean     | -11.5    |
| time/              |          |
|    fps             | 590      |
|    iterations      | 2        |
|    time_elapsed    | 221      |
|    total_timesteps | 131072   |
---------------------------------
Eval num_timesteps=160000, episode_reward=-9.45 +/- 0.82
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50           |
|    mean_reward          | -9.45        |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 160000       |
| train/                  |              |
|    approx_kl            | 0.0039432007 |
|    clip_fraction        | 0.329        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.67        |
|    explained_variance   | 0.698        |
|    learning_rate        | 0.00397      |
|    loss                 | 0.0769       |
|    n_updates            | 24           |
|    policy_gradient_loss | -0.00363     |
|    std                  | 0.997        |
|    value_loss           | 0.125        |
------------------------------------------
New best mean reward!
Elapsed time: 70.3307 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.2     |
|    ep_rew_mean     | -11.2    |
| time/              |          |
|    fps             | 588      |
|    iterations      | 3        |
|    time_elapsed    | 333      |
|    total_timesteps | 196608   |
---------------------------------
Eval num_timesteps=200000, episode_reward=-7.00 +/- 0.44
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | -7          |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 200000      |
| train/                  |             |
|    approx_kl            | 0.004368658 |
|    clip_fraction        | 0.386       |
|    clip_range           | 0.075       |
|    entropy_loss         | -5.64       |
|    explained_variance   | 0.834       |
|    learning_rate        | 0.00397     |
|    loss                 | 0.0859      |
|    n_updates            | 36          |
|    policy_gradient_loss | -0.0102     |
|    std                  | 0.99        |
|    value_loss           | 0.131       |
-----------------------------------------
New best mean reward!
Elapsed time: 70.9302 seconds
Eval num_timesteps=240000, episode_reward=-7.27 +/- 0.52
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -7.27    |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 240000   |
---------------------------------
Elapsed time: 70.1619 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.8     |
|    ep_rew_mean     | -10.5    |
| time/              |          |
|    fps             | 578      |
|    iterations      | 4        |
|    time_elapsed    | 452      |
|    total_timesteps | 262144   |
---------------------------------
TRIAL Finished: Objective Trial Time
Elapsed time: 459.5500 seconds
[32m[I 2022-04-21 12:44:34,062][0m Trial 0 finished with value: -7.272326100000001 and parameters: {'gamma': 0.9256362119519798, 'gae_lambda': 0.8550404419491339, 'lr': 0.003972318402931438, 'n_epochs': 12, 'batch_size': 10, 'net_arch_width_int': 8, 'net_arch_depth': 3, 'n_envs': 4}. Best is trial 0 with value: -7.272326100000001.[0m

 

{'policy': 'MlpPolicy', 'device': 'cuda', 'verbose': 1, 'n_steps': 16384, 'gamma': 0.9081292355254379, 'gae_lambda': 0.8499269813805216, 'learning_rate': 0.0014200338207888323, 'ent_coef': 1e-06, 'max_grad_norm': 0.5, 'vf_coef': 0.75, 'clip_range': 0.075, 'batch_size': 2048, 'n_epochs': 10, 'policy_kwargs': {'net_arch': [{'pi': [128, 128, 128], 'vf': [128, 128, 128]}]}}
n_envs:  3

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
Using cuda device
Eval num_timesteps=30000, episode_reward=-11.24 +/- 1.23
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -11.2    |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
New best mean reward!
Elapsed time: 52.5828 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.9     |
|    ep_rew_mean     | -11.3    |
| time/              |          |
|    fps             | 573      |
|    iterations      | 1        |
|    time_elapsed    | 85       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-11.65 +/- 0.79
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50           |
|    mean_reward          | -11.6        |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 60000        |
| train/                  |              |
|    approx_kl            | 0.0012324172 |
|    clip_fraction        | 0.118        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.68        |
|    explained_variance   | -0.588       |
|    learning_rate        | 0.00142      |
|    loss                 | 0.0858       |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.00248     |
|    std                  | 1            |
|    value_loss           | 0.21         |
------------------------------------------
Elapsed time: 53.8185 seconds
Eval num_timesteps=90000, episode_reward=-11.84 +/- 0.91
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -11.8    |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 90000    |
---------------------------------
Elapsed time: 52.0402 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.2     |
|    ep_rew_mean     | -11.8    |
| time/              |          |
|    fps             | 571      |
|    iterations      | 2        |
|    time_elapsed    | 172      |
|    total_timesteps | 98304    |
---------------------------------
Eval num_timesteps=120000, episode_reward=-7.88 +/- 0.81
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | -7.88       |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 120000      |
| train/                  |             |
|    approx_kl            | 0.002865038 |
|    clip_fraction        | 0.259       |
|    clip_range           | 0.075       |
|    entropy_loss         | -5.67       |
|    explained_variance   | 0.583       |
|    learning_rate        | 0.00142     |
|    loss                 | 0.048       |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0096     |
|    std                  | 1           |
|    value_loss           | 0.108       |
-----------------------------------------
New best mean reward!
Elapsed time: 54.7160 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.2     |
|    ep_rew_mean     | -11.2    |
| time/              |          |
|    fps             | 569      |
|    iterations      | 3        |
|    time_elapsed    | 258      |
|    total_timesteps | 147456   |
---------------------------------
Eval num_timesteps=150000, episode_reward=-5.76 +/- 0.79
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | -5.76       |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 150000      |
| train/                  |             |
|    approx_kl            | 0.004554306 |
|    clip_fraction        | 0.358       |
|    clip_range           | 0.075       |
|    entropy_loss         | -5.66       |
|    explained_variance   | 0.817       |
|    learning_rate        | 0.00142     |
|    loss                 | 0.0586      |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0133     |
|    std                  | 0.996       |
|    value_loss           | 0.122       |
-----------------------------------------
New best mean reward!
Elapsed time: 52.8917 seconds
Eval num_timesteps=180000, episode_reward=-5.45 +/- 0.93
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -5.45    |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 180000   |
---------------------------------
New best mean reward!
Elapsed time: 53.1767 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.6     |
|    ep_rew_mean     | -10.5    |
| time/              |          |
|    fps             | 564      |
|    iterations      | 4        |
|    time_elapsed    | 348      |
|    total_timesteps | 196608   |
---------------------------------
Eval num_timesteps=210000, episode_reward=-3.92 +/- 0.52
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 50         |
|    mean_reward          | -3.92      |
|    success_rate         | 0          |
| time/                   |            |
|    total_timesteps      | 210000     |
| train/                  |            |
|    approx_kl            | 0.00461285 |
|    clip_fraction        | 0.366      |
|    clip_range           | 0.075      |
|    entropy_loss         | -5.64      |
|    explained_variance   | 0.893      |
|    learning_rate        | 0.00142    |
|    loss                 | 0.0683     |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.0125    |
|    std                  | 0.992      |
|    value_loss           | 0.135      |
----------------------------------------
New best mean reward!
Elapsed time: 54.8504 seconds
Eval num_timesteps=240000, episode_reward=-4.16 +/- 0.59
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -4.16    |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 240000   |
---------------------------------
Elapsed time: 53.7422 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.8     |
|    ep_rew_mean     | -9.25    |
| time/              |          |
|    fps             | 561      |
|    iterations      | 5        |
|    time_elapsed    | 437      |
|    total_timesteps | 245760   |
---------------------------------
TRIAL Finished: Objective Trial Time
Elapsed time: 439.4042 seconds
[32m[I 2022-04-21 12:51:53,567][0m Trial 1 finished with value: -4.1640143 and parameters: {'gamma': 0.9081292355254379, 'gae_lambda': 0.8499269813805216, 'lr': 0.0014200338207888323, 'n_epochs': 10, 'batch_size': 11, 'net_arch_width_int': 7, 'net_arch_depth': 3, 'n_envs': 3}. Best is trial 1 with value: -4.1640143.[0m

 

{'policy': 'MlpPolicy', 'device': 'cuda', 'verbose': 1, 'n_steps': 16384, 'gamma': 0.9327161735341573, 'gae_lambda': 0.9092111467586248, 'learning_rate': 0.002907545230257422, 'ent_coef': 1e-06, 'max_grad_norm': 0.5, 'vf_coef': 0.75, 'clip_range': 0.075, 'batch_size': 8192, 'n_epochs': 6, 'policy_kwargs': {'net_arch': [{'pi': [64, 64, 64, 64, 64], 'vf': [64, 64, 64, 64, 64]}]}}
n_envs:  4

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
Using cuda device
Eval num_timesteps=40000, episode_reward=-10.40 +/- 1.19
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -10.4    |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
New best mean reward!
Elapsed time: 72.3969 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.9     |
|    ep_rew_mean     | -12.1    |
| time/              |          |
|    fps             | 557      |
|    iterations      | 1        |
|    time_elapsed    | 117      |
|    total_timesteps | 65536    |
---------------------------------
Eval num_timesteps=80000, episode_reward=-7.25 +/- 1.27
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | -7.25       |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 80000       |
| train/                  |             |
|    approx_kl            | 0.001436592 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.075       |
|    entropy_loss         | -5.68       |
|    explained_variance   | -0.18       |
|    learning_rate        | 0.00291     |
|    loss                 | 0.288       |
|    n_updates            | 6           |
|    policy_gradient_loss | -0.00351    |
|    std                  | 1           |
|    value_loss           | 0.67        |
-----------------------------------------
New best mean reward!
Elapsed time: 71.6708 seconds
Eval num_timesteps=120000, episode_reward=-7.51 +/- 1.54
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -7.51    |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 120000   |
---------------------------------
Elapsed time: 72.7258 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.1     |
|    ep_rew_mean     | -12.1    |
| time/              |          |
|    fps             | 558      |
|    iterations      | 2        |
|    time_elapsed    | 234      |
|    total_timesteps | 131072   |
---------------------------------
Eval num_timesteps=160000, episode_reward=-5.77 +/- 1.13
Episode length: 48.60 +/- 2.94
Success rate: 20.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 48.6         |
|    mean_reward          | -5.77        |
|    success_rate         | 0.2          |
| time/                   |              |
|    total_timesteps      | 160000       |
| train/                  |              |
|    approx_kl            | 0.0019925626 |
|    clip_fraction        | 0.192        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.68        |
|    explained_variance   | 0.441        |
|    learning_rate        | 0.00291      |
|    loss                 | 0.248        |
|    n_updates            | 12           |
|    policy_gradient_loss | -0.00541     |
|    std                  | 0.999        |
|    value_loss           | 0.52         |
------------------------------------------
New best mean reward!
Elapsed time: 66.7922 seconds
 20000 #---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.3     |
|    ep_rew_mean     | -10.9    |
| time/              |          |
|    fps             | 565      |
|    iterations      | 3        |
|    time_elapsed    | 347      |
|    total_timesteps | 196608   |
---------------------------------
Eval num_timesteps=200000, episode_reward=-3.75 +/- 0.86
Episode length: 41.80 +/- 8.70
Success rate: 50.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 41.8        |
|    mean_reward          | -3.75       |
|    success_rate         | 0.5         |
| time/                   |             |
|    total_timesteps      | 200000      |
| train/                  |             |
|    approx_kl            | 0.002679897 |
|    clip_fraction        | 0.254       |
|    clip_range           | 0.075       |
|    entropy_loss         | -5.66       |
|    explained_variance   | 0.728       |
|    learning_rate        | 0.00291     |
|    loss                 | 0.224       |
|    n_updates            | 18          |
|    policy_gradient_loss | -0.00703    |
|    std                  | 0.996       |
|    value_loss           | 0.432       |
-----------------------------------------
New best mean reward!
Elapsed time: 71.0212 seconds
Eval num_timesteps=240000, episode_reward=-3.55 +/- 0.92
Episode length: 40.00 +/- 8.57
Success rate: 60.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40       |
|    mean_reward     | -3.55    |
|    success_rate    | 0.6      |
| time/              |          |
|    total_timesteps | 240000   |
---------------------------------
New best mean reward!
Elapsed time: 67.7367 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 45.5     |
|    ep_rew_mean     | -10      |
| time/              |          |
|    fps             | 567      |
|    iterations      | 4        |
|    time_elapsed    | 461      |
|    total_timesteps | 262144   |
---------------------------------
TRIAL Finished: Objective Trial Time
Elapsed time: 462.5232 seconds
[32m[I 2022-04-21 12:59:36,188][0m Trial 2 finished with value: -3.5529366000000002 and parameters: {'gamma': 0.9327161735341573, 'gae_lambda': 0.9092111467586248, 'lr': 0.002907545230257422, 'n_epochs': 6, 'batch_size': 13, 'net_arch_width_int': 6, 'net_arch_depth': 5, 'n_envs': 4}. Best is trial 2 with value: -3.5529366000000002.[0m

 

{'policy': 'MlpPolicy', 'device': 'cuda', 'verbose': 1, 'n_steps': 16384, 'gamma': 0.95081033874785, 'gae_lambda': 0.9804138006302536, 'learning_rate': 0.002532870111372048, 'ent_coef': 1e-06, 'max_grad_norm': 0.5, 'vf_coef': 0.75, 'clip_range': 0.075, 'batch_size': 1024, 'n_epochs': 11, 'policy_kwargs': {'net_arch': [{'pi': [256, 256, 256, 256, 256], 'vf': [256, 256, 256, 256, 256]}]}}
n_envs:  4

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
Using cuda device
 20000 #Eval num_timesteps=40000, episode_reward=-11.19 +/- 1.66
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -11.2    |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
New best mean reward!
Elapsed time: 69.6070 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.1     |
|    ep_rew_mean     | -11.6    |
| time/              |          |
|    fps             | 592      |
|    iterations      | 1        |
|    time_elapsed    | 110      |
|    total_timesteps | 65536    |
---------------------------------
Eval num_timesteps=80000, episode_reward=-9.92 +/- 1.18
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50           |
|    mean_reward          | -9.92        |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 80000        |
| train/                  |              |
|    approx_kl            | 0.0021094927 |
|    clip_fraction        | 0.243        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.67        |
|    explained_variance   | -0.072       |
|    learning_rate        | 0.00253      |
|    loss                 | 1.04         |
|    n_updates            | 11           |
|    policy_gradient_loss | 0.00208      |
|    std                  | 0.999        |
|    value_loss           | 1.53         |
------------------------------------------
New best mean reward!
Elapsed time: 71.8801 seconds
Eval num_timesteps=120000, episode_reward=-11.46 +/- 1.28
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -11.5    |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 120000   |
---------------------------------
Elapsed time: 69.6813 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.6     |
|    ep_rew_mean     | -11.6    |
| time/              |          |
|    fps             | 571      |
|    iterations      | 2        |
|    time_elapsed    | 229      |
|    total_timesteps | 131072   |
---------------------------------
Eval num_timesteps=160000, episode_reward=-8.42 +/- 1.81
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50           |
|    mean_reward          | -8.42        |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 160000       |
| train/                  |              |
|    approx_kl            | 0.0034061782 |
|    clip_fraction        | 0.278        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.66        |
|    explained_variance   | 0.456        |
|    learning_rate        | 0.00253      |
|    loss                 | 0.677        |
|    n_updates            | 22           |
|    policy_gradient_loss | 0.00144      |
|    std                  | 0.997        |
|    value_loss           | 1.01         |
------------------------------------------
New best mean reward!
Elapsed time: 70.5727 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.4     |
|    ep_rew_mean     | -11.5    |
| time/              |          |
|    fps             | 566      |
|    iterations      | 3        |
|    time_elapsed    | 346      |
|    total_timesteps | 196608   |
---------------------------------
Eval num_timesteps=200000, episode_reward=-9.37 +/- 2.39
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | -9.37       |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 200000      |
| train/                  |             |
|    approx_kl            | 0.003610427 |
|    clip_fraction        | 0.286       |
|    clip_range           | 0.075       |
|    entropy_loss         | -5.65       |
|    explained_variance   | 0.593       |
|    learning_rate        | 0.00253     |
|    loss                 | 0.499       |
|    n_updates            | 33          |
|    policy_gradient_loss | -0.000101   |
|    std                  | 1           |
|    value_loss           | 0.899       |
-----------------------------------------
Elapsed time: 76.5239 seconds
Eval num_timesteps=240000, episode_reward=-7.71 +/- 1.04
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -7.71    |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 240000   |
---------------------------------
New best mean reward!
Elapsed time: 69.4106 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48       |
|    ep_rew_mean     | -9.98    |
| time/              |          |
|    fps             | 561      |
|    iterations      | 4        |
|    time_elapsed    | 466      |
|    total_timesteps | 262144   |
---------------------------------
TRIAL Finished: Objective Trial Time
Elapsed time: 470.8420 seconds
[32m[I 2022-04-21 13:07:27,146][0m Trial 3 finished with value: -7.708672 and parameters: {'gamma': 0.95081033874785, 'gae_lambda': 0.9804138006302536, 'lr': 0.002532870111372048, 'n_epochs': 11, 'batch_size': 10, 'net_arch_width_int': 8, 'net_arch_depth': 5, 'n_envs': 4}. Best is trial 2 with value: -3.5529366000000002.[0m

 

{'policy': 'MlpPolicy', 'device': 'cuda', 'verbose': 1, 'n_steps': 16384, 'gamma': 0.9483005766226962, 'gae_lambda': 0.9400198621647176, 'learning_rate': 0.0011835308477475422, 'ent_coef': 1e-06, 'max_grad_norm': 0.5, 'vf_coef': 0.75, 'clip_range': 0.075, 'batch_size': 2048, 'n_epochs': 8, 'policy_kwargs': {'net_arch': [{'pi': [64, 64, 64, 64, 64], 'vf': [64, 64, 64, 64, 64]}]}}
n_envs:  4

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
Using cuda device
Eval num_timesteps=40000, episode_reward=-10.82 +/- 1.11
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -10.8    |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
New best mean reward!
Elapsed time: 75.8998 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | -12.1    |
| time/              |          |
|    fps             | 548      |
|    iterations      | 1        |
|    time_elapsed    | 119      |
|    total_timesteps | 65536    |
---------------------------------
Eval num_timesteps=80000, episode_reward=-6.49 +/- 1.10
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50           |
|    mean_reward          | -6.49        |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 80000        |
| train/                  |              |
|    approx_kl            | 0.0014650762 |
|    clip_fraction        | 0.124        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.67        |
|    explained_variance   | -0.167       |
|    learning_rate        | 0.00118      |
|    loss                 | 0.481        |
|    n_updates            | 8            |
|    policy_gradient_loss | -0.00326     |
|    std                  | 1            |
|    value_loss           | 1.05         |
------------------------------------------
New best mean reward!
Elapsed time: 70.7339 seconds
Eval num_timesteps=120000, episode_reward=-6.84 +/- 1.09
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -6.84    |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 120000   |
---------------------------------
Elapsed time: 69.5085 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.9     |
|    ep_rew_mean     | -11.7    |
| time/              |          |
|    fps             | 557      |
|    iterations      | 2        |
|    time_elapsed    | 235      |
|    total_timesteps | 131072   |
---------------------------------
Eval num_timesteps=160000, episode_reward=-4.27 +/- 1.08
Episode length: 44.70 +/- 6.68
Success rate: 40.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 44.7         |
|    mean_reward          | -4.27        |
|    success_rate         | 0.4          |
| time/                   |              |
|    total_timesteps      | 160000       |
| train/                  |              |
|    approx_kl            | 0.0024093124 |
|    clip_fraction        | 0.22         |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.67        |
|    explained_variance   | 0.519        |
|    learning_rate        | 0.00118      |
|    loss                 | 0.385        |
|    n_updates            | 16           |
|    policy_gradient_loss | -0.00732     |
|    std                  | 0.999        |
|    value_loss           | 0.652        |
------------------------------------------
New best mean reward!
Elapsed time: 70.4563 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.9     |
|    ep_rew_mean     | -11.4    |
| time/              |          |
|    fps             | 555      |
|    iterations      | 3        |
|    time_elapsed    | 353      |
|    total_timesteps | 196608   |
---------------------------------
Eval num_timesteps=200000, episode_reward=-3.49 +/- 0.45
Episode length: 46.90 +/- 6.52
Success rate: 20.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 46.9         |
|    mean_reward          | -3.49        |
|    success_rate         | 0.2          |
| time/                   |              |
|    total_timesteps      | 200000       |
| train/                  |              |
|    approx_kl            | 0.0029754813 |
|    clip_fraction        | 0.263        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.66        |
|    explained_variance   | 0.711        |
|    learning_rate        | 0.00118      |
|    loss                 | 0.38         |
|    n_updates            | 24           |
|    policy_gradient_loss | -0.00812     |
|    std                  | 0.995        |
|    value_loss           | 0.671        |
------------------------------------------
New best mean reward!
Elapsed time: 76.7759 seconds
Eval num_timesteps=240000, episode_reward=-3.83 +/- 0.97
Episode length: 47.70 +/- 6.90
Success rate: 10.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.7     |
|    mean_reward     | -3.83    |
|    success_rate    | 0.1      |
| time/              |          |
|    total_timesteps | 240000   |
---------------------------------
Elapsed time: 71.2234 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.8     |
|    ep_rew_mean     | -10.3    |
| time/              |          |
|    fps             | 553      |
|    iterations      | 4        |
|    time_elapsed    | 473      |
|    total_timesteps | 262144   |
---------------------------------
TRIAL Finished: Objective Trial Time
Elapsed time: 475.5708 seconds
[32m[I 2022-04-21 13:15:22,814][0m Trial 4 finished with value: -3.8335152999999997 and parameters: {'gamma': 0.9483005766226962, 'gae_lambda': 0.9400198621647176, 'lr': 0.0011835308477475422, 'n_epochs': 8, 'batch_size': 11, 'net_arch_width_int': 6, 'net_arch_depth': 5, 'n_envs': 4}. Best is trial 2 with value: -3.5529366000000002.[0m

 

{'policy': 'MlpPolicy', 'device': 'cuda', 'verbose': 1, 'n_steps': 16384, 'gamma': 0.9603840759399427, 'gae_lambda': 0.8651544633118384, 'learning_rate': 0.0021180747391160203, 'ent_coef': 1e-06, 'max_grad_norm': 0.5, 'vf_coef': 0.75, 'clip_range': 0.075, 'batch_size': 512, 'n_epochs': 11, 'policy_kwargs': {'net_arch': [{'pi': [64, 64, 64, 64, 64], 'vf': [64, 64, 64, 64, 64]}]}}
n_envs:  3

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
Using cuda device
Eval num_timesteps=30000, episode_reward=-10.68 +/- 1.22
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -10.7    |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
New best mean reward!
Elapsed time: 53.1980 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.6     |
|    ep_rew_mean     | -12.7    |
| time/              |          |
|    fps             | 575      |
|    iterations      | 1        |
|    time_elapsed    | 85       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-6.76 +/- 1.45
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50           |
|    mean_reward          | -6.76        |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 60000        |
| train/                  |              |
|    approx_kl            | 0.0021798906 |
|    clip_fraction        | 0.216        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.67        |
|    explained_variance   | -0.193       |
|    learning_rate        | 0.00212      |
|    loss                 | 0.144        |
|    n_updates            | 11           |
|    policy_gradient_loss | -0.00286     |
|    std                  | 0.998        |
|    value_loss           | 0.244        |
------------------------------------------
New best mean reward!
Elapsed time: 61.1081 seconds
Eval num_timesteps=90000, episode_reward=-6.25 +/- 0.90
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -6.25    |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 90000    |
---------------------------------
New best mean reward!
Elapsed time: 57.6681 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47       |
|    ep_rew_mean     | -10.9    |
| time/              |          |
|    fps             | 525      |
|    iterations      | 2        |
|    time_elapsed    | 187      |
|    total_timesteps | 98304    |
---------------------------------
Eval num_timesteps=120000, episode_reward=-4.93 +/- 0.72
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50           |
|    mean_reward          | -4.93        |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 120000       |
| train/                  |              |
|    approx_kl            | 0.0036714033 |
|    clip_fraction        | 0.319        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.66        |
|    explained_variance   | 0.62         |
|    learning_rate        | 0.00212      |
|    loss                 | 0.142        |
|    n_updates            | 22           |
|    policy_gradient_loss | -0.00749     |
|    std                  | 0.992        |
|    value_loss           | 0.257        |
------------------------------------------
New best mean reward!
Elapsed time: 64.5328 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.3     |
|    ep_rew_mean     | -10.5    |
| time/              |          |
|    fps             | 513      |
|    iterations      | 3        |
|    time_elapsed    | 287      |
|    total_timesteps | 147456   |
---------------------------------
Eval num_timesteps=150000, episode_reward=-3.54 +/- 0.56
Episode length: 45.70 +/- 8.60
Success rate: 20.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 45.7         |
|    mean_reward          | -3.54        |
|    success_rate         | 0.2          |
| time/                   |              |
|    total_timesteps      | 150000       |
| train/                  |              |
|    approx_kl            | 0.0045289234 |
|    clip_fraction        | 0.349        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.63        |
|    explained_variance   | 0.792        |
|    learning_rate        | 0.00212      |
|    loss                 | 0.199        |
|    n_updates            | 33           |
|    policy_gradient_loss | -0.0091      |
|    std                  | 0.987        |
|    value_loss           | 0.326        |
------------------------------------------
New best mean reward!
Elapsed time: 63.0895 seconds
Eval num_timesteps=180000, episode_reward=-3.67 +/- 0.49
Episode length: 46.10 +/- 6.20
Success rate: 40.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.1     |
|    mean_reward     | -3.67    |
|    success_rate    | 0.4      |
| time/              |          |
|    total_timesteps | 180000   |
---------------------------------
Elapsed time: 54.9622 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.9     |
|    ep_rew_mean     | -10.1    |
| time/              |          |
|    fps             | 509      |
|    iterations      | 4        |
|    time_elapsed    | 386      |
|    total_timesteps | 196608   |
---------------------------------
Eval num_timesteps=210000, episode_reward=-3.20 +/- 0.40
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | -3.2        |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 210000      |
| train/                  |             |
|    approx_kl            | 0.004781474 |
|    clip_fraction        | 0.359       |
|    clip_range           | 0.075       |
|    entropy_loss         | -5.61       |
|    explained_variance   | 0.843       |
|    learning_rate        | 0.00212     |
|    loss                 | 0.287       |
|    n_updates            | 44          |
|    policy_gradient_loss | -0.00896    |
|    std                  | 0.983       |
|    value_loss           | 0.413       |
-----------------------------------------
New best mean reward!
Elapsed time: 67.6752 seconds
Eval num_timesteps=240000, episode_reward=-3.26 +/- 0.33
Episode length: 49.40 +/- 1.80
Success rate: 20.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | -3.26    |
|    success_rate    | 0.2      |
| time/              |          |
|    total_timesteps | 240000   |
---------------------------------
Elapsed time: 55.4976 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.6     |
|    ep_rew_mean     | -9.98    |
| time/              |          |
|    fps             | 501      |
|    iterations      | 5        |
|    time_elapsed    | 489      |
|    total_timesteps | 245760   |
---------------------------------
TRIAL Finished: Objective Trial Time
Elapsed time: 497.0758 seconds
[32m[I 2022-04-21 13:23:40,000][0m Trial 5 finished with value: -3.2622190000000004 and parameters: {'gamma': 0.9603840759399427, 'gae_lambda': 0.8651544633118384, 'lr': 0.0021180747391160203, 'n_epochs': 11, 'batch_size': 9, 'net_arch_width_int': 6, 'net_arch_depth': 5, 'n_envs': 3}. Best is trial 5 with value: -3.2622190000000004.[0m

 

{'policy': 'MlpPolicy', 'device': 'cuda', 'verbose': 1, 'n_steps': 16384, 'gamma': 0.9873612215702378, 'gae_lambda': 0.9277038807672348, 'learning_rate': 0.0025211863872065653, 'ent_coef': 1e-06, 'max_grad_norm': 0.5, 'vf_coef': 0.75, 'clip_range': 0.075, 'batch_size': 1024, 'n_epochs': 7, 'policy_kwargs': {'net_arch': [{'pi': [128, 128, 128, 128, 128], 'vf': [128, 128, 128, 128, 128]}]}}
n_envs:  3

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
Using cuda device
Eval num_timesteps=30000, episode_reward=-11.10 +/- 0.81
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -11.1    |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
New best mean reward!
Elapsed time: 54.4047 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.8     |
|    ep_rew_mean     | -11.7    |
| time/              |          |
|    fps             | 561      |
|    iterations      | 1        |
|    time_elapsed    | 87       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-9.09 +/- 0.83
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50           |
|    mean_reward          | -9.09        |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 60000        |
| train/                  |              |
|    approx_kl            | 0.0019637372 |
|    clip_fraction        | 0.218        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.67        |
|    explained_variance   | -0.0426      |
|    learning_rate        | 0.00252      |
|    loss                 | 0.638        |
|    n_updates            | 7            |
|    policy_gradient_loss | -2.8e-05     |
|    std                  | 0.999        |
|    value_loss           | 1.18         |
------------------------------------------
New best mean reward!
Elapsed time: 54.2491 seconds
Eval num_timesteps=90000, episode_reward=-9.66 +/- 1.37
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -9.66    |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 90000    |
---------------------------------
Elapsed time: 53.9827 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.7     |
|    ep_rew_mean     | -11.8    |
| time/              |          |
|    fps             | 555      |
|    iterations      | 2        |
|    time_elapsed    | 177      |
|    total_timesteps | 98304    |
---------------------------------
Eval num_timesteps=120000, episode_reward=-6.75 +/- 0.70
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50           |
|    mean_reward          | -6.75        |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 120000       |
| train/                  |              |
|    approx_kl            | 0.0025654603 |
|    clip_fraction        | 0.241        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.67        |
|    explained_variance   | 0.442        |
|    learning_rate        | 0.00252      |
|    loss                 | 0.847        |
|    n_updates            | 14           |
|    policy_gradient_loss | -0.00345     |
|    std                  | 0.996        |
|    value_loss           | 1.41         |
------------------------------------------
New best mean reward!
Elapsed time: 58.5017 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.7     |
|    ep_rew_mean     | -11.2    |
| time/              |          |
|    fps             | 541      |
|    iterations      | 3        |
|    time_elapsed    | 272      |
|    total_timesteps | 147456   |
---------------------------------
Eval num_timesteps=150000, episode_reward=-5.53 +/- 0.64
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50           |
|    mean_reward          | -5.53        |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 150000       |
| train/                  |              |
|    approx_kl            | 0.0027030185 |
|    clip_fraction        | 0.256        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.65        |
|    explained_variance   | 0.639        |
|    learning_rate        | 0.00252      |
|    loss                 | 1.41         |
|    n_updates            | 21           |
|    policy_gradient_loss | -0.00369     |
|    std                  | 0.991        |
|    value_loss           | 1.93         |
------------------------------------------
New best mean reward!
Elapsed time: 59.1015 seconds
Eval num_timesteps=180000, episode_reward=-5.35 +/- 0.76
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -5.35    |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 180000   |
---------------------------------
New best mean reward!
Elapsed time: 54.6169 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.7     |
|    ep_rew_mean     | -11.2    |
| time/              |          |
|    fps             | 535      |
|    iterations      | 4        |
|    time_elapsed    | 367      |
|    total_timesteps | 196608   |
---------------------------------
Eval num_timesteps=210000, episode_reward=-5.73 +/- 0.90
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | -5.73       |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 210000      |
| train/                  |             |
|    approx_kl            | 0.002857582 |
|    clip_fraction        | 0.257       |
|    clip_range           | 0.075       |
|    entropy_loss         | -5.63       |
|    explained_variance   | 0.694       |
|    learning_rate        | 0.00252     |
|    loss                 | 1.25        |
|    n_updates            | 28          |
|    policy_gradient_loss | -0.0027     |
|    std                  | 0.989       |
|    value_loss           | 2.34        |
-----------------------------------------
Elapsed time: 61.9797 seconds
Eval num_timesteps=240000, episode_reward=-5.59 +/- 0.62
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -5.59    |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 240000   |
---------------------------------
Elapsed time: 55.7016 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.4     |
|    ep_rew_mean     | -9.92    |
| time/              |          |
|    fps             | 531      |
|    iterations      | 5        |
|    time_elapsed    | 462      |
|    total_timesteps | 245760   |
---------------------------------
TRIAL Finished: Objective Trial Time
Elapsed time: 464.6183 seconds
[32m[I 2022-04-21 13:31:24,727][0m Trial 6 finished with value: -5.588516199999999 and parameters: {'gamma': 0.9873612215702378, 'gae_lambda': 0.9277038807672348, 'lr': 0.0025211863872065653, 'n_epochs': 7, 'batch_size': 10, 'net_arch_width_int': 7, 'net_arch_depth': 5, 'n_envs': 3}. Best is trial 5 with value: -3.2622190000000004.[0m

 

{'policy': 'MlpPolicy', 'device': 'cuda', 'verbose': 1, 'n_steps': 16384, 'gamma': 0.9911495492049448, 'gae_lambda': 0.8282001305776084, 'learning_rate': 0.0007161633473210814, 'ent_coef': 1e-06, 'max_grad_norm': 0.5, 'vf_coef': 0.75, 'clip_range': 0.075, 'batch_size': 512, 'n_epochs': 6, 'policy_kwargs': {'net_arch': [{'pi': [64, 64, 64], 'vf': [64, 64, 64]}]}}
n_envs:  5

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
Using cuda device
Eval num_timesteps=50000, episode_reward=-11.56 +/- 1.61
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -11.6    |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 50000    |
---------------------------------
New best mean reward!
Elapsed time: 82.9357 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.8     |
|    ep_rew_mean     | -11.6    |
| time/              |          |
|    fps             | 606      |
|    iterations      | 1        |
|    time_elapsed    | 135      |
|    total_timesteps | 81920    |
---------------------------------
Eval num_timesteps=100000, episode_reward=-9.50 +/- 1.18
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50           |
|    mean_reward          | -9.5         |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 100000       |
| train/                  |              |
|    approx_kl            | 0.0015074243 |
|    clip_fraction        | 0.137        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.67        |
|    explained_variance   | -0.323       |
|    learning_rate        | 0.000716     |
|    loss                 | 0.132        |
|    n_updates            | 6            |
|    policy_gradient_loss | -0.0034      |
|    std                  | 1            |
|    value_loss           | 0.353        |
------------------------------------------
New best mean reward!
Elapsed time: 88.1434 seconds
Eval num_timesteps=150000, episode_reward=-8.58 +/- 0.99
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -8.58    |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 150000   |
---------------------------------
New best mean reward!
Elapsed time: 83.4259 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.1     |
|    ep_rew_mean     | -10.7    |
| time/              |          |
|    fps             | 590      |
|    iterations      | 2        |
|    time_elapsed    | 277      |
|    total_timesteps | 163840   |
---------------------------------
Eval num_timesteps=200000, episode_reward=-6.02 +/- 1.09
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50           |
|    mean_reward          | -6.02        |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 200000       |
| train/                  |              |
|    approx_kl            | 0.0024845283 |
|    clip_fraction        | 0.223        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.67        |
|    explained_variance   | 0.567        |
|    learning_rate        | 0.000716     |
|    loss                 | 0.18         |
|    n_updates            | 12           |
|    policy_gradient_loss | -0.0076      |
|    std                  | 1            |
|    value_loss           | 0.331        |
------------------------------------------
New best mean reward!
Elapsed time: 90.0354 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.1     |
|    ep_rew_mean     | -10.4    |
| time/              |          |
|    fps             | 578      |
|    iterations      | 3        |
|    time_elapsed    | 424      |
|    total_timesteps | 245760   |
---------------------------------
TRIAL Finished: Objective Trial Time
Elapsed time: 429.7665 seconds
[32m[I 2022-04-21 13:38:34,588][0m Trial 7 finished with value: -6.0222734 and parameters: {'gamma': 0.9911495492049448, 'gae_lambda': 0.8282001305776084, 'lr': 0.0007161633473210814, 'n_epochs': 6, 'batch_size': 9, 'net_arch_width_int': 6, 'net_arch_depth': 3, 'n_envs': 5}. Best is trial 5 with value: -3.2622190000000004.[0m

 

{'policy': 'MlpPolicy', 'device': 'cuda', 'verbose': 1, 'n_steps': 16384, 'gamma': 0.9676328688359926, 'gae_lambda': 0.8717227433067933, 'learning_rate': 0.0005673495953841259, 'ent_coef': 1e-06, 'max_grad_norm': 0.5, 'vf_coef': 0.75, 'clip_range': 0.075, 'batch_size': 512, 'n_epochs': 14, 'policy_kwargs': {'net_arch': [{'pi': [64, 64, 64, 64], 'vf': [64, 64, 64, 64]}]}}
n_envs:  6

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
Using cuda device
Eval num_timesteps=60000, episode_reward=-10.97 +/- 1.44
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -11      |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 60000    |
---------------------------------
New best mean reward!
Elapsed time: 95.7440 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.7     |
|    ep_rew_mean     | -11.4    |
| time/              |          |
|    fps             | 632      |
|    iterations      | 1        |
|    time_elapsed    | 155      |
|    total_timesteps | 98304    |
---------------------------------
Eval num_timesteps=120000, episode_reward=-7.80 +/- 2.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50           |
|    mean_reward          | -7.8         |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 120000       |
| train/                  |              |
|    approx_kl            | 0.0019369513 |
|    clip_fraction        | 0.154        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.67        |
|    explained_variance   | -0.456       |
|    learning_rate        | 0.000567     |
|    loss                 | 0.14         |
|    n_updates            | 14           |
|    policy_gradient_loss | -0.00454     |
|    std                  | 1            |
|    value_loss           | 0.327        |
------------------------------------------
New best mean reward!
Elapsed time: 110.4675 seconds
Eval num_timesteps=180000, episode_reward=-6.84 +/- 0.76
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -6.84    |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 180000   |
---------------------------------
New best mean reward!
Elapsed time: 98.2417 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.8     |
|    ep_rew_mean     | -11.6    |
| time/              |          |
|    fps             | 589      |
|    iterations      | 2        |
|    time_elapsed    | 333      |
|    total_timesteps | 196608   |
---------------------------------
Eval num_timesteps=240000, episode_reward=-4.23 +/- 0.70
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50           |
|    mean_reward          | -4.23        |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 240000       |
| train/                  |              |
|    approx_kl            | 0.0031974472 |
|    clip_fraction        | 0.286        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.67        |
|    explained_variance   | 0.594        |
|    learning_rate        | 0.000567     |
|    loss                 | 0.208        |
|    n_updates            | 28           |
|    policy_gradient_loss | -0.0107      |
|    std                  | 0.998        |
|    value_loss           | 0.317        |
------------------------------------------
New best mean reward!
Elapsed time: 118.9583 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.8     |
|    ep_rew_mean     | -9.65    |
| time/              |          |
|    fps             | 574      |
|    iterations      | 3        |
|    time_elapsed    | 513      |
|    total_timesteps | 294912   |
---------------------------------
TRIAL Finished: Objective Trial Time
Elapsed time: 529.9548 seconds
[32m[I 2022-04-21 13:47:24,644][0m Trial 8 finished with value: -4.226783300000001 and parameters: {'gamma': 0.9676328688359926, 'gae_lambda': 0.8717227433067933, 'lr': 0.0005673495953841259, 'n_epochs': 14, 'batch_size': 9, 'net_arch_width_int': 6, 'net_arch_depth': 4, 'n_envs': 6}. Best is trial 5 with value: -3.2622190000000004.[0m

 

{'policy': 'MlpPolicy', 'device': 'cuda', 'verbose': 1, 'n_steps': 16384, 'gamma': 0.9707683949559948, 'gae_lambda': 0.8197586624349199, 'learning_rate': 0.0009534653937679456, 'ent_coef': 1e-06, 'max_grad_norm': 0.5, 'vf_coef': 0.75, 'clip_range': 0.075, 'batch_size': 8192, 'n_epochs': 9, 'policy_kwargs': {'net_arch': [{'pi': [128, 128, 128, 128], 'vf': [128, 128, 128, 128]}]}}
n_envs:  3

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
Using cuda device
Eval num_timesteps=30000, episode_reward=-10.51 +/- 0.86
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -10.5    |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
New best mean reward!
Elapsed time: 55.0771 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.2     |
|    ep_rew_mean     | -13.2    |
| time/              |          |
|    fps             | 545      |
|    iterations      | 1        |
|    time_elapsed    | 90       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-8.38 +/- 0.81
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50           |
|    mean_reward          | -8.38        |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 60000        |
| train/                  |              |
|    approx_kl            | 0.0011017333 |
|    clip_fraction        | 0.0914       |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.68        |
|    explained_variance   | -0.262       |
|    learning_rate        | 0.000953     |
|    loss                 | 0.192        |
|    n_updates            | 9            |
|    policy_gradient_loss | -0.00369     |
|    std                  | 1            |
|    value_loss           | 0.435        |
------------------------------------------
New best mean reward!
Elapsed time: 54.4329 seconds
Eval num_timesteps=90000, episode_reward=-8.65 +/- 1.05
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -8.65    |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 90000    |
---------------------------------
Elapsed time: 55.3201 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.8     |
|    ep_rew_mean     | -10.8    |
| time/              |          |
|    fps             | 547      |
|    iterations      | 2        |
|    time_elapsed    | 179      |
|    total_timesteps | 98304    |
---------------------------------
Eval num_timesteps=120000, episode_reward=-6.94 +/- 1.43
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50           |
|    mean_reward          | -6.94        |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 120000       |
| train/                  |              |
|    approx_kl            | 0.0015331851 |
|    clip_fraction        | 0.125        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.68        |
|    explained_variance   | 0.437        |
|    learning_rate        | 0.000953     |
|    loss                 | 0.189        |
|    n_updates            | 18           |
|    policy_gradient_loss | -0.00448     |
|    std                  | 1            |
|    value_loss           | 0.419        |
------------------------------------------
New best mean reward!
Elapsed time: 56.3993 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49       |
|    ep_rew_mean     | -11.5    |
| time/              |          |
|    fps             | 544      |
|    iterations      | 3        |
|    time_elapsed    | 270      |
|    total_timesteps | 147456   |
---------------------------------
Eval num_timesteps=150000, episode_reward=-5.88 +/- 0.92
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50           |
|    mean_reward          | -5.88        |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 150000       |
| train/                  |              |
|    approx_kl            | 0.0017876573 |
|    clip_fraction        | 0.163        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.69        |
|    explained_variance   | 0.679        |
|    learning_rate        | 0.000953     |
|    loss                 | 0.221        |
|    n_updates            | 27           |
|    policy_gradient_loss | -0.00532     |
|    std                  | 1            |
|    value_loss           | 0.409        |
------------------------------------------
New best mean reward!
Elapsed time: 56.3186 seconds
Eval num_timesteps=180000, episode_reward=-5.47 +/- 1.31
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -5.47    |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 180000   |
---------------------------------
New best mean reward!
Elapsed time: 53.6545 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.5     |
|    ep_rew_mean     | -10.9    |
| time/              |          |
|    fps             | 545      |
|    iterations      | 4        |
|    time_elapsed    | 360      |
|    total_timesteps | 196608   |
---------------------------------
Eval num_timesteps=210000, episode_reward=-5.13 +/- 1.34
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50           |
|    mean_reward          | -5.13        |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 210000       |
| train/                  |              |
|    approx_kl            | 0.0018631964 |
|    clip_fraction        | 0.158        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.69        |
|    explained_variance   | 0.795        |
|    learning_rate        | 0.000953     |
|    loss                 | 0.283        |
|    n_updates            | 36           |
|    policy_gradient_loss | -0.00499     |
|    std                  | 1            |
|    value_loss           | 0.482        |
------------------------------------------
New best mean reward!
Elapsed time: 54.6550 seconds
Eval num_timesteps=240000, episode_reward=-4.83 +/- 0.94
Episode length: 49.60 +/- 1.20
Success rate: 10.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.6     |
|    mean_reward     | -4.83    |
|    success_rate    | 0.1      |
| time/              |          |
|    total_timesteps | 240000   |
---------------------------------
New best mean reward!
Elapsed time: 53.1190 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.8     |
|    ep_rew_mean     | -9.84    |
| time/              |          |
|    fps             | 545      |
|    iterations      | 5        |
|    time_elapsed    | 450      |
|    total_timesteps | 245760   |
---------------------------------
TRIAL Finished: Objective Trial Time
Elapsed time: 450.8302 seconds
[32m[I 2022-04-21 13:54:55,576][0m Trial 9 finished with value: -4.8280177 and parameters: {'gamma': 0.9707683949559948, 'gae_lambda': 0.8197586624349199, 'lr': 0.0009534653937679456, 'n_epochs': 9, 'batch_size': 13, 'net_arch_width_int': 7, 'net_arch_depth': 4, 'n_envs': 3}. Best is trial 5 with value: -3.2622190000000004.[0m

 

{'policy': 'MlpPolicy', 'device': 'cuda', 'verbose': 1, 'n_steps': 16384, 'gamma': 0.9660299843749872, 'gae_lambda': 0.8013287374633634, 'learning_rate': 0.004702251223983385, 'ent_coef': 1e-06, 'max_grad_norm': 0.5, 'vf_coef': 0.75, 'clip_range': 0.075, 'batch_size': 4096, 'n_epochs': 14, 'policy_kwargs': {'net_arch': [{'pi': [64, 64, 64, 64], 'vf': [64, 64, 64, 64]}]}}
n_envs:  5

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
Using cuda device
Eval num_timesteps=50000, episode_reward=-11.07 +/- 1.19
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -11.1    |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 50000    |
---------------------------------
New best mean reward!
Elapsed time: 88.8340 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.6     |
|    ep_rew_mean     | -12      |
| time/              |          |
|    fps             | 567      |
|    iterations      | 1        |
|    time_elapsed    | 144      |
|    total_timesteps | 81920    |
---------------------------------
Eval num_timesteps=100000, episode_reward=-9.53 +/- 0.89
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50           |
|    mean_reward          | -9.53        |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 100000       |
| train/                  |              |
|    approx_kl            | 0.0020150386 |
|    clip_fraction        | 0.178        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.68        |
|    explained_variance   | -0.347       |
|    learning_rate        | 0.0047       |
|    loss                 | 0.0578       |
|    n_updates            | 14           |
|    policy_gradient_loss | -0.00311     |
|    std                  | 1            |
|    value_loss           | 0.145        |
------------------------------------------
New best mean reward!
Elapsed time: 88.2602 seconds
Eval num_timesteps=150000, episode_reward=-9.44 +/- 0.67
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -9.44    |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 150000   |
---------------------------------
New best mean reward!
Elapsed time: 85.3297 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.6     |
|    ep_rew_mean     | -11.6    |
| time/              |          |
|    fps             | 572      |
|    iterations      | 2        |
|    time_elapsed    | 285      |
|    total_timesteps | 163840   |
---------------------------------
Eval num_timesteps=200000, episode_reward=-5.44 +/- 1.11
Episode length: 49.90 +/- 0.30
Success rate: 10.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 49.9         |
|    mean_reward          | -5.44        |
|    success_rate         | 0.1          |
| time/                   |              |
|    total_timesteps      | 200000       |
| train/                  |              |
|    approx_kl            | 0.0043294663 |
|    clip_fraction        | 0.338        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.68        |
|    explained_variance   | 0.655        |
|    learning_rate        | 0.0047       |
|    loss                 | 0.0755       |
|    n_updates            | 28           |
|    policy_gradient_loss | -0.0108      |
|    std                  | 1            |
|    value_loss           | 0.161        |
------------------------------------------
New best mean reward!
Elapsed time: 84.5310 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.8     |
|    ep_rew_mean     | -11.2    |
| time/              |          |
|    fps             | 583      |
|    iterations      | 3        |
|    time_elapsed    | 421      |
|    total_timesteps | 245760   |
---------------------------------
TRIAL Finished: Objective Trial Time
Elapsed time: 423.1467 seconds
[32m[I 2022-04-21 14:01:58,821][0m Trial 10 finished with value: -5.4363002 and parameters: {'gamma': 0.9660299843749872, 'gae_lambda': 0.8013287374633634, 'lr': 0.004702251223983385, 'n_epochs': 14, 'batch_size': 12, 'net_arch_width_int': 6, 'net_arch_depth': 4, 'n_envs': 5}. Best is trial 5 with value: -3.2622190000000004.[0m

 

{'policy': 'MlpPolicy', 'device': 'cuda', 'verbose': 1, 'n_steps': 16384, 'gamma': 0.9293887066331005, 'gae_lambda': 0.9032027877980728, 'learning_rate': 0.0022997922174821843, 'ent_coef': 1e-06, 'max_grad_norm': 0.5, 'vf_coef': 0.75, 'clip_range': 0.075, 'batch_size': 8192, 'n_epochs': 6, 'policy_kwargs': {'net_arch': [{'pi': [64, 64, 64, 64, 64], 'vf': [64, 64, 64, 64, 64]}]}}
n_envs:  3

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
Using cuda device
Eval num_timesteps=30000, episode_reward=-10.65 +/- 0.71
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -10.7    |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
New best mean reward!
Elapsed time: 55.2188 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | -12.3    |
| time/              |          |
|    fps             | 537      |
|    iterations      | 1        |
|    time_elapsed    | 91       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-8.38 +/- 1.60
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50           |
|    mean_reward          | -8.38        |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 60000        |
| train/                  |              |
|    approx_kl            | 0.0012857749 |
|    clip_fraction        | 0.113        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.67        |
|    explained_variance   | -0.325       |
|    learning_rate        | 0.0023       |
|    loss                 | 0.319        |
|    n_updates            | 6            |
|    policy_gradient_loss | -0.0033      |
|    std                  | 1            |
|    value_loss           | 0.739        |
------------------------------------------
New best mean reward!
Elapsed time: 56.0012 seconds
Eval num_timesteps=90000, episode_reward=-8.29 +/- 1.36
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -8.29    |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 90000    |
---------------------------------
New best mean reward!
Elapsed time: 52.1025 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.6     |
|    ep_rew_mean     | -12.7    |
| time/              |          |
|    fps             | 553      |
|    iterations      | 2        |
|    time_elapsed    | 177      |
|    total_timesteps | 98304    |
---------------------------------
Eval num_timesteps=120000, episode_reward=-5.58 +/- 0.99
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50           |
|    mean_reward          | -5.58        |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 120000       |
| train/                  |              |
|    approx_kl            | 0.0018769141 |
|    clip_fraction        | 0.167        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.68        |
|    explained_variance   | 0.341        |
|    learning_rate        | 0.0023       |
|    loss                 | 0.269        |
|    n_updates            | 12           |
|    policy_gradient_loss | -0.00427     |
|    std                  | 1            |
|    value_loss           | 0.527        |
------------------------------------------
New best mean reward!
Elapsed time: 57.9869 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.3     |
|    ep_rew_mean     | -10.5    |
| time/              |          |
|    fps             | 548      |
|    iterations      | 3        |
|    time_elapsed    | 269      |
|    total_timesteps | 147456   |
---------------------------------
Eval num_timesteps=150000, episode_reward=-4.66 +/- 0.88
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | -4.66       |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 150000      |
| train/                  |             |
|    approx_kl            | 0.002138416 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.075       |
|    entropy_loss         | -5.67       |
|    explained_variance   | 0.628       |
|    learning_rate        | 0.0023      |
|    loss                 | 0.21        |
|    n_updates            | 18          |
|    policy_gradient_loss | -0.00542    |
|    std                  | 0.999       |
|    value_loss           | 0.375       |
-----------------------------------------
New best mean reward!
Elapsed time: 53.7070 seconds
Eval num_timesteps=180000, episode_reward=-4.56 +/- 0.80
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -4.56    |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 180000   |
---------------------------------
New best mean reward!
Elapsed time: 55.8353 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49       |
|    ep_rew_mean     | -10.7    |
| time/              |          |
|    fps             | 544      |
|    iterations      | 4        |
|    time_elapsed    | 361      |
|    total_timesteps | 196608   |
---------------------------------
Eval num_timesteps=210000, episode_reward=-4.05 +/- 0.37
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50           |
|    mean_reward          | -4.05        |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 210000       |
| train/                  |              |
|    approx_kl            | 0.0019657912 |
|    clip_fraction        | 0.219        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.66        |
|    explained_variance   | 0.742        |
|    learning_rate        | 0.0023       |
|    loss                 | 0.256        |
|    n_updates            | 24           |
|    policy_gradient_loss | -0.0058      |
|    std                  | 0.996        |
|    value_loss           | 0.398        |
------------------------------------------
New best mean reward!
Elapsed time: 54.7351 seconds
Eval num_timesteps=240000, episode_reward=-3.71 +/- 0.69
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -3.71    |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 240000   |
---------------------------------
New best mean reward!
Elapsed time: 54.9704 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.1     |
|    ep_rew_mean     | -9.67    |
| time/              |          |
|    fps             | 545      |
|    iterations      | 5        |
|    time_elapsed    | 450      |
|    total_timesteps | 245760   |
---------------------------------
TRIAL Finished: Objective Trial Time
Elapsed time: 450.8020 seconds
[32m[I 2022-04-21 14:09:29,719][0m Trial 11 finished with value: -3.7094299 and parameters: {'gamma': 0.9293887066331005, 'gae_lambda': 0.9032027877980728, 'lr': 0.0022997922174821843, 'n_epochs': 6, 'batch_size': 13, 'net_arch_width_int': 6, 'net_arch_depth': 5, 'n_envs': 3}. Best is trial 5 with value: -3.2622190000000004.[0m

 

{'policy': 'MlpPolicy', 'device': 'cuda', 'verbose': 1, 'n_steps': 16384, 'gamma': 0.9298605335381712, 'gae_lambda': 0.8969080553898536, 'learning_rate': 0.0019691054626304297, 'ent_coef': 1e-06, 'max_grad_norm': 0.5, 'vf_coef': 0.75, 'clip_range': 0.075, 'batch_size': 4096, 'n_epochs': 8, 'policy_kwargs': {'net_arch': [{'pi': [64, 64, 64, 64, 64], 'vf': [64, 64, 64, 64, 64]}]}}
n_envs:  5

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
Using cuda device
Eval num_timesteps=50000, episode_reward=-11.29 +/- 1.17
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -11.3    |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 50000    |
---------------------------------
New best mean reward!
Elapsed time: 81.1532 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.1     |
|    ep_rew_mean     | -11.8    |
| time/              |          |
|    fps             | 608      |
|    iterations      | 1        |
|    time_elapsed    | 134      |
|    total_timesteps | 81920    |
---------------------------------
Eval num_timesteps=100000, episode_reward=-7.96 +/- 0.96
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50           |
|    mean_reward          | -7.96        |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 100000       |
| train/                  |              |
|    approx_kl            | 0.0017871663 |
|    clip_fraction        | 0.145        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.68        |
|    explained_variance   | -0.203       |
|    learning_rate        | 0.00197      |
|    loss                 | 0.186        |
|    n_updates            | 8            |
|    policy_gradient_loss | -0.00429     |
|    std                  | 1            |
|    value_loss           | 0.425        |
------------------------------------------
New best mean reward!
Elapsed time: 86.2584 seconds
Eval num_timesteps=150000, episode_reward=-8.33 +/- 0.98
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -8.33    |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 150000   |
---------------------------------
Elapsed time: 84.8788 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.4     |
|    ep_rew_mean     | -12      |
| time/              |          |
|    fps             | 590      |
|    iterations      | 2        |
|    time_elapsed    | 277      |
|    total_timesteps | 163840   |
---------------------------------
Eval num_timesteps=200000, episode_reward=-5.04 +/- 0.82
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50           |
|    mean_reward          | -5.04        |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 200000       |
| train/                  |              |
|    approx_kl            | 0.0032757241 |
|    clip_fraction        | 0.281        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.67        |
|    explained_variance   | 0.625        |
|    learning_rate        | 0.00197      |
|    loss                 | 0.142        |
|    n_updates            | 16           |
|    policy_gradient_loss | -0.00944     |
|    std                  | 0.998        |
|    value_loss           | 0.288        |
------------------------------------------
New best mean reward!
Elapsed time: 89.5323 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.5     |
|    ep_rew_mean     | -11      |
| time/              |          |
|    fps             | 583      |
|    iterations      | 3        |
|    time_elapsed    | 421      |
|    total_timesteps | 245760   |
---------------------------------
TRIAL Finished: Objective Trial Time
Elapsed time: 422.5037 seconds
[32m[I 2022-04-21 14:16:32,320][0m Trial 12 finished with value: -5.0418061000000005 and parameters: {'gamma': 0.9298605335381712, 'gae_lambda': 0.8969080553898536, 'lr': 0.0019691054626304297, 'n_epochs': 8, 'batch_size': 12, 'net_arch_width_int': 6, 'net_arch_depth': 5, 'n_envs': 5}. Best is trial 5 with value: -3.2622190000000004.[0m

 

{'policy': 'MlpPolicy', 'device': 'cuda', 'verbose': 1, 'n_steps': 16384, 'gamma': 0.9093373345474445, 'gae_lambda': 0.9290620915181558, 'learning_rate': 0.003576327533197799, 'ent_coef': 1e-06, 'max_grad_norm': 0.5, 'vf_coef': 0.75, 'clip_range': 0.075, 'batch_size': 4096, 'n_epochs': 7, 'policy_kwargs': {'net_arch': [{'pi': [128, 128, 128, 128, 128], 'vf': [128, 128, 128, 128, 128]}]}}
n_envs:  4

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
Using cuda device
Eval num_timesteps=40000, episode_reward=-10.80 +/- 1.12
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -10.8    |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
New best mean reward!
Elapsed time: 67.5055 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.6     |
|    ep_rew_mean     | -12.3    |
| time/              |          |
|    fps             | 591      |
|    iterations      | 1        |
|    time_elapsed    | 110      |
|    total_timesteps | 65536    |
---------------------------------
Eval num_timesteps=80000, episode_reward=-7.22 +/- 1.58
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50           |
|    mean_reward          | -7.22        |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 80000        |
| train/                  |              |
|    approx_kl            | 0.0016946956 |
|    clip_fraction        | 0.184        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.68        |
|    explained_variance   | -0.32        |
|    learning_rate        | 0.00358      |
|    loss                 | 0.201        |
|    n_updates            | 7            |
|    policy_gradient_loss | -0.00154     |
|    std                  | 1            |
|    value_loss           | 0.416        |
------------------------------------------
New best mean reward!
Elapsed time: 70.9175 seconds
Eval num_timesteps=120000, episode_reward=-7.10 +/- 1.39
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -7.1     |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 120000   |
---------------------------------
New best mean reward!
Elapsed time: 68.0647 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.2     |
|    ep_rew_mean     | -12.3    |
| time/              |          |
|    fps             | 580      |
|    iterations      | 2        |
|    time_elapsed    | 225      |
|    total_timesteps | 131072   |
---------------------------------
Eval num_timesteps=160000, episode_reward=-4.69 +/- 1.02
Episode length: 49.80 +/- 0.60
Success rate: 20.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 49.8         |
|    mean_reward          | -4.69        |
|    success_rate         | 0.2          |
| time/                   |              |
|    total_timesteps      | 160000       |
| train/                  |              |
|    approx_kl            | 0.0035259367 |
|    clip_fraction        | 0.308        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.67        |
|    explained_variance   | 0.666        |
|    learning_rate        | 0.00358      |
|    loss                 | 0.156        |
|    n_updates            | 14           |
|    policy_gradient_loss | -0.00796     |
|    std                  | 0.997        |
|    value_loss           | 0.297        |
------------------------------------------
New best mean reward!
Elapsed time: 71.5520 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.4     |
|    ep_rew_mean     | -10.5    |
| time/              |          |
|    fps             | 573      |
|    iterations      | 3        |
|    time_elapsed    | 342      |
|    total_timesteps | 196608   |
---------------------------------
Eval num_timesteps=200000, episode_reward=-3.45 +/- 0.46
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | -3.45       |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 200000      |
| train/                  |             |
|    approx_kl            | 0.005320982 |
|    clip_fraction        | 0.409       |
|    clip_range           | 0.075       |
|    entropy_loss         | -5.65       |
|    explained_variance   | 0.815       |
|    learning_rate        | 0.00358     |
|    loss                 | 0.134       |
|    n_updates            | 21          |
|    policy_gradient_loss | -0.00833    |
|    std                  | 0.991       |
|    value_loss           | 0.224       |
-----------------------------------------
New best mean reward!
Elapsed time: 72.8382 seconds
Eval num_timesteps=240000, episode_reward=-3.00 +/- 0.52
Episode length: 44.60 +/- 8.50
Success rate: 30.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.6     |
|    mean_reward     | -3       |
|    success_rate    | 0.3      |
| time/              |          |
|    total_timesteps | 240000   |
---------------------------------
New best mean reward!
Elapsed time: 70.3942 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.4     |
|    ep_rew_mean     | -10      |
| time/              |          |
|    fps             | 568      |
|    iterations      | 4        |
|    time_elapsed    | 460      |
|    total_timesteps | 262144   |
---------------------------------
TRIAL Finished: Objective Trial Time
Elapsed time: 462.2772 seconds
[32m[I 2022-04-21 14:24:14,706][0m Trial 13 finished with value: -3.0011657 and parameters: {'gamma': 0.9093373345474445, 'gae_lambda': 0.9290620915181558, 'lr': 0.003576327533197799, 'n_epochs': 7, 'batch_size': 12, 'net_arch_width_int': 7, 'net_arch_depth': 5, 'n_envs': 4}. Best is trial 13 with value: -3.0011657.[0m

 

{'policy': 'MlpPolicy', 'device': 'cuda', 'verbose': 1, 'n_steps': 16384, 'gamma': 0.9006087264643433, 'gae_lambda': 0.9603973120051501, 'learning_rate': 0.003315620519661444, 'ent_coef': 1e-06, 'max_grad_norm': 0.5, 'vf_coef': 0.75, 'clip_range': 0.075, 'batch_size': 4096, 'n_epochs': 7, 'policy_kwargs': {'net_arch': [{'pi': [128, 128, 128, 128], 'vf': [128, 128, 128, 128]}]}}
n_envs:  6

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
Using cuda device
Eval num_timesteps=60000, episode_reward=-10.45 +/- 1.26
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -10.5    |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 60000    |
---------------------------------
New best mean reward!
Elapsed time: 100.6782 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.9     |
|    ep_rew_mean     | -11.6    |
| time/              |          |
|    fps             | 608      |
|    iterations      | 1        |
|    time_elapsed    | 161      |
|    total_timesteps | 98304    |
---------------------------------
Eval num_timesteps=120000, episode_reward=-7.79 +/- 1.26
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | -7.79       |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 120000      |
| train/                  |             |
|    approx_kl            | 0.002037125 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.075       |
|    entropy_loss         | -5.68       |
|    explained_variance   | -0.0888     |
|    learning_rate        | 0.00332     |
|    loss                 | 0.29        |
|    n_updates            | 7           |
|    policy_gradient_loss | -0.00217    |
|    std                  | 1           |
|    value_loss           | 0.552       |
-----------------------------------------
New best mean reward!
Elapsed time: 99.1127 seconds
Eval num_timesteps=180000, episode_reward=-7.70 +/- 0.90
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -7.7     |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 180000   |
---------------------------------
New best mean reward!
Elapsed time: 103.7086 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.6     |
|    ep_rew_mean     | -11.4    |
| time/              |          |
|    fps             | 594      |
|    iterations      | 2        |
|    time_elapsed    | 330      |
|    total_timesteps | 196608   |
---------------------------------
Eval num_timesteps=240000, episode_reward=-5.39 +/- 0.46
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | -5.39       |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 240000      |
| train/                  |             |
|    approx_kl            | 0.003937494 |
|    clip_fraction        | 0.304       |
|    clip_range           | 0.075       |
|    entropy_loss         | -5.67       |
|    explained_variance   | 0.647       |
|    learning_rate        | 0.00332     |
|    loss                 | 0.183       |
|    n_updates            | 14          |
|    policy_gradient_loss | -0.00636    |
|    std                  | 0.995       |
|    value_loss           | 0.326       |
-----------------------------------------
New best mean reward!
Elapsed time: 102.8171 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.5     |
|    ep_rew_mean     | -12.2    |
| time/              |          |
|    fps             | 591      |
|    iterations      | 3        |
|    time_elapsed    | 498      |
|    total_timesteps | 294912   |
---------------------------------
TRIAL Finished: Objective Trial Time
Elapsed time: 499.9861 seconds
[32m[I 2022-04-21 14:32:34,800][0m Trial 14 finished with value: -5.388017499999999 and parameters: {'gamma': 0.9006087264643433, 'gae_lambda': 0.9603973120051501, 'lr': 0.003315620519661444, 'n_epochs': 7, 'batch_size': 12, 'net_arch_width_int': 7, 'net_arch_depth': 4, 'n_envs': 6}. Best is trial 13 with value: -3.0011657.[0m

 

{'policy': 'MlpPolicy', 'device': 'cuda', 'verbose': 1, 'n_steps': 16384, 'gamma': 0.91790098248544, 'gae_lambda': 0.8758937044319642, 'learning_rate': 0.0018077837741196346, 'ent_coef': 1e-06, 'max_grad_norm': 0.5, 'vf_coef': 0.75, 'clip_range': 0.075, 'batch_size': 512, 'n_epochs': 12, 'policy_kwargs': {'net_arch': [{'pi': [256, 256, 256, 256, 256], 'vf': [256, 256, 256, 256, 256]}]}}
n_envs:  3

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
Using cuda device
Eval num_timesteps=30000, episode_reward=-11.01 +/- 1.41
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -11      |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
New best mean reward!
Elapsed time: 53.8260 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.6     |
|    ep_rew_mean     | -12.6    |
| time/              |          |
|    fps             | 560      |
|    iterations      | 1        |
|    time_elapsed    | 87       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-9.66 +/- 1.61
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | -9.66       |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.001694963 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.075       |
|    entropy_loss         | -5.68       |
|    explained_variance   | -0.377      |
|    learning_rate        | 0.00181     |
|    loss                 | 0.111       |
|    n_updates            | 12          |
|    policy_gradient_loss | 0.000653    |
|    std                  | 1           |
|    value_loss           | 0.161       |
-----------------------------------------
New best mean reward!
Elapsed time: 62.6085 seconds
Eval num_timesteps=90000, episode_reward=-10.09 +/- 1.39
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -10.1    |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 90000    |
---------------------------------
Elapsed time: 54.7646 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.3     |
|    ep_rew_mean     | -12.1    |
| time/              |          |
|    fps             | 527      |
|    iterations      | 2        |
|    time_elapsed    | 186      |
|    total_timesteps | 98304    |
---------------------------------
Eval num_timesteps=120000, episode_reward=-7.67 +/- 1.87
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50           |
|    mean_reward          | -7.67        |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 120000       |
| train/                  |              |
|    approx_kl            | 0.0030117196 |
|    clip_fraction        | 0.305        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.68        |
|    explained_variance   | 0.692        |
|    learning_rate        | 0.00181      |
|    loss                 | 0.0541       |
|    n_updates            | 24           |
|    policy_gradient_loss | -0.00695     |
|    std                  | 1            |
|    value_loss           | 0.115        |
------------------------------------------
New best mean reward!
Elapsed time: 62.7058 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.6     |
|    ep_rew_mean     | -11.1    |
| time/              |          |
|    fps             | 520      |
|    iterations      | 3        |
|    time_elapsed    | 283      |
|    total_timesteps | 147456   |
---------------------------------
Eval num_timesteps=150000, episode_reward=-5.48 +/- 1.01
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50           |
|    mean_reward          | -5.48        |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 150000       |
| train/                  |              |
|    approx_kl            | 0.0038505842 |
|    clip_fraction        | 0.354        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.66        |
|    explained_variance   | 0.829        |
|    learning_rate        | 0.00181      |
|    loss                 | 0.0738       |
|    n_updates            | 36           |
|    policy_gradient_loss | -0.00904     |
|    std                  | 0.999        |
|    value_loss           | 0.133        |
------------------------------------------
New best mean reward!
Elapsed time: 64.5424 seconds
Eval num_timesteps=180000, episode_reward=-5.61 +/- 1.34
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -5.61    |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 180000   |
---------------------------------
Elapsed time: 54.1999 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.6     |
|    ep_rew_mean     | -10.4    |
| time/              |          |
|    fps             | 513      |
|    iterations      | 4        |
|    time_elapsed    | 383      |
|    total_timesteps | 196608   |
---------------------------------
Eval num_timesteps=210000, episode_reward=-4.65 +/- 0.88
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | -4.65       |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 210000      |
| train/                  |             |
|    approx_kl            | 0.004069718 |
|    clip_fraction        | 0.372       |
|    clip_range           | 0.075       |
|    entropy_loss         | -5.65       |
|    explained_variance   | 0.862       |
|    learning_rate        | 0.00181     |
|    loss                 | 0.0696      |
|    n_updates            | 48          |
|    policy_gradient_loss | -0.00983    |
|    std                  | 0.995       |
|    value_loss           | 0.134       |
-----------------------------------------
New best mean reward!
Elapsed time: 69.3180 seconds
Eval num_timesteps=240000, episode_reward=-5.02 +/- 1.05
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -5.02    |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 240000   |
---------------------------------
Elapsed time: 55.2920 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 46.6     |
|    ep_rew_mean     | -9.29    |
| time/              |          |
|    fps             | 504      |
|    iterations      | 5        |
|    time_elapsed    | 487      |
|    total_timesteps | 245760   |
---------------------------------
TRIAL Finished: Objective Trial Time
Elapsed time: 495.5757 seconds
[32m[I 2022-04-21 14:40:50,478][0m Trial 15 finished with value: -5.0152569 and parameters: {'gamma': 0.91790098248544, 'gae_lambda': 0.8758937044319642, 'lr': 0.0018077837741196346, 'n_epochs': 12, 'batch_size': 9, 'net_arch_width_int': 8, 'net_arch_depth': 5, 'n_envs': 3}. Best is trial 13 with value: -3.0011657.[0m

 

{'policy': 'MlpPolicy', 'device': 'cuda', 'verbose': 1, 'n_steps': 16384, 'gamma': 0.9573393662916276, 'gae_lambda': 0.9300987830769297, 'learning_rate': 0.003910654874486223, 'ent_coef': 1e-06, 'max_grad_norm': 0.5, 'vf_coef': 0.75, 'clip_range': 0.075, 'batch_size': 2048, 'n_epochs': 9, 'policy_kwargs': {'net_arch': [{'pi': [128, 128, 128, 128], 'vf': [128, 128, 128, 128]}]}}
n_envs:  5

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
Using cuda device
Eval num_timesteps=50000, episode_reward=-11.44 +/- 1.46
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -11.4    |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 50000    |
---------------------------------
New best mean reward!
Elapsed time: 86.1117 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.2     |
|    ep_rew_mean     | -12.3    |
| time/              |          |
|    fps             | 575      |
|    iterations      | 1        |
|    time_elapsed    | 142      |
|    total_timesteps | 81920    |
---------------------------------
Eval num_timesteps=100000, episode_reward=-8.78 +/- 0.84
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | -8.78       |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 100000      |
| train/                  |             |
|    approx_kl            | 0.002294584 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.075       |
|    entropy_loss         | -5.67       |
|    explained_variance   | -0.0828     |
|    learning_rate        | 0.00391     |
|    loss                 | 0.407       |
|    n_updates            | 9           |
|    policy_gradient_loss | -0.000441   |
|    std                  | 1           |
|    value_loss           | 0.712       |
-----------------------------------------
New best mean reward!
Elapsed time: 90.9616 seconds
Eval num_timesteps=150000, episode_reward=-9.03 +/- 1.11
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -9.03    |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 150000   |
---------------------------------
Elapsed time: 87.5193 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.4     |
|    ep_rew_mean     | -11.3    |
| time/              |          |
|    fps             | 563      |
|    iterations      | 2        |
|    time_elapsed    | 290      |
|    total_timesteps | 163840   |
---------------------------------
Eval num_timesteps=200000, episode_reward=-6.16 +/- 0.85
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50           |
|    mean_reward          | -6.16        |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 200000       |
| train/                  |              |
|    approx_kl            | 0.0027894974 |
|    clip_fraction        | 0.286        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.69        |
|    explained_variance   | 0.54         |
|    learning_rate        | 0.00391      |
|    loss                 | 0.433        |
|    n_updates            | 18           |
|    policy_gradient_loss | -0.00324     |
|    std                  | 1            |
|    value_loss           | 0.68         |
------------------------------------------
New best mean reward!
Elapsed time: 94.1559 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.2     |
|    ep_rew_mean     | -10.3    |
| time/              |          |
|    fps             | 562      |
|    iterations      | 3        |
|    time_elapsed    | 436      |
|    total_timesteps | 245760   |
---------------------------------
TRIAL Finished: Objective Trial Time
Elapsed time: 440.6885 seconds
[32m[I 2022-04-21 14:48:11,276][0m Trial 16 finished with value: -6.1560668 and parameters: {'gamma': 0.9573393662916276, 'gae_lambda': 0.9300987830769297, 'lr': 0.003910654874486223, 'n_epochs': 9, 'batch_size': 11, 'net_arch_width_int': 7, 'net_arch_depth': 4, 'n_envs': 5}. Best is trial 13 with value: -3.0011657.[0m

 

{'policy': 'MlpPolicy', 'device': 'cuda', 'verbose': 1, 'n_steps': 16384, 'gamma': 0.9379517633696732, 'gae_lambda': 0.8809251789219449, 'learning_rate': 0.004983121594576348, 'ent_coef': 1e-06, 'max_grad_norm': 0.5, 'vf_coef': 0.75, 'clip_range': 0.075, 'batch_size': 4096, 'n_epochs': 7, 'policy_kwargs': {'net_arch': [{'pi': [256, 256, 256, 256, 256], 'vf': [256, 256, 256, 256, 256]}]}}
n_envs:  4

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
Using cuda device
Eval num_timesteps=40000, episode_reward=-10.96 +/- 0.93
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -11      |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
New best mean reward!
Elapsed time: 74.8472 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.4     |
|    ep_rew_mean     | -11.2    |
| time/              |          |
|    fps             | 535      |
|    iterations      | 1        |
|    time_elapsed    | 122      |
|    total_timesteps | 65536    |
---------------------------------
Eval num_timesteps=80000, episode_reward=-9.39 +/- 1.42
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50           |
|    mean_reward          | -9.39        |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 80000        |
| train/                  |              |
|    approx_kl            | 0.0023721969 |
|    clip_fraction        | 0.351        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.68        |
|    explained_variance   | -0.325       |
|    learning_rate        | 0.00498      |
|    loss                 | 0.132        |
|    n_updates            | 7            |
|    policy_gradient_loss | 0.00641      |
|    std                  | 1            |
|    value_loss           | 0.369        |
------------------------------------------
New best mean reward!
Elapsed time: 77.1215 seconds
Eval num_timesteps=120000, episode_reward=-8.51 +/- 1.13
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -8.51    |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 120000   |
---------------------------------
New best mean reward!
Elapsed time: 70.0549 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.7     |
|    ep_rew_mean     | -12.2    |
| time/              |          |
|    fps             | 543      |
|    iterations      | 2        |
|    time_elapsed    | 241      |
|    total_timesteps | 131072   |
---------------------------------
Eval num_timesteps=160000, episode_reward=-6.20 +/- 0.84
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50           |
|    mean_reward          | -6.2         |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 160000       |
| train/                  |              |
|    approx_kl            | 0.0059791426 |
|    clip_fraction        | 0.393        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.67        |
|    explained_variance   | 0.626        |
|    learning_rate        | 0.00498      |
|    loss                 | 0.175        |
|    n_updates            | 14           |
|    policy_gradient_loss | -0.00188     |
|    std                  | 0.998        |
|    value_loss           | 0.332        |
------------------------------------------
New best mean reward!
Elapsed time: 74.5150 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.3     |
|    ep_rew_mean     | -10.7    |
| time/              |          |
|    fps             | 542      |
|    iterations      | 3        |
|    time_elapsed    | 362      |
|    total_timesteps | 196608   |
---------------------------------
Eval num_timesteps=200000, episode_reward=-4.87 +/- 1.00
Episode length: 48.80 +/- 3.60
Success rate: 10.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 48.8         |
|    mean_reward          | -4.87        |
|    success_rate         | 0.1          |
| time/                   |              |
|    total_timesteps      | 200000       |
| train/                  |              |
|    approx_kl            | 0.0045353584 |
|    clip_fraction        | 0.414        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.65        |
|    explained_variance   | 0.801        |
|    learning_rate        | 0.00498      |
|    loss                 | 0.148        |
|    n_updates            | 21           |
|    policy_gradient_loss | -0.00549     |
|    std                  | 0.994        |
|    value_loss           | 0.286        |
------------------------------------------
New best mean reward!
Elapsed time: 74.6231 seconds
Eval num_timesteps=240000, episode_reward=-4.53 +/- 0.49
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -4.53    |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 240000   |
---------------------------------
New best mean reward!
Elapsed time: 74.0069 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48       |
|    ep_rew_mean     | -9.48    |
| time/              |          |
|    fps             | 541      |
|    iterations      | 4        |
|    time_elapsed    | 484      |
|    total_timesteps | 262144   |
---------------------------------
TRIAL Finished: Objective Trial Time
Elapsed time: 485.9508 seconds
[32m[I 2022-04-21 14:56:17,368][0m Trial 17 finished with value: -4.529951200000001 and parameters: {'gamma': 0.9379517633696732, 'gae_lambda': 0.8809251789219449, 'lr': 0.004983121594576348, 'n_epochs': 7, 'batch_size': 12, 'net_arch_width_int': 8, 'net_arch_depth': 5, 'n_envs': 4}. Best is trial 13 with value: -3.0011657.[0m

 

{'policy': 'MlpPolicy', 'device': 'cuda', 'verbose': 1, 'n_steps': 16384, 'gamma': 0.9785639702168824, 'gae_lambda': 0.9552045407749336, 'learning_rate': 0.0014716086718097247, 'ent_coef': 1e-06, 'max_grad_norm': 0.5, 'vf_coef': 0.75, 'clip_range': 0.075, 'batch_size': 1024, 'n_epochs': 10, 'policy_kwargs': {'net_arch': [{'pi': [128, 128, 128, 128, 128], 'vf': [128, 128, 128, 128, 128]}]}}
n_envs:  3

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
Using cuda device
Eval num_timesteps=30000, episode_reward=-9.89 +/- 0.79
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -9.89    |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
New best mean reward!
Elapsed time: 55.7986 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.8     |
|    ep_rew_mean     | -12.3    |
| time/              |          |
|    fps             | 551      |
|    iterations      | 1        |
|    time_elapsed    | 89       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-9.69 +/- 1.73
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | -9.69       |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.001813231 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.075       |
|    entropy_loss         | -5.67       |
|    explained_variance   | -0.0465     |
|    learning_rate        | 0.00147     |
|    loss                 | 1.21        |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00202    |
|    std                  | 0.998       |
|    value_loss           | 1.96        |
-----------------------------------------
New best mean reward!
Elapsed time: 56.7372 seconds
Eval num_timesteps=90000, episode_reward=-8.44 +/- 1.17
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -8.44    |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 90000    |
---------------------------------
New best mean reward!
Elapsed time: 57.1310 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49       |
|    ep_rew_mean     | -11.8    |
| time/              |          |
|    fps             | 534      |
|    iterations      | 2        |
|    time_elapsed    | 184      |
|    total_timesteps | 98304    |
---------------------------------
Eval num_timesteps=120000, episode_reward=-6.52 +/- 0.83
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50           |
|    mean_reward          | -6.52        |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 120000       |
| train/                  |              |
|    approx_kl            | 0.0024622707 |
|    clip_fraction        | 0.218        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.66        |
|    explained_variance   | 0.436        |
|    learning_rate        | 0.00147      |
|    loss                 | 1.2          |
|    n_updates            | 20           |
|    policy_gradient_loss | -0.00388     |
|    std                  | 0.994        |
|    value_loss           | 2.04         |
------------------------------------------
New best mean reward!
Elapsed time: 55.5682 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.3     |
|    ep_rew_mean     | -11.4    |
| time/              |          |
|    fps             | 539      |
|    iterations      | 3        |
|    time_elapsed    | 273      |
|    total_timesteps | 147456   |
---------------------------------
Eval num_timesteps=150000, episode_reward=-5.55 +/- 0.39
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50           |
|    mean_reward          | -5.55        |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 150000       |
| train/                  |              |
|    approx_kl            | 0.0030108213 |
|    clip_fraction        | 0.239        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.64        |
|    explained_variance   | 0.595        |
|    learning_rate        | 0.00147      |
|    loss                 | 1.65         |
|    n_updates            | 30           |
|    policy_gradient_loss | -0.00439     |
|    std                  | 0.992        |
|    value_loss           | 2.36         |
------------------------------------------
New best mean reward!
Elapsed time: 56.4896 seconds
Eval num_timesteps=180000, episode_reward=-5.89 +/- 0.38
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -5.89    |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 180000   |
---------------------------------
Elapsed time: 55.0549 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49       |
|    ep_rew_mean     | -10.9    |
| time/              |          |
|    fps             | 534      |
|    iterations      | 4        |
|    time_elapsed    | 367      |
|    total_timesteps | 196608   |
---------------------------------
Eval num_timesteps=210000, episode_reward=-4.93 +/- 0.51
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | -4.93       |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 210000      |
| train/                  |             |
|    approx_kl            | 0.003172515 |
|    clip_fraction        | 0.26        |
|    clip_range           | 0.075       |
|    entropy_loss         | -5.63       |
|    explained_variance   | 0.664       |
|    learning_rate        | 0.00147     |
|    loss                 | 1.96        |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.00421    |
|    std                  | 0.988       |
|    value_loss           | 2.83        |
-----------------------------------------
New best mean reward!
Elapsed time: 59.2168 seconds
Eval num_timesteps=240000, episode_reward=-5.06 +/- 0.62
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -5.06    |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 240000   |
---------------------------------
Elapsed time: 54.8067 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.4     |
|    ep_rew_mean     | -9.82    |
| time/              |          |
|    fps             | 532      |
|    iterations      | 5        |
|    time_elapsed    | 461      |
|    total_timesteps | 245760   |
---------------------------------
TRIAL Finished: Objective Trial Time
Elapsed time: 465.3850 seconds
[32m[I 2022-04-21 15:04:02,871][0m Trial 18 finished with value: -5.0594602 and parameters: {'gamma': 0.9785639702168824, 'gae_lambda': 0.9552045407749336, 'lr': 0.0014716086718097247, 'n_epochs': 10, 'batch_size': 10, 'net_arch_width_int': 7, 'net_arch_depth': 5, 'n_envs': 3}. Best is trial 13 with value: -3.0011657.[0m

 

{'policy': 'MlpPolicy', 'device': 'cuda', 'verbose': 1, 'n_steps': 16384, 'gamma': 0.9114895911805245, 'gae_lambda': 0.8568750114095723, 'learning_rate': 0.003242039400800422, 'ent_coef': 1e-06, 'max_grad_norm': 0.5, 'vf_coef': 0.75, 'clip_range': 0.075, 'batch_size': 2048, 'n_epochs': 8, 'policy_kwargs': {'net_arch': [{'pi': [128, 128, 128, 128], 'vf': [128, 128, 128, 128]}]}}
n_envs:  5

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
Using cuda device
Eval num_timesteps=50000, episode_reward=-11.25 +/- 1.28
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -11.2    |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 50000    |
---------------------------------
New best mean reward!
Elapsed time: 83.7228 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.7     |
|    ep_rew_mean     | -11.3    |
| time/              |          |
|    fps             | 595      |
|    iterations      | 1        |
|    time_elapsed    | 137      |
|    total_timesteps | 81920    |
---------------------------------
Eval num_timesteps=100000, episode_reward=-8.42 +/- 0.92
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50           |
|    mean_reward          | -8.42        |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 100000       |
| train/                  |              |
|    approx_kl            | 0.0019079618 |
|    clip_fraction        | 0.186        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.68        |
|    explained_variance   | -0.496       |
|    learning_rate        | 0.00324      |
|    loss                 | 0.0801       |
|    n_updates            | 8            |
|    policy_gradient_loss | -0.00089     |
|    std                  | 0.999        |
|    value_loss           | 0.172        |
------------------------------------------
New best mean reward!
Elapsed time: 88.2968 seconds
Eval num_timesteps=150000, episode_reward=-8.92 +/- 1.13
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -8.92    |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 150000   |
---------------------------------
Elapsed time: 85.5916 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.9     |
|    ep_rew_mean     | -11.9    |
| time/              |          |
|    fps             | 581      |
|    iterations      | 2        |
|    time_elapsed    | 281      |
|    total_timesteps | 163840   |
---------------------------------
Eval num_timesteps=200000, episode_reward=-6.42 +/- 1.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | -6.42       |
|    success_rate         | 0           |
| time/                   |             |
|    total_timesteps      | 200000      |
| train/                  |             |
|    approx_kl            | 0.004179134 |
|    clip_fraction        | 0.347       |
|    clip_range           | 0.075       |
|    entropy_loss         | -5.66       |
|    explained_variance   | 0.659       |
|    learning_rate        | 0.00324     |
|    loss                 | 0.064       |
|    n_updates            | 16          |
|    policy_gradient_loss | -0.00863    |
|    std                  | 0.996       |
|    value_loss           | 0.116       |
-----------------------------------------
New best mean reward!
Elapsed time: 89.5397 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.2     |
|    ep_rew_mean     | -11.2    |
| time/              |          |
|    fps             | 574      |
|    iterations      | 3        |
|    time_elapsed    | 427      |
|    total_timesteps | 245760   |
---------------------------------
TRIAL Finished: Objective Trial Time
Elapsed time: 430.6502 seconds
[32m[I 2022-04-21 15:11:13,641][0m Trial 19 finished with value: -6.41596 and parameters: {'gamma': 0.9114895911805245, 'gae_lambda': 0.8568750114095723, 'lr': 0.003242039400800422, 'n_epochs': 8, 'batch_size': 11, 'net_arch_width_int': 7, 'net_arch_depth': 4, 'n_envs': 5}. Best is trial 13 with value: -3.0011657.[0m

 

{'policy': 'MlpPolicy', 'device': 'cuda', 'verbose': 1, 'n_steps': 16384, 'gamma': 0.9389312498546001, 'gae_lambda': 0.9184089156051244, 'learning_rate': 0.001811454484939316, 'ent_coef': 1e-06, 'max_grad_norm': 0.5, 'vf_coef': 0.75, 'clip_range': 0.075, 'batch_size': 4096, 'n_epochs': 10, 'policy_kwargs': {'net_arch': [{'pi': [128, 128, 128, 128, 128], 'vf': [128, 128, 128, 128, 128]}]}}
n_envs:  6

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
Using cuda device
Eval num_timesteps=60000, episode_reward=-11.02 +/- 1.18
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -11      |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 60000    |
---------------------------------
New best mean reward!
Elapsed time: 102.9706 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.2     |
|    ep_rew_mean     | -11.8    |
| time/              |          |
|    fps             | 590      |
|    iterations      | 1        |
|    time_elapsed    | 166      |
|    total_timesteps | 98304    |
---------------------------------
Eval num_timesteps=120000, episode_reward=-9.25 +/- 1.37
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50           |
|    mean_reward          | -9.25        |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 120000       |
| train/                  |              |
|    approx_kl            | 0.0022844078 |
|    clip_fraction        | 0.182        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.68        |
|    explained_variance   | -0.329       |
|    learning_rate        | 0.00181      |
|    loss                 | 0.261        |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.00459     |
|    std                  | 1            |
|    value_loss           | 0.472        |
------------------------------------------
New best mean reward!
Elapsed time: 105.0849 seconds
Eval num_timesteps=180000, episode_reward=-8.75 +/- 1.54
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -8.75    |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 180000   |
---------------------------------
New best mean reward!
Elapsed time: 105.1133 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.6     |
|    ep_rew_mean     | -10.8    |
| time/              |          |
|    fps             | 576      |
|    iterations      | 2        |
|    time_elapsed    | 340      |
|    total_timesteps | 196608   |
---------------------------------
Eval num_timesteps=240000, episode_reward=-4.87 +/- 1.18
Episode length: 49.90 +/- 0.30
Success rate: 10.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 49.9         |
|    mean_reward          | -4.87        |
|    success_rate         | 0.1          |
| time/                   |              |
|    total_timesteps      | 240000       |
| train/                  |              |
|    approx_kl            | 0.0038396094 |
|    clip_fraction        | 0.297        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.67        |
|    explained_variance   | 0.58         |
|    learning_rate        | 0.00181      |
|    loss                 | 0.203        |
|    n_updates            | 20           |
|    policy_gradient_loss | -0.01        |
|    std                  | 0.998        |
|    value_loss           | 0.381        |
------------------------------------------
New best mean reward!
Elapsed time: 104.9704 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.7     |
|    ep_rew_mean     | -10.6    |
| time/              |          |
|    fps             | 574      |
|    iterations      | 3        |
|    time_elapsed    | 513      |
|    total_timesteps | 294912   |
---------------------------------
TRIAL Finished: Objective Trial Time
Elapsed time: 516.0400 seconds
[32m[I 2022-04-21 15:19:49,778][0m Trial 20 finished with value: -4.8650695 and parameters: {'gamma': 0.9389312498546001, 'gae_lambda': 0.9184089156051244, 'lr': 0.001811454484939316, 'n_epochs': 10, 'batch_size': 12, 'net_arch_width_int': 7, 'net_arch_depth': 5, 'n_envs': 6}. Best is trial 13 with value: -3.0011657.[0m

 

{'policy': 'MlpPolicy', 'device': 'cuda', 'verbose': 1, 'n_steps': 16384, 'gamma': 0.9375807530686551, 'gae_lambda': 0.8985384595836667, 'learning_rate': 0.002927032665635272, 'ent_coef': 1e-06, 'max_grad_norm': 0.5, 'vf_coef': 0.75, 'clip_range': 0.075, 'batch_size': 8192, 'n_epochs': 6, 'policy_kwargs': {'net_arch': [{'pi': [64, 64, 64, 64, 64], 'vf': [64, 64, 64, 64, 64]}]}}
n_envs:  4

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
Using cuda device
Eval num_timesteps=40000, episode_reward=-10.70 +/- 1.10
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -10.7    |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
New best mean reward!
Elapsed time: 69.8791 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.2     |
|    ep_rew_mean     | -12.3    |
| time/              |          |
|    fps             | 576      |
|    iterations      | 1        |
|    time_elapsed    | 113      |
|    total_timesteps | 65536    |
---------------------------------
Eval num_timesteps=80000, episode_reward=-7.38 +/- 1.56
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50           |
|    mean_reward          | -7.38        |
|    success_rate         | 0            |
| time/                   |              |
|    total_timesteps      | 80000        |
| train/                  |              |
|    approx_kl            | 0.0014681732 |
|    clip_fraction        | 0.122        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.68        |
|    explained_variance   | -0.324       |
|    learning_rate        | 0.00293      |
|    loss                 | 0.31         |
|    n_updates            | 6            |
|    policy_gradient_loss | -0.00325     |
|    std                  | 1            |
|    value_loss           | 0.678        |
------------------------------------------
New best mean reward!
Elapsed time: 71.5398 seconds
Eval num_timesteps=120000, episode_reward=-8.75 +/- 1.85
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -8.75    |
|    success_rate    | 0        |
| time/              |          |
|    total_timesteps | 120000   |
---------------------------------
Elapsed time: 76.2546 seconds
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.9     |
|    ep_rew_mean     | -11.6    |
| time/              |          |
|    fps             | 551      |
|    iterations      | 2        |
|    time_elapsed    | 237      |
|    total_timesteps | 131072   |
---------------------------------
^C
 

Number of finished trials:  22
Best trial:
  Value:  -3.0011657
  Params: 
    gamma: 0.9093373345474445
    gae_lambda: 0.9290620915181558
    lr: 0.003576327533197799
    n_epochs: 7
    batch_size: 12
    net_arch_width_int: 7
    net_arch_depth: 5
    n_envs: 4
  User attrs:
    vf_coef: 0.75
    clip_range: 0.075
    max_grad_norm: 0.5
    batch_size: 4096
    n_steps: 16384
    ent_coef: 1e-06
    net_arch_width: 128

 
 
 

-----------------------------------------------------------------------------------
TRAINING OPTIMIZED STUDY

 
 
 

{'policy': 'MlpPolicy', 'device': 'cuda', 'verbose': 1, 'gamma': 0.9093373345474445, 'gae_lambda': 0.9290620915181558, 'learning_rate': 0.003576327533197799, 'ent_coef': 1e-06, 'vf_coef': 0.75, 'clip_range': 0.075, 'max_grad_norm': 0.5, 'batch_size': 4096, 'n_steps': 16384, 'n_epochs': 7, 'policy_kwargs': {'net_arch': [{'pi': [128, 128, 128, 128, 128], 'vf': [128, 128, 128, 128, 128]}]}}
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
Using cuda device
Num timesteps: 40000
Best mean reward: -inf - Last mean reward per episode: -12.09
Saving new best model to 21April2022/optimizedVectStudy/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning:

positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.

---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.2     |
|    ep_rew_mean     | -10.9    |
| time/              |          |
|    fps             | 579      |
|    iterations      | 1        |
|    time_elapsed    | 113      |
|    total_timesteps | 65536    |
---------------------------------
Num timesteps: 80000
Best mean reward: -12.09 - Last mean reward per episode: -11.83
Saving new best model to 21April2022/optimizedVectStudy/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning:

positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.

Num timesteps: 120000
Best mean reward: -11.83 - Last mean reward per episode: -11.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 48.9         |
|    ep_rew_mean          | -11.5        |
| time/                   |              |
|    fps                  | 579          |
|    iterations           | 2            |
|    time_elapsed         | 226          |
|    total_timesteps      | 131072       |
| train/                  |              |
|    approx_kl            | 0.0013682295 |
|    clip_fraction        | 0.134        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.67        |
|    explained_variance   | -0.386       |
|    learning_rate        | 0.00358      |
|    loss                 | 0.234        |
|    n_updates            | 7            |
|    policy_gradient_loss | -0.000636    |
|    std                  | 1            |
|    value_loss           | 0.495        |
------------------------------------------
Num timesteps: 160000
Best mean reward: -11.83 - Last mean reward per episode: -11.29
Saving new best model to 21April2022/optimizedVectStudy/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning:

positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.

------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 48.3         |
|    ep_rew_mean          | -11          |
| time/                   |              |
|    fps                  | 568          |
|    iterations           | 3            |
|    time_elapsed         | 345          |
|    total_timesteps      | 196608       |
| train/                  |              |
|    approx_kl            | 0.0027373468 |
|    clip_fraction        | 0.271        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.67        |
|    explained_variance   | 0.655        |
|    learning_rate        | 0.00358      |
|    loss                 | 0.138        |
|    n_updates            | 14           |
|    policy_gradient_loss | -0.00737     |
|    std                  | 1            |
|    value_loss           | 0.261        |
------------------------------------------
Num timesteps: 200000
Best mean reward: -11.29 - Last mean reward per episode: -10.45
Saving new best model to 21April2022/optimizedVectStudy/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning:

positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.

Num timesteps: 240000
Best mean reward: -10.45 - Last mean reward per episode: -10.33
Saving new best model to 21April2022/optimizedVectStudy/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning:

positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.

------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 46.6         |
|    ep_rew_mean          | -9.63        |
| time/                   |              |
|    fps                  | 561          |
|    iterations           | 4            |
|    time_elapsed         | 466          |
|    total_timesteps      | 262144       |
| train/                  |              |
|    approx_kl            | 0.0035687052 |
|    clip_fraction        | 0.326        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.66        |
|    explained_variance   | 0.819        |
|    learning_rate        | 0.00358      |
|    loss                 | 0.153        |
|    n_updates            | 21           |
|    policy_gradient_loss | -0.0088      |
|    std                  | 0.997        |
|    value_loss           | 0.238        |
------------------------------------------
Num timesteps: 280000
Best mean reward: -10.33 - Last mean reward per episode: -9.47
Saving new best model to 21April2022/optimizedVectStudy/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning:

positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.

Num timesteps: 320000
Best mean reward: -9.47 - Last mean reward per episode: -10.05
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 46.7         |
|    ep_rew_mean          | -10.2        |
| time/                   |              |
|    fps                  | 545          |
|    iterations           | 5            |
|    time_elapsed         | 600          |
|    total_timesteps      | 327680       |
| train/                  |              |
|    approx_kl            | 0.0041409936 |
|    clip_fraction        | 0.355        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.65        |
|    explained_variance   | 0.867        |
|    learning_rate        | 0.00358      |
|    loss                 | 0.172        |
|    n_updates            | 28           |
|    policy_gradient_loss | -0.00732     |
|    std                  | 0.994        |
|    value_loss           | 0.268        |
------------------------------------------
Num timesteps: 360000
Best mean reward: -9.47 - Last mean reward per episode: -9.38
Saving new best model to 21April2022/optimizedVectStudy/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning:

positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.

------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 47.9         |
|    ep_rew_mean          | -9.69        |
| time/                   |              |
|    fps                  | 549          |
|    iterations           | 6            |
|    time_elapsed         | 715          |
|    total_timesteps      | 393216       |
| train/                  |              |
|    approx_kl            | 0.0035111802 |
|    clip_fraction        | 0.329        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.64        |
|    explained_variance   | 0.887        |
|    learning_rate        | 0.00358      |
|    loss                 | 0.188        |
|    n_updates            | 35           |
|    policy_gradient_loss | -0.00593     |
|    std                  | 0.991        |
|    value_loss           | 0.284        |
------------------------------------------
Num timesteps: 400000
Best mean reward: -9.38 - Last mean reward per episode: -9.29
Saving new best model to 21April2022/optimizedVectStudy/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning:

positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.

Num timesteps: 440000
Best mean reward: -9.29 - Last mean reward per episode: -8.94
Saving new best model to 21April2022/optimizedVectStudy/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning:

positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.

------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 45.9         |
|    ep_rew_mean          | -8.6         |
| time/                   |              |
|    fps                  | 550          |
|    iterations           | 7            |
|    time_elapsed         | 833          |
|    total_timesteps      | 458752       |
| train/                  |              |
|    approx_kl            | 0.0036005438 |
|    clip_fraction        | 0.328        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.61        |
|    explained_variance   | 0.892        |
|    learning_rate        | 0.00358      |
|    loss                 | 0.199        |
|    n_updates            | 42           |
|    policy_gradient_loss | -0.00383     |
|    std                  | 0.985        |
|    value_loss           | 0.301        |
------------------------------------------
Num timesteps: 480000
Best mean reward: -8.94 - Last mean reward per episode: -8.65
Saving new best model to 21April2022/optimizedVectStudy/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning:

positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.

Num timesteps: 520000
Best mean reward: -8.65 - Last mean reward per episode: -9.58
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 45.4        |
|    ep_rew_mean          | -9.17       |
| time/                   |             |
|    fps                  | 550         |
|    iterations           | 8           |
|    time_elapsed         | 952         |
|    total_timesteps      | 524288      |
| train/                  |             |
|    approx_kl            | 0.003990991 |
|    clip_fraction        | 0.311       |
|    clip_range           | 0.075       |
|    entropy_loss         | -5.59       |
|    explained_variance   | 0.897       |
|    learning_rate        | 0.00358     |
|    loss                 | 0.226       |
|    n_updates            | 49          |
|    policy_gradient_loss | -0.00261    |
|    std                  | 0.98        |
|    value_loss           | 0.328       |
-----------------------------------------
Num timesteps: 560000
Best mean reward: -8.65 - Last mean reward per episode: -9.00
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 46.9        |
|    ep_rew_mean          | -9.1        |
| time/                   |             |
|    fps                  | 550         |
|    iterations           | 9           |
|    time_elapsed         | 1071        |
|    total_timesteps      | 589824      |
| train/                  |             |
|    approx_kl            | 0.004349004 |
|    clip_fraction        | 0.316       |
|    clip_range           | 0.075       |
|    entropy_loss         | -5.57       |
|    explained_variance   | 0.908       |
|    learning_rate        | 0.00358     |
|    loss                 | 0.26        |
|    n_updates            | 56          |
|    policy_gradient_loss | -0.00182    |
|    std                  | 0.976       |
|    value_loss           | 0.332       |
-----------------------------------------
Num timesteps: 600000
Best mean reward: -8.65 - Last mean reward per episode: -7.39
Saving new best model to 21April2022/optimizedVectStudy/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning:

positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.

Num timesteps: 640000
Best mean reward: -7.39 - Last mean reward per episode: -7.78
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 44.5        |
|    ep_rew_mean          | -9.2        |
| time/                   |             |
|    fps                  | 548         |
|    iterations           | 10          |
|    time_elapsed         | 1194        |
|    total_timesteps      | 655360      |
| train/                  |             |
|    approx_kl            | 0.003778471 |
|    clip_fraction        | 0.324       |
|    clip_range           | 0.075       |
|    entropy_loss         | -5.55       |
|    explained_variance   | 0.9         |
|    learning_rate        | 0.00358     |
|    loss                 | 0.234       |
|    n_updates            | 63          |
|    policy_gradient_loss | -0.00146    |
|    std                  | 0.97        |
|    value_loss           | 0.303       |
-----------------------------------------
Num timesteps: 680000
Best mean reward: -7.39 - Last mean reward per episode: -8.24
Num timesteps: 720000
Best mean reward: -7.39 - Last mean reward per episode: -8.67
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 43.9         |
|    ep_rew_mean          | -8.29        |
| time/                   |              |
|    fps                  | 550          |
|    iterations           | 11           |
|    time_elapsed         | 1310         |
|    total_timesteps      | 720896       |
| train/                  |              |
|    approx_kl            | 0.0051992973 |
|    clip_fraction        | 0.354        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.53        |
|    explained_variance   | 0.893        |
|    learning_rate        | 0.00358      |
|    loss                 | 0.2          |
|    n_updates            | 70           |
|    policy_gradient_loss | -0.000179    |
|    std                  | 0.966        |
|    value_loss           | 0.281        |
------------------------------------------
Num timesteps: 760000
Best mean reward: -7.39 - Last mean reward per episode: -6.76
Saving new best model to 21April2022/optimizedVectStudy/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning:

positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.

-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 45.7        |
|    ep_rew_mean          | -7.77       |
| time/                   |             |
|    fps                  | 550         |
|    iterations           | 12          |
|    time_elapsed         | 1429        |
|    total_timesteps      | 786432      |
| train/                  |             |
|    approx_kl            | 0.005704277 |
|    clip_fraction        | 0.366       |
|    clip_range           | 0.075       |
|    entropy_loss         | -5.51       |
|    explained_variance   | 0.898       |
|    learning_rate        | 0.00358     |
|    loss                 | 0.209       |
|    n_updates            | 77          |
|    policy_gradient_loss | 0.000172    |
|    std                  | 0.959       |
|    value_loss           | 0.286       |
-----------------------------------------
Num timesteps: 800000
Best mean reward: -6.76 - Last mean reward per episode: -7.24
Num timesteps: 840000
Best mean reward: -6.76 - Last mean reward per episode: -7.89
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 41.9        |
|    ep_rew_mean          | -6.97       |
| time/                   |             |
|    fps                  | 549         |
|    iterations           | 13          |
|    time_elapsed         | 1550        |
|    total_timesteps      | 851968      |
| train/                  |             |
|    approx_kl            | 0.004975061 |
|    clip_fraction        | 0.352       |
|    clip_range           | 0.075       |
|    entropy_loss         | -5.48       |
|    explained_variance   | 0.898       |
|    learning_rate        | 0.00358     |
|    loss                 | 0.19        |
|    n_updates            | 84          |
|    policy_gradient_loss | -0.000663   |
|    std                  | 0.952       |
|    value_loss           | 0.282       |
-----------------------------------------
Num timesteps: 880000
Best mean reward: -6.76 - Last mean reward per episode: -6.90
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 42.9         |
|    ep_rew_mean          | -6.37        |
| time/                   |              |
|    fps                  | 544          |
|    iterations           | 14           |
|    time_elapsed         | 1685         |
|    total_timesteps      | 917504       |
| train/                  |              |
|    approx_kl            | 0.0051808516 |
|    clip_fraction        | 0.371        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.45        |
|    explained_variance   | 0.895        |
|    learning_rate        | 0.00358      |
|    loss                 | 0.194        |
|    n_updates            | 91           |
|    policy_gradient_loss | 0.000492     |
|    std                  | 0.947        |
|    value_loss           | 0.262        |
------------------------------------------
Num timesteps: 920000
Best mean reward: -6.76 - Last mean reward per episode: -6.45
Saving new best model to 21April2022/optimizedVectStudy/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning:

positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.

Num timesteps: 960000
Best mean reward: -6.45 - Last mean reward per episode: -6.54
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 43.6         |
|    ep_rew_mean          | -6.91        |
| time/                   |              |
|    fps                  | 540          |
|    iterations           | 15           |
|    time_elapsed         | 1818         |
|    total_timesteps      | 983040       |
| train/                  |              |
|    approx_kl            | 0.0068260063 |
|    clip_fraction        | 0.394        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.42        |
|    explained_variance   | 0.893        |
|    learning_rate        | 0.00358      |
|    loss                 | 0.172        |
|    n_updates            | 98           |
|    policy_gradient_loss | 0.000574     |
|    std                  | 0.938        |
|    value_loss           | 0.258        |
------------------------------------------
Num timesteps: 1000000
Best mean reward: -6.45 - Last mean reward per episode: -5.85
Saving new best model to 21April2022/optimizedVectStudy/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning:

positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.

Num timesteps: 1040000
Best mean reward: -5.85 - Last mean reward per episode: -5.55
Saving new best model to 21April2022/optimizedVectStudy/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning:

positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.

------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 40.2         |
|    ep_rew_mean          | -6.67        |
| time/                   |              |
|    fps                  | 538          |
|    iterations           | 16           |
|    time_elapsed         | 1945         |
|    total_timesteps      | 1048576      |
| train/                  |              |
|    approx_kl            | 0.0054990165 |
|    clip_fraction        | 0.393        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.39        |
|    explained_variance   | 0.891        |
|    learning_rate        | 0.00358      |
|    loss                 | 0.177        |
|    n_updates            | 105          |
|    policy_gradient_loss | 0.000724     |
|    std                  | 0.933        |
|    value_loss           | 0.243        |
------------------------------------------
Num timesteps: 1080000
Best mean reward: -5.55 - Last mean reward per episode: -5.04
Saving new best model to 21April2022/optimizedVectStudy/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning:

positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.

-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 40.9        |
|    ep_rew_mean          | -6.38       |
| time/                   |             |
|    fps                  | 536         |
|    iterations           | 17          |
|    time_elapsed         | 2077        |
|    total_timesteps      | 1114112     |
| train/                  |             |
|    approx_kl            | 0.005809024 |
|    clip_fraction        | 0.399       |
|    clip_range           | 0.075       |
|    entropy_loss         | -5.37       |
|    explained_variance   | 0.89        |
|    learning_rate        | 0.00358     |
|    loss                 | 0.166       |
|    n_updates            | 112         |
|    policy_gradient_loss | 0.00156     |
|    std                  | 0.927       |
|    value_loss           | 0.252       |
-----------------------------------------
Num timesteps: 1120000
Best mean reward: -5.04 - Last mean reward per episode: -5.34
Num timesteps: 1160000
Best mean reward: -5.04 - Last mean reward per episode: -4.84
Saving new best model to 21April2022/optimizedVectStudy/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning:

positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.

------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 39.8         |
|    ep_rew_mean          | -6.31        |
| time/                   |              |
|    fps                  | 535          |
|    iterations           | 18           |
|    time_elapsed         | 2204         |
|    total_timesteps      | 1179648      |
| train/                  |              |
|    approx_kl            | 0.0063226307 |
|    clip_fraction        | 0.403        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.33        |
|    explained_variance   | 0.891        |
|    learning_rate        | 0.00358      |
|    loss                 | 0.156        |
|    n_updates            | 119          |
|    policy_gradient_loss | 0.00131      |
|    std                  | 0.92         |
|    value_loss           | 0.234        |
------------------------------------------
Num timesteps: 1200000
Best mean reward: -4.84 - Last mean reward per episode: -5.66
Num timesteps: 1240000
Best mean reward: -4.84 - Last mean reward per episode: -5.40
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 35.4         |
|    ep_rew_mean          | -5.42        |
| time/                   |              |
|    fps                  | 530          |
|    iterations           | 19           |
|    time_elapsed         | 2346         |
|    total_timesteps      | 1245184      |
| train/                  |              |
|    approx_kl            | 0.0059750834 |
|    clip_fraction        | 0.42         |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.31        |
|    explained_variance   | 0.895        |
|    learning_rate        | 0.00358      |
|    loss                 | 0.152        |
|    n_updates            | 126          |
|    policy_gradient_loss | 0.00102      |
|    std                  | 0.913        |
|    value_loss           | 0.213        |
------------------------------------------
Num timesteps: 1280000
Best mean reward: -4.84 - Last mean reward per episode: -4.45
Saving new best model to 21April2022/optimizedVectStudy/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning:

positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.

-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 36.1        |
|    ep_rew_mean          | -5.06       |
| time/                   |             |
|    fps                  | 529         |
|    iterations           | 20          |
|    time_elapsed         | 2473        |
|    total_timesteps      | 1310720     |
| train/                  |             |
|    approx_kl            | 0.005827535 |
|    clip_fraction        | 0.416       |
|    clip_range           | 0.075       |
|    entropy_loss         | -5.27       |
|    explained_variance   | 0.9         |
|    learning_rate        | 0.00358     |
|    loss                 | 0.162       |
|    n_updates            | 133         |
|    policy_gradient_loss | 0.00109     |
|    std                  | 0.905       |
|    value_loss           | 0.227       |
-----------------------------------------
Num timesteps: 1320000
Best mean reward: -4.45 - Last mean reward per episode: -3.77
Saving new best model to 21April2022/optimizedVectStudy/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning:

positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.

Num timesteps: 1360000
Best mean reward: -3.77 - Last mean reward per episode: -4.19
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 29.5         |
|    ep_rew_mean          | -4.09        |
| time/                   |              |
|    fps                  | 529          |
|    iterations           | 21           |
|    time_elapsed         | 2597         |
|    total_timesteps      | 1376256      |
| train/                  |              |
|    approx_kl            | 0.0067559774 |
|    clip_fraction        | 0.423        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.23        |
|    explained_variance   | 0.898        |
|    learning_rate        | 0.00358      |
|    loss                 | 0.152        |
|    n_updates            | 140          |
|    policy_gradient_loss | 0.00238      |
|    std                  | 0.897        |
|    value_loss           | 0.219        |
------------------------------------------
Num timesteps: 1400000
Best mean reward: -3.77 - Last mean reward per episode: -4.25
Num timesteps: 1440000
Best mean reward: -3.77 - Last mean reward per episode: -3.35
Saving new best model to 21April2022/optimizedVectStudy/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning:

positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.

-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 29.7        |
|    ep_rew_mean          | -4.02       |
| time/                   |             |
|    fps                  | 529         |
|    iterations           | 22          |
|    time_elapsed         | 2720        |
|    total_timesteps      | 1441792     |
| train/                  |             |
|    approx_kl            | 0.006897864 |
|    clip_fraction        | 0.426       |
|    clip_range           | 0.075       |
|    entropy_loss         | -5.2        |
|    explained_variance   | 0.904       |
|    learning_rate        | 0.00358     |
|    loss                 | 0.133       |
|    n_updates            | 147         |
|    policy_gradient_loss | 0.00223     |
|    std                  | 0.892       |
|    value_loss           | 0.203       |
-----------------------------------------
Num timesteps: 1480000
Best mean reward: -3.35 - Last mean reward per episode: -3.30
Saving new best model to 21April2022/optimizedVectStudy/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning:

positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.

------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 29.7         |
|    ep_rew_mean          | -4.3         |
| time/                   |              |
|    fps                  | 529          |
|    iterations           | 23           |
|    time_elapsed         | 2845         |
|    total_timesteps      | 1507328      |
| train/                  |              |
|    approx_kl            | 0.0073349467 |
|    clip_fraction        | 0.438        |
|    clip_range           | 0.075        |
|    entropy_loss         | -5.17        |
|    explained_variance   | 0.897        |
|    learning_rate        | 0.00358      |
|    loss                 | 0.131        |
|    n_updates            | 154          |
|    policy_gradient_loss | 0.00213      |
|    std                  | 0.885        |
|    value_loss           | 0.185        |
------------------------------------------
Num timesteps: 1520000
Best mean reward: -3.30 - Last mean reward per episode: -2.47
Saving new best model to 21April2022/optimizedVectStudy/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning:

positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.

Num timesteps: 1560000
Best mean reward: -2.47 - Last mean reward per episode: -3.10
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 25.8        |
|    ep_rew_mean          | -3.39       |
| time/                   |             |
|    fps                  | 528         |
|    iterations           | 24          |
|    time_elapsed         | 2973        |
|    total_timesteps      | 1572864     |
| train/                  |             |
|    approx_kl            | 0.008115169 |
|    clip_fraction        | 0.444       |
|    clip_range           | 0.075       |
|    entropy_loss         | -5.14       |
|    explained_variance   | 0.904       |
|    learning_rate        | 0.00358     |
|    loss                 | 0.119       |
|    n_updates            | 161         |
|    policy_gradient_loss | 0.0018      |
|    std                  | 0.878       |
|    value_loss           | 0.173       |
-----------------------------------------
Num timesteps: 1600000
Best mean reward: -2.47 - Last mean reward per episode: -3.10
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 23          |
|    ep_rew_mean          | -2.86       |
| time/                   |             |
|    fps                  | 528         |
|    iterations           | 25          |
|    time_elapsed         | 3101        |
|    total_timesteps      | 1638400     |
| train/                  |             |
|    approx_kl            | 0.007286839 |
|    clip_fraction        | 0.46        |
|    clip_range           | 0.075       |
|    entropy_loss         | -5.1        |
|    explained_variance   | 0.904       |
|    learning_rate        | 0.00358     |
|    loss                 | 0.126       |
|    n_updates            | 168         |
|    policy_gradient_loss | 0.00264     |
|    std                  | 0.87        |
|    value_loss           | 0.175       |
-----------------------------------------
Num timesteps: 1640000
Best mean reward: -2.47 - Last mean reward per episode: -2.19
Saving new best model to 21April2022/optimizedVectStudy/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning:

positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.

Num timesteps: 1680000
Best mean reward: -2.19 - Last mean reward per episode: -2.15
Saving new best model to 21April2022/optimizedVectStudy/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning:

positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.

-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 24.9        |
|    ep_rew_mean          | -2.81       |
| time/                   |             |
|    fps                  | 527         |
|    iterations           | 26          |
|    time_elapsed         | 3232        |
|    total_timesteps      | 1703936     |
| train/                  |             |
|    approx_kl            | 0.007583531 |
|    clip_fraction        | 0.447       |
|    clip_range           | 0.075       |
|    entropy_loss         | -5.06       |
|    explained_variance   | 0.91        |
|    learning_rate        | 0.00358     |
|    loss                 | 0.114       |
|    n_updates            | 175         |
|    policy_gradient_loss | 0.0024      |
|    std                  | 0.862       |
|    value_loss           | 0.168       |
-----------------------------------------
Num timesteps: 1720000
Best mean reward: -2.15 - Last mean reward per episode: -1.93
Saving new best model to 21April2022/optimizedVectStudy/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning:

positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.

Num timesteps: 1760000
Best mean reward: -1.93 - Last mean reward per episode: -2.22
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.1        |
|    ep_rew_mean          | -2.78       |
| time/                   |             |
|    fps                  | 527         |
|    iterations           | 27          |
|    time_elapsed         | 3356        |
|    total_timesteps      | 1769472     |
| train/                  |             |
|    approx_kl            | 0.007543158 |
|    clip_fraction        | 0.456       |
|    clip_range           | 0.075       |
|    entropy_loss         | -5.02       |
|    explained_variance   | 0.904       |
|    learning_rate        | 0.00358     |
|    loss                 | 0.11        |
|    n_updates            | 182         |
|    policy_gradient_loss | 0.00265     |
|    std                  | 0.853       |
|    value_loss           | 0.168       |
-----------------------------------------
Num timesteps: 1800000
Best mean reward: -1.93 - Last mean reward per episode: -1.64
Saving new best model to 21April2022/optimizedVectStudy/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning:

positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.

-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 16.9        |
|    ep_rew_mean          | -1.85       |
| time/                   |             |
|    fps                  | 526         |
|    iterations           | 28          |
|    time_elapsed         | 3482        |
|    total_timesteps      | 1835008     |
| train/                  |             |
|    approx_kl            | 0.008109222 |
|    clip_fraction        | 0.462       |
|    clip_range           | 0.075       |
|    entropy_loss         | -4.97       |
|    explained_variance   | 0.903       |
|    learning_rate        | 0.00358     |
|    loss                 | 0.124       |
|    n_updates            | 189         |
|    policy_gradient_loss | 0.00246     |
|    std                  | 0.846       |
|    value_loss           | 0.159       |
-----------------------------------------
Num timesteps: 1840000
Best mean reward: -1.64 - Last mean reward per episode: -2.36
Num timesteps: 1880000
Best mean reward: -1.64 - Last mean reward per episode: -2.62
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17.4        |
|    ep_rew_mean          | -2.02       |
| time/                   |             |
|    fps                  | 525         |
|    iterations           | 29          |
|    time_elapsed         | 3614        |
|    total_timesteps      | 1900544     |
| train/                  |             |
|    approx_kl            | 0.008206284 |
|    clip_fraction        | 0.473       |
|    clip_range           | 0.075       |
|    entropy_loss         | -4.94       |
|    explained_variance   | 0.912       |
|    learning_rate        | 0.00358     |
|    loss                 | 0.102       |
|    n_updates            | 196         |
|    policy_gradient_loss | 0.00214     |
|    std                  | 0.836       |
|    value_loss           | 0.138       |
-----------------------------------------
Num timesteps: 1920000
Best mean reward: -1.64 - Last mean reward per episode: -1.37
Saving new best model to 21April2022/optimizedVectStudy/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning:

positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.

Num timesteps: 1960000
Best mean reward: -1.37 - Last mean reward per episode: -1.72
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 16.4        |
|    ep_rew_mean          | -1.92       |
| time/                   |             |
|    fps                  | 526         |
|    iterations           | 30          |
|    time_elapsed         | 3737        |
|    total_timesteps      | 1966080     |
| train/                  |             |
|    approx_kl            | 0.009390654 |
|    clip_fraction        | 0.487       |
|    clip_range           | 0.075       |
|    entropy_loss         | -4.89       |
|    explained_variance   | 0.922       |
|    learning_rate        | 0.00358     |
|    loss                 | 0.115       |
|    n_updates            | 203         |
|    policy_gradient_loss | 0.00284     |
|    std                  | 0.827       |
|    value_loss           | 0.149       |
-----------------------------------------
Num timesteps: 2000000
Best mean reward: -1.37 - Last mean reward per episode: -1.45
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 16.9        |
|    ep_rew_mean          | -2.39       |
| time/                   |             |
|    fps                  | 525         |
|    iterations           | 31          |
|    time_elapsed         | 3862        |
|    total_timesteps      | 2031616     |
| train/                  |             |
|    approx_kl            | 0.007871438 |
|    clip_fraction        | 0.473       |
|    clip_range           | 0.075       |
|    entropy_loss         | -4.84       |
|    explained_variance   | 0.912       |
|    learning_rate        | 0.00358     |
|    loss                 | 0.0992      |
|    n_updates            | 210         |
|    policy_gradient_loss | 0.00222     |
|    std                  | 0.817       |
|    value_loss           | 0.137       |
-----------------------------------------
Num timesteps: 2040000
Best mean reward: -1.37 - Last mean reward per episode: -1.19
Saving new best model to 21April2022/optimizedVectStudy/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning:

positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.

Num timesteps: 2080000
Best mean reward: -1.19 - Last mean reward per episode: -1.53
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 11.7        |
|    ep_rew_mean          | -1.24       |
| time/                   |             |
|    fps                  | 525         |
|    iterations           | 32          |
|    time_elapsed         | 3987        |
|    total_timesteps      | 2097152     |
| train/                  |             |
|    approx_kl            | 0.009092658 |
|    clip_fraction        | 0.499       |
|    clip_range           | 0.075       |
|    entropy_loss         | -4.79       |
|    explained_variance   | 0.921       |
|    learning_rate        | 0.00358     |
|    loss                 | 0.099       |
|    n_updates            | 217         |
|    policy_gradient_loss | 0.00317     |
|    std                  | 0.806       |
|    value_loss           | 0.138       |
-----------------------------------------
Num timesteps: 2120000
Best mean reward: -1.19 - Last mean reward per episode: -1.21
Num timesteps: 2160000
Best mean reward: -1.19 - Last mean reward per episode: -1.17
Saving new best model to 21April2022/optimizedVectStudy/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning:

positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.

-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 10.8        |
|    ep_rew_mean          | -1.08       |
| time/                   |             |
|    fps                  | 525         |
|    iterations           | 33          |
|    time_elapsed         | 4117        |
|    total_timesteps      | 2162688     |
| train/                  |             |
|    approx_kl            | 0.009325947 |
|    clip_fraction        | 0.52        |
|    clip_range           | 0.075       |
|    entropy_loss         | -4.73       |
|    explained_variance   | 0.922       |
|    learning_rate        | 0.00358     |
|    loss                 | 0.0799      |
|    n_updates            | 224         |
|    policy_gradient_loss | 0.00435     |
|    std                  | 0.796       |
|    value_loss           | 0.115       |
-----------------------------------------
Num timesteps: 2200000
Best mean reward: -1.17 - Last mean reward per episode: -0.77
Saving new best model to 21April2022/optimizedVectStudy/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning:

positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.

-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 9.61        |
|    ep_rew_mean          | -0.974      |
| time/                   |             |
|    fps                  | 525         |
|    iterations           | 34          |
|    time_elapsed         | 4238        |
|    total_timesteps      | 2228224     |
| train/                  |             |
|    approx_kl            | 0.011061671 |
|    clip_fraction        | 0.531       |
|    clip_range           | 0.075       |
|    entropy_loss         | -4.68       |
|    explained_variance   | 0.931       |
|    learning_rate        | 0.00358     |
|    loss                 | 0.0831      |
|    n_updates            | 231         |
|    policy_gradient_loss | 0.00432     |
|    std                  | 0.786       |
|    value_loss           | 0.12        |
-----------------------------------------
Num timesteps: 2240000
Best mean reward: -0.77 - Last mean reward per episode: -1.22
Num timesteps: 2280000
Best mean reward: -0.77 - Last mean reward per episode: -0.84
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 10.4        |
|    ep_rew_mean          | -1.21       |
| time/                   |             |
|    fps                  | 525         |
|    iterations           | 35          |
|    time_elapsed         | 4365        |
|    total_timesteps      | 2293760     |
| train/                  |             |
|    approx_kl            | 0.012008316 |
|    clip_fraction        | 0.55        |
|    clip_range           | 0.075       |
|    entropy_loss         | -4.62       |
|    explained_variance   | 0.927       |
|    learning_rate        | 0.00358     |
|    loss                 | 0.0687      |
|    n_updates            | 238         |
|    policy_gradient_loss | 0.00486     |
|    std                  | 0.774       |
|    value_loss           | 0.0974      |
-----------------------------------------
Num timesteps: 2320000
Best mean reward: -0.77 - Last mean reward per episode: -0.75
Saving new best model to 21April2022/optimizedVectStudy/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning:

positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.

-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 8.39        |
|    ep_rew_mean          | -0.972      |
| time/                   |             |
|    fps                  | 520         |
|    iterations           | 36          |
|    time_elapsed         | 4533        |
|    total_timesteps      | 2359296     |
| train/                  |             |
|    approx_kl            | 0.013677441 |
|    clip_fraction        | 0.55        |
|    clip_range           | 0.075       |
|    entropy_loss         | -4.56       |
|    explained_variance   | 0.93        |
|    learning_rate        | 0.00358     |
|    loss                 | 0.0811      |
|    n_updates            | 245         |
|    policy_gradient_loss | 0.00611     |
|    std                  | 0.764       |
|    value_loss           | 0.104       |
-----------------------------------------
Num timesteps: 2360000
Best mean reward: -0.75 - Last mean reward per episode: -0.68
Saving new best model to 21April2022/optimizedVectStudy/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning:

positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.

Num timesteps: 2400000
Best mean reward: -0.68 - Last mean reward per episode: -0.86
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 7.41        |
|    ep_rew_mean          | -0.748      |
| time/                   |             |
|    fps                  | 516         |
|    iterations           | 37          |
|    time_elapsed         | 4692        |
|    total_timesteps      | 2424832     |
| train/                  |             |
|    approx_kl            | 0.017566986 |
|    clip_fraction        | 0.587       |
|    clip_range           | 0.075       |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.93        |
|    learning_rate        | 0.00358     |
|    loss                 | 0.0644      |
|    n_updates            | 252         |
|    policy_gradient_loss | 0.00779     |
|    std                  | 0.752       |
|    value_loss           | 0.0893      |
-----------------------------------------
Num timesteps: 2440000
Best mean reward: -0.68 - Last mean reward per episode: -0.81
Num timesteps: 2480000
Best mean reward: -0.68 - Last mean reward per episode: -0.67
Saving new best model to 21April2022/optimizedVectStudy/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning:

positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.

-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 6.99        |
|    ep_rew_mean          | -0.87       |
| time/                   |             |
|    fps                  | 517         |
|    iterations           | 38          |
|    time_elapsed         | 4809        |
|    total_timesteps      | 2490368     |
| train/                  |             |
|    approx_kl            | 0.018638663 |
|    clip_fraction        | 0.602       |
|    clip_range           | 0.075       |
|    entropy_loss         | -4.43       |
|    explained_variance   | 0.942       |
|    learning_rate        | 0.00358     |
|    loss                 | 0.064       |
|    n_updates            | 259         |
|    policy_gradient_loss | 0.00778     |
|    std                  | 0.741       |
|    value_loss           | 0.0813      |
-----------------------------------------
Num timesteps: 2520000
Best mean reward: -0.67 - Last mean reward per episode: -0.60
Saving new best model to 21April2022/optimizedVectStudy/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning:

positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.

-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 6.73        |
|    ep_rew_mean          | -0.761      |
| time/                   |             |
|    fps                  | 518         |
|    iterations           | 39          |
|    time_elapsed         | 4929        |
|    total_timesteps      | 2555904     |
| train/                  |             |
|    approx_kl            | 0.014277449 |
|    clip_fraction        | 0.604       |
|    clip_range           | 0.075       |
|    entropy_loss         | -4.38       |
|    explained_variance   | 0.938       |
|    learning_rate        | 0.00358     |
|    loss                 | 0.0472      |
|    n_updates            | 266         |
|    policy_gradient_loss | 0.00919     |
|    std                  | 0.729       |
|    value_loss           | 0.067       |
-----------------------------------------
Num timesteps: 2560000
Best mean reward: -0.60 - Last mean reward per episode: -0.57
Saving new best model to 21April2022/optimizedVectStudy/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning:

positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.

Num timesteps: 2600000
Best mean reward: -0.57 - Last mean reward per episode: -0.55
Saving new best model to 21April2022/optimizedVectStudy/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning:

positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.

-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 7.39        |
|    ep_rew_mean          | -0.68       |
| time/                   |             |
|    fps                  | 519         |
|    iterations           | 40          |
|    time_elapsed         | 5047        |
|    total_timesteps      | 2621440     |
| train/                  |             |
|    approx_kl            | 0.021568973 |
|    clip_fraction        | 0.649       |
|    clip_range           | 0.075       |
|    entropy_loss         | -4.3        |
|    explained_variance   | 0.932       |
|    learning_rate        | 0.00358     |
|    loss                 | 0.0439      |
|    n_updates            | 273         |
|    policy_gradient_loss | 0.0105      |
|    std                  | 0.716       |
|    value_loss           | 0.0593      |
-----------------------------------------
Num timesteps: 2640000
Best mean reward: -0.55 - Last mean reward per episode: -0.56
Num timesteps: 2680000
Best mean reward: -0.55 - Last mean reward per episode: -0.59
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 5.94        |
|    ep_rew_mean          | -0.537      |
| time/                   |             |
|    fps                  | 519         |
|    iterations           | 41          |
|    time_elapsed         | 5167        |
|    total_timesteps      | 2686976     |
| train/                  |             |
|    approx_kl            | 0.023571707 |
|    clip_fraction        | 0.643       |
|    clip_range           | 0.075       |
|    entropy_loss         | -4.22       |
|    explained_variance   | 0.936       |
|    learning_rate        | 0.00358     |
|    loss                 | 0.0373      |
|    n_updates            | 280         |
|    policy_gradient_loss | 0.0095      |
|    std                  | 0.702       |
|    value_loss           | 0.0445      |
-----------------------------------------
Num timesteps: 2720000
Best mean reward: -0.55 - Last mean reward per episode: -0.48
Saving new best model to 21April2022/optimizedVectStudy/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning:

positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.

-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 5.39        |
|    ep_rew_mean          | -0.502      |
| time/                   |             |
|    fps                  | 520         |
|    iterations           | 42          |
|    time_elapsed         | 5286        |
|    total_timesteps      | 2752512     |
| train/                  |             |
|    approx_kl            | 0.022735856 |
|    clip_fraction        | 0.671       |
|    clip_range           | 0.075       |
|    entropy_loss         | -4.14       |
|    explained_variance   | 0.932       |
|    learning_rate        | 0.00358     |
|    loss                 | 0.0267      |
|    n_updates            | 287         |
|    policy_gradient_loss | 0.0111      |
|    std                  | 0.69        |
|    value_loss           | 0.036       |
-----------------------------------------
Num timesteps: 2760000
Best mean reward: -0.48 - Last mean reward per episode: -0.47
Saving new best model to 21April2022/optimizedVectStudy/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning:

positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.

Num timesteps: 2800000
Best mean reward: -0.47 - Last mean reward per episode: -0.68
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 5.45        |
|    ep_rew_mean          | -0.489      |
| time/                   |             |
|    fps                  | 521         |
|    iterations           | 43          |
|    time_elapsed         | 5403        |
|    total_timesteps      | 2818048     |
| train/                  |             |
|    approx_kl            | 0.024616055 |
|    clip_fraction        | 0.683       |
|    clip_range           | 0.075       |
|    entropy_loss         | -4.06       |
|    explained_variance   | 0.938       |
|    learning_rate        | 0.00358     |
|    loss                 | 0.026       |
|    n_updates            | 294         |
|    policy_gradient_loss | 0.0133      |
|    std                  | 0.675       |
|    value_loss           | 0.0338      |
-----------------------------------------
Num timesteps: 2840000
Best mean reward: -0.47 - Last mean reward per episode: -0.53
Num timesteps: 2880000
Best mean reward: -0.47 - Last mean reward per episode: -0.53
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4.96        |
|    ep_rew_mean          | -0.455      |
| time/                   |             |
|    fps                  | 522         |
|    iterations           | 44          |
|    time_elapsed         | 5520        |
|    total_timesteps      | 2883584     |
| train/                  |             |
|    approx_kl            | 0.028533807 |
|    clip_fraction        | 0.671       |
|    clip_range           | 0.075       |
|    entropy_loss         | -3.98       |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.00358     |
|    loss                 | 0.0336      |
|    n_updates            | 301         |
|    policy_gradient_loss | 0.0115      |
|    std                  | 0.663       |
|    value_loss           | 0.0334      |
-----------------------------------------
Num timesteps: 2920000
Best mean reward: -0.47 - Last mean reward per episode: -0.54
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 5.32       |
|    ep_rew_mean          | -0.499     |
| time/                   |            |
|    fps                  | 523        |
|    iterations           | 45         |
|    time_elapsed         | 5638       |
|    total_timesteps      | 2949120    |
| train/                  |            |
|    approx_kl            | 0.04175459 |
|    clip_fraction        | 0.696      |
|    clip_range           | 0.075      |
|    entropy_loss         | -3.9       |
|    explained_variance   | 0.946      |
|    learning_rate        | 0.00358    |
|    loss                 | 0.0277     |
|    n_updates            | 308        |
|    policy_gradient_loss | 0.0128     |
|    std                  | 0.646      |
|    value_loss           | 0.0234     |
----------------------------------------
Num timesteps: 2960000
Best mean reward: -0.47 - Last mean reward per episode: -0.44
Saving new best model to 21April2022/optimizedVectStudy/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning:

positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.

Num timesteps: 3000000
Best mean reward: -0.44 - Last mean reward per episode: -0.51
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 5.62       |
|    ep_rew_mean          | -0.507     |
| time/                   |            |
|    fps                  | 523        |
|    iterations           | 46         |
|    time_elapsed         | 5756       |
|    total_timesteps      | 3014656    |
| train/                  |            |
|    approx_kl            | 0.03093155 |
|    clip_fraction        | 0.721      |
|    clip_range           | 0.075      |
|    entropy_loss         | -3.79      |
|    explained_variance   | 0.957      |
|    learning_rate        | 0.00358    |
|    loss                 | 0.0252     |
|    n_updates            | 315        |
|    policy_gradient_loss | 0.0152     |
|    std                  | 0.632      |
|    value_loss           | 0.0241     |
----------------------------------------
Num timesteps: 3040000
Best mean reward: -0.44 - Last mean reward per episode: -0.42
Saving new best model to 21April2022/optimizedVectStudy/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning:

positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.

Num timesteps: 3080000
Best mean reward: -0.42 - Last mean reward per episode: -0.73
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4.94       |
|    ep_rew_mean          | -0.724     |
| time/                   |            |
|    fps                  | 519        |
|    iterations           | 47         |
|    time_elapsed         | 5924       |
|    total_timesteps      | 3080192    |
| train/                  |            |
|    approx_kl            | 0.03802345 |
|    clip_fraction        | 0.723      |
|    clip_range           | 0.075      |
|    entropy_loss         | -3.69      |
|    explained_variance   | 0.941      |
|    learning_rate        | 0.00358    |
|    loss                 | 0.0281     |
|    n_updates            | 322        |
|    policy_gradient_loss | 0.0133     |
|    std                  | 0.617      |
|    value_loss           | 0.0183     |
----------------------------------------
Num timesteps: 3120000
Best mean reward: -0.42 - Last mean reward per episode: -0.48
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4.84        |
|    ep_rew_mean          | -0.45       |
| time/                   |             |
|    fps                  | 517         |
|    iterations           | 48          |
|    time_elapsed         | 6075        |
|    total_timesteps      | 3145728     |
| train/                  |             |
|    approx_kl            | 0.054973383 |
|    clip_fraction        | 0.754       |
|    clip_range           | 0.075       |
|    entropy_loss         | -3.58       |
|    explained_variance   | 0.944       |
|    learning_rate        | 0.00358     |
|    loss                 | 0.0216      |
|    n_updates            | 329         |
|    policy_gradient_loss | 0.0195      |
|    std                  | 0.601       |
|    value_loss           | 0.0111      |
-----------------------------------------
Num timesteps: 3160000
Best mean reward: -0.42 - Last mean reward per episode: -0.43
Num timesteps: 3200000
Best mean reward: -0.42 - Last mean reward per episode: -0.40
Saving new best model to 21April2022/optimizedVectStudy/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning:

positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.

----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4.52       |
|    ep_rew_mean          | -0.403     |
| time/                   |            |
|    fps                  | 518        |
|    iterations           | 49         |
|    time_elapsed         | 6191       |
|    total_timesteps      | 3211264    |
| train/                  |            |
|    approx_kl            | 0.04937953 |
|    clip_fraction        | 0.75       |
|    clip_range           | 0.075      |
|    entropy_loss         | -3.48      |
|    explained_variance   | 0.942      |
|    learning_rate        | 0.00358    |
|    loss                 | 0.0288     |
|    n_updates            | 336        |
|    policy_gradient_loss | 0.0182     |
|    std                  | 0.587      |
|    value_loss           | 0.0125     |
----------------------------------------
Num timesteps: 3240000
Best mean reward: -0.40 - Last mean reward per episode: -0.39
Saving new best model to 21April2022/optimizedVectStudy/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning:

positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.

-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4.5         |
|    ep_rew_mean          | -0.418      |
| time/                   |             |
|    fps                  | 519         |
|    iterations           | 50          |
|    time_elapsed         | 6307        |
|    total_timesteps      | 3276800     |
| train/                  |             |
|    approx_kl            | 0.059208058 |
|    clip_fraction        | 0.784       |
|    clip_range           | 0.075       |
|    entropy_loss         | -3.38       |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.00358     |
|    loss                 | 0.0294      |
|    n_updates            | 343         |
|    policy_gradient_loss | 0.0247      |
|    std                  | 0.574       |
|    value_loss           | 0.0157      |
-----------------------------------------
Num timesteps: 3280000
Best mean reward: -0.39 - Last mean reward per episode: -0.40
Num timesteps: 3320000
Best mean reward: -0.39 - Last mean reward per episode: -0.40
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.22      |
|    ep_rew_mean          | -0.39     |
| time/                   |           |
|    fps                  | 520       |
|    iterations           | 51        |
|    time_elapsed         | 6423      |
|    total_timesteps      | 3342336   |
| train/                  |           |
|    approx_kl            | 0.0925923 |
|    clip_fraction        | 0.791     |
|    clip_range           | 0.075     |
|    entropy_loss         | -3.27     |
|    explained_variance   | 0.952     |
|    learning_rate        | 0.00358   |
|    loss                 | 0.0201    |
|    n_updates            | 350       |
|    policy_gradient_loss | 0.0241    |
|    std                  | 0.557     |
|    value_loss           | 0.011     |
---------------------------------------
Num timesteps: 3360000
Best mean reward: -0.39 - Last mean reward per episode: -0.40
Num timesteps: 3400000
Best mean reward: -0.39 - Last mean reward per episode: -0.39
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.42      |
|    ep_rew_mean          | -0.404    |
| time/                   |           |
|    fps                  | 521       |
|    iterations           | 52        |
|    time_elapsed         | 6537      |
|    total_timesteps      | 3407872   |
| train/                  |           |
|    approx_kl            | 0.0656382 |
|    clip_fraction        | 0.784     |
|    clip_range           | 0.075     |
|    entropy_loss         | -3.14     |
|    explained_variance   | 0.953     |
|    learning_rate        | 0.00358   |
|    loss                 | 0.0258    |
|    n_updates            | 357       |
|    policy_gradient_loss | 0.0215    |
|    std                  | 0.541     |
|    value_loss           | 0.00839   |
---------------------------------------
Num timesteps: 3440000
Best mean reward: -0.39 - Last mean reward per episode: -0.39
Saving new best model to 21April2022/optimizedVectStudy/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning:

positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.

----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4.24       |
|    ep_rew_mean          | -0.387     |
| time/                   |            |
|    fps                  | 522        |
|    iterations           | 53         |
|    time_elapsed         | 6652       |
|    total_timesteps      | 3473408    |
| train/                  |            |
|    approx_kl            | 0.05887796 |
|    clip_fraction        | 0.789      |
|    clip_range           | 0.075      |
|    entropy_loss         | -3.03      |
|    explained_variance   | 0.968      |
|    learning_rate        | 0.00358    |
|    loss                 | 0.00871    |
|    n_updates            | 364        |
|    policy_gradient_loss | 0.0212     |
|    std                  | 0.528      |
|    value_loss           | 0.00808    |
----------------------------------------
Num timesteps: 3480000
Best mean reward: -0.39 - Last mean reward per episode: -0.37
Saving new best model to 21April2022/optimizedVectStudy/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning:

positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.

Num timesteps: 3520000
Best mean reward: -0.37 - Last mean reward per episode: -0.38
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4.27       |
|    ep_rew_mean          | -0.402     |
| time/                   |            |
|    fps                  | 523        |
|    iterations           | 54         |
|    time_elapsed         | 6765       |
|    total_timesteps      | 3538944    |
| train/                  |            |
|    approx_kl            | 0.06613043 |
|    clip_fraction        | 0.784      |
|    clip_range           | 0.075      |
|    entropy_loss         | -2.93      |
|    explained_variance   | 0.941      |
|    learning_rate        | 0.00358    |
|    loss                 | 0.0198     |
|    n_updates            | 371        |
|    policy_gradient_loss | 0.025      |
|    std                  | 0.516      |
|    value_loss           | 0.00384    |
----------------------------------------
Num timesteps: 3560000
Best mean reward: -0.37 - Last mean reward per episode: -0.40
Num timesteps: 3600000
Best mean reward: -0.37 - Last mean reward per episode: -0.38
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4.38       |
|    ep_rew_mean          | -0.391     |
| time/                   |            |
|    fps                  | 523        |
|    iterations           | 55         |
|    time_elapsed         | 6882       |
|    total_timesteps      | 3604480    |
| train/                  |            |
|    approx_kl            | 0.07135095 |
|    clip_fraction        | 0.816      |
|    clip_range           | 0.075      |
|    entropy_loss         | -2.8       |
|    explained_variance   | 0.952      |
|    learning_rate        | 0.00358    |
|    loss                 | 0.0186     |
|    n_updates            | 378        |
|    policy_gradient_loss | 0.0263     |
|    std                  | 0.503      |
|    value_loss           | 0.00445    |
----------------------------------------
Num timesteps: 3640000
Best mean reward: -0.37 - Last mean reward per episode: -0.38
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4.16       |
|    ep_rew_mean          | -0.378     |
| time/                   |            |
|    fps                  | 524        |
|    iterations           | 56         |
|    time_elapsed         | 6998       |
|    total_timesteps      | 3670016    |
| train/                  |            |
|    approx_kl            | 0.09513864 |
|    clip_fraction        | 0.79       |
|    clip_range           | 0.075      |
|    entropy_loss         | -2.71      |
|    explained_variance   | 0.957      |
|    learning_rate        | 0.00358    |
|    loss                 | 0.0353     |
|    n_updates            | 385        |
|    policy_gradient_loss | 0.035      |
|    std                  | 0.488      |
|    value_loss           | 0.00287    |
----------------------------------------
Num timesteps: 3680000
Best mean reward: -0.37 - Last mean reward per episode: -2.79
Num timesteps: 3720000
Best mean reward: -0.37 - Last mean reward per episode: -3.63
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 21.1     |
|    ep_rew_mean          | -3.03    |
| time/                   |          |
|    fps                  | 521      |
|    iterations           | 57       |
|    time_elapsed         | 7159     |
|    total_timesteps      | 3735552  |
| train/                  |          |
|    approx_kl            | 5.047485 |
|    clip_fraction        | 0.9      |
|    clip_range           | 0.075    |
|    entropy_loss         | -2.62    |
|    explained_variance   | 0.925    |
|    learning_rate        | 0.00358  |
|    loss                 | 0.101    |
|    n_updates            | 392      |
|    policy_gradient_loss | 0.0749   |
|    std                  | 0.48     |
|    value_loss           | 0.00704  |
--------------------------------------
Num timesteps: 3760000
Best mean reward: -0.37 - Last mean reward per episode: -4.53
Num timesteps: 3800000
Best mean reward: -0.37 - Last mean reward per episode: -3.24
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 24.9     |
|    ep_rew_mean          | -3.78    |
| time/                   |          |
|    fps                  | 516      |
|    iterations           | 58       |
|    time_elapsed         | 7366     |
|    total_timesteps      | 3801088  |
| train/                  |          |
|    approx_kl            | 30.48748 |
|    clip_fraction        | 0.972    |
|    clip_range           | 0.075    |
|    entropy_loss         | -2.78    |
|    explained_variance   | 0.889    |
|    learning_rate        | 0.00358  |
|    loss                 | 0.25     |
|    n_updates            | 399      |
|    policy_gradient_loss | 0.212    |
|    std                  | 0.503    |
|    value_loss           | 0.176    |
--------------------------------------
Num timesteps: 3840000
Best mean reward: -0.37 - Last mean reward per episode: -4.34
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 26.1       |
|    ep_rew_mean          | -3.82      |
| time/                   |            |
|    fps                  | 514        |
|    iterations           | 59         |
|    time_elapsed         | 7517       |
|    total_timesteps      | 3866624    |
| train/                  |            |
|    approx_kl            | 0.10497621 |
|    clip_fraction        | 0.803      |
|    clip_range           | 0.075      |
|    entropy_loss         | -2.84      |
|    explained_variance   | 0.89       |
|    learning_rate        | 0.00358    |
|    loss                 | 0.17       |
|    n_updates            | 406        |
|    policy_gradient_loss | 0.0836     |
|    std                  | 0.505      |
|    value_loss           | 0.175      |
----------------------------------------
Num timesteps: 3880000
Best mean reward: -0.37 - Last mean reward per episode: -3.49
Num timesteps: 3920000
Best mean reward: -0.37 - Last mean reward per episode: -4.20
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 23         |
|    ep_rew_mean          | -3.41      |
| time/                   |            |
|    fps                  | 513        |
|    iterations           | 60         |
|    time_elapsed         | 7653       |
|    total_timesteps      | 3932160    |
| train/                  |            |
|    approx_kl            | 0.23218244 |
|    clip_fraction        | 0.79       |
|    clip_range           | 0.075      |
|    entropy_loss         | -2.86      |
|    explained_variance   | 0.935      |
|    learning_rate        | 0.00358    |
|    loss                 | 0.172      |
|    n_updates            | 413        |
|    policy_gradient_loss | 0.065      |
|    std                  | 0.509      |
|    value_loss           | 0.131      |
----------------------------------------
Num timesteps: 3960000
Best mean reward: -0.37 - Last mean reward per episode: -2.69
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 24.2       |
|    ep_rew_mean          | -3.69      |
| time/                   |            |
|    fps                  | 512        |
|    iterations           | 61         |
|    time_elapsed         | 7797       |
|    total_timesteps      | 3997696    |
| train/                  |            |
|    approx_kl            | 0.10700377 |
|    clip_fraction        | 0.808      |
|    clip_range           | 0.075      |
|    entropy_loss         | -2.89      |
|    explained_variance   | 0.946      |
|    learning_rate        | 0.00358    |
|    loss                 | 0.138      |
|    n_updates            | 420        |
|    policy_gradient_loss | 0.0704     |
|    std                  | 0.512      |
|    value_loss           | 0.15       |
----------------------------------------
Num timesteps: 4000000
Best mean reward: -0.37 - Last mean reward per episode: -2.71
Num timesteps: 4040000
Best mean reward: -0.37 - Last mean reward per episode: -2.57
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18.6       |
|    ep_rew_mean          | -2.55      |
| time/                   |            |
|    fps                  | 511        |
|    iterations           | 62         |
|    time_elapsed         | 7941       |
|    total_timesteps      | 4063232    |
| train/                  |            |
|    approx_kl            | 0.07309611 |
|    clip_fraction        | 0.753      |
|    clip_range           | 0.075      |
|    entropy_loss         | -2.9       |
|    explained_variance   | 0.942      |
|    learning_rate        | 0.00358    |
|    loss                 | 0.14       |
|    n_updates            | 427        |
|    policy_gradient_loss | 0.0473     |
|    std                  | 0.512      |
|    value_loss           | 0.141      |
----------------------------------------
Num timesteps: 4080000
Best mean reward: -0.37 - Last mean reward per episode: -2.63
Num timesteps: 4120000
Best mean reward: -0.37 - Last mean reward per episode: -2.85
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.8       |
|    ep_rew_mean          | -3.18      |
| time/                   |            |
|    fps                  | 510        |
|    iterations           | 63         |
|    time_elapsed         | 8083       |
|    total_timesteps      | 4128768    |
| train/                  |            |
|    approx_kl            | 0.05987318 |
|    clip_fraction        | 0.736      |
|    clip_range           | 0.075      |
|    entropy_loss         | -2.9       |
|    explained_variance   | 0.939      |
|    learning_rate        | 0.00358    |
|    loss                 | 0.116      |
|    n_updates            | 434        |
|    policy_gradient_loss | 0.0448     |
|    std                  | 0.511      |
|    value_loss           | 0.126      |
----------------------------------------
Num timesteps: 4160000
Best mean reward: -0.37 - Last mean reward per episode: -2.35
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 22.1       |
|    ep_rew_mean          | -3.39      |
| time/                   |            |
|    fps                  | 510        |
|    iterations           | 64         |
|    time_elapsed         | 8223       |
|    total_timesteps      | 4194304    |
| train/                  |            |
|    approx_kl            | 0.03875535 |
|    clip_fraction        | 0.702      |
|    clip_range           | 0.075      |
|    entropy_loss         | -2.9       |
|    explained_variance   | 0.941      |
|    learning_rate        | 0.00358    |
|    loss                 | 0.119      |
|    n_updates            | 441        |
|    policy_gradient_loss | 0.0356     |
|    std                  | 0.512      |
|    value_loss           | 0.12       |
----------------------------------------
Num timesteps: 4200000
Best mean reward: -0.37 - Last mean reward per episode: -2.56
Num timesteps: 4240000
Best mean reward: -0.37 - Last mean reward per episode: -2.37
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.2        |
|    ep_rew_mean          | -2.59       |
| time/                   |             |
|    fps                  | 509         |
|    iterations           | 65          |
|    time_elapsed         | 8360        |
|    total_timesteps      | 4259840     |
| train/                  |             |
|    approx_kl            | 0.055093586 |
|    clip_fraction        | 0.72        |
|    clip_range           | 0.075       |
|    entropy_loss         | -2.9        |
|    explained_variance   | 0.943       |
|    learning_rate        | 0.00358     |
|    loss                 | 0.116       |
|    n_updates            | 448         |
|    policy_gradient_loss | 0.0354      |
|    std                  | 0.513       |
|    value_loss           | 0.127       |
-----------------------------------------
Num timesteps: 4280000
Best mean reward: -0.37 - Last mean reward per episode: -2.27
Num timesteps: 4320000
Best mean reward: -0.37 - Last mean reward per episode: -1.56
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.4       |
|    ep_rew_mean          | -2.77      |
| time/                   |            |
|    fps                  | 508        |
|    iterations           | 66         |
|    time_elapsed         | 8502       |
|    total_timesteps      | 4325376    |
| train/                  |            |
|    approx_kl            | 0.07064685 |
|    clip_fraction        | 0.735      |
|    clip_range           | 0.075      |
|    entropy_loss         | -2.91      |
|    explained_variance   | 0.939      |
|    learning_rate        | 0.00358    |
|    loss                 | 0.112      |
|    n_updates            | 455        |
|    policy_gradient_loss | 0.0404     |
|    std                  | 0.514      |
|    value_loss           | 0.115      |
----------------------------------------
Num timesteps: 4360000
Best mean reward: -0.37 - Last mean reward per episode: -1.46
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 15.3       |
|    ep_rew_mean          | -2.05      |
| time/                   |            |
|    fps                  | 505        |
|    iterations           | 67         |
|    time_elapsed         | 8687       |
|    total_timesteps      | 4390912    |
| train/                  |            |
|    approx_kl            | 0.05385945 |
|    clip_fraction        | 0.705      |
|    clip_range           | 0.075      |
|    entropy_loss         | -2.92      |
|    explained_variance   | 0.939      |
|    learning_rate        | 0.00358    |
|    loss                 | 0.104      |
|    n_updates            | 462        |
|    policy_gradient_loss | 0.0331     |
|    std                  | 0.514      |
|    value_loss           | 0.117      |
----------------------------------------
Num timesteps: 4400000
Best mean reward: -0.37 - Last mean reward per episode: -1.60
Num timesteps: 4440000
Best mean reward: -0.37 - Last mean reward per episode: -1.31
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 14          |
|    ep_rew_mean          | -1.65       |
| time/                   |             |
|    fps                  | 503         |
|    iterations           | 68          |
|    time_elapsed         | 8855        |
|    total_timesteps      | 4456448     |
| train/                  |             |
|    approx_kl            | 0.046839993 |
|    clip_fraction        | 0.672       |
|    clip_range           | 0.075       |
|    entropy_loss         | -2.91       |
|    explained_variance   | 0.939       |
|    learning_rate        | 0.00358     |
|    loss                 | 0.101       |
|    n_updates            | 469         |
|    policy_gradient_loss | 0.0272      |
|    std                  | 0.514       |
|    value_loss           | 0.101       |
-----------------------------------------
Num timesteps: 4480000
Best mean reward: -0.37 - Last mean reward per episode: -1.76
Num timesteps: 4520000
Best mean reward: -0.37 - Last mean reward per episode: -1.19
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 11.7       |
|    ep_rew_mean          | -1.22      |
| time/                   |            |
|    fps                  | 502        |
|    iterations           | 69         |
|    time_elapsed         | 8993       |
|    total_timesteps      | 4521984    |
| train/                  |            |
|    approx_kl            | 0.07676355 |
|    clip_fraction        | 0.726      |
|    clip_range           | 0.075      |
|    entropy_loss         | -2.91      |
|    explained_variance   | 0.93       |
|    learning_rate        | 0.00358    |
|    loss                 | 0.11       |
|    n_updates            | 476        |
|    policy_gradient_loss | 0.0379     |
|    std                  | 0.513      |
|    value_loss           | 0.0897     |
----------------------------------------
Num timesteps: 4560000
Best mean reward: -0.37 - Last mean reward per episode: -1.44
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 10.4       |
|    ep_rew_mean          | -1.1       |
| time/                   |            |
|    fps                  | 502        |
|    iterations           | 70         |
|    time_elapsed         | 9132       |
|    total_timesteps      | 4587520    |
| train/                  |            |
|    approx_kl            | 0.05120024 |
|    clip_fraction        | 0.692      |
|    clip_range           | 0.075      |
|    entropy_loss         | -2.9       |
|    explained_variance   | 0.941      |
|    learning_rate        | 0.00358    |
|    loss                 | 0.0944     |
|    n_updates            | 483        |
|    policy_gradient_loss | 0.0286     |
|    std                  | 0.512      |
|    value_loss           | 0.0874     |
----------------------------------------
Num timesteps: 4600000
Best mean reward: -0.37 - Last mean reward per episode: -1.09
Num timesteps: 4640000
Best mean reward: -0.37 - Last mean reward per episode: -0.62
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 11.6        |
|    ep_rew_mean          | -1.28       |
| time/                   |             |
|    fps                  | 502         |
|    iterations           | 71          |
|    time_elapsed         | 9265        |
|    total_timesteps      | 4653056     |
| train/                  |             |
|    approx_kl            | 0.045258738 |
|    clip_fraction        | 0.729       |
|    clip_range           | 0.075       |
|    entropy_loss         | -2.88       |
|    explained_variance   | 0.936       |
|    learning_rate        | 0.00358     |
|    loss                 | 0.075       |
|    n_updates            | 490         |
|    policy_gradient_loss | 0.034       |
|    std                  | 0.511       |
|    value_loss           | 0.0809      |
-----------------------------------------
Num timesteps: 4680000
Best mean reward: -0.37 - Last mean reward per episode: -0.89
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 7.85        |
|    ep_rew_mean          | -0.926      |
| time/                   |             |
|    fps                  | 502         |
|    iterations           | 72          |
|    time_elapsed         | 9388        |
|    total_timesteps      | 4718592     |
| train/                  |             |
|    approx_kl            | 0.054530546 |
|    clip_fraction        | 0.718       |
|    clip_range           | 0.075       |
|    entropy_loss         | -2.87       |
|    explained_variance   | 0.937       |
|    learning_rate        | 0.00358     |
|    loss                 | 0.056       |
|    n_updates            | 497         |
|    policy_gradient_loss | 0.0292      |
|    std                  | 0.509       |
|    value_loss           | 0.0619      |
-----------------------------------------
Num timesteps: 4720000
Best mean reward: -0.37 - Last mean reward per episode: -0.85
Num timesteps: 4760000
Best mean reward: -0.37 - Last mean reward per episode: -0.76
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 9.26        |
|    ep_rew_mean          | -0.866      |
| time/                   |             |
|    fps                  | 502         |
|    iterations           | 73          |
|    time_elapsed         | 9521        |
|    total_timesteps      | 4784128     |
| train/                  |             |
|    approx_kl            | 0.087150976 |
|    clip_fraction        | 0.758       |
|    clip_range           | 0.075       |
|    entropy_loss         | -2.85       |
|    explained_variance   | 0.939       |
|    learning_rate        | 0.00358     |
|    loss                 | 0.0744      |
|    n_updates            | 504         |
|    policy_gradient_loss | 0.0345      |
|    std                  | 0.507       |
|    value_loss           | 0.0551      |
-----------------------------------------
Num timesteps: 4800000
Best mean reward: -0.37 - Last mean reward per episode: -0.70
Num timesteps: 4840000
Best mean reward: -0.37 - Last mean reward per episode: -0.64
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 7.55       |
|    ep_rew_mean          | -0.781     |
| time/                   |            |
|    fps                  | 502        |
|    iterations           | 74         |
|    time_elapsed         | 9649       |
|    total_timesteps      | 4849664    |
| train/                  |            |
|    approx_kl            | 0.10083282 |
|    clip_fraction        | 0.753      |
|    clip_range           | 0.075      |
|    entropy_loss         | -2.82      |
|    explained_variance   | 0.939      |
|    learning_rate        | 0.00358    |
|    loss                 | 0.0656     |
|    n_updates            | 511        |
|    policy_gradient_loss | 0.0322     |
|    std                  | 0.504      |
|    value_loss           | 0.054      |
----------------------------------------
Num timesteps: 4880000
Best mean reward: -0.37 - Last mean reward per episode: -0.93
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 6.7        |
|    ep_rew_mean          | -0.648     |
| time/                   |            |
|    fps                  | 502        |
|    iterations           | 75         |
|    time_elapsed         | 9773       |
|    total_timesteps      | 4915200    |
| train/                  |            |
|    approx_kl            | 0.31175578 |
|    clip_fraction        | 0.785      |
|    clip_range           | 0.075      |
|    entropy_loss         | -2.8       |
|    explained_variance   | 0.93       |
|    learning_rate        | 0.00358    |
|    loss                 | 0.122      |
|    n_updates            | 518        |
|    policy_gradient_loss | 0.0519     |
|    std                  | 0.503      |
|    value_loss           | 0.0491     |
----------------------------------------
Num timesteps: 4920000
Best mean reward: -0.37 - Last mean reward per episode: -0.64
Num timesteps: 4960000
Best mean reward: -0.37 - Last mean reward per episode: -0.70
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 7.43       |
|    ep_rew_mean          | -0.938     |
| time/                   |            |
|    fps                  | 503        |
|    iterations           | 76         |
|    time_elapsed         | 9894       |
|    total_timesteps      | 4980736    |
| train/                  |            |
|    approx_kl            | 0.06200371 |
|    clip_fraction        | 0.75       |
|    clip_range           | 0.075      |
|    entropy_loss         | -2.8       |
|    explained_variance   | 0.945      |
|    learning_rate        | 0.00358    |
|    loss                 | 0.0551     |
|    n_updates            | 525        |
|    policy_gradient_loss | 0.035      |
|    std                  | 0.502      |
|    value_loss           | 0.0423     |
----------------------------------------
Num timesteps: 5000000
Best mean reward: -0.37 - Last mean reward per episode: -0.68
Num timesteps: 5040000
Best mean reward: -0.37 - Last mean reward per episode: -0.64
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 6.06       |
|    ep_rew_mean          | -0.763     |
| time/                   |            |
|    fps                  | 500        |
|    iterations           | 77         |
|    time_elapsed         | 10072      |
|    total_timesteps      | 5046272    |
| train/                  |            |
|    approx_kl            | 0.10082111 |
|    clip_fraction        | 0.787      |
|    clip_range           | 0.075      |
|    entropy_loss         | -2.78      |
|    explained_variance   | 0.945      |
|    learning_rate        | 0.00358    |
|    loss                 | 0.0472     |
|    n_updates            | 532        |
|    policy_gradient_loss | 0.0352     |
|    std                  | 0.5        |
|    value_loss           | 0.0346     |
----------------------------------------
Num timesteps: 5080000
Best mean reward: -0.37 - Last mean reward per episode: -0.48
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4.92       |
|    ep_rew_mean          | -0.497     |
| time/                   |            |
|    fps                  | 501        |
|    iterations           | 78         |
|    time_elapsed         | 10195      |
|    total_timesteps      | 5111808    |
| train/                  |            |
|    approx_kl            | 0.36453348 |
|    clip_fraction        | 0.805      |
|    clip_range           | 0.075      |
|    entropy_loss         | -2.75      |
|    explained_variance   | 0.945      |
|    learning_rate        | 0.00358    |
|    loss                 | 0.0485     |
|    n_updates            | 539        |
|    policy_gradient_loss | 0.0354     |
|    std                  | 0.498      |
|    value_loss           | 0.0267     |
----------------------------------------
Num timesteps: 5120000
Best mean reward: -0.37 - Last mean reward per episode: -0.52
Num timesteps: 5160000
Best mean reward: -0.37 - Last mean reward per episode: -0.50
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 5.43       |
|    ep_rew_mean          | -0.504     |
| time/                   |            |
|    fps                  | 501        |
|    iterations           | 79         |
|    time_elapsed         | 10325      |
|    total_timesteps      | 5177344    |
| train/                  |            |
|    approx_kl            | 0.09876928 |
|    clip_fraction        | 0.792      |
|    clip_range           | 0.075      |
|    entropy_loss         | -2.72      |
|    explained_variance   | 0.946      |
|    learning_rate        | 0.00358    |
|    loss                 | 0.0423     |
|    n_updates            | 546        |
|    policy_gradient_loss | 0.0328     |
|    std                  | 0.494      |
|    value_loss           | 0.0211     |
----------------------------------------
Num timesteps: 5200000
Best mean reward: -0.37 - Last mean reward per episode: -0.69
Num timesteps: 5240000
Best mean reward: -0.37 - Last mean reward per episode: -0.47
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 5.32       |
|    ep_rew_mean          | -0.518     |
| time/                   |            |
|    fps                  | 502        |
|    iterations           | 80         |
|    time_elapsed         | 10441      |
|    total_timesteps      | 5242880    |
| train/                  |            |
|    approx_kl            | 0.30794257 |
|    clip_fraction        | 0.86       |
|    clip_range           | 0.075      |
|    entropy_loss         | -2.7       |
|    explained_variance   | 0.952      |
|    learning_rate        | 0.00358    |
|    loss                 | 0.0553     |
|    n_updates            | 553        |
|    policy_gradient_loss | 0.0636     |
|    std                  | 0.493      |
|    value_loss           | 0.019      |
----------------------------------------
Num timesteps: 5280000
Best mean reward: -0.37 - Last mean reward per episode: -0.51
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 6.01       |
|    ep_rew_mean          | -0.813     |
| time/                   |            |
|    fps                  | 502        |
|    iterations           | 81         |
|    time_elapsed         | 10568      |
|    total_timesteps      | 5308416    |
| train/                  |            |
|    approx_kl            | 0.38724893 |
|    clip_fraction        | 0.807      |
|    clip_range           | 0.075      |
|    entropy_loss         | -2.65      |
|    explained_variance   | 0.953      |
|    learning_rate        | 0.00358    |
|    loss                 | 0.0418     |
|    n_updates            | 560        |
|    policy_gradient_loss | 0.033      |
|    std                  | 0.488      |
|    value_loss           | 0.0156     |
----------------------------------------
Num timesteps: 5320000
Best mean reward: -0.37 - Last mean reward per episode: -0.45
Num timesteps: 5360000
Best mean reward: -0.37 - Last mean reward per episode: -0.44
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.58      |
|    ep_rew_mean          | -0.43     |
| time/                   |           |
|    fps                  | 502       |
|    iterations           | 82        |
|    time_elapsed         | 10702     |
|    total_timesteps      | 5373952   |
| train/                  |           |
|    approx_kl            | 1.6819766 |
|    clip_fraction        | 0.874     |
|    clip_range           | 0.075     |
|    entropy_loss         | -2.61     |
|    explained_variance   | 0.949     |
|    learning_rate        | 0.00358   |
|    loss                 | 0.0538    |
|    n_updates            | 567       |
|    policy_gradient_loss | 0.0589    |
|    std                  | 0.487     |
|    value_loss           | 0.0134    |
---------------------------------------
Num timesteps: 5400000
Best mean reward: -0.37 - Last mean reward per episode: -0.42
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4.42       |
|    ep_rew_mean          | -0.416     |
| time/                   |            |
|    fps                  | 502        |
|    iterations           | 83         |
|    time_elapsed         | 10818      |
|    total_timesteps      | 5439488    |
| train/                  |            |
|    approx_kl            | 0.15491065 |
|    clip_fraction        | 0.816      |
|    clip_range           | 0.075      |
|    entropy_loss         | -2.6       |
|    explained_variance   | 0.957      |
|    learning_rate        | 0.00358    |
|    loss                 | 0.0447     |
|    n_updates            | 574        |
|    policy_gradient_loss | 0.0401     |
|    std                  | 0.485      |
|    value_loss           | 0.0138     |
----------------------------------------
Num timesteps: 5440000
Best mean reward: -0.37 - Last mean reward per episode: -0.42
Num timesteps: 5480000
Best mean reward: -0.37 - Last mean reward per episode: -0.45
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4.3        |
|    ep_rew_mean          | -0.408     |
| time/                   |            |
|    fps                  | 503        |
|    iterations           | 84         |
|    time_elapsed         | 10935      |
|    total_timesteps      | 5505024    |
| train/                  |            |
|    approx_kl            | 0.87761706 |
|    clip_fraction        | 0.864      |
|    clip_range           | 0.075      |
|    entropy_loss         | -2.57      |
|    explained_variance   | 0.954      |
|    learning_rate        | 0.00358    |
|    loss                 | 0.0641     |
|    n_updates            | 581        |
|    policy_gradient_loss | 0.0517     |
|    std                  | 0.486      |
|    value_loss           | 0.0144     |
----------------------------------------
Num timesteps: 5520000
Best mean reward: -0.37 - Last mean reward per episode: -0.42
Num timesteps: 5560000
Best mean reward: -0.37 - Last mean reward per episode: -0.57
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4.32       |
|    ep_rew_mean          | -0.409     |
| time/                   |            |
|    fps                  | 504        |
|    iterations           | 85         |
|    time_elapsed         | 11051      |
|    total_timesteps      | 5570560    |
| train/                  |            |
|    approx_kl            | 0.35140923 |
|    clip_fraction        | 0.854      |
|    clip_range           | 0.075      |
|    entropy_loss         | -2.55      |
|    explained_variance   | 0.96       |
|    learning_rate        | 0.00358    |
|    loss                 | 0.0354     |
|    n_updates            | 588        |
|    policy_gradient_loss | 0.0442     |
|    std                  | 0.485      |
|    value_loss           | 0.0131     |
----------------------------------------
Num timesteps: 5600000
Best mean reward: -0.37 - Last mean reward per episode: -0.39
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4.32       |
|    ep_rew_mean          | -0.403     |
| time/                   |            |
|    fps                  | 504        |
|    iterations           | 86         |
|    time_elapsed         | 11165      |
|    total_timesteps      | 5636096    |
| train/                  |            |
|    approx_kl            | 0.19800255 |
|    clip_fraction        | 0.834      |
|    clip_range           | 0.075      |
|    entropy_loss         | -2.49      |
|    explained_variance   | 0.965      |
|    learning_rate        | 0.00358    |
|    loss                 | 0.0269     |
|    n_updates            | 595        |
|    policy_gradient_loss | 0.0346     |
|    std                  | 0.478      |
|    value_loss           | 0.0068     |
----------------------------------------
Num timesteps: 5640000
Best mean reward: -0.37 - Last mean reward per episode: -0.40
Num timesteps: 5680000
Best mean reward: -0.37 - Last mean reward per episode: -0.39
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.11      |
|    ep_rew_mean          | -0.382    |
| time/                   |           |
|    fps                  | 505       |
|    iterations           | 87        |
|    time_elapsed         | 11285     |
|    total_timesteps      | 5701632   |
| train/                  |           |
|    approx_kl            | 0.8108511 |
|    clip_fraction        | 0.864     |
|    clip_range           | 0.075     |
|    entropy_loss         | -2.44     |
|    explained_variance   | 0.96      |
|    learning_rate        | 0.00358   |
|    loss                 | 0.0366    |
|    n_updates            | 602       |
|    policy_gradient_loss | 0.037     |
|    std                  | 0.468     |
|    value_loss           | 0.008     |
---------------------------------------
Num timesteps: 5720000
Best mean reward: -0.37 - Last mean reward per episode: -0.39
Num timesteps: 5760000
Best mean reward: -0.37 - Last mean reward per episode: -0.38
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.15      |
|    ep_rew_mean          | -0.384    |
| time/                   |           |
|    fps                  | 503       |
|    iterations           | 88        |
|    time_elapsed         | 11458     |
|    total_timesteps      | 5767168   |
| train/                  |           |
|    approx_kl            | 0.4351254 |
|    clip_fraction        | 0.841     |
|    clip_range           | 0.075     |
|    entropy_loss         | -2.38     |
|    explained_variance   | 0.95      |
|    learning_rate        | 0.00358   |
|    loss                 | 0.0317    |
|    n_updates            | 609       |
|    policy_gradient_loss | 0.0355    |
|    std                  | 0.462     |
|    value_loss           | 0.00595   |
---------------------------------------
Num timesteps: 5800000
Best mean reward: -0.37 - Last mean reward per episode: -0.38
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.15      |
|    ep_rew_mean          | -0.376    |
| time/                   |           |
|    fps                  | 503       |
|    iterations           | 89        |
|    time_elapsed         | 11573     |
|    total_timesteps      | 5832704   |
| train/                  |           |
|    approx_kl            | 1.2200875 |
|    clip_fraction        | 0.898     |
|    clip_range           | 0.075     |
|    entropy_loss         | -2.27     |
|    explained_variance   | 0.973     |
|    learning_rate        | 0.00358   |
|    loss                 | 0.0616    |
|    n_updates            | 616       |
|    policy_gradient_loss | 0.144     |
|    std                  | 0.453     |
|    value_loss           | 0.00734   |
---------------------------------------
Num timesteps: 5840000
Best mean reward: -0.37 - Last mean reward per episode: -0.55
Num timesteps: 5880000
Best mean reward: -0.37 - Last mean reward per episode: -0.74
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 5.48     |
|    ep_rew_mean          | -0.605   |
| time/                   |          |
|    fps                  | 504      |
|    iterations           | 90       |
|    time_elapsed         | 11701    |
|    total_timesteps      | 5898240  |
| train/                  |          |
|    approx_kl            | 5.260324 |
|    clip_fraction        | 0.921    |
|    clip_range           | 0.075    |
|    entropy_loss         | -2.32    |
|    explained_variance   | 0.965    |
|    learning_rate        | 0.00358  |
|    loss                 | 0.0583   |
|    n_updates            | 623      |
|    policy_gradient_loss | 0.0651   |
|    std                  | 0.462    |
|    value_loss           | 0.0101   |
--------------------------------------
Num timesteps: 5920000
Best mean reward: -0.37 - Last mean reward per episode: -0.38
Num timesteps: 5960000
Best mean reward: -0.37 - Last mean reward per episode: -0.42
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 4.25      |
|    ep_rew_mean          | -0.387    |
| time/                   |           |
|    fps                  | 504       |
|    iterations           | 91        |
|    time_elapsed         | 11815     |
|    total_timesteps      | 5963776   |
| train/                  |           |
|    approx_kl            | 3.9921665 |
|    clip_fraction        | 0.915     |
|    clip_range           | 0.075     |
|    entropy_loss         | -2.38     |
|    explained_variance   | 0.962     |
|    learning_rate        | 0.00358   |
|    loss                 | 0.0897    |
|    n_updates            | 630       |
|    policy_gradient_loss | 0.0532    |
|    std                  | 0.462     |
|    value_loss           | 0.0576    |
---------------------------------------
Num timesteps: 6000000
Best mean reward: -0.37 - Last mean reward per episode: -0.37
Saving new best model to 21April2022/optimizedVectStudy/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning:

positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.

----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 4.14       |
|    ep_rew_mean          | -0.39      |
| time/                   |            |
|    fps                  | 505        |
|    iterations           | 92         |
|    time_elapsed         | 11933      |
|    total_timesteps      | 6029312    |
| train/                  |            |
|    approx_kl            | 0.16934341 |
|    clip_fraction        | 0.741      |
|    clip_range           | 0.075      |
|    entropy_loss         | -2.37      |
|    explained_variance   | 0.959      |
|    learning_rate        | 0.00358    |
|    loss                 | 0.0367     |
|    n_updates            | 637        |
|    policy_gradient_loss | 0.0402     |
|    std                  | 0.463      |
|    value_loss           | 0.01       |
----------------------------------------

 
 
 

-----------------------------------------------------------------------------------

 
 
 

RECORDING OPTIMIZED STUDY
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/scripts/stable_baselines/pandaAndrew/pybulletCust.py:94: UserWarning:

The use of the render method is not recommended when the environment has not been created with render=True. The rendering will probably be weird. Prefer making the environment with option `render=True`. For example: `env = gym.make('PandaReach-v2', render=True)`.

/home/hjkwon/scripts/stable_baselines/pandaAndrew/pybulletCust.py:94: UserWarning:

The use of the render method is not recommended when the environment has not been created with render=True. The rendering will probably be weird. Prefer making the environment with option `render=True`. For example: `env = gym.make('PandaReach-v2', render=True)`.

Saving video to /home/hjkwon/scripts/stable_baselines/pandaAndrew/21April2022/videos/ppo-PandaReach-Vect-step-0-to-step-400.mp4
Exception ignored in: <function VecVideoRecorder.__del__ at 0x7f7cb6ce8040>
Traceback (most recent call last):
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/stable_baselines3/common/vec_env/vec_video_recorder.py", line 113, in __del__
    self.close()
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/stable_baselines3/common/vec_env/vec_video_recorder.py", line 109, in close
    VecEnvWrapper.close(self)
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/stable_baselines3/common/vec_env/base_vec_env.py", line 278, in close
    return self.venv.close()
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 67, in close
    env.close()
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/gym/core.py", line 257, in close
    return self.env.close()
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/gym/core.py", line 257, in close
    return self.env.close()
  File "/home/hjkwon/scripts/stable_baselines/pandaAndrew/robottaskenv.py", line 73, in close
    self.sim.close()
  File "/home/hjkwon/scripts/stable_baselines/pandaAndrew/pybulletCust.py", line 59, in close
    self.physics_client.disconnect()
pybullet.error: Not connected to physics server.
Exception ignored in: <function BulletClient.__del__ at 0x7f7cb7e1ae50>
Traceback (most recent call last):
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/pybullet_utils/bullet_client.py", line 39, in __del__
AttributeError: 'NoneType' object has no attribute 'getpid'
Exception ignored in: <function BulletClient.__del__ at 0x7f7cb7e1ae50>
Traceback (most recent call last):
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/pybullet_utils/bullet_client.py", line 39, in __del__
AttributeError: 'NoneType' object has no attribute 'getpid'
Exception ignored in: <function BulletClient.__del__ at 0x7f7cb7e1ae50>
Traceback (most recent call last):
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/pybullet_utils/bullet_client.py", line 39, in __del__
AttributeError: 'NoneType' object has no attribute 'getpid'
Exception ignored in: <function BulletClient.__del__ at 0x7f7cb7e1ae50>
Traceback (most recent call last):
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/pybullet_utils/bullet_client.py", line 39, in __del__
AttributeError: 'NoneType' object has no attribute 'getpid'
(rl_env) ]0;hjkwon@HJK-AI-Robotics: ~/scripts/stable_baselines/pandaAndrew[01;32mhjkwon@HJK-AI-Robotics[00m:[01;34m~/scripts/stable_baselines/pandaAndrew[00m$ [K(rl_env) ]0;hjkwon@HJK-AI-Robotics: ~/scripts/stable_baselines/pandaAndrew[01;32mhjkwon@HJK-AI-Robotics[00m:[01;34m~/scripts/stable_baselines/pandaAndrew[00m$ exit
exit

Script done on 2022-04-22 15:00:41-04:00 [COMMAND_EXIT_CODE="0"]
