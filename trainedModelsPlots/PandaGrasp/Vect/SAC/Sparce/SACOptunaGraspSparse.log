Script started on 2022-07-01 11:28:09-04:00 [TERM="xterm-256color" TTY="/dev/pts/5" COLUMNS="203" LINES="53"]
bash: devel/setup.bash: No such file or directory
bash: /home/hjkwon/catkin_ws/src/moveit/devel/setup.bash: No such file or directory
]0;hjkwon@HJK-AI-Robotics: ~/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-01[01;32mhjkwon@HJK-AI-Robotics[00m:[01;34m~/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-01[00m$ conda activate rl_env
(rl_env) ]0;hjkwon@HJK-AI-Robotics: ~/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-01[01;32mhjkwon@HJK-AI-Robotics[00m:[01;34m~/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-01[00m$ cd ..
(rl_env) ]0;hjkwon@HJK-AI-Robotics: ~/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining[01;32mhjkwon@HJK-AI-Robotics[00m:[01;34m~/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining[00m$ ls
[0m[01;34m2022-04-21[0m  [01;34m2022-04-24[0m  [01;34m2022-06-25[0m  [01;34m2022-06-27[0m  [01;34m2022-06-30[0m  cpuMonitor.log  mainHERVect.py   mainPPOVect.py  mainSACVectTuner.py        mainTD3VectTuner.py  setDirectories.py
[01;34m2022-04-22[0m  [01;34m2022-04-27[0m  [01;34m2022-06-26[0m  [01;34m2022-06-28[0m  [01;34m2022-07-01[0m  gpuMonitor.log  mainPPOVect2.py  mainSACVec.py   mainSACVectTunerSparse.py  [01;34m__pycache__[0m
(rl_env) ]0;hjkwon@HJK-AI-Robotics: ~/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining[01;32mhjkwon@HJK-AI-Robotics[00m:[01;34m~/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining[00m$ python3 mainSACVectTunerSparse.py [K
_XSERVTransSocketUNIXCreateListener: ...SocketCreateListener() failed
_XSERVTransMakeAllCOTSServerListeners: server already running
(EE) 
Fatal server error:
(EE) Cannot establish any listening sockets - Make sure an X server isn't already running(EE) 
pybullet build time: Dec  1 2021 18:34:28


Training SAC Vect Tuner for PandaGraspDepth-v1


[32m[I 2022-07-01 11:28:27,866][0m A new study created in memory with name: no-name-cb568cab-d9a8-4534-8d37-b8e4b4ee0f46[0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.0014379773406417821, 'gamma': 0.987946148027641, 'tau': 0.001155941215380936, 'learning_rate': 0.00042173562680365256, 'batch_size': 256, 'learning_starts': 9000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  3
action_noise:  0.0

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[33m[W 2022-07-01 11:28:28,404][0m Trial 0 failed because of the following error: ValueError('could not broadcast input array from shape (7,) into shape (4,)')
Traceback (most recent call last):
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/optuna/study/_optimize.py", line 213, in _run_trial
    value_or_values = func(trial)
  File "/home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/../SB3/SAC/exploreOptunaSACVect.py", line 147, in objective
    model.learn(total_timesteps = self.n_timesteps, callback=callback)
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/stable_baselines3/sac/sac.py", line 292, in learn
    return super(SAC, self).learn(
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/stable_baselines3/common/off_policy_algorithm.py", line 340, in learn
    total_timesteps, callback = self._setup_learn(
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/stable_baselines3/common/off_policy_algorithm.py", line 316, in _setup_learn
    return super()._setup_learn(
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/stable_baselines3/common/base_class.py", line 429, in _setup_learn
    self._last_obs = self.env.reset()  # pytype: disable=annotation-type-mismatch
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 62, in reset
    self._save_obs(env_idx, obs)
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 92, in _save_obs
    self.buf_obs[key][env_idx] = obs
ValueError: could not broadcast input array from shape (7,) into shape (4,)[0m
Traceback (most recent call last):
  File "/home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/mainSACVectTunerSparse.py", line 101, in <module>
    study.optimize(objective, n_trials=N_TRIALS, gc_after_trial=True) # gc = garbage collection, saves memory
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/optuna/study/study.py", line 400, in optimize
    _optimize(
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/optuna/study/_optimize.py", line 66, in _optimize
    _optimize_sequential(
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/optuna/study/_optimize.py", line 163, in _optimize_sequential
    trial = _run_trial(study, func, catch)
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/optuna/study/_optimize.py", line 264, in _run_trial
    raise func_err
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/optuna/study/_optimize.py", line 213, in _run_trial
    value_or_values = func(trial)
  File "/home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/../SB3/SAC/exploreOptunaSACVect.py", line 147, in objective
    model.learn(total_timesteps = self.n_timesteps, callback=callback)
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/stable_baselines3/sac/sac.py", line 292, in learn
    return super(SAC, self).learn(
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/stable_baselines3/common/off_policy_algorithm.py", line 340, in learn
    total_timesteps, callback = self._setup_learn(
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/stable_baselines3/common/off_policy_algorithm.py", line 316, in _setup_learn
    return super()._setup_learn(
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/stable_baselines3/common/base_class.py", line 429, in _setup_learn
    self._last_obs = self.env.reset()  # pytype: disable=annotation-type-mismatch
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 62, in reset
    self._save_obs(env_idx, obs)
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 92, in _save_obs
    self.buf_obs[key][env_idx] = obs
ValueError: could not broadcast input array from shape (7,) into shape (4,)
(rl_env) ]0;hjkwon@HJK-AI-Robotics: ~/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining[01;32mhjkwon@HJK-AI-Robotics[00m:[01;34m~/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining[00m$ python3 mainSACVectTunerSparse.py
pybullet build time: Dec  1 2021 18:34:28
_XSERVTransSocketUNIXCreateListener: ...SocketCreateListener() failed
_XSERVTransMakeAllCOTSServerListeners: server already running
(EE) 
Fatal server error:
(EE) Cannot establish any listening sockets - Make sure an X server isn't already running(EE) 


Training SAC Vect Tuner for PandaGraspDepth-v1


[32m[I 2022-07-01 11:29:08,307][0m A new study created in memory with name: no-name-12753ea0-0053-455d-a982-9c629d0921ee[0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.001940515979816954, 'gamma': 0.9899851808390701, 'tau': 0.004665627236891077, 'learning_rate': 0.00014345052424590435, 'batch_size': 256, 'learning_starts': 10000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  2
action_noise:  0.10000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=2000, episode_reward=-49.80 +/- 0.60
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=4000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=6000, episode_reward=-49.60 +/- 1.20
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=8000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=10000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=14000, episode_reward=-48.70 +/- 3.03
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=16000, episode_reward=-42.10 +/- 12.68
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=18000, episode_reward=-47.90 +/- 4.66
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=20000, episode_reward=-46.60 +/- 7.68
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=22000, episode_reward=-43.90 +/- 12.78
Episode length: 46.90 +/- 9.30
Success rate: 10.00%
Eval num_timesteps=24000, episode_reward=-49.10 +/- 2.70
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=26000, episode_reward=-42.10 +/- 15.93
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=28000, episode_reward=-33.10 +/- 17.92
Episode length: 43.80 +/- 12.41
Success rate: 20.00%
New best mean reward!
Eval num_timesteps=30000, episode_reward=-42.80 +/- 14.74
Episode length: 43.00 +/- 14.35
Success rate: 20.00%
Eval num_timesteps=32000, episode_reward=-29.70 +/- 20.57
Episode length: 30.40 +/- 19.91
Success rate: 50.00%
New best mean reward!
Eval num_timesteps=34000, episode_reward=-32.60 +/- 18.90
Episode length: 34.40 +/- 19.12
Success rate: 40.00%
Eval num_timesteps=36000, episode_reward=-32.90 +/- 20.74
Episode length: 33.40 +/- 20.33
Success rate: 40.00%
Eval num_timesteps=38000, episode_reward=-43.00 +/- 13.95
Episode length: 43.30 +/- 13.60
Success rate: 30.00%
Eval num_timesteps=40000, episode_reward=-28.30 +/- 21.82
Episode length: 28.80 +/- 21.33
Success rate: 50.00%
New best mean reward!
Eval num_timesteps=42000, episode_reward=-14.20 +/- 17.96
Episode length: 15.00 +/- 17.56
Success rate: 80.00%
New best mean reward!
Eval num_timesteps=44000, episode_reward=-10.30 +/- 14.37
Episode length: 11.20 +/- 14.09
Success rate: 90.00%
New best mean reward!
Eval num_timesteps=46000, episode_reward=-3.40 +/- 0.66
Episode length: 4.40 +/- 0.66
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=48000, episode_reward=-14.40 +/- 17.91
Episode length: 15.20 +/- 17.51
Success rate: 80.00%
Eval num_timesteps=50000, episode_reward=-4.40 +/- 1.28
Episode length: 5.40 +/- 1.28
Success rate: 100.00%
Eval num_timesteps=52000, episode_reward=-3.50 +/- 0.67
Episode length: 4.70 +/- 0.78
Success rate: 100.00%
Eval num_timesteps=54000, episode_reward=-3.90 +/- 0.94
Episode length: 5.00 +/- 1.00
Success rate: 100.00%
Eval num_timesteps=56000, episode_reward=-3.50 +/- 0.92
Episode length: 4.50 +/- 0.92
Success rate: 100.00%
Eval num_timesteps=58000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=60000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
Eval num_timesteps=62000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
Eval num_timesteps=64000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
Eval num_timesteps=66000, episode_reward=-3.10 +/- 0.30
Episode length: 4.10 +/- 0.30
Success rate: 100.00%
^C[33m[W 2022-07-01 11:50:41,694][0m Your study does not have any completed trials.[0m
[33m[W 2022-07-01 11:50:41,705][0m Study instance does not contain trials.[0m
[33m[W 2022-07-01 11:50:41,707][0m Study instance does not contain completed trials.[0m

 
Number of finished trials:  1
Best trial:
Traceback (most recent call last):
  File "/home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/mainSACVectTunerSparse.py", line 110, in <module>
    trial = study.best_trial
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/optuna/study/study.py", line 97, in best_trial
    return copy.deepcopy(self._storage.get_best_trial(self._study_id))
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/optuna/storages/_in_memory.py", line 311, in get_best_trial
    raise ValueError("No trials are completed yet.")
ValueError: No trials are completed yet.
(rl_env) ]0;hjkwon@HJK-AI-Robotics: ~/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining[01;32mhjkwon@HJK-AI-Robotics[00m:[01;34m~/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining[00m$ python3 mainSACVectTunerSparse.py
pybullet build time: Dec  1 2021 18:34:28


Training SAC Vect Tuner for PandaGraspDepth-v1


_XSERVTransSocketUNIXCreateListener: ...SocketCreateListener() failed
_XSERVTransMakeAllCOTSServerListeners: server already running
(EE) 
Fatal server error:
(EE) Cannot establish any listening sockets - Make sure an X server isn't already running(EE) 
[32m[I 2022-07-01 11:50:48,796][0m A new study created in memory with name: no-name-ef683d99-d2f0-4e8c-b27f-bac217d8c3a3[0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.007172235519164878, 'gamma': 0.9900867266757483, 'tau': 0.0011683255543226947, 'learning_rate': 0.0005259416549024535, 'batch_size': 8192, 'learning_starts': 8000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  3
action_noise:  0.05

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=3000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=6000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=9000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=15000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
8192,^C[33m[W 2022-07-01 12:08:40,626][0m Your study does not have any completed trials.[0m
[33m[W 2022-07-01 12:08:40,635][0m Study instance does not contain trials.[0m
[33m[W 2022-07-01 12:08:40,637][0m Study instance does not contain completed trials.[0m

 
Number of finished trials:  1
Best trial:
Traceback (most recent call last):
  File "/home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/mainSACVectTunerSparse.py", line 110, in <module>
    trial = study.best_trial
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/optuna/study/study.py", line 97, in best_trial
    return copy.deepcopy(self._storage.get_best_trial(self._study_id))
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/optuna/storages/_in_memory.py", line 311, in get_best_trial
    raise ValueError("No trials are completed yet.")
ValueError: No trials are completed yet.
(rl_env) ]0;hjkwon@HJK-AI-Robotics: ~/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining[01;32mhjkwon@HJK-AI-Robotics[00m:[01;34m~/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining[00m$ python3 mainSACVectTunerSparse.py
_XSERVTransSocketUNIXCreateListener: ...SocketCreateListener() failed
_XSERVTransMakeAllCOTSServerListeners: server already running
(EE) 
Fatal server error:
(EE) Cannot establish any listening sockets - Make sure an X server isn't already running(EE) 
pybullet build time: Dec  1 2021 18:34:28


Training SAC Vect Tuner for PandaGraspDepth-v1


[32m[I 2022-07-01 12:10:00,732][0m A new study created in memory with name: no-name-ebff2b02-9cc8-4dd1-ad52-2dc0f51ea9f2[0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.001065103006790584, 'gamma': 0.983660753286587, 'tau': 0.0006338048433670077, 'learning_rate': 0.0003775530491462776, 'batch_size': 512, 'learning_starts': 5000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  2
action_noise:  0.0

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=2000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=4000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=6000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=8000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=10000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=14000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=16000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=18000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=20000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=22000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=24000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=26000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=28000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=30000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=32000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=34000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=36000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=38000, episode_reward=-49.00 +/- 3.00
Episode length: 49.10 +/- 2.70
Success rate: 10.00%
New best mean reward!
Eval num_timesteps=40000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=42000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=44000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=46000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=48000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=50000, episode_reward=-46.80 +/- 9.60
Episode length: 46.90 +/- 9.30
Success rate: 10.00%
New best mean reward!
Eval num_timesteps=52000, episode_reward=-43.80 +/- 12.43
Episode length: 44.00 +/- 12.03
Success rate: 20.00%
New best mean reward!
Eval num_timesteps=54000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=56000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=58000, episode_reward=-49.40 +/- 1.80
Episode length: 49.50 +/- 1.50
Success rate: 10.00%
Eval num_timesteps=60000, episode_reward=-46.10 +/- 7.83
Episode length: 46.30 +/- 7.43
Success rate: 20.00%
TRIAL Finished: Objective Trial Time
[32m[I 2022-07-01 12:31:36,587][0m Trial 0 finished with value: -46.1 and parameters: {'gamma': 0.983660753286587, 'tau': 0.0006338048433670077, 'lr': 0.0003775530491462776, 'ent_coef': 0.001065103006790584, 'batch_size_num': 9, 'learning_starts': 5, 'action_noise_int': 1, 'n_envs': 2}. Best is trial 0 with value: -46.1.[0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.002224955997955879, 'gamma': 0.9901730149700864, 'tau': 0.0018907899026279987, 'learning_rate': 0.00012019194623101924, 'batch_size': 512, 'learning_starts': 5000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  4
action_noise:  0.0

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=4000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=8000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=16000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=20000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=24000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=28000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=32000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=36000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=40000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=44000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=48000, episode_reward=-47.90 +/- 6.30
Episode length: 48.00 +/- 6.00
Success rate: 10.00%
New best mean reward!
Eval num_timesteps=52000, episode_reward=-45.10 +/- 9.80
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=56000, episode_reward=-49.30 +/- 2.10
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=60000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
TRIAL Finished: Objective Trial Time
[32m[I 2022-07-01 12:44:33,523][0m Trial 1 finished with value: -50.0 and parameters: {'gamma': 0.9901730149700864, 'tau': 0.0018907899026279987, 'lr': 0.00012019194623101924, 'ent_coef': 0.002224955997955879, 'batch_size_num': 9, 'learning_starts': 5, 'action_noise_int': 1, 'n_envs': 4}. Best is trial 0 with value: -46.1.[0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.00348957638188999, 'gamma': 0.9752095022978555, 'tau': 0.000742732432352837, 'learning_rate': 0.000600229869329565, 'batch_size': 512, 'learning_starts': 10000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  3
action_noise:  0.0

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=3000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=6000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=9000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=15000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=18000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=21000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=24000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=27000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=30000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=33000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=36000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=39000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=42000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=45000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=48000, episode_reward=-37.60 +/- 18.99
Episode length: 37.90 +/- 18.53
Success rate: 30.00%
New best mean reward!
Eval num_timesteps=51000, episode_reward=-42.40 +/- 15.21
Episode length: 42.60 +/- 14.81
Success rate: 20.00%
Eval num_timesteps=54000, episode_reward=-45.50 +/- 13.50
Episode length: 45.60 +/- 13.20
Success rate: 10.00%
Eval num_timesteps=57000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=60000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
TRIAL Finished: Objective Trial Time
[32m[I 2022-07-01 12:59:02,178][0m Trial 2 finished with value: -50.0 and parameters: {'gamma': 0.9752095022978555, 'tau': 0.000742732432352837, 'lr': 0.000600229869329565, 'ent_coef': 0.00348957638188999, 'batch_size_num': 9, 'learning_starts': 10, 'action_noise_int': 1, 'n_envs': 3}. Best is trial 0 with value: -46.1.[0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.2180874531744751, 'gamma': 0.9900566257834797, 'tau': 0.002556244918689341, 'learning_rate': 0.0005424558728964922, 'batch_size': 512, 'learning_starts': 8000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  2
action_noise:  0.0

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=2000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=4000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=6000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=8000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=10000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=14000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=16000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=18000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=20000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=22000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=24000, episode_reward=-46.60 +/- 10.20
Episode length: 46.70 +/- 9.90
Success rate: 10.00%
New best mean reward!
Eval num_timesteps=26000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=28000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=30000, episode_reward=-46.40 +/- 10.80
Episode length: 46.50 +/- 10.50
Success rate: 10.00%
New best mean reward!
Eval num_timesteps=32000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=34000, episode_reward=-47.40 +/- 7.80
Episode length: 47.50 +/- 7.50
Success rate: 10.00%
Eval num_timesteps=36000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=38000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=40000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=42000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=44000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=46000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=48000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=50000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=52000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=54000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=56000, episode_reward=-47.10 +/- 8.70
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=58000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=60000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
TRIAL Finished: Objective Trial Time
[32m[I 2022-07-01 13:18:43,086][0m Trial 3 finished with value: -50.0 and parameters: {'gamma': 0.9900566257834797, 'tau': 0.002556244918689341, 'lr': 0.0005424558728964922, 'ent_coef': 0.2180874531744751, 'batch_size_num': 9, 'learning_starts': 8, 'action_noise_int': 1, 'n_envs': 2}. Best is trial 0 with value: -46.1.[0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.00017174153978215357, 'gamma': 0.989983493856102, 'tau': 0.0022432736634082177, 'learning_rate': 0.0005446489785329785, 'batch_size': 512, 'learning_starts': 5000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  3
action_noise:  0.0

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=3000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=6000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=9000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=15000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=18000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=21000, episode_reward=-48.80 +/- 3.60
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=24000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=27000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=30000, episode_reward=-49.10 +/- 2.70
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=33000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=36000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=39000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=42000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=45000, episode_reward=-47.40 +/- 7.47
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=48000, episode_reward=-47.00 +/- 9.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=51000, episode_reward=-47.40 +/- 7.80
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=54000, episode_reward=-41.30 +/- 12.64
Episode length: 46.60 +/- 10.20
Success rate: 10.00%
New best mean reward!
Eval num_timesteps=57000, episode_reward=-42.70 +/- 12.22
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=60000, episode_reward=-38.60 +/- 12.64
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
TRIAL Finished: Objective Trial Time
[32m[I 2022-07-01 13:33:07,588][0m Trial 4 finished with value: -38.6 and parameters: {'gamma': 0.989983493856102, 'tau': 0.0022432736634082177, 'lr': 0.0005446489785329785, 'ent_coef': 0.00017174153978215357, 'batch_size_num': 9, 'learning_starts': 5, 'action_noise_int': 1, 'n_envs': 3}. Best is trial 4 with value: -38.6.[0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.0004795544319200119, 'gamma': 0.9926363905580753, 'tau': 0.0010676523199045767, 'learning_rate': 0.00016094007782510548, 'batch_size': 1024, 'learning_starts': 5000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  2
action_noise:  0.0

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=2000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=4000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=6000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=8000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=10000, episode_reward=-48.90 +/- 2.30
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=12000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=14000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=16000, episode_reward=-48.90 +/- 3.30
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=18000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=20000, episode_reward=-46.20 +/- 7.36
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=22000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=24000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=26000, episode_reward=-49.10 +/- 2.70
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=28000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=30000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=32000, episode_reward=-49.00 +/- 2.05
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=34000, episode_reward=-49.20 +/- 2.40
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=36000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=38000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=40000, episode_reward=-48.90 +/- 2.98
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=42000, episode_reward=-46.70 +/- 9.90
Episode length: 46.80 +/- 9.60
Success rate: 10.00%
Eval num_timesteps=44000, episode_reward=-46.60 +/- 10.20
Episode length: 46.70 +/- 9.90
Success rate: 10.00%
Eval num_timesteps=46000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=48000, episode_reward=-47.60 +/- 7.20
Episode length: 47.70 +/- 6.90
Success rate: 10.00%
Eval num_timesteps=50000, episode_reward=-47.10 +/- 8.70
Episode length: 47.20 +/- 8.40
Success rate: 10.00%
Eval num_timesteps=52000, episode_reward=-41.80 +/- 12.59
Episode length: 42.10 +/- 12.14
Success rate: 30.00%
New best mean reward!
Eval num_timesteps=54000, episode_reward=-42.90 +/- 13.28
Episode length: 43.50 +/- 13.05
Success rate: 30.00%
Eval num_timesteps=56000, episode_reward=-42.10 +/- 12.64
Episode length: 42.40 +/- 12.20
Success rate: 30.00%
Eval num_timesteps=58000, episode_reward=-47.10 +/- 8.70
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=60000, episode_reward=-43.40 +/- 12.27
Episode length: 44.00 +/- 12.01
Success rate: 20.00%
TRIAL Finished: Objective Trial Time
[32m[I 2022-07-01 14:05:38,589][0m Trial 5 finished with value: -43.4 and parameters: {'gamma': 0.9926363905580753, 'tau': 0.0010676523199045767, 'lr': 0.00016094007782510548, 'ent_coef': 0.0004795544319200119, 'batch_size_num': 10, 'learning_starts': 5, 'action_noise_int': 1, 'n_envs': 2}. Best is trial 4 with value: -38.6.[0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.0001115115540719307, 'gamma': 0.9722939718675218, 'tau': 0.0018057711007900443, 'learning_rate': 0.0005003306648411568, 'batch_size': 512, 'learning_starts': 6000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  4
action_noise:  0.0

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=4000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=8000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=16000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=20000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=24000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=28000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=32000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=36000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=40000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=44000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=48000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=52000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=56000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=60000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
TRIAL Finished: Objective Trial Time
[32m[I 2022-07-01 14:18:28,475][0m Trial 6 finished with value: -50.0 and parameters: {'gamma': 0.9722939718675218, 'tau': 0.0018057711007900443, 'lr': 0.0005003306648411568, 'ent_coef': 0.0001115115540719307, 'batch_size_num': 9, 'learning_starts': 6, 'action_noise_int': 1, 'n_envs': 4}. Best is trial 4 with value: -38.6.[0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.003122907820880991, 'gamma': 0.9890577848160682, 'tau': 0.003752460019604293, 'learning_rate': 0.0002608959494035669, 'batch_size': 256, 'learning_starts': 10000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  3
action_noise:  0.10000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=3000, episode_reward=-48.80 +/- 3.60
Episode length: 48.90 +/- 3.30
Success rate: 10.00%
New best mean reward!
Eval num_timesteps=6000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=9000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=15000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=18000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=21000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=24000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=27000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=30000, episode_reward=-42.90 +/- 14.22
Episode length: 43.10 +/- 13.82
Success rate: 20.00%
New best mean reward!
Eval num_timesteps=33000, episode_reward=-48.20 +/- 5.40
Episode length: 48.30 +/- 5.10
Success rate: 10.00%
Eval num_timesteps=36000, episode_reward=-41.30 +/- 17.40
Episode length: 41.50 +/- 17.00
Success rate: 20.00%
New best mean reward!
Eval num_timesteps=39000, episode_reward=-45.70 +/- 12.90
Episode length: 45.80 +/- 12.60
Success rate: 10.00%
Eval num_timesteps=42000, episode_reward=-45.90 +/- 12.30
Episode length: 46.00 +/- 12.00
Success rate: 10.00%
Eval num_timesteps=45000, episode_reward=-43.10 +/- 13.89
Episode length: 43.30 +/- 13.49
Success rate: 20.00%
Eval num_timesteps=48000, episode_reward=-45.80 +/- 12.60
Episode length: 45.90 +/- 12.30
Success rate: 10.00%
Eval num_timesteps=51000, episode_reward=-41.00 +/- 15.66
Episode length: 41.40 +/- 15.32
Success rate: 30.00%
New best mean reward!
Eval num_timesteps=54000, episode_reward=-17.30 +/- 18.74
Episode length: 23.70 +/- 21.50
Success rate: 60.00%
New best mean reward!
Eval num_timesteps=57000, episode_reward=-32.30 +/- 21.69
Episode length: 32.70 +/- 21.20
Success rate: 40.00%
Eval num_timesteps=60000, episode_reward=-33.70 +/- 19.99
Episode length: 34.20 +/- 19.58
Success rate: 50.00%
TRIAL Finished: Objective Trial Time
[32m[I 2022-07-01 14:32:20,752][0m Trial 7 finished with value: -33.7 and parameters: {'gamma': 0.9890577848160682, 'tau': 0.003752460019604293, 'lr': 0.0002608959494035669, 'ent_coef': 0.003122907820880991, 'batch_size_num': 8, 'learning_starts': 10, 'action_noise_int': 3, 'n_envs': 3}. Best is trial 7 with value: -33.7.[0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.0046467901807592926, 'gamma': 0.9914194915684048, 'tau': 0.002037564368193129, 'learning_rate': 0.00039814249158431675, 'batch_size': 1024, 'learning_starts': 10000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  4
action_noise:  0.0

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=4000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=8000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=16000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=20000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=24000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=28000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=32000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=36000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=40000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=44000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=48000, episode_reward=-49.90 +/- 0.30
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=52000, episode_reward=-47.10 +/- 8.70
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=56000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=60000, episode_reward=-48.90 +/- 3.30
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
TRIAL Finished: Objective Trial Time
[32m[I 2022-07-01 14:48:36,678][0m Trial 8 finished with value: -48.9 and parameters: {'gamma': 0.9914194915684048, 'tau': 0.002037564368193129, 'lr': 0.00039814249158431675, 'ent_coef': 0.0046467901807592926, 'batch_size_num': 10, 'learning_starts': 10, 'action_noise_int': 1, 'n_envs': 4}. Best is trial 7 with value: -33.7.[0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.1772519127622705, 'gamma': 0.9809166588713485, 'tau': 0.0007918027632968752, 'learning_rate': 0.002438390463561256, 'batch_size': 1024, 'learning_starts': 6000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  3
action_noise:  0.05

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=3000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=6000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=9000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=15000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=18000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=21000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=24000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=27000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=30000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=33000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=36000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=39000, episode_reward=-49.30 +/- 2.10
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=42000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=45000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=48000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=51000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=54000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=57000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=60000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
TRIAL Finished: Objective Trial Time
[32m[I 2022-07-01 15:06:45,011][0m Trial 9 finished with value: -50.0 and parameters: {'gamma': 0.9809166588713485, 'tau': 0.0007918027632968752, 'lr': 0.002438390463561256, 'ent_coef': 0.1772519127622705, 'batch_size_num': 10, 'learning_starts': 6, 'action_noise_int': 2, 'n_envs': 3}. Best is trial 7 with value: -33.7.[0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.002981206959412327, 'gamma': 0.9746774522417813, 'tau': 0.0005081879680642787, 'learning_rate': 0.00011879558801751894, 'batch_size': 512, 'learning_starts': 6000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  3
action_noise:  0.0

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=3000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=6000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=9000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=15000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=18000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=21000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=24000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=27000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=30000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=33000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=36000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=39000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=42000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=45000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=48000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=51000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=54000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=57000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=60000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
TRIAL Finished: Objective Trial Time
[32m[I 2022-07-01 15:19:10,255][0m Trial 10 finished with value: -50.0 and parameters: {'gamma': 0.9746774522417813, 'tau': 0.0005081879680642787, 'lr': 0.00011879558801751894, 'ent_coef': 0.002981206959412327, 'batch_size_num': 9, 'learning_starts': 6, 'action_noise_int': 1, 'n_envs': 3}. Best is trial 7 with value: -33.7.[0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.09160284980033567, 'gamma': 0.9837712414506463, 'tau': 0.0009312343867191946, 'learning_rate': 0.00011869024003144656, 'batch_size': 256, 'learning_starts': 10000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  3
action_noise:  0.05

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=3000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=6000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=9000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=15000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=18000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=21000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=24000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=27000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=30000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=33000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=36000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=39000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=42000, episode_reward=-43.20 +/- 13.67
Episode length: 43.50 +/- 13.05
Success rate: 20.00%
New best mean reward!
Eval num_timesteps=45000, episode_reward=-43.10 +/- 14.02
Episode length: 43.30 +/- 13.62
Success rate: 20.00%
New best mean reward!
Eval num_timesteps=48000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=51000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=54000, episode_reward=-48.60 +/- 4.20
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=57000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=60000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
TRIAL Finished: Objective Trial Time
[32m[I 2022-07-01 15:31:02,087][0m Trial 11 finished with value: -50.0 and parameters: {'gamma': 0.9837712414506463, 'tau': 0.0009312343867191946, 'lr': 0.00011869024003144656, 'ent_coef': 0.09160284980033567, 'batch_size_num': 8, 'learning_starts': 10, 'action_noise_int': 2, 'n_envs': 3}. Best is trial 7 with value: -33.7.[0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.003563149424242628, 'gamma': 0.9890242619630928, 'tau': 0.001057861628252091, 'learning_rate': 0.0006334275384187566, 'batch_size': 256, 'learning_starts': 8000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  4
action_noise:  0.0

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=4000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=8000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=16000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=20000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=24000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=28000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=32000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=36000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=40000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=44000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=48000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=52000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=56000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=60000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
TRIAL Finished: Objective Trial Time
[32m[I 2022-07-01 15:40:45,130][0m Trial 12 finished with value: -50.0 and parameters: {'gamma': 0.9890242619630928, 'tau': 0.001057861628252091, 'lr': 0.0006334275384187566, 'ent_coef': 0.003563149424242628, 'batch_size_num': 8, 'learning_starts': 8, 'action_noise_int': 1, 'n_envs': 4}. Best is trial 7 with value: -33.7.[0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.003123818354090943, 'gamma': 0.9748960797057122, 'tau': 0.0031240443017421776, 'learning_rate': 0.00017407673765895283, 'batch_size': 512, 'learning_starts': 7000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  3
action_noise:  0.10000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=3000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=6000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=9000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-49.00 +/- 3.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=15000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=18000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=21000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=24000, episode_reward=-49.70 +/- 0.90
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=27000, episode_reward=-40.90 +/- 17.47
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=30000, episode_reward=-45.60 +/- 13.20
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=33000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=36000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=39000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=42000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=45000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=48000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=51000, episode_reward=-48.00 +/- 4.82
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=54000, episode_reward=-49.50 +/- 1.02
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=57000, episode_reward=-48.10 +/- 4.11
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=60000, episode_reward=-43.80 +/- 10.42
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
TRIAL Finished: Objective Trial Time
[32m[I 2022-07-01 15:59:43,678][0m Trial 13 finished with value: -43.8 and parameters: {'gamma': 0.9748960797057122, 'tau': 0.0031240443017421776, 'lr': 0.00017407673765895283, 'ent_coef': 0.003123818354090943, 'batch_size_num': 9, 'learning_starts': 7, 'action_noise_int': 3, 'n_envs': 3}. Best is trial 7 with value: -33.7.[0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.026073321341067345, 'gamma': 0.9867871832016801, 'tau': 0.004842359454387976, 'learning_rate': 0.001143445103684813, 'batch_size': 256, 'learning_starts': 8000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  5
action_noise:  0.15000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=5000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=10000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=15000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=20000, episode_reward=-45.40 +/- 13.80
Episode length: 45.50 +/- 13.50
Success rate: 10.00%
New best mean reward!
Eval num_timesteps=25000, episode_reward=-45.30 +/- 14.10
Episode length: 45.40 +/- 13.80
Success rate: 10.00%
New best mean reward!
Eval num_timesteps=30000, episode_reward=-22.40 +/- 22.55
Episode length: 23.00 +/- 22.06
Success rate: 60.00%
New best mean reward!
Eval num_timesteps=35000, episode_reward=-23.90 +/- 21.51
Episode length: 24.50 +/- 21.02
Success rate: 60.00%
Eval num_timesteps=40000, episode_reward=-18.00 +/- 16.78
Episode length: 18.80 +/- 16.40
Success rate: 80.00%
New best mean reward!
Eval num_timesteps=45000, episode_reward=-27.90 +/- 22.21
Episode length: 28.40 +/- 21.72
Success rate: 50.00%
Eval num_timesteps=50000, episode_reward=-3.80 +/- 0.87
Episode length: 4.80 +/- 0.87
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=55000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=60000, episode_reward=-3.20 +/- 0.40
Episode length: 4.20 +/- 0.40
Success rate: 100.00%
TRIAL Finished: Objective Trial Time
[32m[I 2022-07-01 16:10:30,761][0m Trial 14 finished with value: -3.2 and parameters: {'gamma': 0.9867871832016801, 'tau': 0.004842359454387976, 'lr': 0.001143445103684813, 'ent_coef': 0.026073321341067345, 'batch_size_num': 8, 'learning_starts': 8, 'action_noise_int': 4, 'n_envs': 5}. Best is trial 14 with value: -3.2.[0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.025733185454457595, 'gamma': 0.986540653857167, 'tau': 0.004561451113450806, 'learning_rate': 0.0012377747178384624, 'batch_size': 256, 'learning_starts': 9000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  5
action_noise:  0.15000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=5000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=10000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=15000, episode_reward=-46.50 +/- 10.50
Episode length: 46.60 +/- 10.20
Success rate: 10.00%
New best mean reward!
Eval num_timesteps=20000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=25000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=30000, episode_reward=-45.50 +/- 13.50
Episode length: 45.60 +/- 13.20
Success rate: 10.00%
New best mean reward!
Eval num_timesteps=35000, episode_reward=-41.00 +/- 18.02
Episode length: 41.20 +/- 17.62
Success rate: 20.00%
New best mean reward!
Eval num_timesteps=40000, episode_reward=-28.20 +/- 21.97
Episode length: 28.70 +/- 21.48
Success rate: 50.00%
New best mean reward!
Eval num_timesteps=45000, episode_reward=-19.40 +/- 19.27
Episode length: 20.20 +/- 18.95
Success rate: 80.00%
New best mean reward!
Eval num_timesteps=50000, episode_reward=-16.60 +/- 16.94
Episode length: 17.40 +/- 16.54
Success rate: 80.00%
New best mean reward!
Eval num_timesteps=55000, episode_reward=-13.70 +/- 18.19
Episode length: 14.50 +/- 17.79
Success rate: 80.00%
New best mean reward!
Eval num_timesteps=60000, episode_reward=-3.70 +/- 1.55
Episode length: 4.70 +/- 1.55
Success rate: 100.00%
New best mean reward!
TRIAL Finished: Objective Trial Time
[32m[I 2022-07-01 16:20:37,033][0m Trial 15 finished with value: -3.7 and parameters: {'gamma': 0.986540653857167, 'tau': 0.004561451113450806, 'lr': 0.0012377747178384624, 'ent_coef': 0.025733185454457595, 'batch_size_num': 8, 'learning_starts': 9, 'action_noise_int': 4, 'n_envs': 5}. Best is trial 14 with value: -3.2.[0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.02646612077664319, 'gamma': 0.9807039116165511, 'tau': 0.0048502767487059685, 'learning_rate': 0.0011945605196217793, 'batch_size': 256, 'learning_starts': 8000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  5
action_noise:  0.15000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=5000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=10000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=15000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=20000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=25000, episode_reward=-45.60 +/- 13.20
Episode length: 45.70 +/- 12.90
Success rate: 10.00%
New best mean reward!
Eval num_timesteps=30000, episode_reward=-38.90 +/- 17.39
Episode length: 39.20 +/- 16.95
Success rate: 30.00%
New best mean reward!
Eval num_timesteps=35000, episode_reward=-46.40 +/- 10.80
Episode length: 46.50 +/- 10.50
Success rate: 10.00%
Eval num_timesteps=40000, episode_reward=-45.30 +/- 14.10
Episode length: 45.40 +/- 13.80
Success rate: 10.00%
Eval num_timesteps=45000, episode_reward=-28.00 +/- 22.24
Episode length: 28.50 +/- 21.75
Success rate: 50.00%
New best mean reward!
Eval num_timesteps=50000, episode_reward=-31.70 +/- 22.44
Episode length: 32.10 +/- 21.95
Success rate: 40.00%
Eval num_timesteps=55000, episode_reward=-33.70 +/- 17.19
Episode length: 34.20 +/- 16.71
Success rate: 50.00%
Eval num_timesteps=60000, episode_reward=-16.20 +/- 18.68
Episode length: 17.00 +/- 18.31
Success rate: 80.00%
New best mean reward!
TRIAL Finished: Objective Trial Time
[32m[I 2022-07-01 16:30:26,926][0m Trial 16 finished with value: -16.2 and parameters: {'gamma': 0.9807039116165511, 'tau': 0.0048502767487059685, 'lr': 0.0011945605196217793, 'ent_coef': 0.02646612077664319, 'batch_size_num': 8, 'learning_starts': 8, 'action_noise_int': 4, 'n_envs': 5}. Best is trial 14 with value: -3.2.[0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.017962617291517873, 'gamma': 0.9861574027211197, 'tau': 0.004870263218582998, 'learning_rate': 0.0013379979198820183, 'batch_size': 256, 'learning_starts': 9000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  5
action_noise:  0.15000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=5000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=10000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=15000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=20000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=25000, episode_reward=-49.00 +/- 2.19
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=30000, episode_reward=-45.30 +/- 14.10
Episode length: 45.40 +/- 13.80
Success rate: 10.00%
New best mean reward!
Eval num_timesteps=35000, episode_reward=-45.40 +/- 13.80
Episode length: 45.50 +/- 13.50
Success rate: 10.00%
Eval num_timesteps=40000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=45000, episode_reward=-42.30 +/- 15.60
Episode length: 42.50 +/- 15.20
Success rate: 20.00%
New best mean reward!
Eval num_timesteps=50000, episode_reward=-45.20 +/- 13.75
Episode length: 45.50 +/- 13.50
Success rate: 10.00%
Eval num_timesteps=55000, episode_reward=-46.00 +/- 12.00
Episode length: 46.10 +/- 11.70
Success rate: 10.00%
Eval num_timesteps=60000, episode_reward=-40.60 +/- 18.80
Episode length: 40.80 +/- 18.40
Success rate: 20.00%
New best mean reward!
TRIAL Finished: Objective Trial Time
[32m[I 2022-07-01 16:40:14,237][0m Trial 17 finished with value: -40.6 and parameters: {'gamma': 0.9861574027211197, 'tau': 0.004870263218582998, 'lr': 0.0013379979198820183, 'ent_coef': 0.017962617291517873, 'batch_size_num': 8, 'learning_starts': 9, 'action_noise_int': 4, 'n_envs': 5}. Best is trial 14 with value: -3.2.[0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.015170809962546588, 'gamma': 0.9942623843518503, 'tau': 0.003649855802287618, 'learning_rate': 0.0010998855332861063, 'batch_size': 256, 'learning_starts': 7000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  5
action_noise:  0.10000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=5000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=10000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=15000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=20000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=25000, episode_reward=-40.70 +/- 18.60
Episode length: 40.90 +/- 18.20
Success rate: 20.00%
New best mean reward!
Eval num_timesteps=30000, episode_reward=-45.40 +/- 13.80
Episode length: 45.50 +/- 13.50
Success rate: 10.00%
Eval num_timesteps=35000, episode_reward=-45.60 +/- 13.20
Episode length: 45.70 +/- 12.90
Success rate: 10.00%
Eval num_timesteps=40000, episode_reward=-23.50 +/- 22.11
Episode length: 24.10 +/- 21.64
Success rate: 60.00%
New best mean reward!
Eval num_timesteps=45000, episode_reward=-35.90 +/- 21.54
Episode length: 36.20 +/- 21.08
Success rate: 30.00%
Eval num_timesteps=50000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=55000, episode_reward=-33.40 +/- 17.56
Episode length: 33.90 +/- 17.09
Success rate: 50.00%
Eval num_timesteps=60000, episode_reward=-14.70 +/- 17.82
Episode length: 15.50 +/- 17.42
Success rate: 80.00%
New best mean reward!
TRIAL Finished: Objective Trial Time
[32m[I 2022-07-01 16:50:11,244][0m Trial 18 finished with value: -14.7 and parameters: {'gamma': 0.9942623843518503, 'tau': 0.003649855802287618, 'lr': 0.0010998855332861063, 'ent_coef': 0.015170809962546588, 'batch_size_num': 8, 'learning_starts': 7, 'action_noise_int': 3, 'n_envs': 5}. Best is trial 14 with value: -3.2.[0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.055383097001926604, 'gamma': 0.986367435643148, 'tau': 0.0014163250623987953, 'learning_rate': 0.0023273049832674775, 'batch_size': 256, 'learning_starts': 9000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  5
action_noise:  0.05

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=5000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=10000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=15000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=20000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=25000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=30000, episode_reward=-49.40 +/- 1.80
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=35000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=40000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=45000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=50000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=55000, episode_reward=-45.70 +/- 12.90
Episode length: 45.80 +/- 12.60
Success rate: 10.00%
New best mean reward!
Eval num_timesteps=60000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
TRIAL Finished: Objective Trial Time
[32m[I 2022-07-01 16:59:38,619][0m Trial 19 finished with value: -50.0 and parameters: {'gamma': 0.986367435643148, 'tau': 0.0014163250623987953, 'lr': 0.0023273049832674775, 'ent_coef': 0.055383097001926604, 'batch_size_num': 8, 'learning_starts': 9, 'action_noise_int': 2, 'n_envs': 5}. Best is trial 14 with value: -3.2.[0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.008820073767094497, 'gamma': 0.9795669800650622, 'tau': 0.0028732553225876887, 'learning_rate': 0.0009069077086214123, 'batch_size': 256, 'learning_starts': 9000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  4
action_noise:  0.15000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=4000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=8000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=16000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=20000, episode_reward=-45.30 +/- 14.10
Episode length: 45.40 +/- 13.80
Success rate: 10.00%
New best mean reward!
Eval num_timesteps=24000, episode_reward=-45.30 +/- 14.10
Episode length: 45.40 +/- 13.80
Success rate: 10.00%
Eval num_timesteps=28000, episode_reward=-36.00 +/- 21.39
Episode length: 36.30 +/- 20.93
Success rate: 30.00%
New best mean reward!
Eval num_timesteps=32000, episode_reward=-26.80 +/- 23.22
Episode length: 27.30 +/- 22.72
Success rate: 50.00%
New best mean reward!
Eval num_timesteps=36000, episode_reward=-31.60 +/- 22.54
Episode length: 32.00 +/- 22.05
Success rate: 40.00%
Eval num_timesteps=40000, episode_reward=-24.00 +/- 21.15
Episode length: 24.70 +/- 20.78
Success rate: 60.00%
New best mean reward!
Eval num_timesteps=44000, episode_reward=-28.00 +/- 22.09
Episode length: 28.50 +/- 21.59
Success rate: 50.00%
Eval num_timesteps=48000, episode_reward=-8.10 +/- 13.99
Episode length: 9.00 +/- 13.69
Success rate: 90.00%
New best mean reward!
Eval num_timesteps=52000, episode_reward=-4.00 +/- 1.00
Episode length: 5.00 +/- 1.00
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=56000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=60000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
TRIAL Finished: Objective Trial Time
[32m[I 2022-07-01 17:10:38,217][0m Trial 20 finished with value: -3.0 and parameters: {'gamma': 0.9795669800650622, 'tau': 0.0028732553225876887, 'lr': 0.0009069077086214123, 'ent_coef': 0.008820073767094497, 'batch_size_num': 8, 'learning_starts': 9, 'action_noise_int': 4, 'n_envs': 4}. Best is trial 20 with value: -3.0.[0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.010627401954708317, 'gamma': 0.9792574824131047, 'tau': 0.003949756573900465, 'learning_rate': 0.000814568315225411, 'batch_size': 256, 'learning_starts': 9000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  4
action_noise:  0.15000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=4000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=8000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-46.80 +/- 9.60
Episode length: 46.90 +/- 9.30
Success rate: 10.00%
New best mean reward!
Eval num_timesteps=16000, episode_reward=-45.30 +/- 14.10
Episode length: 45.40 +/- 13.80
Success rate: 10.00%
New best mean reward!
Eval num_timesteps=20000, episode_reward=-45.90 +/- 12.30
Episode length: 46.00 +/- 12.00
Success rate: 10.00%
Eval num_timesteps=24000, episode_reward=-49.80 +/- 0.60
Episode length: 49.90 +/- 0.30
Success rate: 10.00%
Eval num_timesteps=28000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=32000, episode_reward=-27.60 +/- 22.45
Episode length: 28.10 +/- 21.95
Success rate: 50.00%
New best mean reward!
Eval num_timesteps=36000, episode_reward=-27.50 +/- 22.50
Episode length: 28.00 +/- 22.00
Success rate: 50.00%
New best mean reward!
Eval num_timesteps=40000, episode_reward=-12.60 +/- 18.70
Episode length: 13.40 +/- 18.30
Success rate: 80.00%
New best mean reward!
Eval num_timesteps=44000, episode_reward=-9.00 +/- 13.73
Episode length: 9.90 +/- 13.43
Success rate: 90.00%
New best mean reward!
Eval num_timesteps=48000, episode_reward=-26.70 +/- 23.30
Episode length: 27.20 +/- 22.80
Success rate: 50.00%
Eval num_timesteps=52000, episode_reward=-4.40 +/- 2.06
Episode length: 5.40 +/- 2.06
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=56000, episode_reward=-3.10 +/- 0.30
Episode length: 4.10 +/- 0.30
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=60000, episode_reward=-3.20 +/- 0.40
Episode length: 4.20 +/- 0.40
Success rate: 100.00%
TRIAL Finished: Objective Trial Time
[32m[I 2022-07-01 17:22:09,745][0m Trial 21 finished with value: -3.2 and parameters: {'gamma': 0.9792574824131047, 'tau': 0.003949756573900465, 'lr': 0.000814568315225411, 'ent_coef': 0.010627401954708317, 'batch_size_num': 8, 'learning_starts': 9, 'action_noise_int': 4, 'n_envs': 4}. Best is trial 20 with value: -3.0.[0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.008328012109562181, 'gamma': 0.9785247878158216, 'tau': 0.0028958575569544872, 'learning_rate': 0.0008557889242583103, 'batch_size': 256, 'learning_starts': 8000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  4
action_noise:  0.10000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=4000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=8000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=16000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=20000, episode_reward=-45.30 +/- 14.10
Episode length: 45.40 +/- 13.80
Success rate: 10.00%
New best mean reward!
Eval num_timesteps=24000, episode_reward=-36.80 +/- 20.30
Episode length: 37.10 +/- 19.84
Success rate: 30.00%
New best mean reward!
Eval num_timesteps=28000, episode_reward=-45.70 +/- 12.90
Episode length: 45.80 +/- 12.60
Success rate: 10.00%
Eval num_timesteps=32000, episode_reward=-37.40 +/- 19.63
Episode length: 37.70 +/- 19.18
Success rate: 30.00%
Eval num_timesteps=36000, episode_reward=-33.80 +/- 19.98
Episode length: 34.20 +/- 19.50
Success rate: 40.00%
New best mean reward!
Eval num_timesteps=40000, episode_reward=-37.20 +/- 19.55
Episode length: 37.50 +/- 19.10
Success rate: 30.00%
Eval num_timesteps=44000, episode_reward=-40.70 +/- 18.60
Episode length: 40.90 +/- 18.20
Success rate: 20.00%
Eval num_timesteps=48000, episode_reward=-30.00 +/- 20.32
Episode length: 30.50 +/- 19.83
Success rate: 50.00%
New best mean reward!
Eval num_timesteps=52000, episode_reward=-17.90 +/- 17.07
Episode length: 18.70 +/- 16.70
Success rate: 80.00%
New best mean reward!
Eval num_timesteps=56000, episode_reward=-27.50 +/- 22.61
Episode length: 28.00 +/- 22.11
Success rate: 50.00%
Eval num_timesteps=60000, episode_reward=-17.60 +/- 21.22
Episode length: 18.30 +/- 20.77
Success rate: 70.00%
New best mean reward!
TRIAL Finished: Objective Trial Time
[32m[I 2022-07-01 17:34:59,116][0m Trial 22 finished with value: -17.6 and parameters: {'gamma': 0.9785247878158216, 'tau': 0.0028958575569544872, 'lr': 0.0008557889242583103, 'ent_coef': 0.008328012109562181, 'batch_size_num': 8, 'learning_starts': 8, 'action_noise_int': 3, 'n_envs': 4}. Best is trial 20 with value: -3.0.[0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.01098984744632042, 'gamma': 0.9782656462524881, 'tau': 0.00376309904478976, 'learning_rate': 0.001734643825909677, 'batch_size': 256, 'learning_starts': 9000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  4
action_noise:  0.15000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=4000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=8000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-44.80 +/- 13.15
Episode length: 45.00 +/- 12.84
Success rate: 20.00%
New best mean reward!
Eval num_timesteps=16000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=20000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=24000, episode_reward=-45.40 +/- 13.80
Episode length: 45.50 +/- 13.50
Success rate: 10.00%
Eval num_timesteps=28000, episode_reward=-36.00 +/- 21.39
Episode length: 36.30 +/- 20.93
Success rate: 30.00%
New best mean reward!
Eval num_timesteps=32000, episode_reward=-32.10 +/- 21.94
Episode length: 32.50 +/- 21.45
Success rate: 40.00%
New best mean reward!
Eval num_timesteps=36000, episode_reward=-31.50 +/- 22.67
Episode length: 31.90 +/- 22.18
Success rate: 40.00%
New best mean reward!
Eval num_timesteps=40000, episode_reward=-24.50 +/- 21.62
Episode length: 25.10 +/- 21.15
Success rate: 60.00%
New best mean reward!
Eval num_timesteps=44000, episode_reward=-32.40 +/- 21.68
Episode length: 32.80 +/- 21.19
Success rate: 40.00%
Eval num_timesteps=48000, episode_reward=-17.80 +/- 21.10
Episode length: 18.50 +/- 20.64
Success rate: 70.00%
New best mean reward!
Eval num_timesteps=52000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=56000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
Eval num_timesteps=60000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
TRIAL Finished: Objective Trial Time
[32m[I 2022-07-01 17:45:32,133][0m Trial 23 finished with value: -3.0 and parameters: {'gamma': 0.9782656462524881, 'tau': 0.00376309904478976, 'lr': 0.001734643825909677, 'ent_coef': 0.01098984744632042, 'batch_size_num': 8, 'learning_starts': 9, 'action_noise_int': 4, 'n_envs': 4}. Best is trial 20 with value: -3.0.[0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.00883636061477883, 'gamma': 0.978260970219055, 'tau': 0.003631097579083096, 'learning_rate': 0.0018633581455701062, 'batch_size': 256, 'learning_starts': 9000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  4
action_noise:  0.10000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=4000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=8000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=16000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=20000, episode_reward=-45.30 +/- 14.10
Episode length: 45.40 +/- 13.80
Success rate: 10.00%
New best mean reward!
Eval num_timesteps=24000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=28000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=32000, episode_reward=-40.90 +/- 18.21
Episode length: 41.10 +/- 17.81
Success rate: 20.00%
New best mean reward!
Eval num_timesteps=36000, episode_reward=-28.30 +/- 21.75
Episode length: 28.80 +/- 21.25
Success rate: 50.00%
New best mean reward!
Eval num_timesteps=40000, episode_reward=-31.90 +/- 22.17
Episode length: 32.30 +/- 21.68
Success rate: 40.00%
Eval num_timesteps=44000, episode_reward=-8.00 +/- 14.01
Episode length: 8.90 +/- 13.71
Success rate: 90.00%
New best mean reward!
Eval num_timesteps=48000, episode_reward=-27.40 +/- 22.68
Episode length: 27.90 +/- 22.18
Success rate: 50.00%
Eval num_timesteps=52000, episode_reward=-13.60 +/- 18.25
Episode length: 14.40 +/- 17.86
Success rate: 80.00%
Eval num_timesteps=56000, episode_reward=-3.10 +/- 0.30
Episode length: 4.10 +/- 0.30
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=60000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
New best mean reward!
TRIAL Finished: Objective Trial Time
[32m[I 2022-07-01 17:57:32,683][0m Trial 24 finished with value: -3.0 and parameters: {'gamma': 0.978260970219055, 'tau': 0.003631097579083096, 'lr': 0.0018633581455701062, 'ent_coef': 0.00883636061477883, 'batch_size_num': 8, 'learning_starts': 9, 'action_noise_int': 3, 'n_envs': 4}. Best is trial 20 with value: -3.0.[0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.0011250424836133292, 'gamma': 0.9771769720341564, 'tau': 0.0029860721382987763, 'learning_rate': 0.001756005767956056, 'batch_size': 256, 'learning_starts': 9000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  4
action_noise:  0.10000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=4000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=8000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=16000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=20000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=24000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=28000, episode_reward=-46.40 +/- 10.80
Episode length: 46.50 +/- 10.50
Success rate: 10.00%
New best mean reward!
Eval num_timesteps=32000, episode_reward=-42.00 +/- 16.01
Episode length: 42.20 +/- 15.61
Success rate: 20.00%
New best mean reward!
Eval num_timesteps=36000, episode_reward=-32.90 +/- 21.02
Episode length: 33.30 +/- 20.53
Success rate: 40.00%
New best mean reward!
Eval num_timesteps=40000, episode_reward=-18.70 +/- 20.61
Episode length: 19.40 +/- 20.16
Success rate: 70.00%
New best mean reward!
Eval num_timesteps=44000, episode_reward=-23.80 +/- 18.69
Episode length: 28.70 +/- 21.40
Success rate: 50.00%
Eval num_timesteps=48000, episode_reward=-22.80 +/- 22.27
Episode length: 23.40 +/- 21.78
Success rate: 60.00%
Eval num_timesteps=52000, episode_reward=-13.90 +/- 18.19
Episode length: 14.70 +/- 17.79
Success rate: 80.00%
New best mean reward!
Eval num_timesteps=56000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=60000, episode_reward=-3.20 +/- 0.40
Episode length: 4.20 +/- 0.40
Success rate: 100.00%
TRIAL Finished: Objective Trial Time
[32m[I 2022-07-01 18:09:06,286][0m Trial 25 finished with value: -3.2 and parameters: {'gamma': 0.9771769720341564, 'tau': 0.0029860721382987763, 'lr': 0.001756005767956056, 'ent_coef': 0.0011250424836133292, 'batch_size_num': 8, 'learning_starts': 9, 'action_noise_int': 3, 'n_envs': 4}. Best is trial 20 with value: -3.0.[0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.00756036893278328, 'gamma': 0.9705293706070158, 'tau': 0.0014527769889095849, 'learning_rate': 0.002915564739333065, 'batch_size': 256, 'learning_starts': 7000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  4
action_noise:  0.05

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=4000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=8000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-49.50 +/- 1.50
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=16000, episode_reward=-45.40 +/- 13.80
Episode length: 45.50 +/- 13.50
Success rate: 10.00%
New best mean reward!
Eval num_timesteps=20000, episode_reward=-40.20 +/- 18.64
Episode length: 40.80 +/- 18.40
Success rate: 20.00%
New best mean reward!
Eval num_timesteps=24000, episode_reward=-40.60 +/- 18.80
Episode length: 40.80 +/- 18.40
Success rate: 20.00%
Eval num_timesteps=28000, episode_reward=-35.90 +/- 21.54
Episode length: 36.20 +/- 21.08
Success rate: 30.00%
New best mean reward!
Eval num_timesteps=32000, episode_reward=-26.50 +/- 23.50
Episode length: 27.00 +/- 23.00
Success rate: 50.00%
New best mean reward!
Eval num_timesteps=36000, episode_reward=-40.60 +/- 18.80
Episode length: 40.80 +/- 18.40
Success rate: 20.00%
Eval num_timesteps=40000, episode_reward=-26.60 +/- 23.40
Episode length: 27.10 +/- 22.90
Success rate: 50.00%
Eval num_timesteps=44000, episode_reward=-7.80 +/- 14.07
Episode length: 8.70 +/- 13.77
Success rate: 90.00%
New best mean reward!
Eval num_timesteps=48000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=52000, episode_reward=-3.30 +/- 0.46
Episode length: 4.30 +/- 0.46
Success rate: 100.00%
Eval num_timesteps=56000, episode_reward=-3.10 +/- 0.30
Episode length: 4.10 +/- 0.30
Success rate: 100.00%
Eval num_timesteps=60000, episode_reward=-3.10 +/- 0.30
Episode length: 4.10 +/- 0.30
Success rate: 100.00%
TRIAL Finished: Objective Trial Time
[32m[I 2022-07-01 18:19:39,330][0m Trial 26 finished with value: -3.1 and parameters: {'gamma': 0.9705293706070158, 'tau': 0.0014527769889095849, 'lr': 0.002915564739333065, 'ent_coef': 0.00756036893278328, 'batch_size_num': 8, 'learning_starts': 7, 'action_noise_int': 2, 'n_envs': 4}. Best is trial 20 with value: -3.0.[0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.04884932482526793, 'gamma': 0.976910027945628, 'tau': 0.0024341309264556865, 'learning_rate': 0.001778793688635475, 'batch_size': 256, 'learning_starts': 9000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  4
action_noise:  0.10000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=4000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=8000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=16000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=20000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=24000, episode_reward=-46.40 +/- 9.84
Episode length: 46.60 +/- 9.55
Success rate: 20.00%
New best mean reward!
Eval num_timesteps=28000, episode_reward=-49.40 +/- 1.50
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=32000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=36000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=40000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=44000, episode_reward=-40.90 +/- 18.21
Episode length: 41.10 +/- 17.81
Success rate: 20.00%
New best mean reward!
Eval num_timesteps=48000, episode_reward=-40.90 +/- 18.20
Episode length: 41.10 +/- 17.80
Success rate: 20.00%
Eval num_timesteps=52000, episode_reward=-36.10 +/- 21.24
Episode length: 36.40 +/- 20.78
Success rate: 30.00%
New best mean reward!
Eval num_timesteps=56000, episode_reward=-36.00 +/- 21.39
Episode length: 36.30 +/- 20.93
Success rate: 30.00%
New best mean reward!
Eval num_timesteps=60000, episode_reward=-9.90 +/- 14.15
Episode length: 10.80 +/- 13.87
Success rate: 90.00%
New best mean reward!
TRIAL Finished: Objective Trial Time
[32m[I 2022-07-01 18:30:30,806][0m Trial 27 finished with value: -9.9 and parameters: {'gamma': 0.976910027945628, 'tau': 0.0024341309264556865, 'lr': 0.001778793688635475, 'ent_coef': 0.04884932482526793, 'batch_size_num': 8, 'learning_starts': 9, 'action_noise_int': 3, 'n_envs': 4}. Best is trial 20 with value: -3.0.[0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.001690232461325551, 'gamma': 0.9819249002445413, 'tau': 0.003267037054533623, 'learning_rate': 0.0017413957812320669, 'batch_size': 256, 'learning_starts': 10000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  5
action_noise:  0.15000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=5000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=10000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=15000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=20000, episode_reward=-45.80 +/- 7.17
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=25000, episode_reward=-49.40 +/- 1.28
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=30000, episode_reward=-46.20 +/- 6.14
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=35000, episode_reward=-37.90 +/- 16.34
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=40000, episode_reward=-45.30 +/- 14.10
Episode length: 45.40 +/- 13.80
Success rate: 10.00%
Eval num_timesteps=45000, episode_reward=-27.90 +/- 21.04
Episode length: 36.20 +/- 21.08
Success rate: 30.00%
New best mean reward!
Eval num_timesteps=50000, episode_reward=-3.80 +/- 1.54
Episode length: 4.80 +/- 1.54
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=55000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=60000, episode_reward=-3.10 +/- 0.30
Episode length: 4.10 +/- 0.30
Success rate: 100.00%
TRIAL Finished: Objective Trial Time
[32m[I 2022-07-01 18:40:17,073][0m Trial 28 finished with value: -3.1 and parameters: {'gamma': 0.9819249002445413, 'tau': 0.003267037054533623, 'lr': 0.0017413957812320669, 'ent_coef': 0.001690232461325551, 'batch_size_num': 8, 'learning_starts': 10, 'action_noise_int': 4, 'n_envs': 5}. Best is trial 20 with value: -3.0.[0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.00042760435113858554, 'gamma': 0.9843201405925659, 'tau': 0.003994387026843167, 'learning_rate': 0.0008841276649048388, 'batch_size': 256, 'learning_starts': 8000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  2
action_noise:  0.10000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=2000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=4000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=6000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=8000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=10000, episode_reward=-49.80 +/- 0.60
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=12000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=14000, episode_reward=-46.40 +/- 10.80
Episode length: 46.50 +/- 10.50
Success rate: 10.00%
New best mean reward!
Eval num_timesteps=16000, episode_reward=-46.80 +/- 9.60
Episode length: 46.90 +/- 9.30
Success rate: 10.00%
Eval num_timesteps=18000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=20000, episode_reward=-45.30 +/- 14.10
Episode length: 45.40 +/- 13.80
Success rate: 10.00%
New best mean reward!
Eval num_timesteps=22000, episode_reward=-40.60 +/- 18.80
Episode length: 40.80 +/- 18.40
Success rate: 20.00%
New best mean reward!
Eval num_timesteps=24000, episode_reward=-41.70 +/- 16.78
Episode length: 41.90 +/- 16.39
Success rate: 20.00%
Eval num_timesteps=26000, episode_reward=-40.80 +/- 18.41
Episode length: 41.00 +/- 18.01
Success rate: 20.00%
Eval num_timesteps=28000, episode_reward=-40.70 +/- 18.60
Episode length: 40.90 +/- 18.20
Success rate: 20.00%
Eval num_timesteps=30000, episode_reward=-33.20 +/- 20.70
Episode length: 33.60 +/- 20.21
Success rate: 40.00%
New best mean reward!
Eval num_timesteps=32000, episode_reward=-27.80 +/- 22.29
Episode length: 28.30 +/- 21.79
Success rate: 50.00%
New best mean reward!
Eval num_timesteps=34000, episode_reward=-28.20 +/- 21.90
Episode length: 28.70 +/- 21.41
Success rate: 50.00%
Eval num_timesteps=36000, episode_reward=-13.40 +/- 18.38
Episode length: 14.20 +/- 17.98
Success rate: 80.00%
New best mean reward!
Eval num_timesteps=38000, episode_reward=-8.60 +/- 14.00
Episode length: 9.50 +/- 13.71
Success rate: 90.00%
New best mean reward!
Eval num_timesteps=40000, episode_reward=-8.20 +/- 13.98
Episode length: 9.10 +/- 13.69
Success rate: 90.00%
New best mean reward!
Eval num_timesteps=42000, episode_reward=-8.40 +/- 13.92
Episode length: 13.80 +/- 18.14
Success rate: 80.00%
Eval num_timesteps=44000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=46000, episode_reward=-3.40 +/- 0.66
Episode length: 4.40 +/- 0.66
Success rate: 100.00%
Eval num_timesteps=48000, episode_reward=-3.10 +/- 0.30
Episode length: 4.10 +/- 0.30
Success rate: 100.00%
Eval num_timesteps=50000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
Eval num_timesteps=52000, episode_reward=-3.20 +/- 0.40
Episode length: 4.20 +/- 0.40
Success rate: 100.00%
Eval num_timesteps=54000, episode_reward=-3.10 +/- 0.30
Episode length: 4.10 +/- 0.30
Success rate: 100.00%
Eval num_timesteps=56000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
Eval num_timesteps=58000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
Eval num_timesteps=60000, episode_reward=-3.20 +/- 0.40
Episode length: 4.20 +/- 0.40
Success rate: 100.00%
TRIAL Finished: Objective Trial Time
[32m[I 2022-07-01 18:58:17,004][0m Trial 29 finished with value: -3.2 and parameters: {'gamma': 0.9843201405925659, 'tau': 0.003994387026843167, 'lr': 0.0008841276649048388, 'ent_coef': 0.00042760435113858554, 'batch_size_num': 8, 'learning_starts': 8, 'action_noise_int': 3, 'n_envs': 2}. Best is trial 20 with value: -3.0.[0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.006555985840542277, 'gamma': 0.9793764727834706, 'tau': 0.002756126409680973, 'learning_rate': 0.0015655256429310451, 'batch_size': 1024, 'learning_starts': 9000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  4
action_noise:  0.05

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=4000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=8000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=16000, episode_reward=-45.60 +/- 13.20
Episode length: 45.70 +/- 12.90
Success rate: 10.00%
New best mean reward!
Eval num_timesteps=20000, episode_reward=-48.30 +/- 5.10
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=24000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=28000, episode_reward=-47.20 +/- 8.40
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=32000, episode_reward=-42.00 +/- 15.32
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=36000, episode_reward=-28.50 +/- 20.58
Episode length: 46.20 +/- 11.40
Success rate: 10.00%
New best mean reward!
Eval num_timesteps=40000, episode_reward=-9.50 +/- 7.14
Episode length: 37.80 +/- 18.67
Success rate: 30.00%
New best mean reward!
Eval num_timesteps=44000, episode_reward=-3.90 +/- 0.94
Episode length: 16.60 +/- 18.25
Success rate: 80.00%
New best mean reward!
Eval num_timesteps=48000, episode_reward=-5.30 +/- 1.35
Episode length: 23.70 +/- 21.51
Success rate: 60.00%
Eval num_timesteps=52000, episode_reward=-4.40 +/- 1.11
Episode length: 5.40 +/- 1.11
Success rate: 100.00%
Eval num_timesteps=56000, episode_reward=-3.30 +/- 0.46
Episode length: 4.30 +/- 0.46
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=60000, episode_reward=-3.10 +/- 0.30
Episode length: 4.10 +/- 0.30
Success rate: 100.00%
New best mean reward!
TRIAL Finished: Objective Trial Time
[32m[I 2022-07-01 19:14:23,667][0m Trial 30 finished with value: -3.1 and parameters: {'gamma': 0.9793764727834706, 'tau': 0.002756126409680973, 'lr': 0.0015655256429310451, 'ent_coef': 0.006555985840542277, 'batch_size_num': 10, 'learning_starts': 9, 'action_noise_int': 2, 'n_envs': 4}. Best is trial 20 with value: -3.0.[0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.006132641757211532, 'gamma': 0.9793957936476061, 'tau': 0.0026575496546567927, 'learning_rate': 0.0015732758493850708, 'batch_size': 1024, 'learning_starts': 9000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  4
action_noise:  0.05

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=4000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=8000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=16000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=20000, episode_reward=-41.10 +/- 17.81
Episode length: 41.30 +/- 17.41
Success rate: 20.00%
New best mean reward!
Eval num_timesteps=24000, episode_reward=-31.60 +/- 22.54
Episode length: 32.00 +/- 22.05
Success rate: 40.00%
New best mean reward!
Eval num_timesteps=28000, episode_reward=-28.90 +/- 21.32
Episode length: 29.40 +/- 20.83
Success rate: 50.00%
New best mean reward!
Eval num_timesteps=32000, episode_reward=-45.90 +/- 12.30
Episode length: 46.00 +/- 12.00
Success rate: 10.00%
Eval num_timesteps=36000, episode_reward=-9.60 +/- 13.73
Episode length: 10.50 +/- 13.43
Success rate: 90.00%
New best mean reward!
Eval num_timesteps=40000, episode_reward=-3.10 +/- 0.30
Episode length: 4.10 +/- 0.30
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=44000, episode_reward=-3.10 +/- 0.30
Episode length: 4.10 +/- 0.30
Success rate: 100.00%
Eval num_timesteps=48000, episode_reward=-3.20 +/- 0.40
Episode length: 4.20 +/- 0.40
Success rate: 100.00%
Eval num_timesteps=52000, episode_reward=-3.10 +/- 0.30
Episode length: 4.10 +/- 0.30
Success rate: 100.00%
Eval num_timesteps=56000, episode_reward=-3.10 +/- 0.30
Episode length: 4.10 +/- 0.30
Success rate: 100.00%
Eval num_timesteps=60000, episode_reward=-3.10 +/- 0.30
Episode length: 4.10 +/- 0.30
Success rate: 100.00%
TRIAL Finished: Objective Trial Time
[32m[I 2022-07-01 19:31:54,954][0m Trial 31 finished with value: -3.1 and parameters: {'gamma': 0.9793957936476061, 'tau': 0.0026575496546567927, 'lr': 0.0015732758493850708, 'ent_coef': 0.006132641757211532, 'batch_size_num': 10, 'learning_starts': 9, 'action_noise_int': 2, 'n_envs': 4}. Best is trial 20 with value: -3.0.[0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.011303825379568826, 'gamma': 0.9773835312580536, 'tau': 0.0033904681369461547, 'learning_rate': 0.0019746420257150295, 'batch_size': 1024, 'learning_starts': 9000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  4
action_noise:  0.15000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=4000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=8000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-36.30 +/- 20.93
Episode length: 36.60 +/- 20.47
Success rate: 30.00%
New best mean reward!
Eval num_timesteps=16000, episode_reward=-49.60 +/- 0.92
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=20000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=24000, episode_reward=-41.20 +/- 17.65
Episode length: 41.40 +/- 17.25
Success rate: 20.00%
Eval num_timesteps=28000, episode_reward=-45.60 +/- 13.20
Episode length: 45.70 +/- 12.90
Success rate: 10.00%
Eval num_timesteps=32000, episode_reward=-8.10 +/- 13.98
Episode length: 9.00 +/- 13.68
Success rate: 90.00%
New best mean reward!
Eval num_timesteps=36000, episode_reward=-3.10 +/- 0.30
Episode length: 4.10 +/- 0.30
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=40000, episode_reward=-3.10 +/- 0.30
Episode length: 4.10 +/- 0.30
Success rate: 100.00%
Eval num_timesteps=44000, episode_reward=-3.30 +/- 0.46
Episode length: 4.30 +/- 0.46
Success rate: 100.00%
Eval num_timesteps=48000, episode_reward=-3.10 +/- 0.30
Episode length: 4.10 +/- 0.30
Success rate: 100.00%
Eval num_timesteps=52000, episode_reward=-3.10 +/- 0.30
Episode length: 4.10 +/- 0.30
Success rate: 100.00%
Eval num_timesteps=56000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=60000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
TRIAL Finished: Objective Trial Time
[32m[I 2022-07-01 19:48:01,737][0m Trial 32 finished with value: -3.0 and parameters: {'gamma': 0.9773835312580536, 'tau': 0.0033904681369461547, 'lr': 0.0019746420257150295, 'ent_coef': 0.011303825379568826, 'batch_size_num': 10, 'learning_starts': 9, 'action_noise_int': 4, 'n_envs': 4}. Best is trial 20 with value: -3.0.[0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.0121837813753698, 'gamma': 0.9763494173043968, 'tau': 0.0033029762889598205, 'learning_rate': 0.0021771455868558986, 'batch_size': 1024, 'learning_starts': 10000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  4
action_noise:  0.15000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=4000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=8000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-46.00 +/- 12.00
Episode length: 46.10 +/- 11.70
Success rate: 10.00%
New best mean reward!
Eval num_timesteps=16000, episode_reward=-45.40 +/- 13.80
Episode length: 45.50 +/- 13.50
Success rate: 10.00%
New best mean reward!
Eval num_timesteps=20000, episode_reward=-36.40 +/- 20.78
Episode length: 36.70 +/- 20.32
Success rate: 30.00%
New best mean reward!
Eval num_timesteps=24000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=28000, episode_reward=-9.30 +/- 13.59
Episode length: 10.20 +/- 13.30
Success rate: 90.00%
New best mean reward!
Eval num_timesteps=32000, episode_reward=-3.20 +/- 0.40
Episode length: 4.20 +/- 0.40
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=36000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=40000, episode_reward=-3.10 +/- 0.30
Episode length: 4.10 +/- 0.30
Success rate: 100.00%
Eval num_timesteps=44000, episode_reward=-3.20 +/- 0.40
Episode length: 4.20 +/- 0.40
Success rate: 100.00%
Eval num_timesteps=48000, episode_reward=-3.10 +/- 0.30
Episode length: 4.10 +/- 0.30
Success rate: 100.00%
Eval num_timesteps=52000, episode_reward=-3.10 +/- 0.30
Episode length: 4.10 +/- 0.30
Success rate: 100.00%
Eval num_timesteps=56000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
Eval num_timesteps=60000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
TRIAL Finished: Objective Trial Time
[32m[I 2022-07-01 20:02:25,051][0m Trial 33 finished with value: -3.0 and parameters: {'gamma': 0.9763494173043968, 'tau': 0.0033029762889598205, 'lr': 0.0021771455868558986, 'ent_coef': 0.0121837813753698, 'batch_size_num': 10, 'learning_starts': 10, 'action_noise_int': 4, 'n_envs': 4}. Best is trial 20 with value: -3.0.[0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.04942777708305956, 'gamma': 0.9723347790436927, 'tau': 0.004180139010347649, 'learning_rate': 0.002862835374341565, 'batch_size': 512, 'learning_starts': 10000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  4
action_noise:  0.15000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=4000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=8000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=16000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=20000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=24000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=28000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=32000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=36000, episode_reward=-31.30 +/- 22.90
Episode length: 31.70 +/- 22.41
Success rate: 40.00%
New best mean reward!
Eval num_timesteps=40000, episode_reward=-32.70 +/- 21.38
Episode length: 33.10 +/- 20.89
Success rate: 40.00%
Eval num_timesteps=44000, episode_reward=-26.70 +/- 23.30
Episode length: 27.20 +/- 22.80
Success rate: 50.00%
New best mean reward!
Eval num_timesteps=48000, episode_reward=-17.20 +/- 21.47
Episode length: 17.90 +/- 21.02
Success rate: 70.00%
New best mean reward!
Eval num_timesteps=52000, episode_reward=-14.90 +/- 19.04
Episode length: 15.70 +/- 18.68
Success rate: 80.00%
New best mean reward!
Eval num_timesteps=56000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=60000, episode_reward=-3.20 +/- 0.40
Episode length: 4.20 +/- 0.40
Success rate: 100.00%
TRIAL Finished: Objective Trial Time
[32m[I 2022-07-01 20:14:34,152][0m Trial 34 finished with value: -3.2 and parameters: {'gamma': 0.9723347790436927, 'tau': 0.004180139010347649, 'lr': 0.002862835374341565, 'ent_coef': 0.04942777708305956, 'batch_size_num': 9, 'learning_starts': 10, 'action_noise_int': 4, 'n_envs': 4}. Best is trial 20 with value: -3.0.[0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.013931988371232364, 'gamma': 0.9764761917026226, 'tau': 0.0022315791050665054, 'learning_rate': 0.00237068415321768, 'batch_size': 1024, 'learning_starts': 10000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  5
action_noise:  0.15000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=5000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=10000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=15000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=20000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=25000, episode_reward=-28.50 +/- 21.99
Episode length: 29.00 +/- 21.50
Success rate: 50.00%
New best mean reward!
Eval num_timesteps=30000, episode_reward=-37.90 +/- 18.83
Episode length: 38.20 +/- 18.38
Success rate: 30.00%
Eval num_timesteps=35000, episode_reward=-20.90 +/- 19.49
Episode length: 21.60 +/- 19.04
Success rate: 70.00%
New best mean reward!
Eval num_timesteps=40000, episode_reward=-40.70 +/- 18.60
Episode length: 40.90 +/- 18.20
Success rate: 20.00%
Eval num_timesteps=45000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=50000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
Eval num_timesteps=55000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
Eval num_timesteps=60000, episode_reward=-3.20 +/- 0.40
Episode length: 4.20 +/- 0.40
Success rate: 100.00%
TRIAL Finished: Objective Trial Time
[32m[I 2022-07-01 20:28:08,470][0m Trial 35 finished with value: -3.2 and parameters: {'gamma': 0.9764761917026226, 'tau': 0.0022315791050665054, 'lr': 0.00237068415321768, 'ent_coef': 0.013931988371232364, 'batch_size_num': 10, 'learning_starts': 10, 'action_noise_int': 4, 'n_envs': 5}. Best is trial 20 with value: -3.0.[0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.0017881452564262167, 'gamma': 0.9732706540520775, 'tau': 0.003363744776134631, 'learning_rate': 0.0020563912542552183, 'batch_size': 1024, 'learning_starts': 8000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  4
action_noise:  0.10000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=4000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=8000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-46.50 +/- 10.50
Episode length: 46.60 +/- 10.20
Success rate: 10.00%
New best mean reward!
Eval num_timesteps=16000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=20000, episode_reward=-45.90 +/- 12.30
Episode length: 46.00 +/- 12.00
Success rate: 10.00%
New best mean reward!
Eval num_timesteps=24000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=28000, episode_reward=-36.10 +/- 21.24
Episode length: 36.40 +/- 20.78
Success rate: 30.00%
New best mean reward!
Eval num_timesteps=32000, episode_reward=-45.30 +/- 14.10
Episode length: 45.40 +/- 13.80
Success rate: 10.00%
Eval num_timesteps=36000, episode_reward=-23.00 +/- 22.08
Episode length: 23.60 +/- 21.59
Success rate: 60.00%
New best mean reward!
Eval num_timesteps=40000, episode_reward=-3.90 +/- 1.45
Episode length: 4.90 +/- 1.45
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=44000, episode_reward=-3.30 +/- 0.64
Episode length: 4.30 +/- 0.64
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=48000, episode_reward=-3.10 +/- 0.30
Episode length: 4.10 +/- 0.30
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=52000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=56000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
Eval num_timesteps=60000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
TRIAL Finished: Objective Trial Time
[32m[I 2022-07-01 20:44:54,171][0m Trial 36 finished with value: -3.0 and parameters: {'gamma': 0.9732706540520775, 'tau': 0.003363744776134631, 'lr': 0.0020563912542552183, 'ent_coef': 0.0017881452564262167, 'batch_size_num': 10, 'learning_starts': 8, 'action_noise_int': 3, 'n_envs': 4}. Best is trial 20 with value: -3.0.[0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.0013686098437408084, 'gamma': 0.9738686598446614, 'tau': 0.0017419796432188261, 'learning_rate': 0.0021563916806258145, 'batch_size': 1024, 'learning_starts': 8000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  3
action_noise:  0.10000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=3000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=6000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=9000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=15000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=18000, episode_reward=-49.70 +/- 0.90
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=21000, episode_reward=-47.70 +/- 3.77
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=24000, episode_reward=-47.50 +/- 7.50
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=27000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=30000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
[32m[I 2022-07-01 20:54:32,491][0m Trial 37 pruned. [0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.0006489696868996307, 'gamma': 0.9732764917447252, 'tau': 0.0034199110793127797, 'learning_rate': 0.0013895465138659567, 'batch_size': 1024, 'learning_starts': 10000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  5
action_noise:  0.10000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=5000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=10000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=15000, episode_reward=-49.50 +/- 1.50
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=20000, episode_reward=-48.20 +/- 5.40
Episode length: 48.30 +/- 5.10
Success rate: 10.00%
New best mean reward!
Eval num_timesteps=25000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=30000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=35000, episode_reward=-45.30 +/- 14.10
Episode length: 45.40 +/- 13.80
Success rate: 10.00%
New best mean reward!
Eval num_timesteps=40000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=45000, episode_reward=-36.90 +/- 20.13
Episode length: 37.20 +/- 19.68
Success rate: 30.00%
New best mean reward!
Eval num_timesteps=50000, episode_reward=-29.10 +/- 20.92
Episode length: 29.60 +/- 20.42
Success rate: 50.00%
New best mean reward!
Eval num_timesteps=55000, episode_reward=-19.80 +/- 20.84
Episode length: 20.60 +/- 20.33
Success rate: 70.00%
New best mean reward!
Eval num_timesteps=60000, episode_reward=-7.70 +/- 14.10
Episode length: 8.60 +/- 13.80
Success rate: 90.00%
New best mean reward!
TRIAL Finished: Objective Trial Time
[32m[I 2022-07-01 21:08:36,509][0m Trial 38 finished with value: -7.7 and parameters: {'gamma': 0.9732764917447252, 'tau': 0.0034199110793127797, 'lr': 0.0013895465138659567, 'ent_coef': 0.0006489696868996307, 'batch_size_num': 10, 'learning_starts': 10, 'action_noise_int': 3, 'n_envs': 5}. Best is trial 20 with value: -3.0.[0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.02137409037364704, 'gamma': 0.9777478863456517, 'tau': 0.002368810978724447, 'learning_rate': 0.0029771527439029014, 'batch_size': 512, 'learning_starts': 10000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  3
action_noise:  0.15000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=3000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=6000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=9000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=15000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=18000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=21000, episode_reward=-49.80 +/- 0.60
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=24000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=27000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=30000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
[32m[I 2022-07-01 21:14:43,253][0m Trial 39 pruned. [0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.002049929721284232, 'gamma': 0.9703290456891835, 'tau': 0.004265033381385496, 'learning_rate': 0.002008578242948689, 'batch_size': 1024, 'learning_starts': 8000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  4
action_noise:  0.10000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=4000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=8000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-45.90 +/- 12.30
Episode length: 46.00 +/- 12.00
Success rate: 10.00%
New best mean reward!
Eval num_timesteps=16000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=20000, episode_reward=-45.60 +/- 13.20
Episode length: 45.70 +/- 12.90
Success rate: 10.00%
New best mean reward!
Eval num_timesteps=24000, episode_reward=-45.40 +/- 13.80
Episode length: 45.50 +/- 13.50
Success rate: 10.00%
New best mean reward!
Eval num_timesteps=28000, episode_reward=-27.80 +/- 22.21
Episode length: 28.30 +/- 21.71
Success rate: 50.00%
New best mean reward!
Eval num_timesteps=32000, episode_reward=-5.60 +/- 5.30
Episode length: 9.40 +/- 13.60
Success rate: 90.00%
New best mean reward!
Eval num_timesteps=36000, episode_reward=-17.40 +/- 21.35
Episode length: 18.10 +/- 20.89
Success rate: 70.00%
Eval num_timesteps=40000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=44000, episode_reward=-3.20 +/- 0.40
Episode length: 4.20 +/- 0.40
Success rate: 100.00%
Eval num_timesteps=48000, episode_reward=-3.10 +/- 0.30
Episode length: 4.10 +/- 0.30
Success rate: 100.00%
Eval num_timesteps=52000, episode_reward=-3.10 +/- 0.30
Episode length: 4.10 +/- 0.30
Success rate: 100.00%
Eval num_timesteps=56000, episode_reward=-3.10 +/- 0.30
Episode length: 4.10 +/- 0.30
Success rate: 100.00%
Eval num_timesteps=60000, episode_reward=-3.10 +/- 0.30
Episode length: 4.10 +/- 0.30
Success rate: 100.00%
TRIAL Finished: Objective Trial Time
[32m[I 2022-07-01 21:28:21,977][0m Trial 40 finished with value: -3.1 and parameters: {'gamma': 0.9703290456891835, 'tau': 0.004265033381385496, 'lr': 0.002008578242948689, 'ent_coef': 0.002049929721284232, 'batch_size_num': 10, 'learning_starts': 8, 'action_noise_int': 3, 'n_envs': 4}. Best is trial 20 with value: -3.0.[0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.004653299593267241, 'gamma': 0.9759135963050716, 'tau': 0.00333861256632187, 'learning_rate': 0.001031670976427711, 'batch_size': 1024, 'learning_starts': 9000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  4
action_noise:  0.15000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=4000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=8000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=16000, episode_reward=-49.30 +/- 2.10
Episode length: 49.40 +/- 1.80
Success rate: 10.00%
New best mean reward!
Eval num_timesteps=20000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=24000, episode_reward=-45.50 +/- 13.50
Episode length: 45.60 +/- 13.20
Success rate: 10.00%
New best mean reward!
Eval num_timesteps=28000, episode_reward=-32.00 +/- 22.07
Episode length: 32.40 +/- 21.58
Success rate: 40.00%
New best mean reward!
Eval num_timesteps=32000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=36000, episode_reward=-23.20 +/- 22.05
Episode length: 23.80 +/- 21.57
Success rate: 60.00%
New best mean reward!
Eval num_timesteps=40000, episode_reward=-14.60 +/- 13.92
Episode length: 18.50 +/- 17.18
Success rate: 80.00%
New best mean reward!
Eval num_timesteps=44000, episode_reward=-3.30 +/- 0.46
Episode length: 4.30 +/- 0.46
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=48000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=52000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
Eval num_timesteps=56000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
Eval num_timesteps=60000, episode_reward=-3.20 +/- 0.40
Episode length: 4.20 +/- 0.40
Success rate: 100.00%
TRIAL Finished: Objective Trial Time
[32m[I 2022-07-01 21:43:12,991][0m Trial 41 finished with value: -3.2 and parameters: {'gamma': 0.9759135963050716, 'tau': 0.00333861256632187, 'lr': 0.001031670976427711, 'ent_coef': 0.004653299593267241, 'batch_size_num': 10, 'learning_starts': 9, 'action_noise_int': 4, 'n_envs': 4}. Best is trial 20 with value: -3.0.[0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.009525666447452199, 'gamma': 0.975665963802602, 'tau': 0.0020504872891727605, 'learning_rate': 0.0020787854630737517, 'batch_size': 1024, 'learning_starts': 9000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  4
action_noise:  0.10000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=4000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=8000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=16000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=20000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=24000, episode_reward=-40.80 +/- 18.40
Episode length: 41.00 +/- 18.00
Success rate: 20.00%
New best mean reward!
Eval num_timesteps=28000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=32000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=36000, episode_reward=-40.60 +/- 18.80
Episode length: 40.80 +/- 18.40
Success rate: 20.00%
New best mean reward!
[32m[I 2022-07-01 21:51:24,846][0m Trial 42 pruned. [0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.004346306166101498, 'gamma': 0.9808051043102545, 'tau': 0.003725136182024229, 'learning_rate': 0.0007225989374306367, 'batch_size': 512, 'learning_starts': 10000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  4
action_noise:  0.15000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=4000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=8000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=16000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=20000, episode_reward=-45.30 +/- 14.10
Episode length: 45.40 +/- 13.80
Success rate: 10.00%
New best mean reward!
Eval num_timesteps=24000, episode_reward=-34.30 +/- 19.99
Episode length: 34.70 +/- 19.51
Success rate: 40.00%
New best mean reward!
Eval num_timesteps=28000, episode_reward=-33.50 +/- 20.96
Episode length: 33.90 +/- 20.49
Success rate: 40.00%
New best mean reward!
Eval num_timesteps=32000, episode_reward=-33.30 +/- 20.56
Episode length: 33.70 +/- 20.07
Success rate: 40.00%
New best mean reward!
Eval num_timesteps=36000, episode_reward=-31.40 +/- 22.79
Episode length: 31.80 +/- 22.30
Success rate: 40.00%
New best mean reward!
Eval num_timesteps=40000, episode_reward=-22.20 +/- 22.71
Episode length: 22.80 +/- 22.22
Success rate: 60.00%
New best mean reward!
Eval num_timesteps=44000, episode_reward=-8.10 +/- 13.98
Episode length: 9.00 +/- 13.68
Success rate: 90.00%
New best mean reward!
Eval num_timesteps=48000, episode_reward=-3.10 +/- 0.30
Episode length: 4.10 +/- 0.30
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=52000, episode_reward=-3.20 +/- 0.40
Episode length: 4.20 +/- 0.40
Success rate: 100.00%
Eval num_timesteps=56000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=60000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
TRIAL Finished: Objective Trial Time
[32m[I 2022-07-01 22:02:46,726][0m Trial 43 finished with value: -3.0 and parameters: {'gamma': 0.9808051043102545, 'tau': 0.003725136182024229, 'lr': 0.0007225989374306367, 'ent_coef': 0.004346306166101498, 'batch_size_num': 9, 'learning_starts': 10, 'action_noise_int': 4, 'n_envs': 4}. Best is trial 20 with value: -3.0.[0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.012540073385161159, 'gamma': 0.9821884931037526, 'tau': 0.00369087171795244, 'learning_rate': 0.0003210713939241141, 'batch_size': 512, 'learning_starts': 10000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  4
action_noise:  0.15000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=4000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=8000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=16000, episode_reward=-46.50 +/- 7.42
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=20000, episode_reward=-41.40 +/- 12.13
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=24000, episode_reward=-49.80 +/- 0.60
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=28000, episode_reward=-41.50 +/- 14.19
Episode length: 43.30 +/- 13.71
Success rate: 20.00%
Eval num_timesteps=32000, episode_reward=-43.80 +/- 7.69
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=36000, episode_reward=-45.20 +/- 12.53
Episode length: 45.90 +/- 12.30
Success rate: 10.00%
[32m[I 2022-07-01 22:09:24,330][0m Trial 44 pruned. [0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.0007552001209921338, 'gamma': 0.9803843816006117, 'tau': 0.0026725102291068506, 'learning_rate': 0.0006826801758775682, 'batch_size': 512, 'learning_starts': 8000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  3
action_noise:  0.10000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=3000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=6000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=9000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=15000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=18000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=21000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=24000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=27000, episode_reward=-46.20 +/- 8.82
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
[32m[I 2022-07-01 22:14:12,378][0m Trial 45 pruned. [0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.0024418808178398124, 'gamma': 0.9833118499801266, 'tau': 0.004139654402532814, 'learning_rate': 0.0009666451762322044, 'batch_size': 512, 'learning_starts': 5000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  4
action_noise:  0.10000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=4000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=8000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-48.10 +/- 4.53
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=16000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=20000, episode_reward=-43.40 +/- 14.61
Episode length: 43.60 +/- 14.25
Success rate: 20.00%
New best mean reward!
Eval num_timesteps=24000, episode_reward=-40.40 +/- 16.96
Episode length: 40.70 +/- 16.57
Success rate: 30.00%
New best mean reward!
Eval num_timesteps=28000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=32000, episode_reward=-46.20 +/- 11.40
Episode length: 46.30 +/- 11.10
Success rate: 10.00%
Eval num_timesteps=36000, episode_reward=-31.50 +/- 22.67
Episode length: 31.90 +/- 22.18
Success rate: 40.00%
New best mean reward!
Eval num_timesteps=40000, episode_reward=-27.50 +/- 22.60
Episode length: 28.00 +/- 22.10
Success rate: 50.00%
New best mean reward!
Eval num_timesteps=44000, episode_reward=-8.00 +/- 14.01
Episode length: 9.00 +/- 13.68
Success rate: 90.00%
New best mean reward!
Eval num_timesteps=48000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=52000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
Eval num_timesteps=56000, episode_reward=-3.20 +/- 0.40
Episode length: 4.20 +/- 0.40
Success rate: 100.00%
Eval num_timesteps=60000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
TRIAL Finished: Objective Trial Time
[32m[I 2022-07-01 22:25:20,569][0m Trial 46 finished with value: -3.0 and parameters: {'gamma': 0.9833118499801266, 'tau': 0.004139654402532814, 'lr': 0.0009666451762322044, 'ent_coef': 0.0024418808178398124, 'batch_size_num': 9, 'learning_starts': 5, 'action_noise_int': 3, 'n_envs': 4}. Best is trial 20 with value: -3.0.[0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.004966070175869347, 'gamma': 0.9718375873999623, 'tau': 0.003196256326546768, 'learning_rate': 0.0025459610054708592, 'batch_size': 1024, 'learning_starts': 6000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  3
action_noise:  0.15000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=3000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=6000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=9000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=15000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=18000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=21000, episode_reward=-49.90 +/- 0.30
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=24000, episode_reward=-45.30 +/- 14.10
Episode length: 45.40 +/- 13.80
Success rate: 10.00%
New best mean reward!
Eval num_timesteps=27000, episode_reward=-49.50 +/- 1.50
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=30000, episode_reward=-40.50 +/- 18.75
Episode length: 40.80 +/- 18.40
Success rate: 20.00%
New best mean reward!
[32m[I 2022-07-01 22:33:28,977][0m Trial 47 pruned. [0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.0023924907658313325, 'gamma': 0.9830635593014901, 'tau': 0.004332897132643903, 'learning_rate': 0.0004510571119129241, 'batch_size': 512, 'learning_starts': 5000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  5
action_noise:  0.10000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=5000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=10000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=15000, episode_reward=-47.60 +/- 7.20
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=20000, episode_reward=-46.10 +/- 7.96
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=25000, episode_reward=-45.50 +/- 13.50
Episode length: 45.60 +/- 13.20
Success rate: 10.00%
New best mean reward!
Eval num_timesteps=30000, episode_reward=-41.80 +/- 16.41
Episode length: 42.00 +/- 16.01
Success rate: 20.00%
New best mean reward!
Eval num_timesteps=35000, episode_reward=-25.00 +/- 19.65
Episode length: 25.70 +/- 19.27
Success rate: 70.00%
New best mean reward!
Eval num_timesteps=40000, episode_reward=-22.30 +/- 22.62
Episode length: 22.90 +/- 22.13
Success rate: 60.00%
New best mean reward!
Eval num_timesteps=45000, episode_reward=-24.60 +/- 21.20
Episode length: 25.20 +/- 20.72
Success rate: 60.00%
Eval num_timesteps=50000, episode_reward=-3.20 +/- 0.40
Episode length: 4.20 +/- 0.40
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=55000, episode_reward=-3.30 +/- 0.46
Episode length: 4.30 +/- 0.46
Success rate: 100.00%
Eval num_timesteps=60000, episode_reward=-3.40 +/- 0.80
Episode length: 4.40 +/- 0.80
Success rate: 100.00%
TRIAL Finished: Objective Trial Time
[32m[I 2022-07-01 22:43:12,937][0m Trial 48 finished with value: -3.4 and parameters: {'gamma': 0.9830635593014901, 'tau': 0.004332897132643903, 'lr': 0.0004510571119129241, 'ent_coef': 0.0023924907658313325, 'batch_size_num': 9, 'learning_starts': 5, 'action_noise_int': 3, 'n_envs': 5}. Best is trial 20 with value: -3.0.[0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.03478535924080441, 'gamma': 0.9738914840369669, 'tau': 0.0035198925233876624, 'learning_rate': 0.0019165819909388346, 'batch_size': 1024, 'learning_starts': 7000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  4
action_noise:  0.15000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=4000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=8000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=16000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=20000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=24000, episode_reward=-49.90 +/- 0.30
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=28000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=32000, episode_reward=-49.50 +/- 1.50
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=36000, episode_reward=-48.20 +/- 4.24
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
[32m[I 2022-07-01 22:50:56,866][0m Trial 49 pruned. [0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.004384953035906576, 'gamma': 0.9849900548378915, 'tau': 0.0039019155776343004, 'learning_rate': 0.0006995396700979364, 'batch_size': 512, 'learning_starts': 6000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  5
action_noise:  0.15000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=5000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=10000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=15000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=20000, episode_reward=-49.70 +/- 0.64
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=25000, episode_reward=-40.60 +/- 18.80
Episode length: 40.80 +/- 18.40
Success rate: 20.00%
New best mean reward!
Eval num_timesteps=30000, episode_reward=-45.50 +/- 13.50
Episode length: 45.60 +/- 13.20
Success rate: 10.00%
Eval num_timesteps=35000, episode_reward=-31.20 +/- 23.03
Episode length: 31.60 +/- 22.54
Success rate: 40.00%
New best mean reward!
Eval num_timesteps=40000, episode_reward=-36.00 +/- 21.39
Episode length: 36.30 +/- 20.93
Success rate: 30.00%
Eval num_timesteps=45000, episode_reward=-23.60 +/- 21.81
Episode length: 24.20 +/- 21.33
Success rate: 60.00%
New best mean reward!
Eval num_timesteps=50000, episode_reward=-9.40 +/- 13.75
Episode length: 10.30 +/- 13.45
Success rate: 90.00%
New best mean reward!
Eval num_timesteps=55000, episode_reward=-3.60 +/- 1.80
Episode length: 4.60 +/- 1.80
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=60000, episode_reward=-3.40 +/- 0.80
Episode length: 4.40 +/- 0.80
Success rate: 100.00%
New best mean reward!
TRIAL Finished: Objective Trial Time
[32m[I 2022-07-01 23:00:23,806][0m Trial 50 finished with value: -3.4 and parameters: {'gamma': 0.9849900548378915, 'tau': 0.0039019155776343004, 'lr': 0.0006995396700979364, 'ent_coef': 0.004384953035906576, 'batch_size_num': 9, 'learning_starts': 6, 'action_noise_int': 4, 'n_envs': 5}. Best is trial 20 with value: -3.0.[0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.009370159347080262, 'gamma': 0.9780438421250592, 'tau': 0.00294705834610626, 'learning_rate': 0.0014269918240478433, 'batch_size': 1024, 'learning_starts': 9000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  4
action_noise:  0.15000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=4000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=8000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=16000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=20000, episode_reward=-45.90 +/- 12.30
Episode length: 46.00 +/- 12.00
Success rate: 10.00%
New best mean reward!
Eval num_timesteps=24000, episode_reward=-45.50 +/- 13.50
Episode length: 45.60 +/- 13.20
Success rate: 10.00%
New best mean reward!
Eval num_timesteps=28000, episode_reward=-40.80 +/- 18.41
Episode length: 41.00 +/- 18.01
Success rate: 20.00%
New best mean reward!
Eval num_timesteps=32000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=36000, episode_reward=-41.00 +/- 18.00
Episode length: 41.20 +/- 17.60
Success rate: 20.00%
[32m[I 2022-07-01 23:08:17,531][0m Trial 51 pruned. [0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.0024772369365380098, 'gamma': 0.9820673860791725, 'tau': 0.004514664693564394, 'learning_rate': 0.0005907411046824813, 'batch_size': 512, 'learning_starts': 5000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  4
action_noise:  0.10000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=4000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=8000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=16000, episode_reward=-47.90 +/- 6.30
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=20000, episode_reward=-46.20 +/- 11.40
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=24000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=28000, episode_reward=-35.10 +/- 18.91
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=32000, episode_reward=-34.90 +/- 19.04
Episode length: 41.50 +/- 17.01
Success rate: 20.00%
New best mean reward!
Eval num_timesteps=36000, episode_reward=-34.10 +/- 19.71
Episode length: 34.70 +/- 18.94
Success rate: 40.00%
New best mean reward!
Eval num_timesteps=40000, episode_reward=-14.40 +/- 17.84
Episode length: 15.20 +/- 17.44
Success rate: 80.00%
New best mean reward!
Eval num_timesteps=44000, episode_reward=-22.10 +/- 22.79
Episode length: 22.70 +/- 22.30
Success rate: 60.00%
Eval num_timesteps=48000, episode_reward=-3.20 +/- 0.60
Episode length: 4.20 +/- 0.60
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=52000, episode_reward=-3.80 +/- 1.54
Episode length: 4.80 +/- 1.54
Success rate: 100.00%
Eval num_timesteps=56000, episode_reward=-7.80 +/- 14.07
Episode length: 8.70 +/- 13.77
Success rate: 90.00%
Eval num_timesteps=60000, episode_reward=-7.70 +/- 14.10
Episode length: 8.60 +/- 13.80
Success rate: 90.00%
TRIAL Finished: Objective Trial Time
[32m[I 2022-07-01 23:20:05,887][0m Trial 52 finished with value: -7.7 and parameters: {'gamma': 0.9820673860791725, 'tau': 0.004514664693564394, 'lr': 0.0005907411046824813, 'ent_coef': 0.0024772369365380098, 'batch_size_num': 9, 'learning_starts': 5, 'action_noise_int': 3, 'n_envs': 4}. Best is trial 20 with value: -3.0.[0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.01888582533495285, 'gamma': 0.9748186395231648, 'tau': 0.0030578415871251392, 'learning_rate': 0.0015580662646850847, 'batch_size': 1024, 'learning_starts': 9000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  4
action_noise:  0.15000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=4000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=8000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-45.50 +/- 13.50
Episode length: 45.60 +/- 13.20
Success rate: 10.00%
New best mean reward!
Eval num_timesteps=16000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=20000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=24000, episode_reward=-43.50 +/- 13.00
Episode length: 43.70 +/- 12.60
Success rate: 20.00%
New best mean reward!
Eval num_timesteps=28000, episode_reward=-41.30 +/- 17.44
Episode length: 41.50 +/- 17.04
Success rate: 20.00%
New best mean reward!
Eval num_timesteps=32000, episode_reward=-45.50 +/- 13.50
Episode length: 45.60 +/- 13.20
Success rate: 10.00%
Eval num_timesteps=36000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
[32m[I 2022-07-01 23:27:59,976][0m Trial 53 pruned. [0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.003892245439110143, 'gamma': 0.9810465246470562, 'tau': 0.0038024459057355996, 'learning_rate': 0.0010085217344676404, 'batch_size': 512, 'learning_starts': 10000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  4
action_noise:  0.15000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=4000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=8000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=16000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=20000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=24000, episode_reward=-41.30 +/- 17.40
Episode length: 41.60 +/- 16.80
Success rate: 20.00%
New best mean reward!
Eval num_timesteps=28000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=32000, episode_reward=-34.80 +/- 18.56
Episode length: 37.70 +/- 18.98
Success rate: 30.00%
New best mean reward!
Eval num_timesteps=36000, episode_reward=-36.40 +/- 20.79
Episode length: 36.70 +/- 20.33
Success rate: 30.00%
[32m[I 2022-07-01 23:33:45,321][0m Trial 54 pruned. [0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.033204440642600315, 'gamma': 0.9801500369799383, 'tau': 0.004899451243387511, 'learning_rate': 0.0007624301434035946, 'batch_size': 512, 'learning_starts': 10000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  4
action_noise:  0.15000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=4000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=8000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=16000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=20000, episode_reward=-41.80 +/- 16.50
Episode length: 42.00 +/- 16.10
Success rate: 20.00%
New best mean reward!
Eval num_timesteps=24000, episode_reward=-41.30 +/- 17.40
Episode length: 41.50 +/- 17.00
Success rate: 20.00%
New best mean reward!
Eval num_timesteps=28000, episode_reward=-36.10 +/- 21.23
Episode length: 36.40 +/- 20.78
Success rate: 30.00%
New best mean reward!
Eval num_timesteps=32000, episode_reward=-36.10 +/- 21.23
Episode length: 36.40 +/- 20.78
Success rate: 30.00%
Eval num_timesteps=36000, episode_reward=-19.20 +/- 20.24
Episode length: 20.00 +/- 19.73
Success rate: 70.00%
New best mean reward!
Eval num_timesteps=40000, episode_reward=-22.20 +/- 22.71
Episode length: 22.80 +/- 22.23
Success rate: 60.00%
Eval num_timesteps=44000, episode_reward=-3.10 +/- 0.30
Episode length: 4.10 +/- 0.30
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=48000, episode_reward=-3.10 +/- 0.30
Episode length: 4.10 +/- 0.30
Success rate: 100.00%
Eval num_timesteps=52000, episode_reward=-3.20 +/- 0.40
Episode length: 4.20 +/- 0.40
Success rate: 100.00%
Eval num_timesteps=56000, episode_reward=-3.10 +/- 0.30
Episode length: 4.10 +/- 0.30
Success rate: 100.00%
Eval num_timesteps=60000, episode_reward=-3.20 +/- 0.40
Episode length: 4.20 +/- 0.40
Success rate: 100.00%
TRIAL Finished: Objective Trial Time
[32m[I 2022-07-01 23:43:43,272][0m Trial 55 finished with value: -3.2 and parameters: {'gamma': 0.9801500369799383, 'tau': 0.004899451243387511, 'lr': 0.0007624301434035946, 'ent_coef': 0.033204440642600315, 'batch_size_num': 9, 'learning_starts': 10, 'action_noise_int': 4, 'n_envs': 4}. Best is trial 20 with value: -3.0.[0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.00028535261983986905, 'gamma': 0.9833824625195686, 'tau': 0.00059830257978255, 'learning_rate': 0.0009644264951621336, 'batch_size': 512, 'learning_starts': 10000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  4
action_noise:  0.10000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=4000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=8000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=16000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=20000, episode_reward=-43.70 +/- 14.17
Episode length: 45.50 +/- 13.50
Success rate: 10.00%
New best mean reward!
Eval num_timesteps=24000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=28000, episode_reward=-46.00 +/- 12.00
Episode length: 46.10 +/- 11.70
Success rate: 10.00%
Eval num_timesteps=32000, episode_reward=-44.30 +/- 12.74
Episode length: 46.00 +/- 12.00
Success rate: 10.00%
Eval num_timesteps=36000, episode_reward=-36.60 +/- 20.48
Episode length: 36.90 +/- 20.02
Success rate: 30.00%
New best mean reward!
[32m[I 2022-07-01 23:49:31,166][0m Trial 56 pruned. [0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.0010364592712661682, 'gamma': 0.9785854892694688, 'tau': 0.004418812411783941, 'learning_rate': 0.000519769339053315, 'batch_size': 256, 'learning_starts': 8000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  5
action_noise:  0.05

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=5000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=10000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=15000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=20000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=25000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=30000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=35000, episode_reward=-48.80 +/- 3.60
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=40000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=45000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
[32m[I 2022-07-01 23:54:41,809][0m Trial 57 pruned. [0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.00321410864598886, 'gamma': 0.985348567568347, 'tau': 0.0012580722819653654, 'learning_rate': 0.0004525027723649089, 'batch_size': 256, 'learning_starts': 5000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  2
action_noise:  0.10000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=2000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=4000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=6000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=8000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=10000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=14000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=16000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=18000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
[32m[I 2022-07-01 23:58:13,236][0m Trial 58 pruned. [0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.001651548135275988, 'gamma': 0.9817191946613574, 'tau': 0.003714566019846699, 'learning_rate': 0.0007755301654237512, 'batch_size': 256, 'learning_starts': 7000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  3
action_noise:  0.10000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=3000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=6000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=9000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-45.30 +/- 14.10
Episode length: 45.40 +/- 13.80
Success rate: 10.00%
New best mean reward!
Eval num_timesteps=15000, episode_reward=-36.20 +/- 21.08
Episode length: 36.50 +/- 20.62
Success rate: 30.00%
New best mean reward!
Eval num_timesteps=18000, episode_reward=-40.90 +/- 18.20
Episode length: 41.10 +/- 17.80
Success rate: 20.00%
Eval num_timesteps=21000, episode_reward=-38.90 +/- 17.18
Episode length: 39.20 +/- 16.73
Success rate: 30.00%
Eval num_timesteps=24000, episode_reward=-31.90 +/- 22.17
Episode length: 32.30 +/- 21.68
Success rate: 40.00%
New best mean reward!
Eval num_timesteps=27000, episode_reward=-12.30 +/- 13.15
Episode length: 13.20 +/- 12.87
Success rate: 90.00%
New best mean reward!
Eval num_timesteps=30000, episode_reward=-32.10 +/- 21.96
Episode length: 32.50 +/- 21.47
Success rate: 40.00%
Eval num_timesteps=33000, episode_reward=-32.50 +/- 21.62
Episode length: 32.90 +/- 21.14
Success rate: 40.00%
Eval num_timesteps=36000, episode_reward=-10.00 +/- 13.49
Episode length: 10.90 +/- 13.19
Success rate: 90.00%
New best mean reward!
Eval num_timesteps=39000, episode_reward=-23.70 +/- 21.87
Episode length: 24.30 +/- 21.39
Success rate: 60.00%
Eval num_timesteps=42000, episode_reward=-4.30 +/- 1.35
Episode length: 5.30 +/- 1.35
Success rate: 100.00%
New best mean reward!
[32m[I 2022-07-02 00:05:58,054][0m Trial 59 pruned. [0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.005939107912801819, 'gamma': 0.9798795994632156, 'tau': 0.0021354348385105608, 'learning_rate': 0.0012823395487020538, 'batch_size': 256, 'learning_starts': 8000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  5
action_noise:  0.05

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=5000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=10000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=15000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=20000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=25000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=30000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=35000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=40000, episode_reward=-41.60 +/- 16.89
Episode length: 41.80 +/- 16.50
Success rate: 20.00%
New best mean reward!
Eval num_timesteps=45000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
[32m[I 2022-07-02 00:11:43,315][0m Trial 60 pruned. [0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.008517954270354814, 'gamma': 0.9762928281208554, 'tau': 0.003965688000642549, 'learning_rate': 0.0025534356925455705, 'batch_size': 512, 'learning_starts': 9000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  4
action_noise:  0.15000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=4000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=8000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=16000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=20000, episode_reward=-45.30 +/- 14.10
Episode length: 45.40 +/- 13.80
Success rate: 10.00%
New best mean reward!
Eval num_timesteps=24000, episode_reward=-45.10 +/- 14.05
Episode length: 45.40 +/- 13.80
Success rate: 10.00%
New best mean reward!
Eval num_timesteps=28000, episode_reward=-45.30 +/- 14.10
Episode length: 45.40 +/- 13.80
Success rate: 10.00%
Eval num_timesteps=32000, episode_reward=-32.90 +/- 21.31
Episode length: 33.30 +/- 20.83
Success rate: 40.00%
New best mean reward!
Eval num_timesteps=36000, episode_reward=-17.90 +/- 21.03
Episode length: 18.60 +/- 20.57
Success rate: 70.00%
New best mean reward!
Eval num_timesteps=40000, episode_reward=-19.60 +/- 20.04
Episode length: 20.30 +/- 19.58
Success rate: 70.00%
Eval num_timesteps=44000, episode_reward=-3.30 +/- 0.64
Episode length: 4.30 +/- 0.64
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=48000, episode_reward=-3.10 +/- 0.30
Episode length: 4.10 +/- 0.30
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=52000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=56000, episode_reward=-3.20 +/- 0.40
Episode length: 4.20 +/- 0.40
Success rate: 100.00%
Eval num_timesteps=60000, episode_reward=-3.10 +/- 0.30
Episode length: 4.10 +/- 0.30
Success rate: 100.00%
TRIAL Finished: Objective Trial Time
[32m[I 2022-07-02 00:21:58,026][0m Trial 61 finished with value: -3.1 and parameters: {'gamma': 0.9762928281208554, 'tau': 0.003965688000642549, 'lr': 0.0025534356925455705, 'ent_coef': 0.008517954270354814, 'batch_size_num': 9, 'learning_starts': 9, 'action_noise_int': 4, 'n_envs': 4}. Best is trial 20 with value: -3.0.[0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.012831103983361401, 'gamma': 0.9778536335234111, 'tau': 0.003475887357759185, 'learning_rate': 0.001834269009414798, 'batch_size': 1024, 'learning_starts': 9000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  4
action_noise:  0.15000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=4000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=8000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=16000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=20000, episode_reward=-36.30 +/- 20.95
Episode length: 36.60 +/- 20.49
Success rate: 30.00%
New best mean reward!
Eval num_timesteps=24000, episode_reward=-18.60 +/- 20.68
Episode length: 19.30 +/- 20.23
Success rate: 70.00%
New best mean reward!
Eval num_timesteps=28000, episode_reward=-13.40 +/- 18.33
Episode length: 14.20 +/- 17.93
Success rate: 80.00%
New best mean reward!
Eval num_timesteps=32000, episode_reward=-3.20 +/- 0.40
Episode length: 4.20 +/- 0.40
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=36000, episode_reward=-3.30 +/- 0.46
Episode length: 4.30 +/- 0.46
Success rate: 100.00%
Eval num_timesteps=40000, episode_reward=-3.10 +/- 0.30
Episode length: 4.10 +/- 0.30
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=44000, episode_reward=-3.10 +/- 0.30
Episode length: 4.10 +/- 0.30
Success rate: 100.00%
Eval num_timesteps=48000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=52000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
Eval num_timesteps=56000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
Eval num_timesteps=60000, episode_reward=-3.10 +/- 0.30
Episode length: 4.10 +/- 0.30
Success rate: 100.00%
TRIAL Finished: Objective Trial Time
[32m[I 2022-07-02 00:35:43,286][0m Trial 62 finished with value: -3.1 and parameters: {'gamma': 0.9778536335234111, 'tau': 0.003475887357759185, 'lr': 0.001834269009414798, 'ent_coef': 0.012831103983361401, 'batch_size_num': 10, 'learning_starts': 9, 'action_noise_int': 4, 'n_envs': 4}. Best is trial 20 with value: -3.0.[0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.016273386022876403, 'gamma': 0.9770612467521481, 'tau': 0.003244608172881376, 'learning_rate': 0.002316972073592029, 'batch_size': 1024, 'learning_starts': 10000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  4
action_noise:  0.10000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=4000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=8000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=16000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=20000, episode_reward=-41.10 +/- 17.84
Episode length: 41.30 +/- 17.44
Success rate: 20.00%
New best mean reward!
Eval num_timesteps=24000, episode_reward=-34.70 +/- 19.06
Episode length: 35.10 +/- 18.58
Success rate: 40.00%
New best mean reward!
Eval num_timesteps=28000, episode_reward=-40.60 +/- 18.80
Episode length: 40.80 +/- 18.40
Success rate: 20.00%
Eval num_timesteps=32000, episode_reward=-47.40 +/- 7.80
Episode length: 47.50 +/- 7.50
Success rate: 10.00%
Eval num_timesteps=36000, episode_reward=-33.40 +/- 21.20
Episode length: 33.90 +/- 20.59
Success rate: 40.00%
New best mean reward!
Eval num_timesteps=40000, episode_reward=-3.10 +/- 0.30
Episode length: 4.10 +/- 0.30
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=44000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=48000, episode_reward=-3.80 +/- 2.40
Episode length: 4.80 +/- 2.40
Success rate: 100.00%
Eval num_timesteps=52000, episode_reward=-3.10 +/- 0.30
Episode length: 4.10 +/- 0.30
Success rate: 100.00%
Eval num_timesteps=56000, episode_reward=-3.10 +/- 0.30
Episode length: 4.10 +/- 0.30
Success rate: 100.00%
Eval num_timesteps=60000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
TRIAL Finished: Objective Trial Time
[32m[I 2022-07-02 00:49:42,504][0m Trial 63 finished with value: -3.0 and parameters: {'gamma': 0.9770612467521481, 'tau': 0.003244608172881376, 'lr': 0.002316972073592029, 'ent_coef': 0.016273386022876403, 'batch_size_num': 10, 'learning_starts': 10, 'action_noise_int': 3, 'n_envs': 4}. Best is trial 20 with value: -3.0.[0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.01810331725628196, 'gamma': 0.9880829020660962, 'tau': 0.0024844126974862416, 'learning_rate': 0.002302004912613778, 'batch_size': 1024, 'learning_starts': 10000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  4
action_noise:  0.10000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=4000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=8000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=16000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=20000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=24000, episode_reward=-40.60 +/- 18.80
Episode length: 40.80 +/- 18.40
Success rate: 20.00%
New best mean reward!
Eval num_timesteps=28000, episode_reward=-45.30 +/- 14.10
Episode length: 45.40 +/- 13.80
Success rate: 10.00%
Eval num_timesteps=32000, episode_reward=-36.60 +/- 20.48
Episode length: 36.90 +/- 20.02
Success rate: 30.00%
New best mean reward!
Eval num_timesteps=36000, episode_reward=-31.20 +/- 23.03
Episode length: 31.60 +/- 22.54
Success rate: 40.00%
New best mean reward!
Eval num_timesteps=40000, episode_reward=-13.70 +/- 18.28
Episode length: 14.50 +/- 17.88
Success rate: 80.00%
New best mean reward!
Eval num_timesteps=44000, episode_reward=-3.50 +/- 0.67
Episode length: 4.50 +/- 0.67
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=48000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=52000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
Eval num_timesteps=56000, episode_reward=-3.20 +/- 0.40
Episode length: 4.20 +/- 0.40
Success rate: 100.00%
Eval num_timesteps=60000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
TRIAL Finished: Objective Trial Time
[32m[I 2022-07-02 01:03:50,430][0m Trial 64 finished with value: -3.0 and parameters: {'gamma': 0.9880829020660962, 'tau': 0.0024844126974862416, 'lr': 0.002302004912613778, 'ent_coef': 0.01810331725628196, 'batch_size_num': 10, 'learning_starts': 10, 'action_noise_int': 3, 'n_envs': 4}. Best is trial 20 with value: -3.0.[0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.00667603706886869, 'gamma': 0.9772175556160724, 'tau': 0.003178563835685421, 'learning_rate': 0.00018115638751514614, 'batch_size': 1024, 'learning_starts': 10000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  4
action_noise:  0.10000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=4000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=8000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=16000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=20000, episode_reward=-47.70 +/- 5.02
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=24000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=28000, episode_reward=-48.90 +/- 3.30
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=32000, episode_reward=-49.30 +/- 2.10
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=36000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
[32m[I 2022-07-02 01:11:26,761][0m Trial 65 pruned. [0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.022134167866512802, 'gamma': 0.987789040056161, 'tau': 0.0023948207923918173, 'learning_rate': 0.0015240089734794057, 'batch_size': 1024, 'learning_starts': 10000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  4
action_noise:  0.15000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=4000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=8000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=16000, episode_reward=-46.30 +/- 11.10
Episode length: 46.40 +/- 10.80
Success rate: 10.00%
New best mean reward!
Eval num_timesteps=20000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=24000, episode_reward=-48.00 +/- 6.00
Episode length: 48.10 +/- 5.70
Success rate: 10.00%
Eval num_timesteps=28000, episode_reward=-36.70 +/- 17.43
Episode length: 37.10 +/- 16.97
Success rate: 40.00%
New best mean reward!
Eval num_timesteps=32000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=36000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
[32m[I 2022-07-02 01:19:03,785][0m Trial 66 pruned. [0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.011505448762958146, 'gamma': 0.9788010179992276, 'tau': 0.002812831831627131, 'learning_rate': 0.0011806944160735384, 'batch_size': 256, 'learning_starts': 9000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  4
action_noise:  0.15000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=4000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=8000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=16000, episode_reward=-45.40 +/- 13.80
Episode length: 45.50 +/- 13.50
Success rate: 10.00%
New best mean reward!
Eval num_timesteps=20000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=24000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=28000, episode_reward=-40.60 +/- 18.80
Episode length: 40.80 +/- 18.40
Success rate: 20.00%
New best mean reward!
Eval num_timesteps=32000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=36000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
[32m[I 2022-07-02 01:24:07,351][0m Trial 67 pruned. [0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.10755039349536949, 'gamma': 0.9743924190374491, 'tau': 0.004090729754830397, 'learning_rate': 0.0016743119838334107, 'batch_size': 256, 'learning_starts': 9000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  5
action_noise:  0.15000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=5000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=10000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=15000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=20000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=25000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=30000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=35000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=40000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=45000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
[32m[I 2022-07-02 01:29:33,525][0m Trial 68 pruned. [0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.007507231268613951, 'gamma': 0.9814328758494001, 'tau': 0.004629779599693443, 'learning_rate': 0.0009633194623273532, 'batch_size': 256, 'learning_starts': 6000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  4
action_noise:  0.15000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=4000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=8000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=16000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=20000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=24000, episode_reward=-33.20 +/- 20.63
Episode length: 33.60 +/- 20.15
Success rate: 40.00%
New best mean reward!
Eval num_timesteps=28000, episode_reward=-33.10 +/- 20.75
Episode length: 33.50 +/- 20.26
Success rate: 40.00%
New best mean reward!
Eval num_timesteps=32000, episode_reward=-22.50 +/- 22.46
Episode length: 23.10 +/- 21.97
Success rate: 60.00%
New best mean reward!
Eval num_timesteps=36000, episode_reward=-41.70 +/- 16.61
Episode length: 41.90 +/- 16.21
Success rate: 20.00%
[32m[I 2022-07-02 01:34:43,048][0m Trial 69 pruned. [0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.034273544011721155, 'gamma': 0.9827520239489314, 'tau': 0.003581360369758497, 'learning_rate': 0.001093132661832721, 'batch_size': 256, 'learning_starts': 9000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  3
action_noise:  0.15000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=3000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=6000, episode_reward=-49.40 +/- 1.80
Episode length: 49.50 +/- 1.50
Success rate: 10.00%
New best mean reward!
Eval num_timesteps=9000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-41.90 +/- 16.20
Episode length: 42.10 +/- 15.80
Success rate: 20.00%
New best mean reward!
Eval num_timesteps=15000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=18000, episode_reward=-46.90 +/- 9.30
Episode length: 47.00 +/- 9.00
Success rate: 10.00%
Eval num_timesteps=21000, episode_reward=-47.60 +/- 7.20
Episode length: 47.70 +/- 6.90
Success rate: 10.00%
Eval num_timesteps=24000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=27000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
[32m[I 2022-07-02 01:39:22,308][0m Trial 70 pruned. [0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.0030526822909891934, 'gamma': 0.9753561095719444, 'tau': 0.003033172321572248, 'learning_rate': 0.0020741122582588986, 'batch_size': 1024, 'learning_starts': 9000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  4
action_noise:  0.15000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=4000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=8000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=16000, episode_reward=-45.30 +/- 14.10
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=20000, episode_reward=-49.70 +/- 0.64
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=24000, episode_reward=-38.20 +/- 18.21
Episode length: 38.50 +/- 17.76
Success rate: 30.00%
New best mean reward!
Eval num_timesteps=28000, episode_reward=-28.10 +/- 22.24
Episode length: 28.60 +/- 21.74
Success rate: 50.00%
New best mean reward!
Eval num_timesteps=32000, episode_reward=-3.30 +/- 0.46
Episode length: 4.30 +/- 0.46
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=36000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=40000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
Eval num_timesteps=44000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
Eval num_timesteps=48000, episode_reward=-3.20 +/- 0.40
Episode length: 4.20 +/- 0.40
Success rate: 100.00%
Eval num_timesteps=52000, episode_reward=-3.10 +/- 0.30
Episode length: 4.20 +/- 0.40
Success rate: 100.00%
Eval num_timesteps=56000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
Eval num_timesteps=60000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
TRIAL Finished: Objective Trial Time
[32m[I 2022-07-02 01:54:25,497][0m Trial 71 finished with value: -3.0 and parameters: {'gamma': 0.9753561095719444, 'tau': 0.003033172321572248, 'lr': 0.0020741122582588986, 'ent_coef': 0.0030526822909891934, 'batch_size_num': 10, 'learning_starts': 9, 'action_noise_int': 4, 'n_envs': 4}. Best is trial 20 with value: -3.0.[0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.009883723312755574, 'gamma': 0.9727685713895964, 'tau': 0.0029834474777501283, 'learning_rate': 0.0026874083344781245, 'batch_size': 1024, 'learning_starts': 9000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  4
action_noise:  0.15000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=4000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=8000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=16000, episode_reward=-46.50 +/- 10.50
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=20000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=24000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=28000, episode_reward=-44.60 +/- 13.21
Episode length: 45.70 +/- 12.90
Success rate: 10.00%
New best mean reward!
Eval num_timesteps=32000, episode_reward=-40.60 +/- 18.80
Episode length: 40.80 +/- 18.40
Success rate: 20.00%
New best mean reward!
Eval num_timesteps=36000, episode_reward=-36.40 +/- 20.81
Episode length: 36.70 +/- 20.36
Success rate: 30.00%
New best mean reward!
[32m[I 2022-07-02 02:02:56,163][0m Trial 72 pruned. [0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.0027949650763637547, 'gamma': 0.9756918502608338, 'tau': 0.00260624898812092, 'learning_rate': 0.0021862303661286065, 'batch_size': 1024, 'learning_starts': 8000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  4
action_noise:  0.10000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=4000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=8000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=16000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=20000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=24000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=28000, episode_reward=-45.40 +/- 13.80
Episode length: 45.50 +/- 13.50
Success rate: 10.00%
New best mean reward!
Eval num_timesteps=32000, episode_reward=-45.30 +/- 14.10
Episode length: 45.40 +/- 13.80
Success rate: 10.00%
New best mean reward!
Eval num_timesteps=36000, episode_reward=-40.60 +/- 18.80
Episode length: 40.80 +/- 18.40
Success rate: 20.00%
New best mean reward!
[32m[I 2022-07-02 02:11:20,243][0m Trial 73 pruned. [0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.005310303575223982, 'gamma': 0.9795670914179455, 'tau': 0.0034049240600962523, 'learning_rate': 0.0019247983509628095, 'batch_size': 256, 'learning_starts': 10000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  4
action_noise:  0.15000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=4000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=8000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-49.40 +/- 1.80
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=16000, episode_reward=-45.70 +/- 12.90
Episode length: 45.80 +/- 12.60
Success rate: 10.00%
New best mean reward!
Eval num_timesteps=20000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=24000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=28000, episode_reward=-40.60 +/- 18.80
Episode length: 40.80 +/- 18.40
Success rate: 20.00%
New best mean reward!
Eval num_timesteps=32000, episode_reward=-41.00 +/- 18.02
Episode length: 41.20 +/- 17.62
Success rate: 20.00%
Eval num_timesteps=36000, episode_reward=-45.30 +/- 14.10
Episode length: 45.40 +/- 13.80
Success rate: 10.00%
[32m[I 2022-07-02 02:16:42,255][0m Trial 74 pruned. [0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.015560789126625495, 'gamma': 0.9910301674233077, 'tau': 0.002553037268015817, 'learning_rate': 0.002329791456764394, 'batch_size': 1024, 'learning_starts': 10000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  4
action_noise:  0.10000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=4000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=8000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-49.80 +/- 0.60
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=16000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=20000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=24000, episode_reward=-47.00 +/- 4.24
Episode length: 49.20 +/- 2.40
Success rate: 10.00%
New best mean reward!
Eval num_timesteps=28000, episode_reward=-49.20 +/- 1.66
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=32000, episode_reward=-34.50 +/- 20.44
Episode length: 37.50 +/- 19.27
Success rate: 30.00%
New best mean reward!
Eval num_timesteps=36000, episode_reward=-13.00 +/- 18.51
Episode length: 18.40 +/- 20.70
Success rate: 70.00%
New best mean reward!
Eval num_timesteps=40000, episode_reward=-3.60 +/- 0.66
Episode length: 4.60 +/- 0.66
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=44000, episode_reward=-3.40 +/- 0.49
Episode length: 4.40 +/- 0.49
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=48000, episode_reward=-3.10 +/- 0.30
Episode length: 4.10 +/- 0.30
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=52000, episode_reward=-3.10 +/- 0.30
Episode length: 4.10 +/- 0.30
Success rate: 100.00%
Eval num_timesteps=56000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=60000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
TRIAL Finished: Objective Trial Time
[32m[I 2022-07-02 02:32:33,853][0m Trial 75 finished with value: -3.0 and parameters: {'gamma': 0.9910301674233077, 'tau': 0.002553037268015817, 'lr': 0.002329791456764394, 'ent_coef': 0.015560789126625495, 'batch_size_num': 10, 'learning_starts': 10, 'action_noise_int': 3, 'n_envs': 4}. Best is trial 20 with value: -3.0.[0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.003869623059386591, 'gamma': 0.9949319030433315, 'tau': 0.0018121643440300883, 'learning_rate': 0.0026994954487637056, 'batch_size': 1024, 'learning_starts': 10000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  4
action_noise:  0.10000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=4000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=8000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-45.30 +/- 14.10
Episode length: 45.40 +/- 13.80
Success rate: 10.00%
New best mean reward!
Eval num_timesteps=16000, episode_reward=-47.90 +/- 5.37
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=20000, episode_reward=-40.80 +/- 18.40
Episode length: 41.00 +/- 18.00
Success rate: 20.00%
New best mean reward!
Eval num_timesteps=24000, episode_reward=-37.40 +/- 16.64
Episode length: 37.80 +/- 16.19
Success rate: 40.00%
New best mean reward!
Eval num_timesteps=28000, episode_reward=-27.40 +/- 22.64
Episode length: 27.90 +/- 22.14
Success rate: 50.00%
New best mean reward!
Eval num_timesteps=32000, episode_reward=-18.30 +/- 20.40
Episode length: 19.20 +/- 20.24
Success rate: 70.00%
New best mean reward!
Eval num_timesteps=36000, episode_reward=-3.40 +/- 0.66
Episode length: 4.40 +/- 0.66
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=40000, episode_reward=-3.10 +/- 0.30
Episode length: 4.10 +/- 0.30
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=44000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=48000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
Eval num_timesteps=52000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
Eval num_timesteps=56000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
Eval num_timesteps=60000, episode_reward=-3.10 +/- 0.30
Episode length: 4.10 +/- 0.30
Success rate: 100.00%
TRIAL Finished: Objective Trial Time
[32m[I 2022-07-02 02:47:47,964][0m Trial 76 finished with value: -3.1 and parameters: {'gamma': 0.9949319030433315, 'tau': 0.0018121643440300883, 'lr': 0.0026994954487637056, 'ent_coef': 0.003869623059386591, 'batch_size_num': 10, 'learning_starts': 10, 'action_noise_int': 3, 'n_envs': 4}. Best is trial 20 with value: -3.0.[0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.016263314383183184, 'gamma': 0.9888290868631976, 'tau': 0.0031567397726884926, 'learning_rate': 0.002084645508580967, 'batch_size': 1024, 'learning_starts': 10000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  4
action_noise:  0.10000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=4000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=8000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-41.50 +/- 17.04
Episode length: 41.70 +/- 16.64
Success rate: 20.00%
New best mean reward!
Eval num_timesteps=16000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=20000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=24000, episode_reward=-46.20 +/- 11.40
Episode length: 46.30 +/- 11.10
Success rate: 10.00%
Eval num_timesteps=28000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=32000, episode_reward=-24.60 +/- 20.89
Episode length: 25.20 +/- 20.40
Success rate: 60.00%
New best mean reward!
Eval num_timesteps=36000, episode_reward=-13.80 +/- 16.73
Episode length: 14.70 +/- 16.52
Success rate: 90.00%
New best mean reward!
Eval num_timesteps=40000, episode_reward=-3.10 +/- 0.30
Episode length: 4.10 +/- 0.30
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=44000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=48000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
Eval num_timesteps=52000, episode_reward=-3.20 +/- 0.40
Episode length: 4.20 +/- 0.40
Success rate: 100.00%
Eval num_timesteps=56000, episode_reward=-3.10 +/- 0.30
Episode length: 4.10 +/- 0.30
Success rate: 100.00%
Eval num_timesteps=60000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
TRIAL Finished: Objective Trial Time
[32m[I 2022-07-02 03:03:08,987][0m Trial 77 finished with value: -3.0 and parameters: {'gamma': 0.9888290868631976, 'tau': 0.0031567397726884926, 'lr': 0.002084645508580967, 'ent_coef': 0.016263314383183184, 'batch_size_num': 10, 'learning_starts': 10, 'action_noise_int': 3, 'n_envs': 4}. Best is trial 20 with value: -3.0.[0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.0015089457353599212, 'gamma': 0.9929779025594007, 'tau': 0.004183396372248283, 'learning_rate': 0.000907477469704637, 'batch_size': 512, 'learning_starts': 7000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  4
action_noise:  0.05

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=4000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=8000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=16000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=20000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=24000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=28000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=32000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=36000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
[32m[I 2022-07-02 03:09:38,827][0m Trial 78 pruned. [0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.024349151837662533, 'gamma': 0.9913475866447881, 'tau': 0.0027217744692663165, 'learning_rate': 0.0013774672219040796, 'batch_size': 1024, 'learning_starts': 9000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  4
action_noise:  0.10000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=4000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=8000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-42.00 +/- 16.00
Episode length: 42.20 +/- 15.60
Success rate: 20.00%
New best mean reward!
Eval num_timesteps=16000, episode_reward=-45.50 +/- 13.50
Episode length: 45.60 +/- 13.20
Success rate: 10.00%
Eval num_timesteps=20000, episode_reward=-45.30 +/- 14.10
Episode length: 45.40 +/- 13.80
Success rate: 10.00%
Eval num_timesteps=24000, episode_reward=-45.30 +/- 14.10
Episode length: 45.40 +/- 13.80
Success rate: 10.00%
Eval num_timesteps=28000, episode_reward=-45.50 +/- 13.50
Episode length: 45.60 +/- 13.20
Success rate: 10.00%
Eval num_timesteps=32000, episode_reward=-31.20 +/- 23.03
Episode length: 31.60 +/- 22.54
Success rate: 40.00%
New best mean reward!
Eval num_timesteps=36000, episode_reward=-36.00 +/- 21.39
Episode length: 36.30 +/- 20.93
Success rate: 30.00%
[32m[I 2022-07-02 03:19:07,313][0m Trial 79 pruned. [0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.014761112295643992, 'gamma': 0.9917034548355372, 'tau': 0.0028567288382357627, 'learning_rate': 0.0021546692161016983, 'batch_size': 1024, 'learning_starts': 10000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  4
action_noise:  0.15000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=4000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=8000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=16000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=20000, episode_reward=-31.20 +/- 23.03
Episode length: 31.60 +/- 22.54
Success rate: 40.00%
New best mean reward!
Eval num_timesteps=24000, episode_reward=-45.30 +/- 14.10
Episode length: 45.40 +/- 13.80
Success rate: 10.00%
Eval num_timesteps=28000, episode_reward=-31.60 +/- 22.54
Episode length: 32.00 +/- 22.05
Success rate: 40.00%
Eval num_timesteps=32000, episode_reward=-36.40 +/- 20.79
Episode length: 36.70 +/- 20.33
Success rate: 30.00%
Eval num_timesteps=36000, episode_reward=-26.50 +/- 23.50
Episode length: 27.00 +/- 23.00
Success rate: 50.00%
New best mean reward!
Eval num_timesteps=40000, episode_reward=-31.20 +/- 23.03
Episode length: 31.60 +/- 22.54
Success rate: 40.00%
Eval num_timesteps=44000, episode_reward=-3.30 +/- 0.46
Episode length: 4.30 +/- 0.46
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=48000, episode_reward=-3.40 +/- 0.49
Episode length: 4.40 +/- 0.49
Success rate: 100.00%
Eval num_timesteps=52000, episode_reward=-3.20 +/- 0.40
Episode length: 4.20 +/- 0.40
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=56000, episode_reward=-3.10 +/- 0.30
Episode length: 4.10 +/- 0.30
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=60000, episode_reward=-3.10 +/- 0.30
Episode length: 4.10 +/- 0.30
Success rate: 100.00%
TRIAL Finished: Objective Trial Time
[32m[I 2022-07-02 03:35:52,801][0m Trial 80 finished with value: -3.1 and parameters: {'gamma': 0.9917034548355372, 'tau': 0.0028567288382357627, 'lr': 0.0021546692161016983, 'ent_coef': 0.014761112295643992, 'batch_size_num': 10, 'learning_starts': 10, 'action_noise_int': 4, 'n_envs': 4}. Best is trial 20 with value: -3.0.[0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.011119583019042227, 'gamma': 0.9770849061070971, 'tau': 0.003671813265606878, 'learning_rate': 0.0006148338869484776, 'batch_size': 512, 'learning_starts': 9000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  4
action_noise:  0.15000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=4000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=8000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=16000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=20000, episode_reward=-46.10 +/- 11.70
Episode length: 46.20 +/- 11.40
Success rate: 10.00%
New best mean reward!
Eval num_timesteps=24000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=28000, episode_reward=-40.70 +/- 18.60
Episode length: 40.90 +/- 18.20
Success rate: 20.00%
New best mean reward!
Eval num_timesteps=32000, episode_reward=-27.70 +/- 22.33
Episode length: 28.20 +/- 21.83
Success rate: 50.00%
New best mean reward!
Eval num_timesteps=36000, episode_reward=-32.40 +/- 19.59
Episode length: 32.90 +/- 19.14
Success rate: 50.00%
Eval num_timesteps=40000, episode_reward=-18.50 +/- 20.74
Episode length: 19.20 +/- 20.29
Success rate: 70.00%
New best mean reward!
Eval num_timesteps=44000, episode_reward=-6.10 +/- 3.62
Episode length: 7.10 +/- 3.62
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=48000, episode_reward=-3.30 +/- 0.46
Episode length: 4.30 +/- 0.46
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=52000, episode_reward=-3.20 +/- 0.40
Episode length: 4.20 +/- 0.40
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=56000, episode_reward=-3.10 +/- 0.30
Episode length: 4.10 +/- 0.30
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=60000, episode_reward=-3.10 +/- 0.30
Episode length: 4.10 +/- 0.30
Success rate: 100.00%
TRIAL Finished: Objective Trial Time
[32m[I 2022-07-02 03:49:11,838][0m Trial 81 finished with value: -3.1 and parameters: {'gamma': 0.9770849061070971, 'tau': 0.003671813265606878, 'lr': 0.0006148338869484776, 'ent_coef': 0.011119583019042227, 'batch_size_num': 9, 'learning_starts': 9, 'action_noise_int': 4, 'n_envs': 4}. Best is trial 20 with value: -3.0.[0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.00777647724048625, 'gamma': 0.9790152702204524, 'tau': 0.003315641041459658, 'learning_rate': 0.001735001263864278, 'batch_size': 256, 'learning_starts': 8000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  4
action_noise:  0.15000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=4000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=8000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=16000, episode_reward=-46.20 +/- 11.40
Episode length: 46.30 +/- 11.10
Success rate: 10.00%
New best mean reward!
Eval num_timesteps=20000, episode_reward=-41.60 +/- 16.89
Episode length: 41.80 +/- 16.50
Success rate: 20.00%
New best mean reward!
Eval num_timesteps=24000, episode_reward=-36.80 +/- 20.23
Episode length: 37.10 +/- 19.77
Success rate: 30.00%
New best mean reward!
Eval num_timesteps=28000, episode_reward=-41.10 +/- 17.84
Episode length: 41.30 +/- 17.44
Success rate: 20.00%
Eval num_timesteps=32000, episode_reward=-29.70 +/- 20.68
Episode length: 30.20 +/- 20.19
Success rate: 50.00%
New best mean reward!
Eval num_timesteps=36000, episode_reward=-32.60 +/- 21.35
Episode length: 33.00 +/- 20.86
Success rate: 40.00%
Eval num_timesteps=40000, episode_reward=-4.30 +/- 1.35
Episode length: 5.30 +/- 1.35
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=44000, episode_reward=-3.60 +/- 0.80
Episode length: 4.60 +/- 0.80
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=48000, episode_reward=-3.10 +/- 0.30
Episode length: 4.10 +/- 0.30
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=52000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=56000, episode_reward=-3.10 +/- 0.30
Episode length: 4.10 +/- 0.30
Success rate: 100.00%
Eval num_timesteps=60000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
TRIAL Finished: Objective Trial Time
[32m[I 2022-07-02 04:00:10,860][0m Trial 82 finished with value: -3.0 and parameters: {'gamma': 0.9790152702204524, 'tau': 0.003315641041459658, 'lr': 0.001735001263864278, 'ent_coef': 0.00777647724048625, 'batch_size_num': 8, 'learning_starts': 8, 'action_noise_int': 4, 'n_envs': 4}. Best is trial 20 with value: -3.0.[0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.01642133636907184, 'gamma': 0.9893148723930152, 'tau': 0.0025388256891499183, 'learning_rate': 0.0023283390346958777, 'batch_size': 1024, 'learning_starts': 10000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  4
action_noise:  0.10000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=4000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=8000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-41.70 +/- 16.60
Episode length: 41.90 +/- 16.20
Success rate: 20.00%
New best mean reward!
Eval num_timesteps=16000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=20000, episode_reward=-45.50 +/- 13.50
Episode length: 45.60 +/- 13.20
Success rate: 10.00%
Eval num_timesteps=24000, episode_reward=-45.30 +/- 14.10
Episode length: 45.40 +/- 13.80
Success rate: 10.00%
Eval num_timesteps=28000, episode_reward=-32.80 +/- 21.52
Episode length: 33.20 +/- 21.04
Success rate: 40.00%
New best mean reward!
Eval num_timesteps=32000, episode_reward=-19.50 +/- 20.07
Episode length: 20.20 +/- 19.61
Success rate: 70.00%
New best mean reward!
Eval num_timesteps=36000, episode_reward=-21.80 +/- 23.03
Episode length: 22.40 +/- 22.54
Success rate: 60.00%
Eval num_timesteps=40000, episode_reward=-3.20 +/- 0.40
Episode length: 4.20 +/- 0.40
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=44000, episode_reward=-3.10 +/- 0.30
Episode length: 4.10 +/- 0.30
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=48000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=52000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
Eval num_timesteps=56000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
Eval num_timesteps=60000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
TRIAL Finished: Objective Trial Time
[32m[I 2022-07-02 04:17:23,106][0m Trial 83 finished with value: -3.0 and parameters: {'gamma': 0.9893148723930152, 'tau': 0.0025388256891499183, 'lr': 0.0023283390346958777, 'ent_coef': 0.01642133636907184, 'batch_size_num': 10, 'learning_starts': 10, 'action_noise_int': 3, 'n_envs': 4}. Best is trial 20 with value: -3.0.[0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.2953568479053875, 'gamma': 0.9714696732708163, 'tau': 0.003814475872311075, 'learning_rate': 0.0016696787894890756, 'batch_size': 1024, 'learning_starts': 9000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  4
action_noise:  0.15000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=4000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=8000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=16000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=20000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=24000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=28000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=32000, episode_reward=-48.10 +/- 5.70
Episode length: 48.20 +/- 5.40
Success rate: 10.00%
New best mean reward!
Eval num_timesteps=36000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
[32m[I 2022-07-02 04:26:36,259][0m Trial 84 pruned. [0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.001905705375211807, 'gamma': 0.9781440347450688, 'tau': 0.0033133739777588783, 'learning_rate': 0.0019096660869106627, 'batch_size': 1024, 'learning_starts': 8000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  4
action_noise:  0.15000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=4000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=8000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=16000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=20000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=24000, episode_reward=-33.30 +/- 20.63
Episode length: 33.70 +/- 20.14
Success rate: 40.00%
New best mean reward!
Eval num_timesteps=28000, episode_reward=-45.30 +/- 14.10
Episode length: 45.40 +/- 13.80
Success rate: 10.00%
Eval num_timesteps=32000, episode_reward=-24.40 +/- 21.44
Episode length: 25.00 +/- 20.96
Success rate: 60.00%
New best mean reward!
Eval num_timesteps=36000, episode_reward=-12.90 +/- 18.57
Episode length: 13.70 +/- 18.17
Success rate: 80.00%
New best mean reward!
Eval num_timesteps=40000, episode_reward=-3.70 +/- 1.19
Episode length: 4.70 +/- 1.19
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=44000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=48000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
Eval num_timesteps=52000, episode_reward=-3.20 +/- 0.40
Episode length: 4.20 +/- 0.40
Success rate: 100.00%
Eval num_timesteps=56000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
Eval num_timesteps=60000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
TRIAL Finished: Objective Trial Time
[32m[I 2022-07-02 04:44:32,709][0m Trial 85 finished with value: -3.0 and parameters: {'gamma': 0.9781440347450688, 'tau': 0.0033133739777588783, 'lr': 0.0019096660869106627, 'ent_coef': 0.001905705375211807, 'batch_size_num': 10, 'learning_starts': 8, 'action_noise_int': 4, 'n_envs': 4}. Best is trial 20 with value: -3.0.[0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.0021051141124285846, 'gamma': 0.9766017884271916, 'tau': 0.0033654228335569594, 'learning_rate': 0.0019312718198291258, 'batch_size': 1024, 'learning_starts': 8000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  4
action_noise:  0.0

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=4000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=8000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=16000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=20000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=24000, episode_reward=-49.10 +/- 1.92
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=28000, episode_reward=-49.40 +/- 1.80
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=32000, episode_reward=-48.60 +/- 4.20
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=36000, episode_reward=-49.10 +/- 2.39
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
[32m[I 2022-07-02 04:54:14,416][0m Trial 86 pruned. [0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.001185866335172942, 'gamma': 0.9806127108625683, 'tau': 0.003533027670277019, 'learning_rate': 0.0027372953319159867, 'batch_size': 1024, 'learning_starts': 7000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  4
action_noise:  0.10000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=4000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=8000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=16000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=20000, episode_reward=-48.40 +/- 3.67
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=24000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=28000, episode_reward=-28.30 +/- 21.85
Episode length: 28.80 +/- 21.35
Success rate: 50.00%
New best mean reward!
Eval num_timesteps=32000, episode_reward=-33.80 +/- 19.97
Episode length: 34.20 +/- 19.48
Success rate: 40.00%
Eval num_timesteps=36000, episode_reward=-32.70 +/- 21.23
Episode length: 33.10 +/- 20.74
Success rate: 40.00%
[32m[I 2022-07-02 05:04:28,631][0m Trial 87 pruned. [0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.029800714394461598, 'gamma': 0.9841534152342644, 'tau': 0.0008025951190863798, 'learning_rate': 0.0007023071960484221, 'batch_size': 1024, 'learning_starts': 10000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  4
action_noise:  0.10000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=4000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=8000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=16000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=20000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=24000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=28000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=32000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=36000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
[32m[I 2022-07-02 05:13:37,545][0m Trial 88 pruned. [0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.01813631951565081, 'gamma': 0.9874555428649476, 'tau': 0.0022759612638856472, 'learning_rate': 0.0023483080415890264, 'batch_size': 1024, 'learning_starts': 10000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  4
action_noise:  0.10000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=4000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=8000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-45.80 +/- 12.60
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=16000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=20000, episode_reward=-49.20 +/- 2.40
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=24000, episode_reward=-45.30 +/- 14.10
Episode length: 45.40 +/- 13.80
Success rate: 10.00%
New best mean reward!
Eval num_timesteps=28000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=32000, episode_reward=-44.60 +/- 14.02
Episode length: 45.40 +/- 13.80
Success rate: 10.00%
New best mean reward!
Eval num_timesteps=36000, episode_reward=-32.40 +/- 21.66
Episode length: 32.80 +/- 21.17
Success rate: 40.00%
New best mean reward!
[32m[I 2022-07-02 05:23:11,334][0m Trial 89 pruned. [0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.0028065844787036743, 'gamma': 0.989751091436023, 'tau': 0.0030925734867440174, 'learning_rate': 0.0025118355784077688, 'batch_size': 1024, 'learning_starts': 6000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  5
action_noise:  0.15000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=5000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=10000, episode_reward=-36.50 +/- 20.64
Episode length: 36.80 +/- 20.18
Success rate: 30.00%
New best mean reward!
Eval num_timesteps=15000, episode_reward=-36.00 +/- 21.39
Episode length: 36.30 +/- 20.93
Success rate: 30.00%
New best mean reward!
Eval num_timesteps=20000, episode_reward=-34.50 +/- 20.47
Episode length: 34.90 +/- 20.01
Success rate: 40.00%
New best mean reward!
Eval num_timesteps=25000, episode_reward=-28.20 +/- 22.03
Episode length: 32.70 +/- 21.40
Success rate: 40.00%
New best mean reward!
Eval num_timesteps=30000, episode_reward=-45.40 +/- 13.80
Episode length: 45.50 +/- 13.50
Success rate: 10.00%
Eval num_timesteps=35000, episode_reward=-3.10 +/- 0.30
Episode length: 4.10 +/- 0.30
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=40000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=45000, episode_reward=-3.30 +/- 0.46
Episode length: 4.30 +/- 0.46
Success rate: 100.00%
Eval num_timesteps=50000, episode_reward=-3.20 +/- 0.40
Episode length: 4.20 +/- 0.40
Success rate: 100.00%
Eval num_timesteps=55000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
Eval num_timesteps=60000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
TRIAL Finished: Objective Trial Time
[32m[I 2022-07-02 05:38:40,144][0m Trial 90 finished with value: -3.0 and parameters: {'gamma': 0.989751091436023, 'tau': 0.0030925734867440174, 'lr': 0.0025118355784077688, 'ent_coef': 0.0028065844787036743, 'batch_size_num': 10, 'learning_starts': 6, 'action_noise_int': 4, 'n_envs': 5}. Best is trial 20 with value: -3.0.[0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.004318943412507697, 'gamma': 0.9775295102361267, 'tau': 0.0031106807295951384, 'learning_rate': 0.002475150077734203, 'batch_size': 1024, 'learning_starts': 6000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  5
action_noise:  0.15000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=5000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=10000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=15000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=20000, episode_reward=-38.10 +/- 18.41
Episode length: 38.40 +/- 17.96
Success rate: 30.00%
New best mean reward!
Eval num_timesteps=25000, episode_reward=-45.40 +/- 13.80
Episode length: 45.50 +/- 13.50
Success rate: 10.00%
Eval num_timesteps=30000, episode_reward=-22.00 +/- 22.87
Episode length: 22.60 +/- 22.38
Success rate: 60.00%
New best mean reward!
Eval num_timesteps=35000, episode_reward=-3.20 +/- 0.40
Episode length: 4.20 +/- 0.40
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=40000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=45000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
Eval num_timesteps=50000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
Eval num_timesteps=55000, episode_reward=-3.10 +/- 0.30
Episode length: 4.10 +/- 0.30
Success rate: 100.00%
Eval num_timesteps=60000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
TRIAL Finished: Objective Trial Time
[32m[I 2022-07-02 05:54:16,528][0m Trial 91 finished with value: -3.0 and parameters: {'gamma': 0.9775295102361267, 'tau': 0.0031106807295951384, 'lr': 0.002475150077734203, 'ent_coef': 0.004318943412507697, 'batch_size_num': 10, 'learning_starts': 6, 'action_noise_int': 4, 'n_envs': 5}. Best is trial 20 with value: -3.0.[0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.005831675271217744, 'gamma': 0.9775338931110913, 'tau': 0.004021803958885307, 'learning_rate': 0.0024580552041300937, 'batch_size': 1024, 'learning_starts': 6000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  5
action_noise:  0.15000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=5000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=10000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=15000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=20000, episode_reward=-45.90 +/- 12.30
Episode length: 46.00 +/- 12.00
Success rate: 10.00%
New best mean reward!
Eval num_timesteps=25000, episode_reward=-45.40 +/- 13.80
Episode length: 45.50 +/- 13.50
Success rate: 10.00%
New best mean reward!
Eval num_timesteps=30000, episode_reward=-31.40 +/- 22.78
Episode length: 31.80 +/- 22.29
Success rate: 40.00%
New best mean reward!
Eval num_timesteps=35000, episode_reward=-26.70 +/- 23.30
Episode length: 27.20 +/- 22.80
Success rate: 50.00%
New best mean reward!
Eval num_timesteps=40000, episode_reward=-18.50 +/- 20.74
Episode length: 19.20 +/- 20.28
Success rate: 70.00%
New best mean reward!
Eval num_timesteps=45000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=50000, episode_reward=-3.20 +/- 0.40
Episode length: 4.20 +/- 0.40
Success rate: 100.00%
Eval num_timesteps=55000, episode_reward=-3.00 +/- 0.00
Episode length: 9.50 +/- 13.76
Success rate: 90.00%
Eval num_timesteps=60000, episode_reward=-3.00 +/- 0.00
Episode length: 8.60 +/- 13.80
Success rate: 90.00%
TRIAL Finished: Objective Trial Time
[32m[I 2022-07-02 06:10:32,625][0m Trial 92 finished with value: -3.0 and parameters: {'gamma': 0.9775338931110913, 'tau': 0.004021803958885307, 'lr': 0.0024580552041300937, 'ent_coef': 0.005831675271217744, 'batch_size_num': 10, 'learning_starts': 6, 'action_noise_int': 4, 'n_envs': 5}. Best is trial 20 with value: -3.0.[0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.0041692354889746736, 'gamma': 0.9752266182412858, 'tau': 0.003015045639065114, 'learning_rate': 0.002190812841364743, 'batch_size': 1024, 'learning_starts': 10000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  5
action_noise:  0.10000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=5000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=10000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=15000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=20000, episode_reward=-47.20 +/- 4.83
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=25000, episode_reward=-49.40 +/- 1.80
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=30000, episode_reward=-29.20 +/- 21.79
Episode length: 31.70 +/- 22.41
Success rate: 40.00%
New best mean reward!
Eval num_timesteps=35000, episode_reward=-8.20 +/- 5.65
Episode length: 15.30 +/- 17.49
Success rate: 80.00%
New best mean reward!
Eval num_timesteps=40000, episode_reward=-9.80 +/- 13.69
Episode length: 10.70 +/- 13.39
Success rate: 90.00%
Eval num_timesteps=45000, episode_reward=-3.10 +/- 0.30
Episode length: 4.10 +/- 0.30
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=50000, episode_reward=-3.20 +/- 0.40
Episode length: 4.20 +/- 0.40
Success rate: 100.00%
Eval num_timesteps=55000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=60000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
TRIAL Finished: Objective Trial Time
[32m[I 2022-07-02 06:24:36,601][0m Trial 93 finished with value: -3.0 and parameters: {'gamma': 0.9752266182412858, 'tau': 0.003015045639065114, 'lr': 0.002190812841364743, 'ent_coef': 0.0041692354889746736, 'batch_size_num': 10, 'learning_starts': 10, 'action_noise_int': 3, 'n_envs': 5}. Best is trial 20 with value: -3.0.[0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.0055901808029921556, 'gamma': 0.976102769061779, 'tau': 0.004093845495178884, 'learning_rate': 0.002928453915807003, 'batch_size': 1024, 'learning_starts': 5000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  4
action_noise:  0.10000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=4000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=8000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-45.60 +/- 13.20
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=16000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=20000, episode_reward=-40.60 +/- 14.22
Episode length: 43.50 +/- 13.98
Success rate: 20.00%
New best mean reward!
Eval num_timesteps=24000, episode_reward=-9.40 +/- 13.61
Episode length: 10.30 +/- 13.31
Success rate: 90.00%
New best mean reward!
Eval num_timesteps=28000, episode_reward=-3.20 +/- 0.40
Episode length: 4.20 +/- 0.40
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=32000, episode_reward=-3.10 +/- 0.30
Episode length: 4.10 +/- 0.30
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=36000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=40000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
Eval num_timesteps=44000, episode_reward=-3.10 +/- 0.30
Episode length: 4.10 +/- 0.30
Success rate: 100.00%
Eval num_timesteps=48000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
Eval num_timesteps=52000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
Eval num_timesteps=56000, episode_reward=-3.10 +/- 0.30
Episode length: 4.10 +/- 0.30
Success rate: 100.00%
Eval num_timesteps=60000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
TRIAL Finished: Objective Trial Time
[32m[I 2022-07-02 06:43:12,854][0m Trial 94 finished with value: -3.0 and parameters: {'gamma': 0.976102769061779, 'tau': 0.004093845495178884, 'lr': 0.002928453915807003, 'ent_coef': 0.0055901808029921556, 'batch_size_num': 10, 'learning_starts': 5, 'action_noise_int': 3, 'n_envs': 4}. Best is trial 20 with value: -3.0.[0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.012879588686173818, 'gamma': 0.9740348617627824, 'tau': 0.0024520365091942995, 'learning_rate': 0.0014505562483914187, 'batch_size': 1024, 'learning_starts': 9000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  4
action_noise:  0.10000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=4000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=8000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-46.20 +/- 11.40
Episode length: 46.30 +/- 11.10
Success rate: 10.00%
New best mean reward!
Eval num_timesteps=16000, episode_reward=-49.10 +/- 1.92
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=20000, episode_reward=-45.00 +/- 14.03
Episode length: 45.40 +/- 13.80
Success rate: 10.00%
New best mean reward!
Eval num_timesteps=24000, episode_reward=-49.10 +/- 1.45
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=28000, episode_reward=-41.70 +/- 16.67
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=32000, episode_reward=-14.10 +/- 18.22
Episode length: 14.90 +/- 17.83
Success rate: 80.00%
New best mean reward!
Eval num_timesteps=36000, episode_reward=-6.00 +/- 3.03
Episode length: 11.80 +/- 13.01
Success rate: 90.00%
New best mean reward!
Eval num_timesteps=40000, episode_reward=-3.40 +/- 0.66
Episode length: 4.40 +/- 0.66
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=44000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=48000, episode_reward=-3.10 +/- 0.30
Episode length: 4.10 +/- 0.30
Success rate: 100.00%
Eval num_timesteps=52000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
Eval num_timesteps=56000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
Eval num_timesteps=60000, episode_reward=-3.10 +/- 0.30
Episode length: 4.10 +/- 0.30
Success rate: 100.00%
TRIAL Finished: Objective Trial Time
[32m[I 2022-07-02 07:00:51,394][0m Trial 95 finished with value: -3.1 and parameters: {'gamma': 0.9740348617627824, 'tau': 0.0024520365091942995, 'lr': 0.0014505562483914187, 'ent_coef': 0.012879588686173818, 'batch_size_num': 10, 'learning_starts': 9, 'action_noise_int': 3, 'n_envs': 4}. Best is trial 20 with value: -3.0.[0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.040572551755511915, 'gamma': 0.9883561627172545, 'tau': 0.0028285523995323627, 'learning_rate': 0.0021475655684763274, 'batch_size': 1024, 'learning_starts': 10000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  4
action_noise:  0.10000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=4000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=8000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=16000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=20000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=24000, episode_reward=-47.70 +/- 6.26
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=28000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=32000, episode_reward=-44.70 +/- 10.66
Episode length: 44.90 +/- 10.26
Success rate: 20.00%
New best mean reward!
Eval num_timesteps=36000, episode_reward=-45.40 +/- 13.80
Episode length: 45.50 +/- 13.50
Success rate: 10.00%
[32m[I 2022-07-02 07:10:09,690][0m Trial 96 pruned. [0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.02216202590869805, 'gamma': 0.9910157886282569, 'tau': 0.0019849340796357757, 'learning_rate': 0.0020232534421590347, 'batch_size': 1024, 'learning_starts': 9000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  4
action_noise:  0.15000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=4000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=8000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=16000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=20000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=24000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=28000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=32000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=36000, episode_reward=-36.00 +/- 21.39
Episode length: 36.30 +/- 20.93
Success rate: 30.00%
New best mean reward!
[32m[I 2022-07-02 07:19:44,097][0m Trial 97 pruned. [0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.0068263423089180665, 'gamma': 0.992640254754514, 'tau': 0.004685999574591736, 'learning_rate': 0.0016025690033082056, 'batch_size': 512, 'learning_starts': 9000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  4
action_noise:  0.15000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=4000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=8000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=12000, episode_reward=-42.60 +/- 14.83
Episode length: 42.80 +/- 14.43
Success rate: 20.00%
New best mean reward!
Eval num_timesteps=16000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=20000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=24000, episode_reward=-49.20 +/- 2.40
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=28000, episode_reward=-45.30 +/- 14.10
Episode length: 45.40 +/- 13.80
Success rate: 10.00%
Eval num_timesteps=32000, episode_reward=-41.80 +/- 16.41
Episode length: 42.00 +/- 16.01
Success rate: 20.00%
New best mean reward!
Eval num_timesteps=36000, episode_reward=-32.20 +/- 21.86
Episode length: 32.60 +/- 21.37
Success rate: 40.00%
New best mean reward!
[32m[I 2022-07-02 07:27:27,267][0m Trial 98 pruned. [0m

 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.003385434793459635, 'gamma': 0.9749659212232031, 'tau': 0.003841408821822759, 'learning_rate': 0.0018081991423505635, 'batch_size': 256, 'learning_starts': 9000, 'buffer_size': 50000, 'train_freq': 50, 'gradient_steps': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
n_envs:  5
action_noise:  0.10000000000000002

 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Eval num_timesteps=5000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=10000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=15000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=20000, episode_reward=-50.00 +/- 0.00
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
Eval num_timesteps=25000, episode_reward=-48.50 +/- 3.07
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=30000, episode_reward=-43.10 +/- 12.28
Episode length: 50.00 +/- 0.00
Success rate: 0.00%
New best mean reward!
Eval num_timesteps=35000, episode_reward=-25.80 +/- 20.03
Episode length: 46.80 +/- 9.60
Success rate: 10.00%
New best mean reward!
Eval num_timesteps=40000, episode_reward=-13.10 +/- 18.01
Episode length: 45.70 +/- 12.90
Success rate: 10.00%
New best mean reward!
Eval num_timesteps=45000, episode_reward=-18.30 +/- 20.90
Episode length: 19.00 +/- 20.44
Success rate: 70.00%
Eval num_timesteps=50000, episode_reward=-3.00 +/- 0.00
Episode length: 4.00 +/- 0.00
Success rate: 100.00%
New best mean reward!
Eval num_timesteps=55000, episode_reward=-3.10 +/- 0.30
Episode length: 4.60 +/- 0.49
Success rate: 100.00%
Eval num_timesteps=60000, episode_reward=-3.00 +/- 0.00
Episode length: 17.80 +/- 21.08
Success rate: 70.00%
TRIAL Finished: Objective Trial Time
[32m[I 2022-07-02 07:38:04,133][0m Trial 99 finished with value: -3.0 and parameters: {'gamma': 0.9749659212232031, 'tau': 0.003841408821822759, 'lr': 0.0018081991423505635, 'ent_coef': 0.003385434793459635, 'batch_size_num': 8, 'learning_starts': 9, 'action_noise_int': 3, 'n_envs': 5}. Best is trial 20 with value: -3.0.[0m

 
Number of finished trials:  100
Best trial:
Value:  -3.0
Params: 
    gamma: 0.9795669800650622
    tau: 0.0028732553225876887
    lr: 0.0009069077086214123
    ent_coef: 0.008820073767094497
    batch_size_num: 8
    learning_starts: 9
    action_noise_int: 4
    n_envs: 4
User attrs:
    learning_rate: 0.0009069077086214123
    batch_size: 256
    learning_starts: 9000
    buffer_size: 50000
    train_freq: 50
    net_arch_width: 256
    net_arch_depth: 3
    gradient_steps: 50
    action_noise: 0.15000000000000002
    ent_coef: 0.008820073767094497
    n_envs: 4

 
TRAINING OPTIMIZED STUDY 
 

{'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 0, 'use_sde': 'True', 'use_sde_at_warmup': 'True', 'ent_coef': 0.008820073767094497, 'gradient_steps': 50, 'gamma': 0.9795669800650622, 'tau': 0.0028732553225876887, 'learning_rate': 0.0009069077086214123, 'batch_size': 256, 'learning_starts': 9000, 'buffer_size': 50000, 'train_freq': 50, 'policy_kwargs': {'net_arch': [256, 256, 256]}}
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning:

positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.

Num timesteps: 4000
Best mean reward: -inf - Last mean reward per episode: -50.00

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-01/SACOptunaPandaGraspDepth-v1/optimizedCallbackDir/model_20220702_073818_numTimesteps_4000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning:

positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.

Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-01/SACOptunaPandaGraspDepth-v1/optimizedCallbackDir/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning:

positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.

Num timesteps: 8000
Best mean reward: -50.00 - Last mean reward per episode: -50.00
Num timesteps: 12000
Best mean reward: -50.00 - Last mean reward per episode: -50.00
Num timesteps: 16000
Best mean reward: -50.00 - Last mean reward per episode: -49.67

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-01/SACOptunaPandaGraspDepth-v1/optimizedCallbackDir/model_20220702_073942_numTimesteps_16000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning:

positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.

Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-01/SACOptunaPandaGraspDepth-v1/optimizedCallbackDir/best_model
Num timesteps: 20000
Best mean reward: -49.67 - Last mean reward per episode: -49.96
Num timesteps: 24000
Best mean reward: -49.67 - Last mean reward per episode: -48.12

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-01/SACOptunaPandaGraspDepth-v1/optimizedCallbackDir/model_20220702_074109_numTimesteps_24000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning:

positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.

Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-01/SACOptunaPandaGraspDepth-v1/optimizedCallbackDir/best_model
Num timesteps: 28000
Best mean reward: -48.12 - Last mean reward per episode: -45.09

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-01/SACOptunaPandaGraspDepth-v1/optimizedCallbackDir/model_20220702_074148_numTimesteps_28000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning:

positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.

Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-01/SACOptunaPandaGraspDepth-v1/optimizedCallbackDir/best_model
Num timesteps: 32000
Best mean reward: -45.09 - Last mean reward per episode: -43.71

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-01/SACOptunaPandaGraspDepth-v1/optimizedCallbackDir/model_20220702_074227_numTimesteps_32000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning:

positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.

Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-01/SACOptunaPandaGraspDepth-v1/optimizedCallbackDir/best_model
Num timesteps: 36000
Best mean reward: -43.71 - Last mean reward per episode: -37.30

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-01/SACOptunaPandaGraspDepth-v1/optimizedCallbackDir/model_20220702_074306_numTimesteps_36000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning:

positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.

Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-01/SACOptunaPandaGraspDepth-v1/optimizedCallbackDir/best_model
Num timesteps: 40000
Best mean reward: -37.30 - Last mean reward per episode: -35.15

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-01/SACOptunaPandaGraspDepth-v1/optimizedCallbackDir/model_20220702_074344_numTimesteps_40000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning:

positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.

Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-01/SACOptunaPandaGraspDepth-v1/optimizedCallbackDir/best_model
Num timesteps: 44000
Best mean reward: -35.15 - Last mean reward per episode: -33.51

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-01/SACOptunaPandaGraspDepth-v1/optimizedCallbackDir/model_20220702_074424_numTimesteps_44000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning:

positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.

Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-01/SACOptunaPandaGraspDepth-v1/optimizedCallbackDir/best_model
Num timesteps: 48000
Best mean reward: -33.51 - Last mean reward per episode: -28.41

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-01/SACOptunaPandaGraspDepth-v1/optimizedCallbackDir/model_20220702_074507_numTimesteps_48000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning:

positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.

Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-01/SACOptunaPandaGraspDepth-v1/optimizedCallbackDir/best_model
Num timesteps: 52000
Best mean reward: -28.41 - Last mean reward per episode: -25.32

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-01/SACOptunaPandaGraspDepth-v1/optimizedCallbackDir/model_20220702_074544_numTimesteps_52000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning:

positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.

Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-01/SACOptunaPandaGraspDepth-v1/optimizedCallbackDir/best_model
Num timesteps: 56000
Best mean reward: -25.32 - Last mean reward per episode: -18.50

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-01/SACOptunaPandaGraspDepth-v1/optimizedCallbackDir/model_20220702_074631_numTimesteps_56000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning:

positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.

Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-01/SACOptunaPandaGraspDepth-v1/optimizedCallbackDir/best_model
Num timesteps: 60000
Best mean reward: -18.50 - Last mean reward per episode: -8.20

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-01/SACOptunaPandaGraspDepth-v1/optimizedCallbackDir/model_20220702_074713_numTimesteps_60000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning:

positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.

Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-01/SACOptunaPandaGraspDepth-v1/optimizedCallbackDir/best_model
Num timesteps: 64000
Best mean reward: -8.20 - Last mean reward per episode: -5.35

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-01/SACOptunaPandaGraspDepth-v1/optimizedCallbackDir/model_20220702_074752_numTimesteps_64000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning:

positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.

Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-01/SACOptunaPandaGraspDepth-v1/optimizedCallbackDir/best_model
Num timesteps: 68000
Best mean reward: -5.35 - Last mean reward per episode: -3.17

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-01/SACOptunaPandaGraspDepth-v1/optimizedCallbackDir/model_20220702_074832_numTimesteps_68000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning:

positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.

Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-01/SACOptunaPandaGraspDepth-v1/optimizedCallbackDir/best_model
Num timesteps: 72000
Best mean reward: -3.17 - Last mean reward per episode: -3.21
Num timesteps: 76000
Best mean reward: -3.17 - Last mean reward per episode: -3.32
Num timesteps: 80000
Best mean reward: -3.17 - Last mean reward per episode: -3.17
Num timesteps: 84000
Best mean reward: -3.17 - Last mean reward per episode: -3.10

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-01/SACOptunaPandaGraspDepth-v1/optimizedCallbackDir/model_20220702_075052_numTimesteps_84000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning:

positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.

Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-01/SACOptunaPandaGraspDepth-v1/optimizedCallbackDir/best_model
Num timesteps: 88000
Best mean reward: -3.10 - Last mean reward per episode: -3.15
Num timesteps: 92000
Best mean reward: -3.10 - Last mean reward per episode: -3.28
Num timesteps: 96000
Best mean reward: -3.10 - Last mean reward per episode: -3.26
Num timesteps: 100000
Best mean reward: -3.10 - Last mean reward per episode: -3.11
Num timesteps: 104000
Best mean reward: -3.10 - Last mean reward per episode: -3.20
Num timesteps: 108000
Best mean reward: -3.10 - Last mean reward per episode: -3.11
Num timesteps: 112000
Best mean reward: -3.10 - Last mean reward per episode: -3.12
Num timesteps: 116000
Best mean reward: -3.10 - Last mean reward per episode: -3.24
Num timesteps: 120000
Best mean reward: -3.10 - Last mean reward per episode: -3.22
Num timesteps: 124000
Best mean reward: -3.10 - Last mean reward per episode: -3.10
Num timesteps: 128000
Best mean reward: -3.10 - Last mean reward per episode: -3.10
Num timesteps: 132000
Best mean reward: -3.10 - Last mean reward per episode: -3.19
Num timesteps: 136000
Best mean reward: -3.10 - Last mean reward per episode: -3.09

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-01/SACOptunaPandaGraspDepth-v1/optimizedCallbackDir/model_20220702_075827_numTimesteps_136000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning:

positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.

Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-01/SACOptunaPandaGraspDepth-v1/optimizedCallbackDir/best_model
Num timesteps: 140000
Best mean reward: -3.09 - Last mean reward per episode: -3.14
Num timesteps: 144000
Best mean reward: -3.09 - Last mean reward per episode: -3.30
Num timesteps: 148000
Best mean reward: -3.09 - Last mean reward per episode: -3.22
Num timesteps: 152000
Best mean reward: -3.09 - Last mean reward per episode: -3.17
Num timesteps: 156000
Best mean reward: -3.09 - Last mean reward per episode: -3.16
Num timesteps: 160000
Best mean reward: -3.09 - Last mean reward per episode: -3.18
Num timesteps: 164000
Best mean reward: -3.09 - Last mean reward per episode: -3.17
Num timesteps: 168000
Best mean reward: -3.09 - Last mean reward per episode: -3.17
Num timesteps: 172000
Best mean reward: -3.09 - Last mean reward per episode: -3.19
Num timesteps: 176000
Best mean reward: -3.09 - Last mean reward per episode: -3.21
Num timesteps: 180000
Best mean reward: -3.09 - Last mean reward per episode: -3.22
Num timesteps: 184000
Best mean reward: -3.09 - Last mean reward per episode: -3.27
Num timesteps: 188000
Best mean reward: -3.09 - Last mean reward per episode: -3.26
Num timesteps: 192000
Best mean reward: -3.09 - Last mean reward per episode: -3.15
Num timesteps: 196000
Best mean reward: -3.09 - Last mean reward per episode: -3.13
Num timesteps: 200000
Best mean reward: -3.09 - Last mean reward per episode: -3.18
Num timesteps: 204000
Best mean reward: -3.09 - Last mean reward per episode: -3.14
Num timesteps: 208000
Best mean reward: -3.09 - Last mean reward per episode: -3.21
Num timesteps: 212000
Best mean reward: -3.09 - Last mean reward per episode: -3.11
Num timesteps: 216000
Best mean reward: -3.09 - Last mean reward per episode: -3.16
Num timesteps: 220000
Best mean reward: -3.09 - Last mean reward per episode: -3.63
Num timesteps: 224000
Best mean reward: -3.09 - Last mean reward per episode: -3.13
Num timesteps: 228000
Best mean reward: -3.09 - Last mean reward per episode: -3.25
Num timesteps: 232000
Best mean reward: -3.09 - Last mean reward per episode: -3.27
Num timesteps: 236000
Best mean reward: -3.09 - Last mean reward per episode: -3.17
Num timesteps: 240000
Best mean reward: -3.09 - Last mean reward per episode: -3.29
Num timesteps: 244000
Best mean reward: -3.09 - Last mean reward per episode: -3.21
Num timesteps: 248000
Best mean reward: -3.09 - Last mean reward per episode: -3.12
Num timesteps: 252000
Best mean reward: -3.09 - Last mean reward per episode: -3.06

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-01/SACOptunaPandaGraspDepth-v1/optimizedCallbackDir/model_20220702_081539_numTimesteps_252000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning:

positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.

Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-01/SACOptunaPandaGraspDepth-v1/optimizedCallbackDir/best_model
Num timesteps: 256000
Best mean reward: -3.06 - Last mean reward per episode: -3.38
Num timesteps: 260000
Best mean reward: -3.06 - Last mean reward per episode: -4.00
Num timesteps: 264000
Best mean reward: -3.06 - Last mean reward per episode: -3.21
Num timesteps: 268000
Best mean reward: -3.06 - Last mean reward per episode: -3.66
Num timesteps: 272000
Best mean reward: -3.06 - Last mean reward per episode: -3.15
Num timesteps: 276000
Best mean reward: -3.06 - Last mean reward per episode: -3.14
Num timesteps: 280000
Best mean reward: -3.06 - Last mean reward per episode: -3.12
Num timesteps: 284000
Best mean reward: -3.06 - Last mean reward per episode: -3.18
Num timesteps: 288000
Best mean reward: -3.06 - Last mean reward per episode: -3.18
Num timesteps: 292000
Best mean reward: -3.06 - Last mean reward per episode: -3.40
Num timesteps: 296000
Best mean reward: -3.06 - Last mean reward per episode: -3.57
Num timesteps: 300000
Best mean reward: -3.06 - Last mean reward per episode: -3.33
Num timesteps: 304000
Best mean reward: -3.06 - Last mean reward per episode: -3.13
Num timesteps: 308000
Best mean reward: -3.06 - Last mean reward per episode: -3.32
Num timesteps: 312000
Best mean reward: -3.06 - Last mean reward per episode: -3.13
Num timesteps: 316000
Best mean reward: -3.06 - Last mean reward per episode: -3.36
Num timesteps: 320000
Best mean reward: -3.06 - Last mean reward per episode: -3.67
Num timesteps: 324000
Best mean reward: -3.06 - Last mean reward per episode: -3.12
Num timesteps: 328000
Best mean reward: -3.06 - Last mean reward per episode: -3.17
Num timesteps: 332000
Best mean reward: -3.06 - Last mean reward per episode: -3.25
Num timesteps: 336000
Best mean reward: -3.06 - Last mean reward per episode: -3.24
Num timesteps: 340000
Best mean reward: -3.06 - Last mean reward per episode: -3.28
Num timesteps: 344000
Best mean reward: -3.06 - Last mean reward per episode: -3.39
Num timesteps: 348000
Best mean reward: -3.06 - Last mean reward per episode: -3.15
Num timesteps: 352000
Best mean reward: -3.06 - Last mean reward per episode: -3.25
Num timesteps: 356000
Best mean reward: -3.06 - Last mean reward per episode: -3.24
Num timesteps: 360000
Best mean reward: -3.06 - Last mean reward per episode: -3.20
Num timesteps: 364000
Best mean reward: -3.06 - Last mean reward per episode: -3.24
Num timesteps: 368000
Best mean reward: -3.06 - Last mean reward per episode: -3.13
Num timesteps: 372000
Best mean reward: -3.06 - Last mean reward per episode: -3.16
Num timesteps: 376000
Best mean reward: -3.06 - Last mean reward per episode: -3.10
Num timesteps: 380000
Best mean reward: -3.06 - Last mean reward per episode: -3.14
Num timesteps: 384000
Best mean reward: -3.06 - Last mean reward per episode: -3.23
Num timesteps: 388000
Best mean reward: -3.06 - Last mean reward per episode: -3.21
Num timesteps: 392000
Best mean reward: -3.06 - Last mean reward per episode: -3.16
Num timesteps: 396000
Best mean reward: -3.06 - Last mean reward per episode: -3.11
Num timesteps: 400000
Best mean reward: -3.06 - Last mean reward per episode: -3.24
Num timesteps: 404000
Best mean reward: -3.06 - Last mean reward per episode: -3.11
Num timesteps: 408000
Best mean reward: -3.06 - Last mean reward per episode: -3.15
Num timesteps: 412000
Best mean reward: -3.06 - Last mean reward per episode: -3.62
Num timesteps: 416000
Best mean reward: -3.06 - Last mean reward per episode: -3.50
Num timesteps: 420000
Best mean reward: -3.06 - Last mean reward per episode: -3.21
Num timesteps: 424000
Best mean reward: -3.06 - Last mean reward per episode: -3.10
Num timesteps: 428000
Best mean reward: -3.06 - Last mean reward per episode: -3.15
Num timesteps: 432000
Best mean reward: -3.06 - Last mean reward per episode: -3.26
Num timesteps: 436000
Best mean reward: -3.06 - Last mean reward per episode: -3.10
Num timesteps: 440000
Best mean reward: -3.06 - Last mean reward per episode: -3.10
Num timesteps: 444000
Best mean reward: -3.06 - Last mean reward per episode: -3.21
Num timesteps: 448000
Best mean reward: -3.06 - Last mean reward per episode: -3.54
Num timesteps: 452000
Best mean reward: -3.06 - Last mean reward per episode: -3.13
Num timesteps: 456000
Best mean reward: -3.06 - Last mean reward per episode: -3.16
Num timesteps: 460000
Best mean reward: -3.06 - Last mean reward per episode: -3.11
Num timesteps: 464000
Best mean reward: -3.06 - Last mean reward per episode: -3.07
Num timesteps: 468000
Best mean reward: -3.06 - Last mean reward per episode: -3.11
Num timesteps: 472000
Best mean reward: -3.06 - Last mean reward per episode: -3.20
Num timesteps: 476000
Best mean reward: -3.06 - Last mean reward per episode: -3.08
Num timesteps: 480000
Best mean reward: -3.06 - Last mean reward per episode: -3.15
Num timesteps: 484000
Best mean reward: -3.06 - Last mean reward per episode: -3.59
Num timesteps: 488000
Best mean reward: -3.06 - Last mean reward per episode: -3.10
Num timesteps: 492000
Best mean reward: -3.06 - Last mean reward per episode: -3.04

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-01/SACOptunaPandaGraspDepth-v1/optimizedCallbackDir/model_20220702_085202_numTimesteps_492000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning:

positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.

Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-01/SACOptunaPandaGraspDepth-v1/optimizedCallbackDir/best_model
Num timesteps: 496000
Best mean reward: -3.04 - Last mean reward per episode: -3.32
Num timesteps: 500000
Best mean reward: -3.04 - Last mean reward per episode: -3.30

 
RECORDING OPTIMIZED STUDY 
 

argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
/home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/../env/pybulletCust.py:95: UserWarning:

The use of the render method is not recommended when the environment has not been created with render=True. The rendering will probably be weird. Prefer making the environment with option `render=True`. For example: `env = gym.make('PandaReach-v2', render=True)`.

/home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/../env/pybulletCust.py:95: UserWarning:

The use of the render method is not recommended when the environment has not been created with render=True. The rendering will probably be weird. Prefer making the environment with option `render=True`. For example: `env = gym.make('PandaReach-v2', render=True)`.

Saving video to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-01/SACOptunaPandaGraspDepth-v1/videos/SAC-Vect-PandaGraspDepth-v1-step-0-to-step-400.mp4
Exception ignored in: <function BulletClient.__del__ at 0x7faa0e698f70>
Traceback (most recent call last):
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/pybullet_utils/bullet_client.py", line 39, in __del__
AttributeError: 'NoneType' object has no attribute 'getpid'
Exception ignored in: <function BulletClient.__del__ at 0x7faa0e698f70>
Traceback (most recent call last):
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/pybullet_utils/bullet_client.py", line 39, in __del__
AttributeError: 'NoneType' object has no attribute 'getpid'
Exception ignored in: <function BulletClient.__del__ at 0x7faa0e698f70>
Traceback (most recent call last):
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/pybullet_utils/bullet_client.py", line 39, in __del__
AttributeError: 'NoneType' object has no attribute 'getpid'
Exception ignored in: <function BulletClient.__del__ at 0x7faa0e698f70>
Traceback (most recent call last):
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/pybullet_utils/bullet_client.py", line 39, in __del__
AttributeError: