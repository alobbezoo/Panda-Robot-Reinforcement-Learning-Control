Script started on 2022-07-02 19:14:58-04:00 [TERM="xterm-256color" TTY="/dev/pts/9" COLUMNS="211" LINES="55"]
bash: devel/setup.bash: No such file or directory
bash: /home/hjkwon/catkin_ws/src/moveit/devel/setup.bash: No such file or directory
]0;hjkwon@HJK-AI-Robotics: ~/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02[01;32mhjkwon@HJK-AI-Robotics[00m:[01;34m~/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02[00m$ cd ..
]0;hjkwon@HJK-AI-Robotics: ~/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining[01;32mhjkwon@HJK-AI-Robotics[00m:[01;34m~/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining[00m$ conda [K activate lr_[K[K[Krl_env
(rl_env) ]0;hjkwon@HJK-AI-Robotics: ~/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining[01;32mhjkwon@HJK-AI-Robotics[00m:[01;34m~/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining[00m$ python3 mi[KainPPOVect2.py 
pybullet build time: Dec  1 2021 18:34:28
_XSERVTransSocketUNIXCreateListener: ...SocketCreateListener() failed
_XSERVTransMakeAllCOTSServerListeners: server already running
(EE) 
Fatal server error:
(EE) Cannot establish any listening sockets - Make sure an X server isn't already running(EE) 
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413

 
 kwargs:  {'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 1, 'gamma': 0.9270335099881851, 'gae_lambda': 0.8237685826071883, 'learning_rate': 0.0013378119211869901, 'ent_coef': 1e-06, 'max_grad_norm': 0.5, 'vf_coef': 0.75, 'clip_range': 0.075, 'batch_size': 1024, 'n_steps': 32768, 'n_epochs': 9, 'policy_kwargs': {'net_arch': [{'pi': [128, 128, 128, 128, 128], 'vf': [128, 128, 128, 128, 128]}]}}
n_envs:  3
NOTE: Testing the model which performed well for grasping with the reach problem


Using cpu device
Num timesteps: 3000
Best mean reward: -inf - Last mean reward per episode: -49.63

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_191523_numTimesteps_3000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 6000
Best mean reward: -49.63 - Last mean reward per episode: -49.59

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_191529_numTimesteps_6000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 9000
Best mean reward: -49.59 - Last mean reward per episode: -49.55

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_191536_numTimesteps_9000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 12000
Best mean reward: -49.55 - Last mean reward per episode: -49.54

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_191541_numTimesteps_12000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 15000
Best mean reward: -49.54 - Last mean reward per episode: -49.54
Num timesteps: 18000
Best mean reward: -49.54 - Last mean reward per episode: -49.46

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_191553_numTimesteps_18000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 21000
Best mean reward: -49.46 - Last mean reward per episode: -49.72
Num timesteps: 24000
Best mean reward: -49.46 - Last mean reward per episode: -49.63
Num timesteps: 27000
Best mean reward: -49.46 - Last mean reward per episode: -49.68
Num timesteps: 30000
Best mean reward: -49.46 - Last mean reward per episode: -49.74
Num timesteps: 33000
Best mean reward: -49.46 - Last mean reward per episode: -49.61
Num timesteps: 36000
Best mean reward: -49.46 - Last mean reward per episode: -49.52
Num timesteps: 39000
Best mean reward: -49.46 - Last mean reward per episode: -49.63
Num timesteps: 42000
Best mean reward: -49.46 - Last mean reward per episode: -49.64
Num timesteps: 45000
Best mean reward: -49.46 - Last mean reward per episode: -49.39

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_191643_numTimesteps_45000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 48000
Best mean reward: -49.39 - Last mean reward per episode: -49.49
Num timesteps: 51000
Best mean reward: -49.39 - Last mean reward per episode: -49.63
Num timesteps: 54000
Best mean reward: -49.39 - Last mean reward per episode: -49.51
Num timesteps: 57000
Best mean reward: -49.39 - Last mean reward per episode: -49.34

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_191709_numTimesteps_57000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 60000
Best mean reward: -49.34 - Last mean reward per episode: -49.45
Num timesteps: 63000
Best mean reward: -49.34 - Last mean reward per episode: -49.66
Num timesteps: 66000
Best mean reward: -49.34 - Last mean reward per episode: -49.66
Num timesteps: 69000
Best mean reward: -49.34 - Last mean reward per episode: -49.50
Num timesteps: 72000
Best mean reward: -49.34 - Last mean reward per episode: -49.61
Num timesteps: 75000
Best mean reward: -49.34 - Last mean reward per episode: -49.50
Num timesteps: 78000
Best mean reward: -49.34 - Last mean reward per episode: -49.66
Num timesteps: 81000
Best mean reward: -49.34 - Last mean reward per episode: -49.51
Num timesteps: 84000
Best mean reward: -49.34 - Last mean reward per episode: -49.47
Num timesteps: 87000
Best mean reward: -49.34 - Last mean reward per episode: -49.50
Num timesteps: 90000
Best mean reward: -49.34 - Last mean reward per episode: -49.47
Num timesteps: 93000
Best mean reward: -49.34 - Last mean reward per episode: -49.49
Num timesteps: 96000
Best mean reward: -49.34 - Last mean reward per episode: -49.19

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_191827_numTimesteps_96000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | -49.2    |
| time/              |          |
|    fps             | 504      |
|    iterations      | 1        |
|    time_elapsed    | 194      |
|    total_timesteps | 98304    |
---------------------------------
Num timesteps: 99000
Best mean reward: -49.19 - Last mean reward per episode: -49.36
Num timesteps: 102000
Best mean reward: -49.19 - Last mean reward per episode: -49.32
Num timesteps: 105000
Best mean reward: -49.19 - Last mean reward per episode: -49.42
Num timesteps: 108000
Best mean reward: -49.19 - Last mean reward per episode: -49.42
Num timesteps: 111000
Best mean reward: -49.19 - Last mean reward per episode: -49.51
Num timesteps: 114000
Best mean reward: -49.19 - Last mean reward per episode: -49.71
Num timesteps: 117000
Best mean reward: -49.19 - Last mean reward per episode: -49.64
Num timesteps: 120000
Best mean reward: -49.19 - Last mean reward per episode: -49.69
Num timesteps: 123000
Best mean reward: -49.19 - Last mean reward per episode: -49.61
Num timesteps: 126000
Best mean reward: -49.19 - Last mean reward per episode: -49.56
Num timesteps: 129000
Best mean reward: -49.19 - Last mean reward per episode: -49.58
Num timesteps: 132000
Best mean reward: -49.19 - Last mean reward per episode: -49.64
Num timesteps: 135000
Best mean reward: -49.19 - Last mean reward per episode: -49.50
Num timesteps: 138000
Best mean reward: -49.19 - Last mean reward per episode: -49.38
Num timesteps: 141000
Best mean reward: -49.19 - Last mean reward per episode: -49.40
Num timesteps: 144000
Best mean reward: -49.19 - Last mean reward per episode: -49.43
Num timesteps: 147000
Best mean reward: -49.19 - Last mean reward per episode: -49.50
Num timesteps: 150000
Best mean reward: -49.19 - Last mean reward per episode: -49.59
Num timesteps: 153000
Best mean reward: -49.19 - Last mean reward per episode: -49.45
Num timesteps: 156000
Best mean reward: -49.19 - Last mean reward per episode: -49.48
Num timesteps: 159000
Best mean reward: -49.19 - Last mean reward per episode: -49.71
Num timesteps: 162000
Best mean reward: -49.19 - Last mean reward per episode: -49.62
Num timesteps: 165000
Best mean reward: -49.19 - Last mean reward per episode: -49.36
Num timesteps: 168000
Best mean reward: -49.19 - Last mean reward per episode: -49.68
Num timesteps: 171000
Best mean reward: -49.19 - Last mean reward per episode: -49.49
Num timesteps: 174000
Best mean reward: -49.19 - Last mean reward per episode: -49.47
Num timesteps: 177000
Best mean reward: -49.19 - Last mean reward per episode: -49.69
Num timesteps: 180000
Best mean reward: -49.19 - Last mean reward per episode: -49.57
Num timesteps: 183000
Best mean reward: -49.19 - Last mean reward per episode: -49.37
Num timesteps: 186000
Best mean reward: -49.19 - Last mean reward per episode: -49.68
Num timesteps: 189000
Best mean reward: -49.19 - Last mean reward per episode: -49.72
Num timesteps: 192000
Best mean reward: -49.19 - Last mean reward per episode: -49.66
Num timesteps: 195000
Best mean reward: -49.19 - Last mean reward per episode: -49.60
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 50            |
|    ep_rew_mean          | -49.5         |
| time/                   |               |
|    fps                  | 462           |
|    iterations           | 2             |
|    time_elapsed         | 425           |
|    total_timesteps      | 196608        |
| train/                  |               |
|    approx_kl            | 0.00048541572 |
|    clip_fraction        | 0.026         |
|    clip_range           | 0.075         |
|    entropy_loss         | -4.25         |
|    explained_variance   | -0.0286       |
|    learning_rate        | 0.00134       |
|    loss                 | 0.273         |
|    n_updates            | 9             |
|    policy_gradient_loss | -0.000196     |
|    std                  | 0.999         |
|    value_loss           | 0.514         |
-------------------------------------------
Num timesteps: 198000
Best mean reward: -49.19 - Last mean reward per episode: -49.44
Num timesteps: 201000
Best mean reward: -49.19 - Last mean reward per episode: -49.67
Num timesteps: 204000
Best mean reward: -49.19 - Last mean reward per episode: -49.79
Num timesteps: 207000
Best mean reward: -49.19 - Last mean reward per episode: -49.63
Num timesteps: 210000
Best mean reward: -49.19 - Last mean reward per episode: -49.60
Num timesteps: 213000
Best mean reward: -49.19 - Last mean reward per episode: -49.47
Num timesteps: 216000
Best mean reward: -49.19 - Last mean reward per episode: -49.30
Num timesteps: 219000
Best mean reward: -49.19 - Last mean reward per episode: -49.46
Num timesteps: 222000
Best mean reward: -49.19 - Last mean reward per episode: -49.73
Num timesteps: 225000
Best mean reward: -49.19 - Last mean reward per episode: -49.48
Num timesteps: 228000
Best mean reward: -49.19 - Last mean reward per episode: -49.52
Num timesteps: 231000
Best mean reward: -49.19 - Last mean reward per episode: -49.59
Num timesteps: 234000
Best mean reward: -49.19 - Last mean reward per episode: -49.58
Num timesteps: 237000
Best mean reward: -49.19 - Last mean reward per episode: -49.79
Num timesteps: 240000
Best mean reward: -49.19 - Last mean reward per episode: -49.76
Num timesteps: 243000
Best mean reward: -49.19 - Last mean reward per episode: -49.44
Num timesteps: 246000
Best mean reward: -49.19 - Last mean reward per episode: -49.41
Num timesteps: 249000
Best mean reward: -49.19 - Last mean reward per episode: -49.72
Num timesteps: 252000
Best mean reward: -49.19 - Last mean reward per episode: -49.28
Num timesteps: 255000
Best mean reward: -49.19 - Last mean reward per episode: -49.42
Num timesteps: 258000
Best mean reward: -49.19 - Last mean reward per episode: -49.38
Num timesteps: 261000
Best mean reward: -49.19 - Last mean reward per episode: -49.57
Num timesteps: 264000
Best mean reward: -49.19 - Last mean reward per episode: -49.56
Num timesteps: 267000
Best mean reward: -49.19 - Last mean reward per episode: -49.47
Num timesteps: 270000
Best mean reward: -49.19 - Last mean reward per episode: -49.62
Num timesteps: 273000
Best mean reward: -49.19 - Last mean reward per episode: -49.58
Num timesteps: 276000
Best mean reward: -49.19 - Last mean reward per episode: -49.64
Num timesteps: 279000
Best mean reward: -49.19 - Last mean reward per episode: -49.53
Num timesteps: 282000
Best mean reward: -49.19 - Last mean reward per episode: -49.41
Num timesteps: 285000
Best mean reward: -49.19 - Last mean reward per episode: -49.48
Num timesteps: 288000
Best mean reward: -49.19 - Last mean reward per episode: -49.67
Num timesteps: 291000
Best mean reward: -49.19 - Last mean reward per episode: -49.50
Num timesteps: 294000
Best mean reward: -49.19 - Last mean reward per episode: -49.43
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 50            |
|    ep_rew_mean          | -49.5         |
| time/                   |               |
|    fps                  | 458           |
|    iterations           | 3             |
|    time_elapsed         | 642           |
|    total_timesteps      | 294912        |
| train/                  |               |
|    approx_kl            | 0.00067326723 |
|    clip_fraction        | 0.0457        |
|    clip_range           | 0.075         |
|    entropy_loss         | -4.25         |
|    explained_variance   | 0.162         |
|    learning_rate        | 0.00134       |
|    loss                 | 0.178         |
|    n_updates            | 18            |
|    policy_gradient_loss | -0.000684     |
|    std                  | 1             |
|    value_loss           | 0.318         |
-------------------------------------------
Num timesteps: 297000
Best mean reward: -49.19 - Last mean reward per episode: -49.32
Num timesteps: 300000
Best mean reward: -49.19 - Last mean reward per episode: -49.13

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_192627_numTimesteps_300000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 303000
Best mean reward: -49.13 - Last mean reward per episode: -49.43
Num timesteps: 306000
Best mean reward: -49.13 - Last mean reward per episode: -49.71
Num timesteps: 309000
Best mean reward: -49.13 - Last mean reward per episode: -49.67
Num timesteps: 312000
Best mean reward: -49.13 - Last mean reward per episode: -49.16
Num timesteps: 315000
Best mean reward: -49.13 - Last mean reward per episode: -49.44
Num timesteps: 318000
Best mean reward: -49.13 - Last mean reward per episode: -49.75
Num timesteps: 321000
Best mean reward: -49.13 - Last mean reward per episode: -49.52
Num timesteps: 324000
Best mean reward: -49.13 - Last mean reward per episode: -49.50
Num timesteps: 327000
Best mean reward: -49.13 - Last mean reward per episode: -49.44
Num timesteps: 330000
Best mean reward: -49.13 - Last mean reward per episode: -49.18
Num timesteps: 333000
Best mean reward: -49.13 - Last mean reward per episode: -49.65
Num timesteps: 336000
Best mean reward: -49.13 - Last mean reward per episode: -49.88
Num timesteps: 339000
Best mean reward: -49.13 - Last mean reward per episode: -49.61
Num timesteps: 342000
Best mean reward: -49.13 - Last mean reward per episode: -49.41
Num timesteps: 345000
Best mean reward: -49.13 - Last mean reward per episode: -49.46
Num timesteps: 348000
Best mean reward: -49.13 - Last mean reward per episode: -49.71
Num timesteps: 351000
Best mean reward: -49.13 - Last mean reward per episode: -49.68
Num timesteps: 354000
Best mean reward: -49.13 - Last mean reward per episode: -49.64
Num timesteps: 357000
Best mean reward: -49.13 - Last mean reward per episode: -49.35
Num timesteps: 360000
Best mean reward: -49.13 - Last mean reward per episode: -49.49
Num timesteps: 363000
Best mean reward: -49.13 - Last mean reward per episode: -49.72
Num timesteps: 366000
Best mean reward: -49.13 - Last mean reward per episode: -49.63
Num timesteps: 369000
Best mean reward: -49.13 - Last mean reward per episode: -49.54
Num timesteps: 372000
Best mean reward: -49.13 - Last mean reward per episode: -49.46
Num timesteps: 375000
Best mean reward: -49.13 - Last mean reward per episode: -49.52
Num timesteps: 378000
Best mean reward: -49.13 - Last mean reward per episode: -49.80
Num timesteps: 381000
Best mean reward: -49.13 - Last mean reward per episode: -49.62
Num timesteps: 384000
Best mean reward: -49.13 - Last mean reward per episode: -49.31
Num timesteps: 387000
Best mean reward: -49.13 - Last mean reward per episode: -49.27
Num timesteps: 390000
Best mean reward: -49.13 - Last mean reward per episode: -49.30
Num timesteps: 393000
Best mean reward: -49.13 - Last mean reward per episode: -49.44
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 50           |
|    ep_rew_mean          | -49.5        |
| time/                   |              |
|    fps                  | 458          |
|    iterations           | 4            |
|    time_elapsed         | 857          |
|    total_timesteps      | 393216       |
| train/                  |              |
|    approx_kl            | 0.0008914881 |
|    clip_fraction        | 0.0754       |
|    clip_range           | 0.075        |
|    entropy_loss         | -4.26        |
|    explained_variance   | 0.321        |
|    learning_rate        | 0.00134      |
|    loss                 | 0.105        |
|    n_updates            | 27           |
|    policy_gradient_loss | -0.00157     |
|    std                  | 0.998        |
|    value_loss           | 0.181        |
------------------------------------------
Num timesteps: 396000
Best mean reward: -49.13 - Last mean reward per episode: -49.64
Num timesteps: 399000
Best mean reward: -49.13 - Last mean reward per episode: -49.38
Num timesteps: 402000
Best mean reward: -49.13 - Last mean reward per episode: -49.56
Num timesteps: 405000
Best mean reward: -49.13 - Last mean reward per episode: -49.43
Num timesteps: 408000
Best mean reward: -49.13 - Last mean reward per episode: -49.24
Num timesteps: 411000
Best mean reward: -49.13 - Last mean reward per episode: -49.19
Num timesteps: 414000
Best mean reward: -49.13 - Last mean reward per episode: -49.57
Num timesteps: 417000
Best mean reward: -49.13 - Last mean reward per episode: -49.42
Num timesteps: 420000
Best mean reward: -49.13 - Last mean reward per episode: -49.50
Num timesteps: 423000
Best mean reward: -49.13 - Last mean reward per episode: -49.58
Num timesteps: 426000
Best mean reward: -49.13 - Last mean reward per episode: -49.34
Num timesteps: 429000
Best mean reward: -49.13 - Last mean reward per episode: -49.46
Num timesteps: 432000
Best mean reward: -49.13 - Last mean reward per episode: -49.47
Num timesteps: 435000
Best mean reward: -49.13 - Last mean reward per episode: -49.54
Num timesteps: 438000
Best mean reward: -49.13 - Last mean reward per episode: -49.53
Num timesteps: 441000
Best mean reward: -49.13 - Last mean reward per episode: -49.62
Num timesteps: 444000
Best mean reward: -49.13 - Last mean reward per episode: -49.58
Num timesteps: 447000
Best mean reward: -49.13 - Last mean reward per episode: -49.44
Num timesteps: 450000
Best mean reward: -49.13 - Last mean reward per episode: -49.60
Num timesteps: 453000
Best mean reward: -49.13 - Last mean reward per episode: -49.65
Num timesteps: 456000
Best mean reward: -49.13 - Last mean reward per episode: -49.64
Num timesteps: 459000
Best mean reward: -49.13 - Last mean reward per episode: -49.37
Num timesteps: 462000
Best mean reward: -49.13 - Last mean reward per episode: -49.46
Num timesteps: 465000
Best mean reward: -49.13 - Last mean reward per episode: -49.38
Num timesteps: 468000
Best mean reward: -49.13 - Last mean reward per episode: -49.43
Num timesteps: 471000
Best mean reward: -49.13 - Last mean reward per episode: -49.34
Num timesteps: 474000
Best mean reward: -49.13 - Last mean reward per episode: -49.45
Num timesteps: 477000
Best mean reward: -49.13 - Last mean reward per episode: -49.41
Num timesteps: 480000
Best mean reward: -49.13 - Last mean reward per episode: -49.34
Num timesteps: 483000
Best mean reward: -49.13 - Last mean reward per episode: -49.18
Num timesteps: 486000
Best mean reward: -49.13 - Last mean reward per episode: -49.03

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_193247_numTimesteps_486000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 489000
Best mean reward: -49.03 - Last mean reward per episode: -49.19
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 50          |
|    ep_rew_mean          | -49.6       |
| time/                   |             |
|    fps                  | 463         |
|    iterations           | 5           |
|    time_elapsed         | 1059        |
|    total_timesteps      | 491520      |
| train/                  |             |
|    approx_kl            | 0.001352887 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.075       |
|    entropy_loss         | -4.25       |
|    explained_variance   | 0.464       |
|    learning_rate        | 0.00134     |
|    loss                 | 0.0852      |
|    n_updates            | 36          |
|    policy_gradient_loss | -0.00272    |
|    std                  | 1           |
|    value_loss           | 0.126       |
-----------------------------------------
Num timesteps: 492000
Best mean reward: -49.03 - Last mean reward per episode: -49.67
Num timesteps: 495000
Best mean reward: -49.03 - Last mean reward per episode: -49.60
Num timesteps: 498000
Best mean reward: -49.03 - Last mean reward per episode: -49.56
Num timesteps: 501000
Best mean reward: -49.03 - Last mean reward per episode: -49.59
Num timesteps: 504000
Best mean reward: -49.03 - Last mean reward per episode: -49.40
Num timesteps: 507000
Best mean reward: -49.03 - Last mean reward per episode: -49.59
Num timesteps: 510000
Best mean reward: -49.03 - Last mean reward per episode: -49.45
Num timesteps: 513000
Best mean reward: -49.03 - Last mean reward per episode: -49.57
Num timesteps: 516000
Best mean reward: -49.03 - Last mean reward per episode: -49.66
Num timesteps: 519000
Best mean reward: -49.03 - Last mean reward per episode: -49.61
Num timesteps: 522000
Best mean reward: -49.03 - Last mean reward per episode: -49.55
Num timesteps: 525000
Best mean reward: -49.03 - Last mean reward per episode: -49.67
Num timesteps: 528000
Best mean reward: -49.03 - Last mean reward per episode: -49.65
Num timesteps: 531000
Best mean reward: -49.03 - Last mean reward per episode: -49.45
Num timesteps: 534000
Best mean reward: -49.03 - Last mean reward per episode: -49.18
Num timesteps: 537000
Best mean reward: -49.03 - Last mean reward per episode: -49.33
Num timesteps: 540000
Best mean reward: -49.03 - Last mean reward per episode: -49.09
Num timesteps: 543000
Best mean reward: -49.03 - Last mean reward per episode: -49.13
Num timesteps: 546000
Best mean reward: -49.03 - Last mean reward per episode: -49.18
Num timesteps: 549000
Best mean reward: -49.03 - Last mean reward per episode: -48.75

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_193457_numTimesteps_549000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 552000
Best mean reward: -48.75 - Last mean reward per episode: -49.41
Num timesteps: 555000
Best mean reward: -48.75 - Last mean reward per episode: -49.80
Num timesteps: 558000
Best mean reward: -48.75 - Last mean reward per episode: -49.57
Num timesteps: 561000
Best mean reward: -48.75 - Last mean reward per episode: -49.46
Num timesteps: 564000
Best mean reward: -48.75 - Last mean reward per episode: -49.54
Num timesteps: 567000
Best mean reward: -48.75 - Last mean reward per episode: -49.08
Num timesteps: 570000
Best mean reward: -48.75 - Last mean reward per episode: -49.30
Num timesteps: 573000
Best mean reward: -48.75 - Last mean reward per episode: -49.48
Num timesteps: 576000
Best mean reward: -48.75 - Last mean reward per episode: -49.71
Num timesteps: 579000
Best mean reward: -48.75 - Last mean reward per episode: -49.71
Num timesteps: 582000
Best mean reward: -48.75 - Last mean reward per episode: -49.71
Num timesteps: 585000
Best mean reward: -48.75 - Last mean reward per episode: -49.41
Num timesteps: 588000
Best mean reward: -48.75 - Last mean reward per episode: -49.22
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 50           |
|    ep_rew_mean          | -49.5        |
| time/                   |              |
|    fps                  | 468          |
|    iterations           | 6            |
|    time_elapsed         | 1258         |
|    total_timesteps      | 589824       |
| train/                  |              |
|    approx_kl            | 0.0016877285 |
|    clip_fraction        | 0.165        |
|    clip_range           | 0.075        |
|    entropy_loss         | -4.26        |
|    explained_variance   | 0.565        |
|    learning_rate        | 0.00134      |
|    loss                 | 0.061        |
|    n_updates            | 45           |
|    policy_gradient_loss | -0.00328     |
|    std                  | 0.999        |
|    value_loss           | 0.106        |
------------------------------------------
Num timesteps: 591000
Best mean reward: -48.75 - Last mean reward per episode: -49.53
Num timesteps: 594000
Best mean reward: -48.75 - Last mean reward per episode: -49.42
Num timesteps: 597000
Best mean reward: -48.75 - Last mean reward per episode: -49.40
Num timesteps: 600000
Best mean reward: -48.75 - Last mean reward per episode: -49.30
Num timesteps: 603000
Best mean reward: -48.75 - Last mean reward per episode: -49.53
Num timesteps: 606000
Best mean reward: -48.75 - Last mean reward per episode: -49.61
Num timesteps: 609000
Best mean reward: -48.75 - Last mean reward per episode: -49.59
Num timesteps: 612000
Best mean reward: -48.75 - Last mean reward per episode: -49.47
Num timesteps: 615000
Best mean reward: -48.75 - Last mean reward per episode: -49.18
Num timesteps: 618000
Best mean reward: -48.75 - Last mean reward per episode: -49.27
Num timesteps: 621000
Best mean reward: -48.75 - Last mean reward per episode: -49.40
Num timesteps: 624000
Best mean reward: -48.75 - Last mean reward per episode: -49.29
Num timesteps: 627000
Best mean reward: -48.75 - Last mean reward per episode: -49.25
Num timesteps: 630000
Best mean reward: -48.75 - Last mean reward per episode: -49.32
Num timesteps: 633000
Best mean reward: -48.75 - Last mean reward per episode: -49.25
Num timesteps: 636000
Best mean reward: -48.75 - Last mean reward per episode: -49.47
Num timesteps: 639000
Best mean reward: -48.75 - Last mean reward per episode: -49.66
Num timesteps: 642000
Best mean reward: -48.75 - Last mean reward per episode: -49.60
Num timesteps: 645000
Best mean reward: -48.75 - Last mean reward per episode: -49.39
Num timesteps: 648000
Best mean reward: -48.75 - Last mean reward per episode: -49.41
Num timesteps: 651000
Best mean reward: -48.75 - Last mean reward per episode: -49.18
Num timesteps: 654000
Best mean reward: -48.75 - Last mean reward per episode: -49.35
Num timesteps: 657000
Best mean reward: -48.75 - Last mean reward per episode: -49.59
Num timesteps: 660000
Best mean reward: -48.75 - Last mean reward per episode: -49.30
Num timesteps: 663000
Best mean reward: -48.75 - Last mean reward per episode: -49.28
Num timesteps: 666000
Best mean reward: -48.75 - Last mean reward per episode: -49.41
Num timesteps: 669000
Best mean reward: -48.75 - Last mean reward per episode: -49.38
Num timesteps: 672000
Best mean reward: -48.75 - Last mean reward per episode: -49.46
Num timesteps: 675000
Best mean reward: -48.75 - Last mean reward per episode: -49.30
Num timesteps: 678000
Best mean reward: -48.75 - Last mean reward per episode: -49.26
Num timesteps: 681000
Best mean reward: -48.75 - Last mean reward per episode: -49.34
Num timesteps: 684000
Best mean reward: -48.75 - Last mean reward per episode: -49.28
Num timesteps: 687000
Best mean reward: -48.75 - Last mean reward per episode: -49.19
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 50           |
|    ep_rew_mean          | -49.2        |
| time/                   |              |
|    fps                  | 467          |
|    iterations           | 7            |
|    time_elapsed         | 1472         |
|    total_timesteps      | 688128       |
| train/                  |              |
|    approx_kl            | 0.0021322921 |
|    clip_fraction        | 0.193        |
|    clip_range           | 0.075        |
|    entropy_loss         | -4.25        |
|    explained_variance   | 0.569        |
|    learning_rate        | 0.00134      |
|    loss                 | 0.0671       |
|    n_updates            | 54           |
|    policy_gradient_loss | -0.00379     |
|    std                  | 0.997        |
|    value_loss           | 0.0879       |
------------------------------------------
Num timesteps: 690000
Best mean reward: -48.75 - Last mean reward per episode: -49.13
Num timesteps: 693000
Best mean reward: -48.75 - Last mean reward per episode: -49.30
Num timesteps: 696000
Best mean reward: -48.75 - Last mean reward per episode: -49.32
Num timesteps: 699000
Best mean reward: -48.75 - Last mean reward per episode: -49.25
Num timesteps: 702000
Best mean reward: -48.75 - Last mean reward per episode: -49.31
Num timesteps: 705000
Best mean reward: -48.75 - Last mean reward per episode: -49.02
Num timesteps: 708000
Best mean reward: -48.75 - Last mean reward per episode: -48.96
Num timesteps: 711000
Best mean reward: -48.75 - Last mean reward per episode: -48.97
Num timesteps: 714000
Best mean reward: -48.75 - Last mean reward per episode: -49.18
Num timesteps: 717000
Best mean reward: -48.75 - Last mean reward per episode: -49.07
Num timesteps: 720000
Best mean reward: -48.75 - Last mean reward per episode: -48.89
Num timesteps: 723000
Best mean reward: -48.75 - Last mean reward per episode: -48.82
Num timesteps: 726000
Best mean reward: -48.75 - Last mean reward per episode: -48.83
Num timesteps: 729000
Best mean reward: -48.75 - Last mean reward per episode: -49.26
Num timesteps: 732000
Best mean reward: -48.75 - Last mean reward per episode: -49.36
Num timesteps: 735000
Best mean reward: -48.75 - Last mean reward per episode: -49.33
Num timesteps: 738000
Best mean reward: -48.75 - Last mean reward per episode: -49.27
Num timesteps: 741000
Best mean reward: -48.75 - Last mean reward per episode: -49.20
Num timesteps: 744000
Best mean reward: -48.75 - Last mean reward per episode: -49.22
Num timesteps: 747000
Best mean reward: -48.75 - Last mean reward per episode: -49.17
Num timesteps: 750000
Best mean reward: -48.75 - Last mean reward per episode: -49.24
Num timesteps: 753000
Best mean reward: -48.75 - Last mean reward per episode: -49.14
Num timesteps: 756000
Best mean reward: -48.75 - Last mean reward per episode: -49.27
Num timesteps: 759000
Best mean reward: -48.75 - Last mean reward per episode: -49.16
Num timesteps: 762000
Best mean reward: -48.75 - Last mean reward per episode: -49.11
Num timesteps: 765000
Best mean reward: -48.75 - Last mean reward per episode: -49.16
Num timesteps: 768000
Best mean reward: -48.75 - Last mean reward per episode: -49.31
Num timesteps: 771000
Best mean reward: -48.75 - Last mean reward per episode: -49.33
Num timesteps: 774000
Best mean reward: -48.75 - Last mean reward per episode: -48.87
Num timesteps: 777000
Best mean reward: -48.75 - Last mean reward per episode: -48.53

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_194317_numTimesteps_777000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 780000
Best mean reward: -48.53 - Last mean reward per episode: -49.01
Num timesteps: 783000
Best mean reward: -48.53 - Last mean reward per episode: -49.13
Num timesteps: 786000
Best mean reward: -48.53 - Last mean reward per episode: -48.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 50           |
|    ep_rew_mean          | -48.9        |
| time/                   |              |
|    fps                  | 462          |
|    iterations           | 8            |
|    time_elapsed         | 1699         |
|    total_timesteps      | 786432       |
| train/                  |              |
|    approx_kl            | 0.0032195237 |
|    clip_fraction        | 0.266        |
|    clip_range           | 0.075        |
|    entropy_loss         | -4.24        |
|    explained_variance   | 0.557        |
|    learning_rate        | 0.00134      |
|    loss                 | 0.0485       |
|    n_updates            | 63           |
|    policy_gradient_loss | -0.00588     |
|    std                  | 0.994        |
|    value_loss           | 0.0891       |
------------------------------------------
Num timesteps: 789000
Best mean reward: -48.53 - Last mean reward per episode: -49.39
Num timesteps: 792000
Best mean reward: -48.53 - Last mean reward per episode: -48.90
Num timesteps: 795000
Best mean reward: -48.53 - Last mean reward per episode: -48.32

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_194413_numTimesteps_795000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 798000
Best mean reward: -48.32 - Last mean reward per episode: -48.15

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_194420_numTimesteps_798000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 801000
Best mean reward: -48.15 - Last mean reward per episode: -48.79
Num timesteps: 804000
Best mean reward: -48.15 - Last mean reward per episode: -48.80
Num timesteps: 807000
Best mean reward: -48.15 - Last mean reward per episode: -48.96
Num timesteps: 810000
Best mean reward: -48.15 - Last mean reward per episode: -49.09
Num timesteps: 813000
Best mean reward: -48.15 - Last mean reward per episode: -49.09
Num timesteps: 816000
Best mean reward: -48.15 - Last mean reward per episode: -48.43
Num timesteps: 819000
Best mean reward: -48.15 - Last mean reward per episode: -48.47
Num timesteps: 822000
Best mean reward: -48.15 - Last mean reward per episode: -49.18
Num timesteps: 825000
Best mean reward: -48.15 - Last mean reward per episode: -48.87
Num timesteps: 828000
Best mean reward: -48.15 - Last mean reward per episode: -48.72
Num timesteps: 831000
Best mean reward: -48.15 - Last mean reward per episode: -48.83
Num timesteps: 834000
Best mean reward: -48.15 - Last mean reward per episode: -49.33
Num timesteps: 837000
Best mean reward: -48.15 - Last mean reward per episode: -49.19
Num timesteps: 840000
Best mean reward: -48.15 - Last mean reward per episode: -49.02
Num timesteps: 843000
Best mean reward: -48.15 - Last mean reward per episode: -48.88
Num timesteps: 846000
Best mean reward: -48.15 - Last mean reward per episode: -48.78
Num timesteps: 849000
Best mean reward: -48.15 - Last mean reward per episode: -48.48
Num timesteps: 852000
Best mean reward: -48.15 - Last mean reward per episode: -48.62
Num timesteps: 855000
Best mean reward: -48.15 - Last mean reward per episode: -48.36
Num timesteps: 858000
Best mean reward: -48.15 - Last mean reward per episode: -49.05
Num timesteps: 861000
Best mean reward: -48.15 - Last mean reward per episode: -49.11
Num timesteps: 864000
Best mean reward: -48.15 - Last mean reward per episode: -48.88
Num timesteps: 867000
Best mean reward: -48.15 - Last mean reward per episode: -48.80
Num timesteps: 870000
Best mean reward: -48.15 - Last mean reward per episode: -48.80
Num timesteps: 873000
Best mean reward: -48.15 - Last mean reward per episode: -48.60
Num timesteps: 876000
Best mean reward: -48.15 - Last mean reward per episode: -48.57
Num timesteps: 879000
Best mean reward: -48.15 - Last mean reward per episode: -48.72
Num timesteps: 882000
Best mean reward: -48.15 - Last mean reward per episode: -48.80
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 50           |
|    ep_rew_mean          | -48.8        |
| time/                   |              |
|    fps                  | 458          |
|    iterations           | 9            |
|    time_elapsed         | 1928         |
|    total_timesteps      | 884736       |
| train/                  |              |
|    approx_kl            | 0.0032845575 |
|    clip_fraction        | 0.304        |
|    clip_range           | 0.075        |
|    entropy_loss         | -4.22        |
|    explained_variance   | 0.563        |
|    learning_rate        | 0.00134      |
|    loss                 | 0.0543       |
|    n_updates            | 72           |
|    policy_gradient_loss | -0.00747     |
|    std                  | 0.986        |
|    value_loss           | 0.105        |
------------------------------------------
Num timesteps: 885000
Best mean reward: -48.15 - Last mean reward per episode: -48.80
Num timesteps: 888000
Best mean reward: -48.15 - Last mean reward per episode: -48.07

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_194751_numTimesteps_888000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 891000
Best mean reward: -48.07 - Last mean reward per episode: -47.41

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_194756_numTimesteps_891000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 894000
Best mean reward: -47.41 - Last mean reward per episode: -47.92
Num timesteps: 897000
Best mean reward: -47.41 - Last mean reward per episode: -48.59
Num timesteps: 900000
Best mean reward: -47.41 - Last mean reward per episode: -48.11
Num timesteps: 903000
Best mean reward: -47.41 - Last mean reward per episode: -47.73
Num timesteps: 906000
Best mean reward: -47.41 - Last mean reward per episode: -47.57
Num timesteps: 909000
Best mean reward: -47.41 - Last mean reward per episode: -47.62
Num timesteps: 912000
Best mean reward: -47.41 - Last mean reward per episode: -48.39
Num timesteps: 915000
Best mean reward: -47.41 - Last mean reward per episode: -48.55
Num timesteps: 918000
Best mean reward: -47.41 - Last mean reward per episode: -48.44
Num timesteps: 921000
Best mean reward: -47.41 - Last mean reward per episode: -48.03
Num timesteps: 924000
Best mean reward: -47.41 - Last mean reward per episode: -47.91
Num timesteps: 927000
Best mean reward: -47.41 - Last mean reward per episode: -48.08
Num timesteps: 930000
Best mean reward: -47.41 - Last mean reward per episode: -48.19
Num timesteps: 933000
Best mean reward: -47.41 - Last mean reward per episode: -47.99
Num timesteps: 936000
Best mean reward: -47.41 - Last mean reward per episode: -47.74
Num timesteps: 939000
Best mean reward: -47.41 - Last mean reward per episode: -48.31
Num timesteps: 942000
Best mean reward: -47.41 - Last mean reward per episode: -48.20
Num timesteps: 945000
Best mean reward: -47.41 - Last mean reward per episode: -48.29
Num timesteps: 948000
Best mean reward: -47.41 - Last mean reward per episode: -47.96
Num timesteps: 951000
Best mean reward: -47.41 - Last mean reward per episode: -47.87
Num timesteps: 954000
Best mean reward: -47.41 - Last mean reward per episode: -48.27
Num timesteps: 957000
Best mean reward: -47.41 - Last mean reward per episode: -48.69
Num timesteps: 960000
Best mean reward: -47.41 - Last mean reward per episode: -48.68
Num timesteps: 963000
Best mean reward: -47.41 - Last mean reward per episode: -48.45
Num timesteps: 966000
Best mean reward: -47.41 - Last mean reward per episode: -48.71
Num timesteps: 969000
Best mean reward: -47.41 - Last mean reward per episode: -47.56
Num timesteps: 972000
Best mean reward: -47.41 - Last mean reward per episode: -47.71
Num timesteps: 975000
Best mean reward: -47.41 - Last mean reward per episode: -48.33
Num timesteps: 978000
Best mean reward: -47.41 - Last mean reward per episode: -48.01
Num timesteps: 981000
Best mean reward: -47.41 - Last mean reward per episode: -48.62
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 50           |
|    ep_rew_mean          | -48.7        |
| time/                   |              |
|    fps                  | 458          |
|    iterations           | 10           |
|    time_elapsed         | 2142         |
|    total_timesteps      | 983040       |
| train/                  |              |
|    approx_kl            | 0.0037677046 |
|    clip_fraction        | 0.314        |
|    clip_range           | 0.075        |
|    entropy_loss         | -4.19        |
|    explained_variance   | 0.57         |
|    learning_rate        | 0.00134      |
|    loss                 | 0.086        |
|    n_updates            | 81           |
|    policy_gradient_loss | -0.00741     |
|    std                  | 0.978        |
|    value_loss           | 0.131        |
------------------------------------------
Num timesteps: 984000
Best mean reward: -47.41 - Last mean reward per episode: -48.36
Num timesteps: 987000
Best mean reward: -47.41 - Last mean reward per episode: -47.67
Num timesteps: 990000
Best mean reward: -47.41 - Last mean reward per episode: -46.51

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_195131_numTimesteps_990000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 993000
Best mean reward: -46.51 - Last mean reward per episode: -46.38

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_195137_numTimesteps_993000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 996000
Best mean reward: -46.38 - Last mean reward per episode: -47.39
Num timesteps: 999000
Best mean reward: -46.38 - Last mean reward per episode: -47.59
Num timesteps: 1002000
Best mean reward: -46.38 - Last mean reward per episode: -47.45
Num timesteps: 1005000
Best mean reward: -46.38 - Last mean reward per episode: -47.23
Num timesteps: 1008000
Best mean reward: -46.38 - Last mean reward per episode: -46.37

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_195205_numTimesteps_1008000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 1011000
Best mean reward: -46.37 - Last mean reward per episode: -46.02

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_195211_numTimesteps_1011000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 1014000
Best mean reward: -46.02 - Last mean reward per episode: -47.40
Num timesteps: 1017000
Best mean reward: -46.02 - Last mean reward per episode: -47.62
Num timesteps: 1020000
Best mean reward: -46.02 - Last mean reward per episode: -46.84
Num timesteps: 1023000
Best mean reward: -46.02 - Last mean reward per episode: -47.50
Num timesteps: 1026000
Best mean reward: -46.02 - Last mean reward per episode: -47.53
Num timesteps: 1029000
Best mean reward: -46.02 - Last mean reward per episode: -46.71
Num timesteps: 1032000
Best mean reward: -46.02 - Last mean reward per episode: -46.63
Num timesteps: 1035000
Best mean reward: -46.02 - Last mean reward per episode: -46.05
Num timesteps: 1038000
Best mean reward: -46.02 - Last mean reward per episode: -46.56
Num timesteps: 1041000
Best mean reward: -46.02 - Last mean reward per episode: -47.04
Num timesteps: 1044000
Best mean reward: -46.02 - Last mean reward per episode: -47.19
Num timesteps: 1047000
Best mean reward: -46.02 - Last mean reward per episode: -47.49
Num timesteps: 1050000
Best mean reward: -46.02 - Last mean reward per episode: -47.64
Num timesteps: 1053000
Best mean reward: -46.02 - Last mean reward per episode: -46.52
Num timesteps: 1056000
Best mean reward: -46.02 - Last mean reward per episode: -47.54
Num timesteps: 1059000
Best mean reward: -46.02 - Last mean reward per episode: -47.21
Num timesteps: 1062000
Best mean reward: -46.02 - Last mean reward per episode: -47.51
Num timesteps: 1065000
Best mean reward: -46.02 - Last mean reward per episode: -47.66
Num timesteps: 1068000
Best mean reward: -46.02 - Last mean reward per episode: -47.37
Num timesteps: 1071000
Best mean reward: -46.02 - Last mean reward per episode: -47.59
Num timesteps: 1074000
Best mean reward: -46.02 - Last mean reward per episode: -47.16
Num timesteps: 1077000
Best mean reward: -46.02 - Last mean reward per episode: -46.67
Num timesteps: 1080000
Best mean reward: -46.02 - Last mean reward per episode: -46.09
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 50           |
|    ep_rew_mean          | -45.5        |
| time/                   |              |
|    fps                  | 459          |
|    iterations           | 11           |
|    time_elapsed         | 2351         |
|    total_timesteps      | 1081344      |
| train/                  |              |
|    approx_kl            | 0.0035988248 |
|    clip_fraction        | 0.293        |
|    clip_range           | 0.075        |
|    entropy_loss         | -4.16        |
|    explained_variance   | 0.621        |
|    learning_rate        | 0.00134      |
|    loss                 | 0.135        |
|    n_updates            | 90           |
|    policy_gradient_loss | -0.00797     |
|    std                  | 0.968        |
|    value_loss           | 0.196        |
------------------------------------------
Num timesteps: 1083000
Best mean reward: -46.02 - Last mean reward per episode: -45.80

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_195449_numTimesteps_1083000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 1086000
Best mean reward: -45.80 - Last mean reward per episode: -46.32
Num timesteps: 1089000
Best mean reward: -45.80 - Last mean reward per episode: -45.71

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_195500_numTimesteps_1089000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 1092000
Best mean reward: -45.71 - Last mean reward per episode: -45.82
Num timesteps: 1095000
Best mean reward: -45.71 - Last mean reward per episode: -45.62

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_195511_numTimesteps_1095000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 1098000
Best mean reward: -45.62 - Last mean reward per episode: -46.35
Num timesteps: 1101000
Best mean reward: -45.62 - Last mean reward per episode: -45.36

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_195526_numTimesteps_1101000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 1104000
Best mean reward: -45.36 - Last mean reward per episode: -46.08
Num timesteps: 1107000
Best mean reward: -45.36 - Last mean reward per episode: -46.10
Num timesteps: 1110000
Best mean reward: -45.36 - Last mean reward per episode: -46.00
Num timesteps: 1113000
Best mean reward: -45.36 - Last mean reward per episode: -47.14
Num timesteps: 1116000
Best mean reward: -45.36 - Last mean reward per episode: -47.03
Num timesteps: 1119000
Best mean reward: -45.36 - Last mean reward per episode: -46.33
Num timesteps: 1122000
Best mean reward: -45.36 - Last mean reward per episode: -46.20
Num timesteps: 1125000
Best mean reward: -45.36 - Last mean reward per episode: -46.10
Num timesteps: 1128000
Best mean reward: -45.36 - Last mean reward per episode: -46.12
Num timesteps: 1131000
Best mean reward: -45.36 - Last mean reward per episode: -46.22
Num timesteps: 1134000
Best mean reward: -45.36 - Last mean reward per episode: -46.47
Num timesteps: 1137000
Best mean reward: -45.36 - Last mean reward per episode: -46.61
Num timesteps: 1140000
Best mean reward: -45.36 - Last mean reward per episode: -46.30
Num timesteps: 1143000
Best mean reward: -45.36 - Last mean reward per episode: -46.37
Num timesteps: 1146000
Best mean reward: -45.36 - Last mean reward per episode: -46.60
Num timesteps: 1149000
Best mean reward: -45.36 - Last mean reward per episode: -46.76
Num timesteps: 1152000
Best mean reward: -45.36 - Last mean reward per episode: -46.86
Num timesteps: 1155000
Best mean reward: -45.36 - Last mean reward per episode: -46.52
Num timesteps: 1158000
Best mean reward: -45.36 - Last mean reward per episode: -46.82
Num timesteps: 1161000
Best mean reward: -45.36 - Last mean reward per episode: -46.91
Num timesteps: 1164000
Best mean reward: -45.36 - Last mean reward per episode: -47.57
Num timesteps: 1167000
Best mean reward: -45.36 - Last mean reward per episode: -46.43
Num timesteps: 1170000
Best mean reward: -45.36 - Last mean reward per episode: -46.02
Num timesteps: 1173000
Best mean reward: -45.36 - Last mean reward per episode: -45.83
Num timesteps: 1176000
Best mean reward: -45.36 - Last mean reward per episode: -46.42
Num timesteps: 1179000
Best mean reward: -45.36 - Last mean reward per episode: -47.26
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 50          |
|    ep_rew_mean          | -46.8       |
| time/                   |             |
|    fps                  | 457         |
|    iterations           | 12          |
|    time_elapsed         | 2579        |
|    total_timesteps      | 1179648     |
| train/                  |             |
|    approx_kl            | 0.003400009 |
|    clip_fraction        | 0.277       |
|    clip_range           | 0.075       |
|    entropy_loss         | -4.12       |
|    explained_variance   | 0.675       |
|    learning_rate        | 0.00134     |
|    loss                 | 0.219       |
|    n_updates            | 99          |
|    policy_gradient_loss | -0.00912    |
|    std                  | 0.956       |
|    value_loss           | 0.311       |
-----------------------------------------
Num timesteps: 1182000
Best mean reward: -45.36 - Last mean reward per episode: -46.01
Num timesteps: 1185000
Best mean reward: -45.36 - Last mean reward per episode: -43.52

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_195843_numTimesteps_1185000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 1188000
Best mean reward: -43.52 - Last mean reward per episode: -45.35
Num timesteps: 1191000
Best mean reward: -43.52 - Last mean reward per episode: -44.53
Num timesteps: 1194000
Best mean reward: -43.52 - Last mean reward per episode: -44.57
Num timesteps: 1197000
Best mean reward: -43.52 - Last mean reward per episode: -44.83
Num timesteps: 1200000
Best mean reward: -43.52 - Last mean reward per episode: -44.67
Num timesteps: 1203000
Best mean reward: -43.52 - Last mean reward per episode: -45.59
Num timesteps: 1206000
Best mean reward: -43.52 - Last mean reward per episode: -45.30
Num timesteps: 1209000
Best mean reward: -43.52 - Last mean reward per episode: -44.99
Num timesteps: 1212000
Best mean reward: -43.52 - Last mean reward per episode: -44.50
Num timesteps: 1215000
Best mean reward: -43.52 - Last mean reward per episode: -45.96
Num timesteps: 1218000
Best mean reward: -43.52 - Last mean reward per episode: -45.35
Num timesteps: 1221000
Best mean reward: -43.52 - Last mean reward per episode: -44.82
Num timesteps: 1224000
Best mean reward: -43.52 - Last mean reward per episode: -45.03
Num timesteps: 1227000
Best mean reward: -43.52 - Last mean reward per episode: -44.74
Num timesteps: 1230000
Best mean reward: -43.52 - Last mean reward per episode: -43.58
Num timesteps: 1233000
Best mean reward: -43.52 - Last mean reward per episode: -43.86
Num timesteps: 1236000
Best mean reward: -43.52 - Last mean reward per episode: -45.34
Num timesteps: 1239000
Best mean reward: -43.52 - Last mean reward per episode: -45.26
Num timesteps: 1242000
Best mean reward: -43.52 - Last mean reward per episode: -44.24
Num timesteps: 1245000
Best mean reward: -43.52 - Last mean reward per episode: -44.56
Num timesteps: 1248000
Best mean reward: -43.52 - Last mean reward per episode: -45.68
Num timesteps: 1251000
Best mean reward: -43.52 - Last mean reward per episode: -44.88
Num timesteps: 1254000
Best mean reward: -43.52 - Last mean reward per episode: -42.77

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_200104_numTimesteps_1254000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 1257000
Best mean reward: -42.77 - Last mean reward per episode: -43.94
Num timesteps: 1260000
Best mean reward: -42.77 - Last mean reward per episode: -45.12
Num timesteps: 1263000
Best mean reward: -42.77 - Last mean reward per episode: -45.16
Num timesteps: 1266000
Best mean reward: -42.77 - Last mean reward per episode: -45.31
Num timesteps: 1269000
Best mean reward: -42.77 - Last mean reward per episode: -45.10
Num timesteps: 1272000
Best mean reward: -42.77 - Last mean reward per episode: -44.79
Num timesteps: 1275000
Best mean reward: -42.77 - Last mean reward per episode: -44.54
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 50           |
|    ep_rew_mean          | -44.5        |
| time/                   |              |
|    fps                  | 456          |
|    iterations           | 13           |
|    time_elapsed         | 2797         |
|    total_timesteps      | 1277952      |
| train/                  |              |
|    approx_kl            | 0.0037331798 |
|    clip_fraction        | 0.292        |
|    clip_range           | 0.075        |
|    entropy_loss         | -4.08        |
|    explained_variance   | 0.723        |
|    learning_rate        | 0.00134      |
|    loss                 | 0.249        |
|    n_updates            | 108          |
|    policy_gradient_loss | -0.00893     |
|    std                  | 0.942        |
|    value_loss           | 0.358        |
------------------------------------------
Num timesteps: 1278000
Best mean reward: -42.77 - Last mean reward per episode: -44.37
Num timesteps: 1281000
Best mean reward: -42.77 - Last mean reward per episode: -44.64
Num timesteps: 1284000
Best mean reward: -42.77 - Last mean reward per episode: -42.88
Num timesteps: 1287000
Best mean reward: -42.77 - Last mean reward per episode: -42.37

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_200234_numTimesteps_1287000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 1290000
Best mean reward: -42.37 - Last mean reward per episode: -44.22
Num timesteps: 1293000
Best mean reward: -42.37 - Last mean reward per episode: -44.85
Num timesteps: 1296000
Best mean reward: -42.37 - Last mean reward per episode: -43.40
Num timesteps: 1299000
Best mean reward: -42.37 - Last mean reward per episode: -43.66
Num timesteps: 1302000
Best mean reward: -42.37 - Last mean reward per episode: -44.14
Num timesteps: 1305000
Best mean reward: -42.37 - Last mean reward per episode: -45.14
Num timesteps: 1308000
Best mean reward: -42.37 - Last mean reward per episode: -43.23
Num timesteps: 1311000
Best mean reward: -42.37 - Last mean reward per episode: -42.04

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_200323_numTimesteps_1311000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 1314000
Best mean reward: -42.04 - Last mean reward per episode: -43.66
Num timesteps: 1317000
Best mean reward: -42.04 - Last mean reward per episode: -44.22
Num timesteps: 1320000
Best mean reward: -42.04 - Last mean reward per episode: -43.42
Num timesteps: 1323000
Best mean reward: -42.04 - Last mean reward per episode: -43.68
Num timesteps: 1326000
Best mean reward: -42.04 - Last mean reward per episode: -43.50
Num timesteps: 1329000
Best mean reward: -42.04 - Last mean reward per episode: -44.69
Num timesteps: 1332000
Best mean reward: -42.04 - Last mean reward per episode: -43.88
Num timesteps: 1335000
Best mean reward: -42.04 - Last mean reward per episode: -43.76
Num timesteps: 1338000
Best mean reward: -42.04 - Last mean reward per episode: -43.46
Num timesteps: 1341000
Best mean reward: -42.04 - Last mean reward per episode: -43.25
Num timesteps: 1344000
Best mean reward: -42.04 - Last mean reward per episode: -42.74
Num timesteps: 1347000
Best mean reward: -42.04 - Last mean reward per episode: -43.37
Num timesteps: 1350000
Best mean reward: -42.04 - Last mean reward per episode: -43.28
Num timesteps: 1353000
Best mean reward: -42.04 - Last mean reward per episode: -44.42
Num timesteps: 1356000
Best mean reward: -42.04 - Last mean reward per episode: -44.76
Num timesteps: 1359000
Best mean reward: -42.04 - Last mean reward per episode: -45.58
Num timesteps: 1362000
Best mean reward: -42.04 - Last mean reward per episode: -45.59
Num timesteps: 1365000
Best mean reward: -42.04 - Last mean reward per episode: -45.01
Num timesteps: 1368000
Best mean reward: -42.04 - Last mean reward per episode: -44.29
Num timesteps: 1371000
Best mean reward: -42.04 - Last mean reward per episode: -43.25
Num timesteps: 1374000
Best mean reward: -42.04 - Last mean reward per episode: -43.02
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 50           |
|    ep_rew_mean          | -43.6        |
| time/                   |              |
|    fps                  | 454          |
|    iterations           | 14           |
|    time_elapsed         | 3025         |
|    total_timesteps      | 1376256      |
| train/                  |              |
|    approx_kl            | 0.0036927008 |
|    clip_fraction        | 0.296        |
|    clip_range           | 0.075        |
|    entropy_loss         | -4.03        |
|    explained_variance   | 0.805        |
|    learning_rate        | 0.00134      |
|    loss                 | 0.349        |
|    n_updates            | 117          |
|    policy_gradient_loss | -0.01        |
|    std                  | 0.929        |
|    value_loss           | 0.467        |
------------------------------------------
Num timesteps: 1377000
Best mean reward: -42.04 - Last mean reward per episode: -43.42
Num timesteps: 1380000
Best mean reward: -42.04 - Last mean reward per episode: -43.31
Num timesteps: 1383000
Best mean reward: -42.04 - Last mean reward per episode: -43.52
Num timesteps: 1386000
Best mean reward: -42.04 - Last mean reward per episode: -42.38
Num timesteps: 1389000
Best mean reward: -42.04 - Last mean reward per episode: -41.69

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_200624_numTimesteps_1389000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 1392000
Best mean reward: -41.69 - Last mean reward per episode: -42.12
Num timesteps: 1395000
Best mean reward: -41.69 - Last mean reward per episode: -42.78
Num timesteps: 1398000
Best mean reward: -41.69 - Last mean reward per episode: -42.85
Num timesteps: 1401000
Best mean reward: -41.69 - Last mean reward per episode: -41.81
Num timesteps: 1404000
Best mean reward: -41.69 - Last mean reward per episode: -42.65
Num timesteps: 1407000
Best mean reward: -41.69 - Last mean reward per episode: -43.35
Num timesteps: 1410000
Best mean reward: -41.69 - Last mean reward per episode: -43.14
Num timesteps: 1413000
Best mean reward: -41.69 - Last mean reward per episode: -40.41

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_200715_numTimesteps_1413000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 1416000
Best mean reward: -40.41 - Last mean reward per episode: -39.15

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_200722_numTimesteps_1416000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 1419000
Best mean reward: -39.15 - Last mean reward per episode: -40.96
Num timesteps: 1422000
Best mean reward: -39.15 - Last mean reward per episode: -41.30
Num timesteps: 1425000
Best mean reward: -39.15 - Last mean reward per episode: -41.47
Num timesteps: 1428000
Best mean reward: -39.15 - Last mean reward per episode: -40.74
Num timesteps: 1431000
Best mean reward: -39.15 - Last mean reward per episode: -41.40
Num timesteps: 1434000
Best mean reward: -39.15 - Last mean reward per episode: -42.97
Num timesteps: 1437000
Best mean reward: -39.15 - Last mean reward per episode: -42.52
Num timesteps: 1440000
Best mean reward: -39.15 - Last mean reward per episode: -42.31
Num timesteps: 1443000
Best mean reward: -39.15 - Last mean reward per episode: -41.33
Num timesteps: 1446000
Best mean reward: -39.15 - Last mean reward per episode: -42.13
Num timesteps: 1449000
Best mean reward: -39.15 - Last mean reward per episode: -42.75
Num timesteps: 1452000
Best mean reward: -39.15 - Last mean reward per episode: -41.34
Num timesteps: 1455000
Best mean reward: -39.15 - Last mean reward per episode: -41.73
Num timesteps: 1458000
Best mean reward: -39.15 - Last mean reward per episode: -42.38
Num timesteps: 1461000
Best mean reward: -39.15 - Last mean reward per episode: -41.44
Num timesteps: 1464000
Best mean reward: -39.15 - Last mean reward per episode: -41.73
Num timesteps: 1467000
Best mean reward: -39.15 - Last mean reward per episode: -43.78
Num timesteps: 1470000
Best mean reward: -39.15 - Last mean reward per episode: -44.49
Num timesteps: 1473000
Best mean reward: -39.15 - Last mean reward per episode: -42.59
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 50           |
|    ep_rew_mean          | -43          |
| time/                   |              |
|    fps                  | 456          |
|    iterations           | 15           |
|    time_elapsed         | 3230         |
|    total_timesteps      | 1474560      |
| train/                  |              |
|    approx_kl            | 0.0041560917 |
|    clip_fraction        | 0.314        |
|    clip_range           | 0.075        |
|    entropy_loss         | -3.99        |
|    explained_variance   | 0.844        |
|    learning_rate        | 0.00134      |
|    loss                 | 0.346        |
|    n_updates            | 126          |
|    policy_gradient_loss | -0.00949     |
|    std                  | 0.914        |
|    value_loss           | 0.509        |
------------------------------------------
Num timesteps: 1476000
Best mean reward: -39.15 - Last mean reward per episode: -42.72
Num timesteps: 1479000
Best mean reward: -39.15 - Last mean reward per episode: -40.82
Num timesteps: 1482000
Best mean reward: -39.15 - Last mean reward per episode: -38.87

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_200939_numTimesteps_1482000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 1485000
Best mean reward: -38.87 - Last mean reward per episode: -39.84
Num timesteps: 1488000
Best mean reward: -38.87 - Last mean reward per episode: -39.73
Num timesteps: 1491000
Best mean reward: -38.87 - Last mean reward per episode: -39.73
Num timesteps: 1494000
Best mean reward: -38.87 - Last mean reward per episode: -41.79
Num timesteps: 1497000
Best mean reward: -38.87 - Last mean reward per episode: -40.74
Num timesteps: 1500000
Best mean reward: -38.87 - Last mean reward per episode: -40.83
Num timesteps: 1503000
Best mean reward: -38.87 - Last mean reward per episode: -41.89
Num timesteps: 1506000
Best mean reward: -38.87 - Last mean reward per episode: -39.71
Num timesteps: 1509000
Best mean reward: -38.87 - Last mean reward per episode: -39.22
Num timesteps: 1512000
Best mean reward: -38.87 - Last mean reward per episode: -41.55
Num timesteps: 1515000
Best mean reward: -38.87 - Last mean reward per episode: -41.53
Num timesteps: 1518000
Best mean reward: -38.87 - Last mean reward per episode: -41.16
Num timesteps: 1521000
Best mean reward: -38.87 - Last mean reward per episode: -40.76
Num timesteps: 1524000
Best mean reward: -38.87 - Last mean reward per episode: -39.05
Num timesteps: 1527000
Best mean reward: -38.87 - Last mean reward per episode: -40.91
Num timesteps: 1530000
Best mean reward: -38.87 - Last mean reward per episode: -40.97
Num timesteps: 1533000
Best mean reward: -38.87 - Last mean reward per episode: -40.98
Num timesteps: 1536000
Best mean reward: -38.87 - Last mean reward per episode: -40.93
Num timesteps: 1539000
Best mean reward: -38.87 - Last mean reward per episode: -40.83
Num timesteps: 1542000
Best mean reward: -38.87 - Last mean reward per episode: -41.29
Num timesteps: 1545000
Best mean reward: -38.87 - Last mean reward per episode: -40.06
Num timesteps: 1548000
Best mean reward: -38.87 - Last mean reward per episode: -42.19
Num timesteps: 1551000
Best mean reward: -38.87 - Last mean reward per episode: -42.19
Num timesteps: 1554000
Best mean reward: -38.87 - Last mean reward per episode: -41.72
Num timesteps: 1557000
Best mean reward: -38.87 - Last mean reward per episode: -40.66
Num timesteps: 1560000
Best mean reward: -38.87 - Last mean reward per episode: -40.93
Num timesteps: 1563000
Best mean reward: -38.87 - Last mean reward per episode: -41.45
Num timesteps: 1566000
Best mean reward: -38.87 - Last mean reward per episode: -41.50
Num timesteps: 1569000
Best mean reward: -38.87 - Last mean reward per episode: -42.48
Num timesteps: 1572000
Best mean reward: -38.87 - Last mean reward per episode: -40.42
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 50          |
|    ep_rew_mean          | -40.6       |
| time/                   |             |
|    fps                  | 457         |
|    iterations           | 16          |
|    time_elapsed         | 3435        |
|    total_timesteps      | 1572864     |
| train/                  |             |
|    approx_kl            | 0.004357087 |
|    clip_fraction        | 0.329       |
|    clip_range           | 0.075       |
|    entropy_loss         | -3.94       |
|    explained_variance   | 0.872       |
|    learning_rate        | 0.00134     |
|    loss                 | 0.412       |
|    n_updates            | 135         |
|    policy_gradient_loss | -0.0103     |
|    std                  | 0.9         |
|    value_loss           | 0.584       |
-----------------------------------------
Num timesteps: 1575000
Best mean reward: -38.87 - Last mean reward per episode: -39.65
Num timesteps: 1578000
Best mean reward: -38.87 - Last mean reward per episode: -39.07
Num timesteps: 1581000
Best mean reward: -38.87 - Last mean reward per episode: -39.56
Num timesteps: 1584000
Best mean reward: -38.87 - Last mean reward per episode: -39.35
Num timesteps: 1587000
Best mean reward: -38.87 - Last mean reward per episode: -37.57

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_201324_numTimesteps_1587000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 1590000
Best mean reward: -37.57 - Last mean reward per episode: -40.13
Num timesteps: 1593000
Best mean reward: -37.57 - Last mean reward per episode: -40.23
Num timesteps: 1596000
Best mean reward: -37.57 - Last mean reward per episode: -39.50
Num timesteps: 1599000
Best mean reward: -37.57 - Last mean reward per episode: -41.94
Num timesteps: 1602000
Best mean reward: -37.57 - Last mean reward per episode: -40.67
Num timesteps: 1605000
Best mean reward: -37.57 - Last mean reward per episode: -39.42
Num timesteps: 1608000
Best mean reward: -37.57 - Last mean reward per episode: -39.83
Num timesteps: 1611000
Best mean reward: -37.57 - Last mean reward per episode: -38.18
Num timesteps: 1614000
Best mean reward: -37.57 - Last mean reward per episode: -39.35
Num timesteps: 1617000
Best mean reward: -37.57 - Last mean reward per episode: -41.17
Num timesteps: 1620000
Best mean reward: -37.57 - Last mean reward per episode: -41.47
Num timesteps: 1623000
Best mean reward: -37.57 - Last mean reward per episode: -40.53
Num timesteps: 1626000
Best mean reward: -37.57 - Last mean reward per episode: -39.70
Num timesteps: 1629000
Best mean reward: -37.57 - Last mean reward per episode: -38.82
Num timesteps: 1632000
Best mean reward: -37.57 - Last mean reward per episode: -37.84
Num timesteps: 1635000
Best mean reward: -37.57 - Last mean reward per episode: -38.30
Num timesteps: 1638000
Best mean reward: -37.57 - Last mean reward per episode: -40.04
Num timesteps: 1641000
Best mean reward: -37.57 - Last mean reward per episode: -39.76
Num timesteps: 1644000
Best mean reward: -37.57 - Last mean reward per episode: -41.17
Num timesteps: 1647000
Best mean reward: -37.57 - Last mean reward per episode: -40.05
Num timesteps: 1650000
Best mean reward: -37.57 - Last mean reward per episode: -40.88
Num timesteps: 1653000
Best mean reward: -37.57 - Last mean reward per episode: -39.52
Num timesteps: 1656000
Best mean reward: -37.57 - Last mean reward per episode: -40.28
Num timesteps: 1659000
Best mean reward: -37.57 - Last mean reward per episode: -40.20
Num timesteps: 1662000
Best mean reward: -37.57 - Last mean reward per episode: -40.66
Num timesteps: 1665000
Best mean reward: -37.57 - Last mean reward per episode: -40.73
Num timesteps: 1668000
Best mean reward: -37.57 - Last mean reward per episode: -40.69
Num timesteps: 1671000
Best mean reward: -37.57 - Last mean reward per episode: -40.06
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 50          |
|    ep_rew_mean          | -40.4       |
| time/                   |             |
|    fps                  | 456         |
|    iterations           | 17          |
|    time_elapsed         | 3663        |
|    total_timesteps      | 1671168     |
| train/                  |             |
|    approx_kl            | 0.004714995 |
|    clip_fraction        | 0.35        |
|    clip_range           | 0.075       |
|    entropy_loss         | -3.89       |
|    explained_variance   | 0.875       |
|    learning_rate        | 0.00134     |
|    loss                 | 0.484       |
|    n_updates            | 144         |
|    policy_gradient_loss | -0.0104     |
|    std                  | 0.883       |
|    value_loss           | 0.664       |
-----------------------------------------
Num timesteps: 1674000
Best mean reward: -37.57 - Last mean reward per episode: -40.49
Num timesteps: 1677000
Best mean reward: -37.57 - Last mean reward per episode: -38.50
Num timesteps: 1680000
Best mean reward: -37.57 - Last mean reward per episode: -35.17

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_201655_numTimesteps_1680000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 1683000
Best mean reward: -35.17 - Last mean reward per episode: -37.76
Num timesteps: 1686000
Best mean reward: -35.17 - Last mean reward per episode: -36.66
Num timesteps: 1689000
Best mean reward: -35.17 - Last mean reward per episode: -37.49
Num timesteps: 1692000
Best mean reward: -35.17 - Last mean reward per episode: -39.47
Num timesteps: 1695000
Best mean reward: -35.17 - Last mean reward per episode: -39.37
Num timesteps: 1698000
Best mean reward: -35.17 - Last mean reward per episode: -37.51
Num timesteps: 1701000
Best mean reward: -35.17 - Last mean reward per episode: -37.76
Num timesteps: 1704000
Best mean reward: -35.17 - Last mean reward per episode: -38.33
Num timesteps: 1707000
Best mean reward: -35.17 - Last mean reward per episode: -38.54
Num timesteps: 1710000
Best mean reward: -35.17 - Last mean reward per episode: -37.19
Num timesteps: 1713000
Best mean reward: -35.17 - Last mean reward per episode: -36.14
Num timesteps: 1716000
Best mean reward: -35.17 - Last mean reward per episode: -36.86
Num timesteps: 1719000
Best mean reward: -35.17 - Last mean reward per episode: -35.45
Num timesteps: 1722000
Best mean reward: -35.17 - Last mean reward per episode: -37.51
Num timesteps: 1725000
Best mean reward: -35.17 - Last mean reward per episode: -37.93
Num timesteps: 1728000
Best mean reward: -35.17 - Last mean reward per episode: -35.39
Num timesteps: 1731000
Best mean reward: -35.17 - Last mean reward per episode: -37.11
Num timesteps: 1734000
Best mean reward: -35.17 - Last mean reward per episode: -37.26
Num timesteps: 1737000
Best mean reward: -35.17 - Last mean reward per episode: -35.52
Num timesteps: 1740000
Best mean reward: -35.17 - Last mean reward per episode: -35.72
Num timesteps: 1743000
Best mean reward: -35.17 - Last mean reward per episode: -36.68
Num timesteps: 1746000
Best mean reward: -35.17 - Last mean reward per episode: -36.53
Num timesteps: 1749000
Best mean reward: -35.17 - Last mean reward per episode: -38.69
Num timesteps: 1752000
Best mean reward: -35.17 - Last mean reward per episode: -38.75
Num timesteps: 1755000
Best mean reward: -35.17 - Last mean reward per episode: -37.78
Num timesteps: 1758000
Best mean reward: -35.17 - Last mean reward per episode: -37.36
Num timesteps: 1761000
Best mean reward: -35.17 - Last mean reward per episode: -37.74
Num timesteps: 1764000
Best mean reward: -35.17 - Last mean reward per episode: -37.57
Num timesteps: 1767000
Best mean reward: -35.17 - Last mean reward per episode: -34.55

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_201951_numTimesteps_1767000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 50           |
|    ep_rew_mean          | -37.3        |
| time/                   |              |
|    fps                  | 456          |
|    iterations           | 18           |
|    time_elapsed         | 3878         |
|    total_timesteps      | 1769472      |
| train/                  |              |
|    approx_kl            | 0.0047632908 |
|    clip_fraction        | 0.36         |
|    clip_range           | 0.075        |
|    entropy_loss         | -3.83        |
|    explained_variance   | 0.883        |
|    learning_rate        | 0.00134      |
|    loss                 | 0.488        |
|    n_updates            | 153          |
|    policy_gradient_loss | -0.0104      |
|    std                  | 0.867        |
|    value_loss           | 0.729        |
------------------------------------------
Num timesteps: 1770000
Best mean reward: -34.55 - Last mean reward per episode: -37.93
Num timesteps: 1773000
Best mean reward: -34.55 - Last mean reward per episode: -36.88
Num timesteps: 1776000
Best mean reward: -34.55 - Last mean reward per episode: -36.34
Num timesteps: 1779000
Best mean reward: -34.55 - Last mean reward per episode: -37.00
Num timesteps: 1782000
Best mean reward: -34.55 - Last mean reward per episode: -36.20
Num timesteps: 1785000
Best mean reward: -34.55 - Last mean reward per episode: -36.39
Num timesteps: 1788000
Best mean reward: -34.55 - Last mean reward per episode: -34.89
Num timesteps: 1791000
Best mean reward: -34.55 - Last mean reward per episode: -33.09

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_202057_numTimesteps_1791000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 1794000
Best mean reward: -33.09 - Last mean reward per episode: -34.43
Num timesteps: 1797000
Best mean reward: -33.09 - Last mean reward per episode: -35.10
Num timesteps: 1800000
Best mean reward: -33.09 - Last mean reward per episode: -35.30
Num timesteps: 1803000
Best mean reward: -33.09 - Last mean reward per episode: -33.84
Num timesteps: 1806000
Best mean reward: -33.09 - Last mean reward per episode: -34.80
Num timesteps: 1809000
Best mean reward: -33.09 - Last mean reward per episode: -35.03
Num timesteps: 1812000
Best mean reward: -33.09 - Last mean reward per episode: -36.75
Num timesteps: 1815000
Best mean reward: -33.09 - Last mean reward per episode: -36.31
Num timesteps: 1818000
Best mean reward: -33.09 - Last mean reward per episode: -36.79
Num timesteps: 1821000
Best mean reward: -33.09 - Last mean reward per episode: -35.85
Num timesteps: 1824000
Best mean reward: -33.09 - Last mean reward per episode: -35.14
Num timesteps: 1827000
Best mean reward: -33.09 - Last mean reward per episode: -35.87
Num timesteps: 1830000
Best mean reward: -33.09 - Last mean reward per episode: -37.75
Num timesteps: 1833000
Best mean reward: -33.09 - Last mean reward per episode: -36.19
Num timesteps: 1836000
Best mean reward: -33.09 - Last mean reward per episode: -36.19
Num timesteps: 1839000
Best mean reward: -33.09 - Last mean reward per episode: -35.71
Num timesteps: 1842000
Best mean reward: -33.09 - Last mean reward per episode: -35.09
Num timesteps: 1845000
Best mean reward: -33.09 - Last mean reward per episode: -34.11
Num timesteps: 1848000
Best mean reward: -33.09 - Last mean reward per episode: -36.03
Num timesteps: 1851000
Best mean reward: -33.09 - Last mean reward per episode: -36.49
Num timesteps: 1854000
Best mean reward: -33.09 - Last mean reward per episode: -35.71
Num timesteps: 1857000
Best mean reward: -33.09 - Last mean reward per episode: -34.41
Num timesteps: 1860000
Best mean reward: -33.09 - Last mean reward per episode: -34.74
Num timesteps: 1863000
Best mean reward: -33.09 - Last mean reward per episode: -37.25
Num timesteps: 1866000
Best mean reward: -33.09 - Last mean reward per episode: -35.79
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 50          |
|    ep_rew_mean          | -34.6       |
| time/                   |             |
|    fps                  | 454         |
|    iterations           | 19          |
|    time_elapsed         | 4112        |
|    total_timesteps      | 1867776     |
| train/                  |             |
|    approx_kl            | 0.004826762 |
|    clip_fraction        | 0.378       |
|    clip_range           | 0.075       |
|    entropy_loss         | -3.77       |
|    explained_variance   | 0.88        |
|    learning_rate        | 0.00134     |
|    loss                 | 0.616       |
|    n_updates            | 162         |
|    policy_gradient_loss | -0.0115     |
|    std                  | 0.85        |
|    value_loss           | 0.879       |
-----------------------------------------
Num timesteps: 1869000
Best mean reward: -33.09 - Last mean reward per episode: -34.21
Num timesteps: 1872000
Best mean reward: -33.09 - Last mean reward per episode: -33.68
Num timesteps: 1875000
Best mean reward: -33.09 - Last mean reward per episode: -33.80
Num timesteps: 1878000
Best mean reward: -33.09 - Last mean reward per episode: -32.83

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_202425_numTimesteps_1878000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 1881000
Best mean reward: -32.83 - Last mean reward per episode: -32.85
Num timesteps: 1884000
Best mean reward: -32.83 - Last mean reward per episode: -32.09

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_202436_numTimesteps_1884000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 1887000
Best mean reward: -32.09 - Last mean reward per episode: -31.88

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_202442_numTimesteps_1887000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 1890000
Best mean reward: -31.88 - Last mean reward per episode: -32.95
Num timesteps: 1893000
Best mean reward: -31.88 - Last mean reward per episode: -32.88
Num timesteps: 1896000
Best mean reward: -31.88 - Last mean reward per episode: -32.70
Num timesteps: 1899000
Best mean reward: -31.88 - Last mean reward per episode: -32.25
Num timesteps: 1902000
Best mean reward: -31.88 - Last mean reward per episode: -33.58
Num timesteps: 1905000
Best mean reward: -31.88 - Last mean reward per episode: -32.67
Num timesteps: 1908000
Best mean reward: -31.88 - Last mean reward per episode: -32.67
Num timesteps: 1911000
Best mean reward: -31.88 - Last mean reward per episode: -32.72
Num timesteps: 1914000
Best mean reward: -31.88 - Last mean reward per episode: -32.61
Num timesteps: 1917000
Best mean reward: -31.88 - Last mean reward per episode: -32.40
Num timesteps: 1920000
Best mean reward: -31.88 - Last mean reward per episode: -29.94

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_202552_numTimesteps_1920000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 1923000
Best mean reward: -29.94 - Last mean reward per episode: -32.06
Num timesteps: 1926000
Best mean reward: -29.94 - Last mean reward per episode: -33.89
Num timesteps: 1929000
Best mean reward: -29.94 - Last mean reward per episode: -33.60
Num timesteps: 1932000
Best mean reward: -29.94 - Last mean reward per episode: -31.74
Num timesteps: 1935000
Best mean reward: -29.94 - Last mean reward per episode: -31.26
Num timesteps: 1938000
Best mean reward: -29.94 - Last mean reward per episode: -30.20
Num timesteps: 1941000
Best mean reward: -29.94 - Last mean reward per episode: -31.43
Num timesteps: 1944000
Best mean reward: -29.94 - Last mean reward per episode: -33.88
Num timesteps: 1947000
Best mean reward: -29.94 - Last mean reward per episode: -34.35
Num timesteps: 1950000
Best mean reward: -29.94 - Last mean reward per episode: -32.79
Num timesteps: 1953000
Best mean reward: -29.94 - Last mean reward per episode: -32.42
Num timesteps: 1956000
Best mean reward: -29.94 - Last mean reward per episode: -30.48
Num timesteps: 1959000
Best mean reward: -29.94 - Last mean reward per episode: -29.23

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_202714_numTimesteps_1959000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 1962000
Best mean reward: -29.23 - Last mean reward per episode: -32.14
Num timesteps: 1965000
Best mean reward: -29.23 - Last mean reward per episode: -32.45
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 50           |
|    ep_rew_mean          | -32.5        |
| time/                   |              |
|    fps                  | 454          |
|    iterations           | 20           |
|    time_elapsed         | 4329         |
|    total_timesteps      | 1966080      |
| train/                  |              |
|    approx_kl            | 0.0052588726 |
|    clip_fraction        | 0.396        |
|    clip_range           | 0.075        |
|    entropy_loss         | -3.7         |
|    explained_variance   | 0.879        |
|    learning_rate        | 0.00134      |
|    loss                 | 0.731        |
|    n_updates            | 171          |
|    policy_gradient_loss | -0.0116      |
|    std                  | 0.832        |
|    value_loss           | 0.994        |
------------------------------------------
Num timesteps: 1968000
Best mean reward: -29.23 - Last mean reward per episode: -29.66
Num timesteps: 1971000
Best mean reward: -29.23 - Last mean reward per episode: -30.88
Num timesteps: 1974000
Best mean reward: -29.23 - Last mean reward per episode: -31.06
Num timesteps: 1977000
Best mean reward: -29.23 - Last mean reward per episode: -30.01
Num timesteps: 1980000
Best mean reward: -29.23 - Last mean reward per episode: -31.15
Num timesteps: 1983000
Best mean reward: -29.23 - Last mean reward per episode: -32.06
Num timesteps: 1986000
Best mean reward: -29.23 - Last mean reward per episode: -29.55
Num timesteps: 1989000
Best mean reward: -29.23 - Last mean reward per episode: -32.74
Num timesteps: 1992000
Best mean reward: -29.23 - Last mean reward per episode: -32.71
Num timesteps: 1995000
Best mean reward: -29.23 - Last mean reward per episode: -30.02
Num timesteps: 1998000
Best mean reward: -29.23 - Last mean reward per episode: -29.31
Num timesteps: 2001000
Best mean reward: -29.23 - Last mean reward per episode: -29.79
Num timesteps: 2004000
Best mean reward: -29.23 - Last mean reward per episode: -29.73
Num timesteps: 2007000
Best mean reward: -29.23 - Last mean reward per episode: -29.19

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_202910_numTimesteps_2007000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 2010000
Best mean reward: -29.19 - Last mean reward per episode: -30.47
Num timesteps: 2013000
Best mean reward: -29.19 - Last mean reward per episode: -30.24
Num timesteps: 2016000
Best mean reward: -29.19 - Last mean reward per episode: -30.32
Num timesteps: 2019000
Best mean reward: -29.19 - Last mean reward per episode: -31.18
Num timesteps: 2022000
Best mean reward: -29.19 - Last mean reward per episode: -30.94
Num timesteps: 2025000
Best mean reward: -29.19 - Last mean reward per episode: -30.97
Num timesteps: 2028000
Best mean reward: -29.19 - Last mean reward per episode: -31.46
Num timesteps: 2031000
Best mean reward: -29.19 - Last mean reward per episode: -30.05
Num timesteps: 2034000
Best mean reward: -29.19 - Last mean reward per episode: -31.13
Num timesteps: 2037000
Best mean reward: -29.19 - Last mean reward per episode: -31.29
Num timesteps: 2040000
Best mean reward: -29.19 - Last mean reward per episode: -30.98
Num timesteps: 2043000
Best mean reward: -29.19 - Last mean reward per episode: -30.31
Num timesteps: 2046000
Best mean reward: -29.19 - Last mean reward per episode: -31.06
Num timesteps: 2049000
Best mean reward: -29.19 - Last mean reward per episode: -29.93
Num timesteps: 2052000
Best mean reward: -29.19 - Last mean reward per episode: -29.93
Num timesteps: 2055000
Best mean reward: -29.19 - Last mean reward per episode: -31.10
Num timesteps: 2058000
Best mean reward: -29.19 - Last mean reward per episode: -32.10
Num timesteps: 2061000
Best mean reward: -29.19 - Last mean reward per episode: -32.39
Num timesteps: 2064000
Best mean reward: -29.19 - Last mean reward per episode: -33.59
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 50          |
|    ep_rew_mean          | -33.5       |
| time/                   |             |
|    fps                  | 453         |
|    iterations           | 21          |
|    time_elapsed         | 4551        |
|    total_timesteps      | 2064384     |
| train/                  |             |
|    approx_kl            | 0.005320189 |
|    clip_fraction        | 0.401       |
|    clip_range           | 0.075       |
|    entropy_loss         | -3.64       |
|    explained_variance   | 0.88        |
|    learning_rate        | 0.00134     |
|    loss                 | 0.826       |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.012      |
|    std                  | 0.814       |
|    value_loss           | 1.09        |
-----------------------------------------
Num timesteps: 2067000
Best mean reward: -29.19 - Last mean reward per episode: -30.47
Num timesteps: 2070000
Best mean reward: -29.19 - Last mean reward per episode: -28.61

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_203146_numTimesteps_2070000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 2073000
Best mean reward: -28.61 - Last mean reward per episode: -27.10

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_203153_numTimesteps_2073000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 2076000
Best mean reward: -27.10 - Last mean reward per episode: -26.36

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_203159_numTimesteps_2076000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 2079000
Best mean reward: -26.36 - Last mean reward per episode: -26.83
Num timesteps: 2082000
Best mean reward: -26.36 - Last mean reward per episode: -28.47
Num timesteps: 2085000
Best mean reward: -26.36 - Last mean reward per episode: -27.18
Num timesteps: 2088000
Best mean reward: -26.36 - Last mean reward per episode: -27.52
Num timesteps: 2091000
Best mean reward: -26.36 - Last mean reward per episode: -28.80
Num timesteps: 2094000
Best mean reward: -26.36 - Last mean reward per episode: -27.52
Num timesteps: 2097000
Best mean reward: -26.36 - Last mean reward per episode: -27.02
Num timesteps: 2100000
Best mean reward: -26.36 - Last mean reward per episode: -29.39
Num timesteps: 2103000
Best mean reward: -26.36 - Last mean reward per episode: -28.59
Num timesteps: 2106000
Best mean reward: -26.36 - Last mean reward per episode: -26.14

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_203255_numTimesteps_2106000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 2109000
Best mean reward: -26.14 - Last mean reward per episode: -26.13

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_203302_numTimesteps_2109000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 2112000
Best mean reward: -26.13 - Last mean reward per episode: -26.85
Num timesteps: 2115000
Best mean reward: -26.13 - Last mean reward per episode: -27.98
Num timesteps: 2118000
Best mean reward: -26.13 - Last mean reward per episode: -27.58
Num timesteps: 2121000
Best mean reward: -26.13 - Last mean reward per episode: -28.45
Num timesteps: 2124000
Best mean reward: -26.13 - Last mean reward per episode: -28.18
Num timesteps: 2127000
Best mean reward: -26.13 - Last mean reward per episode: -27.68
Num timesteps: 2130000
Best mean reward: -26.13 - Last mean reward per episode: -27.69
Num timesteps: 2133000
Best mean reward: -26.13 - Last mean reward per episode: -28.48
Num timesteps: 2136000
Best mean reward: -26.13 - Last mean reward per episode: -26.79
Num timesteps: 2139000
Best mean reward: -26.13 - Last mean reward per episode: -25.20

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_203405_numTimesteps_2139000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 2142000
Best mean reward: -25.20 - Last mean reward per episode: -27.44
Num timesteps: 2145000
Best mean reward: -25.20 - Last mean reward per episode: -28.49
Num timesteps: 2148000
Best mean reward: -25.20 - Last mean reward per episode: -28.28
Num timesteps: 2151000
Best mean reward: -25.20 - Last mean reward per episode: -27.89
Num timesteps: 2154000
Best mean reward: -25.20 - Last mean reward per episode: -28.43
Num timesteps: 2157000
Best mean reward: -25.20 - Last mean reward per episode: -27.29
Num timesteps: 2160000
Best mean reward: -25.20 - Last mean reward per episode: -25.90
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 50           |
|    ep_rew_mean          | -26.5        |
| time/                   |              |
|    fps                  | 452          |
|    iterations           | 22           |
|    time_elapsed         | 4782         |
|    total_timesteps      | 2162688      |
| train/                  |              |
|    approx_kl            | 0.0054302537 |
|    clip_fraction        | 0.411        |
|    clip_range           | 0.075        |
|    entropy_loss         | -3.57        |
|    explained_variance   | 0.863        |
|    learning_rate        | 0.00134      |
|    loss                 | 0.83         |
|    n_updates            | 189          |
|    policy_gradient_loss | -0.0113      |
|    std                  | 0.794        |
|    value_loss           | 1.21         |
------------------------------------------
Num timesteps: 2163000
Best mean reward: -25.20 - Last mean reward per episode: -26.12
Num timesteps: 2166000
Best mean reward: -25.20 - Last mean reward per episode: -27.10
Num timesteps: 2169000
Best mean reward: -25.20 - Last mean reward per episode: -25.75
Num timesteps: 2172000
Best mean reward: -25.20 - Last mean reward per episode: -25.48
Num timesteps: 2175000
Best mean reward: -25.20 - Last mean reward per episode: -25.91
Num timesteps: 2178000
Best mean reward: -25.20 - Last mean reward per episode: -26.26
Num timesteps: 2181000
Best mean reward: -25.20 - Last mean reward per episode: -25.92
Num timesteps: 2184000
Best mean reward: -25.20 - Last mean reward per episode: -26.01
Num timesteps: 2187000
Best mean reward: -25.20 - Last mean reward per episode: -25.44
Num timesteps: 2190000
Best mean reward: -25.20 - Last mean reward per episode: -25.97
Num timesteps: 2193000
Best mean reward: -25.20 - Last mean reward per episode: -24.28

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_203623_numTimesteps_2193000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 2196000
Best mean reward: -24.28 - Last mean reward per episode: -23.40

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_203630_numTimesteps_2196000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 2199000
Best mean reward: -23.40 - Last mean reward per episode: -25.64
Num timesteps: 2202000
Best mean reward: -23.40 - Last mean reward per episode: -26.12
Num timesteps: 2205000
Best mean reward: -23.40 - Last mean reward per episode: -27.18
Num timesteps: 2208000
Best mean reward: -23.40 - Last mean reward per episode: -27.68
Num timesteps: 2211000
Best mean reward: -23.40 - Last mean reward per episode: -27.06
Num timesteps: 2214000
Best mean reward: -23.40 - Last mean reward per episode: -27.38
Num timesteps: 2217000
Best mean reward: -23.40 - Last mean reward per episode: -26.13
Num timesteps: 2220000
Best mean reward: -23.40 - Last mean reward per episode: -27.02
Num timesteps: 2223000
Best mean reward: -23.40 - Last mean reward per episode: -25.83
Num timesteps: 2226000
Best mean reward: -23.40 - Last mean reward per episode: -24.53
Num timesteps: 2229000
Best mean reward: -23.40 - Last mean reward per episode: -23.96
Num timesteps: 2232000
Best mean reward: -23.40 - Last mean reward per episode: -25.31
Num timesteps: 2235000
Best mean reward: -23.40 - Last mean reward per episode: -25.18
Num timesteps: 2238000
Best mean reward: -23.40 - Last mean reward per episode: -24.12
Num timesteps: 2241000
Best mean reward: -23.40 - Last mean reward per episode: -24.65
Num timesteps: 2244000
Best mean reward: -23.40 - Last mean reward per episode: -25.10
Num timesteps: 2247000
Best mean reward: -23.40 - Last mean reward per episode: -26.33
Num timesteps: 2250000
Best mean reward: -23.40 - Last mean reward per episode: -25.47
Num timesteps: 2253000
Best mean reward: -23.40 - Last mean reward per episode: -25.79
Num timesteps: 2256000
Best mean reward: -23.40 - Last mean reward per episode: -25.23
Num timesteps: 2259000
Best mean reward: -23.40 - Last mean reward per episode: -25.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 50           |
|    ep_rew_mean          | -25.8        |
| time/                   |              |
|    fps                  | 450          |
|    iterations           | 23           |
|    time_elapsed         | 5015         |
|    total_timesteps      | 2260992      |
| train/                  |              |
|    approx_kl            | 0.0055797375 |
|    clip_fraction        | 0.422        |
|    clip_range           | 0.075        |
|    entropy_loss         | -3.49        |
|    explained_variance   | 0.858        |
|    learning_rate        | 0.00134      |
|    loss                 | 0.879        |
|    n_updates            | 198          |
|    policy_gradient_loss | -0.0107      |
|    std                  | 0.776        |
|    value_loss           | 1.23         |
------------------------------------------
Num timesteps: 2262000
Best mean reward: -23.40 - Last mean reward per episode: -25.00
Num timesteps: 2265000
Best mean reward: -23.40 - Last mean reward per episode: -22.98

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_203924_numTimesteps_2265000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 2268000
Best mean reward: -22.98 - Last mean reward per episode: -23.17
Num timesteps: 2271000
Best mean reward: -22.98 - Last mean reward per episode: -24.00
Num timesteps: 2274000
Best mean reward: -22.98 - Last mean reward per episode: -23.43
Num timesteps: 2277000
Best mean reward: -22.98 - Last mean reward per episode: -21.65

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_203945_numTimesteps_2277000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 2280000
Best mean reward: -21.65 - Last mean reward per episode: -22.11
Num timesteps: 2283000
Best mean reward: -21.65 - Last mean reward per episode: -23.88
Num timesteps: 2286000
Best mean reward: -21.65 - Last mean reward per episode: -22.44
Num timesteps: 2289000
Best mean reward: -21.65 - Last mean reward per episode: -23.33
Num timesteps: 2292000
Best mean reward: -21.65 - Last mean reward per episode: -24.50
Num timesteps: 2295000
Best mean reward: -21.65 - Last mean reward per episode: -24.52
Num timesteps: 2298000
Best mean reward: -21.65 - Last mean reward per episode: -24.06
Num timesteps: 2301000
Best mean reward: -21.65 - Last mean reward per episode: -23.42
Num timesteps: 2304000
Best mean reward: -21.65 - Last mean reward per episode: -22.52
Num timesteps: 2307000
Best mean reward: -21.65 - Last mean reward per episode: -23.20
Num timesteps: 2310000
Best mean reward: -21.65 - Last mean reward per episode: -23.06
Num timesteps: 2313000
Best mean reward: -21.65 - Last mean reward per episode: -22.78
Num timesteps: 2316000
Best mean reward: -21.65 - Last mean reward per episode: -23.45
Num timesteps: 2319000
Best mean reward: -21.65 - Last mean reward per episode: -21.73
Num timesteps: 2322000
Best mean reward: -21.65 - Last mean reward per episode: -21.44

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_204112_numTimesteps_2322000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 2325000
Best mean reward: -21.44 - Last mean reward per episode: -22.34
Num timesteps: 2328000
Best mean reward: -21.44 - Last mean reward per episode: -23.28
Num timesteps: 2331000
Best mean reward: -21.44 - Last mean reward per episode: -23.48
Num timesteps: 2334000
Best mean reward: -21.44 - Last mean reward per episode: -23.10
Num timesteps: 2337000
Best mean reward: -21.44 - Last mean reward per episode: -23.24
Num timesteps: 2340000
Best mean reward: -21.44 - Last mean reward per episode: -23.49
Num timesteps: 2343000
Best mean reward: -21.44 - Last mean reward per episode: -24.06
Num timesteps: 2346000
Best mean reward: -21.44 - Last mean reward per episode: -24.15
Num timesteps: 2349000
Best mean reward: -21.44 - Last mean reward per episode: -22.58
Num timesteps: 2352000
Best mean reward: -21.44 - Last mean reward per episode: -21.23

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_204219_numTimesteps_2352000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 2355000
Best mean reward: -21.23 - Last mean reward per episode: -23.60
Num timesteps: 2358000
Best mean reward: -21.23 - Last mean reward per episode: -23.55
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 50          |
|    ep_rew_mean          | -23.6       |
| time/                   |             |
|    fps                  | 450         |
|    iterations           | 24          |
|    time_elapsed         | 5238        |
|    total_timesteps      | 2359296     |
| train/                  |             |
|    approx_kl            | 0.005275564 |
|    clip_fraction        | 0.417       |
|    clip_range           | 0.075       |
|    entropy_loss         | -3.42       |
|    explained_variance   | 0.834       |
|    learning_rate        | 0.00134     |
|    loss                 | 0.815       |
|    n_updates            | 207         |
|    policy_gradient_loss | -0.0107     |
|    std                  | 0.756       |
|    value_loss           | 1.27        |
-----------------------------------------
Num timesteps: 2361000
Best mean reward: -21.23 - Last mean reward per episode: -22.31
Num timesteps: 2364000
Best mean reward: -21.23 - Last mean reward per episode: -22.19
Num timesteps: 2367000
Best mean reward: -21.23 - Last mean reward per episode: -20.61

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_204317_numTimesteps_2367000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 2370000
Best mean reward: -20.61 - Last mean reward per episode: -20.32

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_204323_numTimesteps_2370000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 2373000
Best mean reward: -20.32 - Last mean reward per episode: -19.83

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_204329_numTimesteps_2373000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 2376000
Best mean reward: -19.83 - Last mean reward per episode: -20.67
Num timesteps: 2379000
Best mean reward: -19.83 - Last mean reward per episode: -21.81
Num timesteps: 2382000
Best mean reward: -19.83 - Last mean reward per episode: -21.81
Num timesteps: 2385000
Best mean reward: -19.83 - Last mean reward per episode: -21.29
Num timesteps: 2388000
Best mean reward: -19.83 - Last mean reward per episode: -19.38

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_204400_numTimesteps_2388000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 2391000
Best mean reward: -19.38 - Last mean reward per episode: -20.61
Num timesteps: 2394000
Best mean reward: -19.38 - Last mean reward per episode: -21.86
Num timesteps: 2397000
Best mean reward: -19.38 - Last mean reward per episode: -21.55
Num timesteps: 2400000
Best mean reward: -19.38 - Last mean reward per episode: -20.75
Num timesteps: 2403000
Best mean reward: -19.38 - Last mean reward per episode: -19.93
Num timesteps: 2406000
Best mean reward: -19.38 - Last mean reward per episode: -20.09
Num timesteps: 2409000
Best mean reward: -19.38 - Last mean reward per episode: -21.06
Num timesteps: 2412000
Best mean reward: -19.38 - Last mean reward per episode: -20.93
Num timesteps: 2415000
Best mean reward: -19.38 - Last mean reward per episode: -20.19
Num timesteps: 2418000
Best mean reward: -19.38 - Last mean reward per episode: -19.59
Num timesteps: 2421000
Best mean reward: -19.38 - Last mean reward per episode: -20.43
Num timesteps: 2424000
Best mean reward: -19.38 - Last mean reward per episode: -21.05
Num timesteps: 2427000
Best mean reward: -19.38 - Last mean reward per episode: -22.02
Num timesteps: 2430000
Best mean reward: -19.38 - Last mean reward per episode: -21.65
Num timesteps: 2433000
Best mean reward: -19.38 - Last mean reward per episode: -21.14
Num timesteps: 2436000
Best mean reward: -19.38 - Last mean reward per episode: -21.63
Num timesteps: 2439000
Best mean reward: -19.38 - Last mean reward per episode: -21.71
Num timesteps: 2442000
Best mean reward: -19.38 - Last mean reward per episode: -21.52
Num timesteps: 2445000
Best mean reward: -19.38 - Last mean reward per episode: -21.64
Num timesteps: 2448000
Best mean reward: -19.38 - Last mean reward per episode: -21.81
Num timesteps: 2451000
Best mean reward: -19.38 - Last mean reward per episode: -21.17
Num timesteps: 2454000
Best mean reward: -19.38 - Last mean reward per episode: -20.87
Num timesteps: 2457000
Best mean reward: -19.38 - Last mean reward per episode: -20.23
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 50          |
|    ep_rew_mean          | -20.1       |
| time/                   |             |
|    fps                  | 448         |
|    iterations           | 25          |
|    time_elapsed         | 5478        |
|    total_timesteps      | 2457600     |
| train/                  |             |
|    approx_kl            | 0.005157601 |
|    clip_fraction        | 0.422       |
|    clip_range           | 0.075       |
|    entropy_loss         | -3.34       |
|    explained_variance   | 0.812       |
|    learning_rate        | 0.00134     |
|    loss                 | 0.84        |
|    n_updates            | 216         |
|    policy_gradient_loss | -0.00967    |
|    std                  | 0.737       |
|    value_loss           | 1.22        |
-----------------------------------------
Num timesteps: 2460000
Best mean reward: -19.38 - Last mean reward per episode: -19.64
Num timesteps: 2463000
Best mean reward: -19.38 - Last mean reward per episode: -19.21

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_204705_numTimesteps_2463000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 2466000
Best mean reward: -19.21 - Last mean reward per episode: -18.33

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_204710_numTimesteps_2466000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 2469000
Best mean reward: -18.33 - Last mean reward per episode: -17.94

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_204717_numTimesteps_2469000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 2472000
Best mean reward: -17.94 - Last mean reward per episode: -18.48
Num timesteps: 2475000
Best mean reward: -17.94 - Last mean reward per episode: -17.53

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_204731_numTimesteps_2475000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 2478000
Best mean reward: -17.53 - Last mean reward per episode: -19.37
Num timesteps: 2481000
Best mean reward: -17.53 - Last mean reward per episode: -20.21
Num timesteps: 2484000
Best mean reward: -17.53 - Last mean reward per episode: -19.95
Num timesteps: 2487000
Best mean reward: -17.53 - Last mean reward per episode: -18.39
Num timesteps: 2490000
Best mean reward: -17.53 - Last mean reward per episode: -19.79
Num timesteps: 2493000
Best mean reward: -17.53 - Last mean reward per episode: -20.01
Num timesteps: 2496000
Best mean reward: -17.53 - Last mean reward per episode: -20.07
Num timesteps: 2499000
Best mean reward: -17.53 - Last mean reward per episode: -20.18
Num timesteps: 2502000
Best mean reward: -17.53 - Last mean reward per episode: -20.00
Num timesteps: 2505000
Best mean reward: -17.53 - Last mean reward per episode: -19.39
Num timesteps: 2508000
Best mean reward: -17.53 - Last mean reward per episode: -18.86
Num timesteps: 2511000
Best mean reward: -17.53 - Last mean reward per episode: -19.07
Num timesteps: 2514000
Best mean reward: -17.53 - Last mean reward per episode: -19.62
Num timesteps: 2517000
Best mean reward: -17.53 - Last mean reward per episode: -17.59
Num timesteps: 2520000
Best mean reward: -17.53 - Last mean reward per episode: -18.64
Num timesteps: 2523000
Best mean reward: -17.53 - Last mean reward per episode: -18.99
Num timesteps: 2526000
Best mean reward: -17.53 - Last mean reward per episode: -18.66
Num timesteps: 2529000
Best mean reward: -17.53 - Last mean reward per episode: -19.52
Num timesteps: 2532000
Best mean reward: -17.53 - Last mean reward per episode: -18.46
Num timesteps: 2535000
Best mean reward: -17.53 - Last mean reward per episode: -17.99
Num timesteps: 2538000
Best mean reward: -17.53 - Last mean reward per episode: -19.00
Num timesteps: 2541000
Best mean reward: -17.53 - Last mean reward per episode: -18.20
Num timesteps: 2544000
Best mean reward: -17.53 - Last mean reward per episode: -19.74
Num timesteps: 2547000
Best mean reward: -17.53 - Last mean reward per episode: -19.55
Num timesteps: 2550000
Best mean reward: -17.53 - Last mean reward per episode: -19.42
Num timesteps: 2553000
Best mean reward: -17.53 - Last mean reward per episode: -20.09
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 50           |
|    ep_rew_mean          | -20.3        |
| time/                   |              |
|    fps                  | 448          |
|    iterations           | 26           |
|    time_elapsed         | 5697         |
|    total_timesteps      | 2555904      |
| train/                  |              |
|    approx_kl            | 0.0050848206 |
|    clip_fraction        | 0.411        |
|    clip_range           | 0.075        |
|    entropy_loss         | -3.27        |
|    explained_variance   | 0.798        |
|    learning_rate        | 0.00134      |
|    loss                 | 0.772        |
|    n_updates            | 225          |
|    policy_gradient_loss | -0.00913     |
|    std                  | 0.718        |
|    value_loss           | 1.16         |
------------------------------------------
Num timesteps: 2556000
Best mean reward: -17.53 - Last mean reward per episode: -20.27
Num timesteps: 2559000
Best mean reward: -17.53 - Last mean reward per episode: -19.13
Num timesteps: 2562000
Best mean reward: -17.53 - Last mean reward per episode: -17.84
Num timesteps: 2565000
Best mean reward: -17.53 - Last mean reward per episode: -16.89

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_205049_numTimesteps_2565000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 2568000
Best mean reward: -16.89 - Last mean reward per episode: -16.98
Num timesteps: 2571000
Best mean reward: -16.89 - Last mean reward per episode: -17.84
Num timesteps: 2574000
Best mean reward: -16.89 - Last mean reward per episode: -17.96
Num timesteps: 2577000
Best mean reward: -16.89 - Last mean reward per episode: -18.84
Num timesteps: 2580000
Best mean reward: -16.89 - Last mean reward per episode: -17.07
Num timesteps: 2583000
Best mean reward: -16.89 - Last mean reward per episode: -16.33

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_205128_numTimesteps_2583000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 2586000
Best mean reward: -16.33 - Last mean reward per episode: -17.12
Num timesteps: 2589000
Best mean reward: -16.33 - Last mean reward per episode: -17.21
Num timesteps: 2592000
Best mean reward: -16.33 - Last mean reward per episode: -17.01
Num timesteps: 2595000
Best mean reward: -16.33 - Last mean reward per episode: -17.44
Num timesteps: 2598000
Best mean reward: -16.33 - Last mean reward per episode: -16.82
Num timesteps: 2601000
Best mean reward: -16.33 - Last mean reward per episode: -16.98
Num timesteps: 2604000
Best mean reward: -16.33 - Last mean reward per episode: -17.70
Num timesteps: 2607000
Best mean reward: -16.33 - Last mean reward per episode: -17.91
Num timesteps: 2610000
Best mean reward: -16.33 - Last mean reward per episode: -18.40
Num timesteps: 2613000
Best mean reward: -16.33 - Last mean reward per episode: -18.60
Num timesteps: 2616000
Best mean reward: -16.33 - Last mean reward per episode: -18.22
Num timesteps: 2619000
Best mean reward: -16.33 - Last mean reward per episode: -17.36
Num timesteps: 2622000
Best mean reward: -16.33 - Last mean reward per episode: -17.82
Num timesteps: 2625000
Best mean reward: -16.33 - Last mean reward per episode: -18.51
Num timesteps: 2628000
Best mean reward: -16.33 - Last mean reward per episode: -18.19
Num timesteps: 2631000
Best mean reward: -16.33 - Last mean reward per episode: -19.04
Num timesteps: 2634000
Best mean reward: -16.33 - Last mean reward per episode: -18.29
Num timesteps: 2637000
Best mean reward: -16.33 - Last mean reward per episode: -16.44
Num timesteps: 2640000
Best mean reward: -16.33 - Last mean reward per episode: -17.07
Num timesteps: 2643000
Best mean reward: -16.33 - Last mean reward per episode: -18.41
Num timesteps: 2646000
Best mean reward: -16.33 - Last mean reward per episode: -16.75
Num timesteps: 2649000
Best mean reward: -16.33 - Last mean reward per episode: -17.22
Num timesteps: 2652000
Best mean reward: -16.33 - Last mean reward per episode: -18.50
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 50          |
|    ep_rew_mean          | -17.7       |
| time/                   |             |
|    fps                  | 448         |
|    iterations           | 27          |
|    time_elapsed         | 5912        |
|    total_timesteps      | 2654208     |
| train/                  |             |
|    approx_kl            | 0.005700425 |
|    clip_fraction        | 0.409       |
|    clip_range           | 0.075       |
|    entropy_loss         | -3.19       |
|    explained_variance   | 0.79        |
|    learning_rate        | 0.00134     |
|    loss                 | 0.779       |
|    n_updates            | 234         |
|    policy_gradient_loss | -0.00839    |
|    std                  | 0.7         |
|    value_loss           | 1.08        |
-----------------------------------------
Num timesteps: 2655000
Best mean reward: -16.33 - Last mean reward per episode: -17.34
Num timesteps: 2658000
Best mean reward: -16.33 - Last mean reward per episode: -16.35
Num timesteps: 2661000
Best mean reward: -16.33 - Last mean reward per episode: -16.29

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_205424_numTimesteps_2661000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 2664000
Best mean reward: -16.29 - Last mean reward per episode: -16.62
Num timesteps: 2667000
Best mean reward: -16.29 - Last mean reward per episode: -16.00

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_205435_numTimesteps_2667000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 2670000
Best mean reward: -16.00 - Last mean reward per episode: -16.15
Num timesteps: 2673000
Best mean reward: -16.00 - Last mean reward per episode: -16.05
Num timesteps: 2676000
Best mean reward: -16.00 - Last mean reward per episode: -16.33
Num timesteps: 2679000
Best mean reward: -16.00 - Last mean reward per episode: -16.42
Num timesteps: 2682000
Best mean reward: -16.00 - Last mean reward per episode: -15.97

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_205503_numTimesteps_2682000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 2685000
Best mean reward: -15.97 - Last mean reward per episode: -16.33
Num timesteps: 2688000
Best mean reward: -15.97 - Last mean reward per episode: -17.01
Num timesteps: 2691000
Best mean reward: -15.97 - Last mean reward per episode: -17.28
Num timesteps: 2694000
Best mean reward: -15.97 - Last mean reward per episode: -16.90
Num timesteps: 2697000
Best mean reward: -15.97 - Last mean reward per episode: -16.64
Num timesteps: 2700000
Best mean reward: -15.97 - Last mean reward per episode: -15.70

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_205544_numTimesteps_2700000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 2703000
Best mean reward: -15.70 - Last mean reward per episode: -15.44

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_205551_numTimesteps_2703000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 2706000
Best mean reward: -15.44 - Last mean reward per episode: -15.72
Num timesteps: 2709000
Best mean reward: -15.44 - Last mean reward per episode: -16.63
Num timesteps: 2712000
Best mean reward: -15.44 - Last mean reward per episode: -15.88
Num timesteps: 2715000
Best mean reward: -15.44 - Last mean reward per episode: -16.00
Num timesteps: 2718000
Best mean reward: -15.44 - Last mean reward per episode: -16.98
Num timesteps: 2721000
Best mean reward: -15.44 - Last mean reward per episode: -16.83
Num timesteps: 2724000
Best mean reward: -15.44 - Last mean reward per episode: -16.44
Num timesteps: 2727000
Best mean reward: -15.44 - Last mean reward per episode: -16.28
Num timesteps: 2730000
Best mean reward: -15.44 - Last mean reward per episode: -16.60
Num timesteps: 2733000
Best mean reward: -15.44 - Last mean reward per episode: -15.71
Num timesteps: 2736000
Best mean reward: -15.44 - Last mean reward per episode: -16.29
Num timesteps: 2739000
Best mean reward: -15.44 - Last mean reward per episode: -16.80
Num timesteps: 2742000
Best mean reward: -15.44 - Last mean reward per episode: -16.93
Num timesteps: 2745000
Best mean reward: -15.44 - Last mean reward per episode: -16.39
Num timesteps: 2748000
Best mean reward: -15.44 - Last mean reward per episode: -16.09
Num timesteps: 2751000
Best mean reward: -15.44 - Last mean reward per episode: -14.88

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_205735_numTimesteps_2751000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 50           |
|    ep_rew_mean          | -15.4        |
| time/                   |              |
|    fps                  | 448          |
|    iterations           | 28           |
|    time_elapsed         | 6141         |
|    total_timesteps      | 2752512      |
| train/                  |              |
|    approx_kl            | 0.0051334836 |
|    clip_fraction        | 0.413        |
|    clip_range           | 0.075        |
|    entropy_loss         | -3.11        |
|    explained_variance   | 0.768        |
|    learning_rate        | 0.00134      |
|    loss                 | 0.702        |
|    n_updates            | 243          |
|    policy_gradient_loss | -0.00743     |
|    std                  | 0.683        |
|    value_loss           | 0.967        |
------------------------------------------
Num timesteps: 2754000
Best mean reward: -14.88 - Last mean reward per episode: -15.37
Num timesteps: 2757000
Best mean reward: -14.88 - Last mean reward per episode: -15.71
Num timesteps: 2760000
Best mean reward: -14.88 - Last mean reward per episode: -14.04

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_205810_numTimesteps_2760000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 2763000
Best mean reward: -14.04 - Last mean reward per episode: -13.25

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_205815_numTimesteps_2763000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 2766000
Best mean reward: -13.25 - Last mean reward per episode: -14.68
Num timesteps: 2769000
Best mean reward: -13.25 - Last mean reward per episode: -15.08
Num timesteps: 2772000
Best mean reward: -13.25 - Last mean reward per episode: -14.63
Num timesteps: 2775000
Best mean reward: -13.25 - Last mean reward per episode: -15.55
Num timesteps: 2778000
Best mean reward: -13.25 - Last mean reward per episode: -15.51
Num timesteps: 2781000
Best mean reward: -13.25 - Last mean reward per episode: -15.64
Num timesteps: 2784000
Best mean reward: -13.25 - Last mean reward per episode: -14.47
Num timesteps: 2787000
Best mean reward: -13.25 - Last mean reward per episode: -14.62
Num timesteps: 2790000
Best mean reward: -13.25 - Last mean reward per episode: -15.46
Num timesteps: 2793000
Best mean reward: -13.25 - Last mean reward per episode: -15.76
Num timesteps: 2796000
Best mean reward: -13.25 - Last mean reward per episode: -14.95
Num timesteps: 2799000
Best mean reward: -13.25 - Last mean reward per episode: -14.39
Num timesteps: 2802000
Best mean reward: -13.25 - Last mean reward per episode: -15.13
Num timesteps: 2805000
Best mean reward: -13.25 - Last mean reward per episode: -15.15
Num timesteps: 2808000
Best mean reward: -13.25 - Last mean reward per episode: -15.48
Num timesteps: 2811000
Best mean reward: -13.25 - Last mean reward per episode: -15.05
Num timesteps: 2814000
Best mean reward: -13.25 - Last mean reward per episode: -15.11
Num timesteps: 2817000
Best mean reward: -13.25 - Last mean reward per episode: -15.03
Num timesteps: 2820000
Best mean reward: -13.25 - Last mean reward per episode: -15.26
Num timesteps: 2823000
Best mean reward: -13.25 - Last mean reward per episode: -15.85
Num timesteps: 2826000
Best mean reward: -13.25 - Last mean reward per episode: -15.86
Num timesteps: 2829000
Best mean reward: -13.25 - Last mean reward per episode: -14.59
Num timesteps: 2832000
Best mean reward: -13.25 - Last mean reward per episode: -14.21
Num timesteps: 2835000
Best mean reward: -13.25 - Last mean reward per episode: -15.22
Num timesteps: 2838000
Best mean reward: -13.25 - Last mean reward per episode: -15.39
Num timesteps: 2841000
Best mean reward: -13.25 - Last mean reward per episode: -15.20
Num timesteps: 2844000
Best mean reward: -13.25 - Last mean reward per episode: -16.26
Num timesteps: 2847000
Best mean reward: -13.25 - Last mean reward per episode: -15.62
Num timesteps: 2850000
Best mean reward: -13.25 - Last mean reward per episode: -14.97
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 50          |
|    ep_rew_mean          | -15.3       |
| time/                   |             |
|    fps                  | 448         |
|    iterations           | 29          |
|    time_elapsed         | 6356        |
|    total_timesteps      | 2850816     |
| train/                  |             |
|    approx_kl            | 0.005568811 |
|    clip_fraction        | 0.418       |
|    clip_range           | 0.075       |
|    entropy_loss         | -3.03       |
|    explained_variance   | 0.747       |
|    learning_rate        | 0.00134     |
|    loss                 | 0.72        |
|    n_updates            | 252         |
|    policy_gradient_loss | -0.00646    |
|    std                  | 0.665       |
|    value_loss           | 0.878       |
-----------------------------------------
Num timesteps: 2853000
Best mean reward: -13.25 - Last mean reward per episode: -14.44
Num timesteps: 2856000
Best mean reward: -13.25 - Last mean reward per episode: -13.35
Num timesteps: 2859000
Best mean reward: -13.25 - Last mean reward per episode: -13.32
Num timesteps: 2862000
Best mean reward: -13.25 - Last mean reward per episode: -12.83

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_210204_numTimesteps_2862000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 2865000
Best mean reward: -12.83 - Last mean reward per episode: -13.83
Num timesteps: 2868000
Best mean reward: -12.83 - Last mean reward per episode: -14.13
Num timesteps: 2871000
Best mean reward: -12.83 - Last mean reward per episode: -14.47
Num timesteps: 2874000
Best mean reward: -12.83 - Last mean reward per episode: -14.45
Num timesteps: 2877000
Best mean reward: -12.83 - Last mean reward per episode: -13.78
Num timesteps: 2880000
Best mean reward: -12.83 - Last mean reward per episode: -13.81
Num timesteps: 2883000
Best mean reward: -12.83 - Last mean reward per episode: -14.25
Num timesteps: 2886000
Best mean reward: -12.83 - Last mean reward per episode: -13.68
Num timesteps: 2889000
Best mean reward: -12.83 - Last mean reward per episode: -13.06
Num timesteps: 2892000
Best mean reward: -12.83 - Last mean reward per episode: -13.71
Num timesteps: 2895000
Best mean reward: -12.83 - Last mean reward per episode: -13.69
Num timesteps: 2898000
Best mean reward: -12.83 - Last mean reward per episode: -14.32
Num timesteps: 2901000
Best mean reward: -12.83 - Last mean reward per episode: -14.62
Num timesteps: 2904000
Best mean reward: -12.83 - Last mean reward per episode: -14.19
Num timesteps: 2907000
Best mean reward: -12.83 - Last mean reward per episode: -14.14
Num timesteps: 2910000
Best mean reward: -12.83 - Last mean reward per episode: -13.69
Num timesteps: 2913000
Best mean reward: -12.83 - Last mean reward per episode: -13.21
Num timesteps: 2916000
Best mean reward: -12.83 - Last mean reward per episode: -12.43

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_210405_numTimesteps_2916000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 2919000
Best mean reward: -12.43 - Last mean reward per episode: -13.13
Num timesteps: 2922000
Best mean reward: -12.43 - Last mean reward per episode: -14.75
Num timesteps: 2925000
Best mean reward: -12.43 - Last mean reward per episode: -13.68
Num timesteps: 2928000
Best mean reward: -12.43 - Last mean reward per episode: -14.09
Num timesteps: 2931000
Best mean reward: -12.43 - Last mean reward per episode: -14.45
Num timesteps: 2934000
Best mean reward: -12.43 - Last mean reward per episode: -13.99
Num timesteps: 2937000
Best mean reward: -12.43 - Last mean reward per episode: -14.77
Num timesteps: 2940000
Best mean reward: -12.43 - Last mean reward per episode: -14.30
Num timesteps: 2943000
Best mean reward: -12.43 - Last mean reward per episode: -12.78
Num timesteps: 2946000
Best mean reward: -12.43 - Last mean reward per episode: -14.19
Num timesteps: 2949000
Best mean reward: -12.43 - Last mean reward per episode: -14.03
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 50          |
|    ep_rew_mean          | -14         |
| time/                   |             |
|    fps                  | 447         |
|    iterations           | 30          |
|    time_elapsed         | 6590        |
|    total_timesteps      | 2949120     |
| train/                  |             |
|    approx_kl            | 0.005290314 |
|    clip_fraction        | 0.401       |
|    clip_range           | 0.075       |
|    entropy_loss         | -2.95       |
|    explained_variance   | 0.718       |
|    learning_rate        | 0.00134     |
|    loss                 | 0.576       |
|    n_updates            | 261         |
|    policy_gradient_loss | -0.00582    |
|    std                  | 0.649       |
|    value_loss           | 0.816       |
-----------------------------------------
Num timesteps: 2952000
Best mean reward: -12.43 - Last mean reward per episode: -13.19
Num timesteps: 2955000
Best mean reward: -12.43 - Last mean reward per episode: -12.98
Num timesteps: 2958000
Best mean reward: -12.43 - Last mean reward per episode: -12.96
Num timesteps: 2961000
Best mean reward: -12.43 - Last mean reward per episode: -12.96
Num timesteps: 2964000
Best mean reward: -12.43 - Last mean reward per episode: -12.69
Num timesteps: 2967000
Best mean reward: -12.43 - Last mean reward per episode: -12.50
Num timesteps: 2970000
Best mean reward: -12.43 - Last mean reward per episode: -12.65
Num timesteps: 2973000
Best mean reward: -12.43 - Last mean reward per episode: -13.19
Num timesteps: 2976000
Best mean reward: -12.43 - Last mean reward per episode: -13.54
Num timesteps: 2979000
Best mean reward: -12.43 - Last mean reward per episode: -13.55
Num timesteps: 2982000
Best mean reward: -12.43 - Last mean reward per episode: -13.47
Num timesteps: 2985000
Best mean reward: -12.43 - Last mean reward per episode: -12.69
Num timesteps: 2988000
Best mean reward: -12.43 - Last mean reward per episode: -13.08
Num timesteps: 2991000
Best mean reward: -12.43 - Last mean reward per episode: -13.14
Num timesteps: 2994000
Best mean reward: -12.43 - Last mean reward per episode: -13.28
Num timesteps: 2997000
Best mean reward: -12.43 - Last mean reward per episode: -13.69
Num timesteps: 3000000
Best mean reward: -12.43 - Last mean reward per episode: -12.65
Num timesteps: 3003000
Best mean reward: -12.43 - Last mean reward per episode: -12.78
Num timesteps: 3006000
Best mean reward: -12.43 - Last mean reward per episode: -12.69
Num timesteps: 3009000
Best mean reward: -12.43 - Last mean reward per episode: -13.36
Num timesteps: 3012000
Best mean reward: -12.43 - Last mean reward per episode: -13.39
Num timesteps: 3015000
Best mean reward: -12.43 - Last mean reward per episode: -13.11
Num timesteps: 3018000
Best mean reward: -12.43 - Last mean reward per episode: -13.05
Num timesteps: 3021000
Best mean reward: -12.43 - Last mean reward per episode: -13.36
Num timesteps: 3024000
Best mean reward: -12.43 - Last mean reward per episode: -13.13
Num timesteps: 3027000
Best mean reward: -12.43 - Last mean reward per episode: -12.92
Num timesteps: 3030000
Best mean reward: -12.43 - Last mean reward per episode: -13.64
Num timesteps: 3033000
Best mean reward: -12.43 - Last mean reward per episode: -12.83
Num timesteps: 3036000
Best mean reward: -12.43 - Last mean reward per episode: -12.81
Num timesteps: 3039000
Best mean reward: -12.43 - Last mean reward per episode: -14.09
Num timesteps: 3042000
Best mean reward: -12.43 - Last mean reward per episode: -13.75
Num timesteps: 3045000
Best mean reward: -12.43 - Last mean reward per episode: -14.01
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 50           |
|    ep_rew_mean          | -14.2        |
| time/                   |              |
|    fps                  | 446          |
|    iterations           | 31           |
|    time_elapsed         | 6828         |
|    total_timesteps      | 3047424      |
| train/                  |              |
|    approx_kl            | 0.0051622377 |
|    clip_fraction        | 0.415        |
|    clip_range           | 0.075        |
|    entropy_loss         | -2.88        |
|    explained_variance   | 0.737        |
|    learning_rate        | 0.00134      |
|    loss                 | 0.58         |
|    n_updates            | 270          |
|    policy_gradient_loss | -0.00474     |
|    std                  | 0.632        |
|    value_loss           | 0.708        |
------------------------------------------
Num timesteps: 3048000
Best mean reward: -12.43 - Last mean reward per episode: -14.06
Num timesteps: 3051000
Best mean reward: -12.43 - Last mean reward per episode: -12.64
Num timesteps: 3054000
Best mean reward: -12.43 - Last mean reward per episode: -12.49
Num timesteps: 3057000
Best mean reward: -12.43 - Last mean reward per episode: -12.26

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_210946_numTimesteps_3057000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 3060000
Best mean reward: -12.26 - Last mean reward per episode: -11.81

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_210952_numTimesteps_3060000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 3063000
Best mean reward: -11.81 - Last mean reward per episode: -11.93
Num timesteps: 3066000
Best mean reward: -11.81 - Last mean reward per episode: -12.09
Num timesteps: 3069000
Best mean reward: -11.81 - Last mean reward per episode: -11.85
Num timesteps: 3072000
Best mean reward: -11.81 - Last mean reward per episode: -11.88
Num timesteps: 3075000
Best mean reward: -11.81 - Last mean reward per episode: -12.14
Num timesteps: 3078000
Best mean reward: -11.81 - Last mean reward per episode: -12.11
Num timesteps: 3081000
Best mean reward: -11.81 - Last mean reward per episode: -12.50
Num timesteps: 3084000
Best mean reward: -11.81 - Last mean reward per episode: -12.16
Num timesteps: 3087000
Best mean reward: -11.81 - Last mean reward per episode: -11.43

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_211048_numTimesteps_3087000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 3090000
Best mean reward: -11.43 - Last mean reward per episode: -11.68
Num timesteps: 3093000
Best mean reward: -11.43 - Last mean reward per episode: -11.92
Num timesteps: 3096000
Best mean reward: -11.43 - Last mean reward per episode: -10.98

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_211107_numTimesteps_3096000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 3099000
Best mean reward: -10.98 - Last mean reward per episode: -11.72
Num timesteps: 3102000
Best mean reward: -10.98 - Last mean reward per episode: -12.55
Num timesteps: 3105000
Best mean reward: -10.98 - Last mean reward per episode: -12.16
Num timesteps: 3108000
Best mean reward: -10.98 - Last mean reward per episode: -11.96
Num timesteps: 3111000
Best mean reward: -10.98 - Last mean reward per episode: -11.55
Num timesteps: 3114000
Best mean reward: -10.98 - Last mean reward per episode: -11.88
Num timesteps: 3117000
Best mean reward: -10.98 - Last mean reward per episode: -12.63
Num timesteps: 3120000
Best mean reward: -10.98 - Last mean reward per episode: -12.61
Num timesteps: 3123000
Best mean reward: -10.98 - Last mean reward per episode: -11.95
Num timesteps: 3126000
Best mean reward: -10.98 - Last mean reward per episode: -11.62
Num timesteps: 3129000
Best mean reward: -10.98 - Last mean reward per episode: -11.93
Num timesteps: 3132000
Best mean reward: -10.98 - Last mean reward per episode: -12.47
Num timesteps: 3135000
Best mean reward: -10.98 - Last mean reward per episode: -11.45
Num timesteps: 3138000
Best mean reward: -10.98 - Last mean reward per episode: -11.72
Num timesteps: 3141000
Best mean reward: -10.98 - Last mean reward per episode: -11.82
Num timesteps: 3144000
Best mean reward: -10.98 - Last mean reward per episode: -11.49
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 50          |
|    ep_rew_mean          | -12.3       |
| time/                   |             |
|    fps                  | 446         |
|    iterations           | 32          |
|    time_elapsed         | 7053        |
|    total_timesteps      | 3145728     |
| train/                  |             |
|    approx_kl            | 0.005201714 |
|    clip_fraction        | 0.413       |
|    clip_range           | 0.075       |
|    entropy_loss         | -2.8        |
|    explained_variance   | 0.717       |
|    learning_rate        | 0.00134     |
|    loss                 | 0.461       |
|    n_updates            | 279         |
|    policy_gradient_loss | -0.004      |
|    std                  | 0.617       |
|    value_loss           | 0.673       |
-----------------------------------------
Num timesteps: 3147000
Best mean reward: -10.98 - Last mean reward per episode: -11.20
Num timesteps: 3150000
Best mean reward: -10.98 - Last mean reward per episode: -11.34
Num timesteps: 3153000
Best mean reward: -10.98 - Last mean reward per episode: -10.71

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_211326_numTimesteps_3153000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 3156000
Best mean reward: -10.71 - Last mean reward per episode: -11.31
Num timesteps: 3159000
Best mean reward: -10.71 - Last mean reward per episode: -11.03
Num timesteps: 3162000
Best mean reward: -10.71 - Last mean reward per episode: -11.22
Num timesteps: 3165000
Best mean reward: -10.71 - Last mean reward per episode: -11.20
Num timesteps: 3168000
Best mean reward: -10.71 - Last mean reward per episode: -11.52
Num timesteps: 3171000
Best mean reward: -10.71 - Last mean reward per episode: -11.39
Num timesteps: 3174000
Best mean reward: -10.71 - Last mean reward per episode: -12.39
Num timesteps: 3177000
Best mean reward: -10.71 - Last mean reward per episode: -11.44
Num timesteps: 3180000
Best mean reward: -10.71 - Last mean reward per episode: -10.72
Num timesteps: 3183000
Best mean reward: -10.71 - Last mean reward per episode: -11.95
Num timesteps: 3186000
Best mean reward: -10.71 - Last mean reward per episode: -12.48
Num timesteps: 3189000
Best mean reward: -10.71 - Last mean reward per episode: -12.09
Num timesteps: 3192000
Best mean reward: -10.71 - Last mean reward per episode: -12.34
Num timesteps: 3195000
Best mean reward: -10.71 - Last mean reward per episode: -11.30
Num timesteps: 3198000
Best mean reward: -10.71 - Last mean reward per episode: -10.51

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_211505_numTimesteps_3198000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 3201000
Best mean reward: -10.51 - Last mean reward per episode: -11.08
Num timesteps: 3204000
Best mean reward: -10.51 - Last mean reward per episode: -10.90
Num timesteps: 3207000
Best mean reward: -10.51 - Last mean reward per episode: -11.36
Num timesteps: 3210000
Best mean reward: -10.51 - Last mean reward per episode: -11.82
Num timesteps: 3213000
Best mean reward: -10.51 - Last mean reward per episode: -11.44
Num timesteps: 3216000
Best mean reward: -10.51 - Last mean reward per episode: -10.89
Num timesteps: 3219000
Best mean reward: -10.51 - Last mean reward per episode: -10.76
Num timesteps: 3222000
Best mean reward: -10.51 - Last mean reward per episode: -11.22
Num timesteps: 3225000
Best mean reward: -10.51 - Last mean reward per episode: -11.46
Num timesteps: 3228000
Best mean reward: -10.51 - Last mean reward per episode: -11.85
Num timesteps: 3231000
Best mean reward: -10.51 - Last mean reward per episode: -11.62
Num timesteps: 3234000
Best mean reward: -10.51 - Last mean reward per episode: -11.62
Num timesteps: 3237000
Best mean reward: -10.51 - Last mean reward per episode: -10.92
Num timesteps: 3240000
Best mean reward: -10.51 - Last mean reward per episode: -10.76
Num timesteps: 3243000
Best mean reward: -10.51 - Last mean reward per episode: -11.40
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 50          |
|    ep_rew_mean          | -11.8       |
| time/                   |             |
|    fps                  | 445         |
|    iterations           | 33          |
|    time_elapsed         | 7278        |
|    total_timesteps      | 3244032     |
| train/                  |             |
|    approx_kl            | 0.005304633 |
|    clip_fraction        | 0.405       |
|    clip_range           | 0.075       |
|    entropy_loss         | -2.73       |
|    explained_variance   | 0.705       |
|    learning_rate        | 0.00134     |
|    loss                 | 0.451       |
|    n_updates            | 288         |
|    policy_gradient_loss | -0.0037     |
|    std                  | 0.601       |
|    value_loss           | 0.616       |
-----------------------------------------
Num timesteps: 3246000
Best mean reward: -10.51 - Last mean reward per episode: -10.95
Num timesteps: 3249000
Best mean reward: -10.51 - Last mean reward per episode: -9.94

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_211709_numTimesteps_3249000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 3252000
Best mean reward: -9.94 - Last mean reward per episode: -10.41
Num timesteps: 3255000
Best mean reward: -9.94 - Last mean reward per episode: -10.66
Num timesteps: 3258000
Best mean reward: -9.94 - Last mean reward per episode: -10.69
Num timesteps: 3261000
Best mean reward: -9.94 - Last mean reward per episode: -10.79
Num timesteps: 3264000
Best mean reward: -9.94 - Last mean reward per episode: -9.83

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_211740_numTimesteps_3264000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 3267000
Best mean reward: -9.83 - Last mean reward per episode: -10.08
Num timesteps: 3270000
Best mean reward: -9.83 - Last mean reward per episode: -9.63

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_211750_numTimesteps_3270000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 3273000
Best mean reward: -9.63 - Last mean reward per episode: -9.70
Num timesteps: 3276000
Best mean reward: -9.63 - Last mean reward per episode: -10.41
Num timesteps: 3279000
Best mean reward: -9.63 - Last mean reward per episode: -10.70
Num timesteps: 3282000
Best mean reward: -9.63 - Last mean reward per episode: -10.40
Num timesteps: 3285000
Best mean reward: -9.63 - Last mean reward per episode: -10.27
Num timesteps: 3288000
Best mean reward: -9.63 - Last mean reward per episode: -10.29
Num timesteps: 3291000
Best mean reward: -9.63 - Last mean reward per episode: -11.05
Num timesteps: 3294000
Best mean reward: -9.63 - Last mean reward per episode: -11.59
Num timesteps: 3297000
Best mean reward: -9.63 - Last mean reward per episode: -10.49
Num timesteps: 3300000
Best mean reward: -9.63 - Last mean reward per episode: -10.29
Num timesteps: 3303000
Best mean reward: -9.63 - Last mean reward per episode: -10.26
Num timesteps: 3306000
Best mean reward: -9.63 - Last mean reward per episode: -10.43
Num timesteps: 3309000
Best mean reward: -9.63 - Last mean reward per episode: -10.08
Num timesteps: 3312000
Best mean reward: -9.63 - Last mean reward per episode: -9.72
Num timesteps: 3315000
Best mean reward: -9.63 - Last mean reward per episode: -10.49
Num timesteps: 3318000
Best mean reward: -9.63 - Last mean reward per episode: -10.96
Num timesteps: 3321000
Best mean reward: -9.63 - Last mean reward per episode: -10.47
Num timesteps: 3324000
Best mean reward: -9.63 - Last mean reward per episode: -10.70
Num timesteps: 3327000
Best mean reward: -9.63 - Last mean reward per episode: -11.08
Num timesteps: 3330000
Best mean reward: -9.63 - Last mean reward per episode: -10.71
Num timesteps: 3333000
Best mean reward: -9.63 - Last mean reward per episode: -9.93
Num timesteps: 3336000
Best mean reward: -9.63 - Last mean reward per episode: -10.95
Num timesteps: 3339000
Best mean reward: -9.63 - Last mean reward per episode: -9.74
Num timesteps: 3342000
Best mean reward: -9.63 - Last mean reward per episode: -9.62

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_212012_numTimesteps_3342000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 50           |
|    ep_rew_mean          | -9.85        |
| time/                   |              |
|    fps                  | 445          |
|    iterations           | 34           |
|    time_elapsed         | 7496         |
|    total_timesteps      | 3342336      |
| train/                  |              |
|    approx_kl            | 0.0052178116 |
|    clip_fraction        | 0.4          |
|    clip_range           | 0.075        |
|    entropy_loss         | -2.65        |
|    explained_variance   | 0.674        |
|    learning_rate        | 0.00134      |
|    loss                 | 0.413        |
|    n_updates            | 297          |
|    policy_gradient_loss | -0.00318     |
|    std                  | 0.586        |
|    value_loss           | 0.591        |
------------------------------------------
Num timesteps: 3345000
Best mean reward: -9.62 - Last mean reward per episode: -10.23
Num timesteps: 3348000
Best mean reward: -9.62 - Last mean reward per episode: -10.36
Num timesteps: 3351000
Best mean reward: -9.62 - Last mean reward per episode: -10.14
Num timesteps: 3354000
Best mean reward: -9.62 - Last mean reward per episode: -10.24
Num timesteps: 3357000
Best mean reward: -9.62 - Last mean reward per episode: -9.73
Num timesteps: 3360000
Best mean reward: -9.62 - Last mean reward per episode: -9.29

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_212110_numTimesteps_3360000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 3363000
Best mean reward: -9.29 - Last mean reward per episode: -9.87
Num timesteps: 3366000
Best mean reward: -9.29 - Last mean reward per episode: -10.08
Num timesteps: 3369000
Best mean reward: -9.29 - Last mean reward per episode: -9.28

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_212129_numTimesteps_3369000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 3372000
Best mean reward: -9.28 - Last mean reward per episode: -9.19

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_212136_numTimesteps_3372000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 3375000
Best mean reward: -9.19 - Last mean reward per episode: -9.30
Num timesteps: 3378000
Best mean reward: -9.19 - Last mean reward per episode: -9.88
Num timesteps: 3381000
Best mean reward: -9.19 - Last mean reward per episode: -10.06
Num timesteps: 3384000
Best mean reward: -9.19 - Last mean reward per episode: -10.47
Num timesteps: 3387000
Best mean reward: -9.19 - Last mean reward per episode: -10.10
Num timesteps: 3390000
Best mean reward: -9.19 - Last mean reward per episode: -10.41
Num timesteps: 3393000
Best mean reward: -9.19 - Last mean reward per episode: -10.36
Num timesteps: 3396000
Best mean reward: -9.19 - Last mean reward per episode: -9.06

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_212230_numTimesteps_3396000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 3399000
Best mean reward: -9.06 - Last mean reward per episode: -9.07
Num timesteps: 3402000
Best mean reward: -9.06 - Last mean reward per episode: -9.32
Num timesteps: 3405000
Best mean reward: -9.06 - Last mean reward per episode: -9.61
Num timesteps: 3408000
Best mean reward: -9.06 - Last mean reward per episode: -9.73
Num timesteps: 3411000
Best mean reward: -9.06 - Last mean reward per episode: -10.62
Num timesteps: 3414000
Best mean reward: -9.06 - Last mean reward per episode: -10.66
Num timesteps: 3417000
Best mean reward: -9.06 - Last mean reward per episode: -9.86
Num timesteps: 3420000
Best mean reward: -9.06 - Last mean reward per episode: -9.95
Num timesteps: 3423000
Best mean reward: -9.06 - Last mean reward per episode: -10.11
Num timesteps: 3426000
Best mean reward: -9.06 - Last mean reward per episode: -9.65
Num timesteps: 3429000
Best mean reward: -9.06 - Last mean reward per episode: -10.00
Num timesteps: 3432000
Best mean reward: -9.06 - Last mean reward per episode: -9.77
Num timesteps: 3435000
Best mean reward: -9.06 - Last mean reward per episode: -9.08
Num timesteps: 3438000
Best mean reward: -9.06 - Last mean reward per episode: -9.93
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 50          |
|    ep_rew_mean          | -9.76       |
| time/                   |             |
|    fps                  | 445         |
|    iterations           | 35          |
|    time_elapsed         | 7719        |
|    total_timesteps      | 3440640     |
| train/                  |             |
|    approx_kl            | 0.006055347 |
|    clip_fraction        | 0.414       |
|    clip_range           | 0.075       |
|    entropy_loss         | -2.57       |
|    explained_variance   | 0.692       |
|    learning_rate        | 0.00134     |
|    loss                 | 0.414       |
|    n_updates            | 306         |
|    policy_gradient_loss | -0.00238    |
|    std                  | 0.571       |
|    value_loss           | 0.532       |
-----------------------------------------
Num timesteps: 3441000
Best mean reward: -9.06 - Last mean reward per episode: -9.88
Num timesteps: 3444000
Best mean reward: -9.06 - Last mean reward per episode: -9.22
Num timesteps: 3447000
Best mean reward: -9.06 - Last mean reward per episode: -8.52

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_212435_numTimesteps_3447000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 3450000
Best mean reward: -8.52 - Last mean reward per episode: -9.04
Num timesteps: 3453000
Best mean reward: -8.52 - Last mean reward per episode: -9.57
Num timesteps: 3456000
Best mean reward: -8.52 - Last mean reward per episode: -8.72
Num timesteps: 3459000
Best mean reward: -8.52 - Last mean reward per episode: -8.40

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_212503_numTimesteps_3459000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 3462000
Best mean reward: -8.40 - Last mean reward per episode: -8.14

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_212509_numTimesteps_3462000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 3465000
Best mean reward: -8.14 - Last mean reward per episode: -8.72
Num timesteps: 3468000
Best mean reward: -8.14 - Last mean reward per episode: -8.62
Num timesteps: 3471000
Best mean reward: -8.14 - Last mean reward per episode: -9.08
Num timesteps: 3474000
Best mean reward: -8.14 - Last mean reward per episode: -8.72
Num timesteps: 3477000
Best mean reward: -8.14 - Last mean reward per episode: -9.05
Num timesteps: 3480000
Best mean reward: -8.14 - Last mean reward per episode: -9.04
Num timesteps: 3483000
Best mean reward: -8.14 - Last mean reward per episode: -8.58
Num timesteps: 3486000
Best mean reward: -8.14 - Last mean reward per episode: -8.65
Num timesteps: 3489000
Best mean reward: -8.14 - Last mean reward per episode: -8.83
Num timesteps: 3492000
Best mean reward: -8.14 - Last mean reward per episode: -9.02
Num timesteps: 3495000
Best mean reward: -8.14 - Last mean reward per episode: -8.94
Num timesteps: 3498000
Best mean reward: -8.14 - Last mean reward per episode: -9.66
Num timesteps: 3501000
Best mean reward: -8.14 - Last mean reward per episode: -9.20
Num timesteps: 3504000
Best mean reward: -8.14 - Last mean reward per episode: -9.61
Num timesteps: 3507000
Best mean reward: -8.14 - Last mean reward per episode: -9.63
Num timesteps: 3510000
Best mean reward: -8.14 - Last mean reward per episode: -9.56
Num timesteps: 3513000
Best mean reward: -8.14 - Last mean reward per episode: -9.34
Num timesteps: 3516000
Best mean reward: -8.14 - Last mean reward per episode: -8.87
Num timesteps: 3519000
Best mean reward: -8.14 - Last mean reward per episode: -9.76
Num timesteps: 3522000
Best mean reward: -8.14 - Last mean reward per episode: -9.77
Num timesteps: 3525000
Best mean reward: -8.14 - Last mean reward per episode: -9.34
Num timesteps: 3528000
Best mean reward: -8.14 - Last mean reward per episode: -9.09
Num timesteps: 3531000
Best mean reward: -8.14 - Last mean reward per episode: -8.45
Num timesteps: 3534000
Best mean reward: -8.14 - Last mean reward per episode: -8.51
Num timesteps: 3537000
Best mean reward: -8.14 - Last mean reward per episode: -8.61
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 50           |
|    ep_rew_mean          | -8.44        |
| time/                   |              |
|    fps                  | 445          |
|    iterations           | 36           |
|    time_elapsed         | 7952         |
|    total_timesteps      | 3538944      |
| train/                  |              |
|    approx_kl            | 0.0060963915 |
|    clip_fraction        | 0.407        |
|    clip_range           | 0.075        |
|    entropy_loss         | -2.5         |
|    explained_variance   | 0.661        |
|    learning_rate        | 0.00134      |
|    loss                 | 0.344        |
|    n_updates            | 315          |
|    policy_gradient_loss | -0.00162     |
|    std                  | 0.556        |
|    value_loss           | 0.503        |
------------------------------------------
Num timesteps: 3540000
Best mean reward: -8.14 - Last mean reward per episode: -8.37
Num timesteps: 3543000
Best mean reward: -8.14 - Last mean reward per episode: -8.48
Num timesteps: 3546000
Best mean reward: -8.14 - Last mean reward per episode: -8.61
Num timesteps: 3549000
Best mean reward: -8.14 - Last mean reward per episode: -8.31
Num timesteps: 3552000
Best mean reward: -8.14 - Last mean reward per episode: -8.22
Num timesteps: 3555000
Best mean reward: -8.14 - Last mean reward per episode: -8.24
Num timesteps: 3558000
Best mean reward: -8.14 - Last mean reward per episode: -8.88
Num timesteps: 3561000
Best mean reward: -8.14 - Last mean reward per episode: -8.70
Num timesteps: 3564000
Best mean reward: -8.14 - Last mean reward per episode: -8.20
Num timesteps: 3567000
Best mean reward: -8.14 - Last mean reward per episode: -8.16
Num timesteps: 3570000
Best mean reward: -8.14 - Last mean reward per episode: -8.20
Num timesteps: 3573000
Best mean reward: -8.14 - Last mean reward per episode: -7.92

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_212920_numTimesteps_3573000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 3576000
Best mean reward: -7.92 - Last mean reward per episode: -8.82
Num timesteps: 3579000
Best mean reward: -7.92 - Last mean reward per episode: -8.83
Num timesteps: 3582000
Best mean reward: -7.92 - Last mean reward per episode: -8.59
Num timesteps: 3585000
Best mean reward: -7.92 - Last mean reward per episode: -8.93
Num timesteps: 3588000
Best mean reward: -7.92 - Last mean reward per episode: -8.59
Num timesteps: 3591000
Best mean reward: -7.92 - Last mean reward per episode: -9.00
Num timesteps: 3594000
Best mean reward: -7.92 - Last mean reward per episode: -8.98
Num timesteps: 3597000
Best mean reward: -7.92 - Last mean reward per episode: -8.63
Num timesteps: 3600000
Best mean reward: -7.92 - Last mean reward per episode: -7.95
Num timesteps: 3603000
Best mean reward: -7.92 - Last mean reward per episode: -7.97
Num timesteps: 3606000
Best mean reward: -7.92 - Last mean reward per episode: -8.69
Num timesteps: 3609000
Best mean reward: -7.92 - Last mean reward per episode: -9.25
Num timesteps: 3612000
Best mean reward: -7.92 - Last mean reward per episode: -8.39
Num timesteps: 3615000
Best mean reward: -7.92 - Last mean reward per episode: -8.22
Num timesteps: 3618000
Best mean reward: -7.92 - Last mean reward per episode: -8.92
Num timesteps: 3621000
Best mean reward: -7.92 - Last mean reward per episode: -9.08
Num timesteps: 3624000
Best mean reward: -7.92 - Last mean reward per episode: -9.11
Num timesteps: 3627000
Best mean reward: -7.92 - Last mean reward per episode: -8.14
Num timesteps: 3630000
Best mean reward: -7.92 - Last mean reward per episode: -8.05
Num timesteps: 3633000
Best mean reward: -7.92 - Last mean reward per episode: -8.03
Num timesteps: 3636000
Best mean reward: -7.92 - Last mean reward per episode: -8.09
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 50           |
|    ep_rew_mean          | -7.57        |
| time/                   |              |
|    fps                  | 445          |
|    iterations           | 37           |
|    time_elapsed         | 8171         |
|    total_timesteps      | 3637248      |
| train/                  |              |
|    approx_kl            | 0.0050679124 |
|    clip_fraction        | 0.407        |
|    clip_range           | 0.075        |
|    entropy_loss         | -2.42        |
|    explained_variance   | 0.662        |
|    learning_rate        | 0.00134      |
|    loss                 | 0.346        |
|    n_updates            | 324          |
|    policy_gradient_loss | -0.00127     |
|    std                  | 0.543        |
|    value_loss           | 0.454        |
------------------------------------------
Num timesteps: 3639000
Best mean reward: -7.92 - Last mean reward per episode: -7.36

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_213150_numTimesteps_3639000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 3642000
Best mean reward: -7.36 - Last mean reward per episode: -7.67
Num timesteps: 3645000
Best mean reward: -7.36 - Last mean reward per episode: -7.72
Num timesteps: 3648000
Best mean reward: -7.36 - Last mean reward per episode: -7.93
Num timesteps: 3651000
Best mean reward: -7.36 - Last mean reward per episode: -7.57
Num timesteps: 3654000
Best mean reward: -7.36 - Last mean reward per episode: -7.89
Num timesteps: 3657000
Best mean reward: -7.36 - Last mean reward per episode: -8.90
Num timesteps: 3660000
Best mean reward: -7.36 - Last mean reward per episode: -9.13
Num timesteps: 3663000
Best mean reward: -7.36 - Last mean reward per episode: -8.11
Num timesteps: 3666000
Best mean reward: -7.36 - Last mean reward per episode: -8.04
Num timesteps: 3669000
Best mean reward: -7.36 - Last mean reward per episode: -8.04
Num timesteps: 3672000
Best mean reward: -7.36 - Last mean reward per episode: -8.50
Num timesteps: 3675000
Best mean reward: -7.36 - Last mean reward per episode: -8.15
Num timesteps: 3678000
Best mean reward: -7.36 - Last mean reward per episode: -7.50
Num timesteps: 3681000
Best mean reward: -7.36 - Last mean reward per episode: -7.78
Num timesteps: 3684000
Best mean reward: -7.36 - Last mean reward per episode: -7.62
Num timesteps: 3687000
Best mean reward: -7.36 - Last mean reward per episode: -7.84
Num timesteps: 3690000
Best mean reward: -7.36 - Last mean reward per episode: -7.89
Num timesteps: 3693000
Best mean reward: -7.36 - Last mean reward per episode: -8.37
Num timesteps: 3696000
Best mean reward: -7.36 - Last mean reward per episode: -8.18
Num timesteps: 3699000
Best mean reward: -7.36 - Last mean reward per episode: -7.79
Num timesteps: 3702000
Best mean reward: -7.36 - Last mean reward per episode: -7.56
Num timesteps: 3705000
Best mean reward: -7.36 - Last mean reward per episode: -8.06
Num timesteps: 3708000
Best mean reward: -7.36 - Last mean reward per episode: -7.75
Num timesteps: 3711000
Best mean reward: -7.36 - Last mean reward per episode: -7.37
Num timesteps: 3714000
Best mean reward: -7.36 - Last mean reward per episode: -7.73
Num timesteps: 3717000
Best mean reward: -7.36 - Last mean reward per episode: -7.66
Num timesteps: 3720000
Best mean reward: -7.36 - Last mean reward per episode: -7.93
Num timesteps: 3723000
Best mean reward: -7.36 - Last mean reward per episode: -8.06
Num timesteps: 3726000
Best mean reward: -7.36 - Last mean reward per episode: -8.10
Num timesteps: 3729000
Best mean reward: -7.36 - Last mean reward per episode: -7.49
Num timesteps: 3732000
Best mean reward: -7.36 - Last mean reward per episode: -7.88
Num timesteps: 3735000
Best mean reward: -7.36 - Last mean reward per episode: -8.35
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 50           |
|    ep_rew_mean          | -8.3         |
| time/                   |              |
|    fps                  | 445          |
|    iterations           | 38           |
|    time_elapsed         | 8381         |
|    total_timesteps      | 3735552      |
| train/                  |              |
|    approx_kl            | 0.0059350715 |
|    clip_fraction        | 0.412        |
|    clip_range           | 0.075        |
|    entropy_loss         | -2.34        |
|    explained_variance   | 0.637        |
|    learning_rate        | 0.00134      |
|    loss                 | 0.318        |
|    n_updates            | 333          |
|    policy_gradient_loss | -0.000991    |
|    std                  | 0.53         |
|    value_loss           | 0.433        |
------------------------------------------
Num timesteps: 3738000
Best mean reward: -7.36 - Last mean reward per episode: -8.63
Num timesteps: 3741000
Best mean reward: -7.36 - Last mean reward per episode: -8.29
Num timesteps: 3744000
Best mean reward: -7.36 - Last mean reward per episode: -7.70
Num timesteps: 3747000
Best mean reward: -7.36 - Last mean reward per episode: -7.80
Num timesteps: 3750000
Best mean reward: -7.36 - Last mean reward per episode: -7.52
Num timesteps: 3753000
Best mean reward: -7.36 - Last mean reward per episode: -7.60
Num timesteps: 3756000
Best mean reward: -7.36 - Last mean reward per episode: -7.33

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_213558_numTimesteps_3756000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 3759000
Best mean reward: -7.33 - Last mean reward per episode: -7.34
Num timesteps: 3762000
Best mean reward: -7.33 - Last mean reward per episode: -7.36
Num timesteps: 3765000
Best mean reward: -7.33 - Last mean reward per episode: -7.84
Num timesteps: 3768000
Best mean reward: -7.33 - Last mean reward per episode: -7.66
Num timesteps: 3771000
Best mean reward: -7.33 - Last mean reward per episode: -7.65
Num timesteps: 3774000
Best mean reward: -7.33 - Last mean reward per episode: -7.55
Num timesteps: 3777000
Best mean reward: -7.33 - Last mean reward per episode: -7.45
Num timesteps: 3780000
Best mean reward: -7.33 - Last mean reward per episode: -7.80
Num timesteps: 3783000
Best mean reward: -7.33 - Last mean reward per episode: -7.44
Num timesteps: 3786000
Best mean reward: -7.33 - Last mean reward per episode: -7.06

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_213655_numTimesteps_3786000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 3789000
Best mean reward: -7.06 - Last mean reward per episode: -7.60
Num timesteps: 3792000
Best mean reward: -7.06 - Last mean reward per episode: -7.16
Num timesteps: 3795000
Best mean reward: -7.06 - Last mean reward per episode: -6.94

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_213713_numTimesteps_3795000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 3798000
Best mean reward: -6.94 - Last mean reward per episode: -7.18
Num timesteps: 3801000
Best mean reward: -6.94 - Last mean reward per episode: -7.21
Num timesteps: 3804000
Best mean reward: -6.94 - Last mean reward per episode: -7.02
Num timesteps: 3807000
Best mean reward: -6.94 - Last mean reward per episode: -7.22
Num timesteps: 3810000
Best mean reward: -6.94 - Last mean reward per episode: -7.46
Num timesteps: 3813000
Best mean reward: -6.94 - Last mean reward per episode: -7.19
Num timesteps: 3816000
Best mean reward: -6.94 - Last mean reward per episode: -7.41
Num timesteps: 3819000
Best mean reward: -6.94 - Last mean reward per episode: -7.24
Num timesteps: 3822000
Best mean reward: -6.94 - Last mean reward per episode: -7.10
Num timesteps: 3825000
Best mean reward: -6.94 - Last mean reward per episode: -7.10
Num timesteps: 3828000
Best mean reward: -6.94 - Last mean reward per episode: -7.44
Num timesteps: 3831000
Best mean reward: -6.94 - Last mean reward per episode: -6.58

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_213819_numTimesteps_3831000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 50          |
|    ep_rew_mean          | -6.78       |
| time/                   |             |
|    fps                  | 446         |
|    iterations           | 39          |
|    time_elapsed         | 8587        |
|    total_timesteps      | 3833856     |
| train/                  |             |
|    approx_kl            | 0.005958961 |
|    clip_fraction        | 0.416       |
|    clip_range           | 0.075       |
|    entropy_loss         | -2.27       |
|    explained_variance   | 0.613       |
|    learning_rate        | 0.00134     |
|    loss                 | 0.28        |
|    n_updates            | 342         |
|    policy_gradient_loss | -0.00073    |
|    std                  | 0.517       |
|    value_loss           | 0.404       |
-----------------------------------------
Num timesteps: 3834000
Best mean reward: -6.58 - Last mean reward per episode: -6.88
Num timesteps: 3837000
Best mean reward: -6.58 - Last mean reward per episode: -6.96
Num timesteps: 3840000
Best mean reward: -6.58 - Last mean reward per episode: -7.19
Num timesteps: 3843000
Best mean reward: -6.58 - Last mean reward per episode: -7.14
Num timesteps: 3846000
Best mean reward: -6.58 - Last mean reward per episode: -7.01
Num timesteps: 3849000
Best mean reward: -6.58 - Last mean reward per episode: -7.22
Num timesteps: 3852000
Best mean reward: -6.58 - Last mean reward per episode: -7.39
Num timesteps: 3855000
Best mean reward: -6.58 - Last mean reward per episode: -7.44
Num timesteps: 3858000
Best mean reward: -6.58 - Last mean reward per episode: -7.60
Num timesteps: 3861000
Best mean reward: -6.58 - Last mean reward per episode: -7.70
Num timesteps: 3864000
Best mean reward: -6.58 - Last mean reward per episode: -7.23
Num timesteps: 3867000
Best mean reward: -6.58 - Last mean reward per episode: -6.55

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_213947_numTimesteps_3867000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 3870000
Best mean reward: -6.55 - Last mean reward per episode: -6.64
Num timesteps: 3873000
Best mean reward: -6.55 - Last mean reward per episode: -6.84
Num timesteps: 3876000
Best mean reward: -6.55 - Last mean reward per episode: -6.71
Num timesteps: 3879000
Best mean reward: -6.55 - Last mean reward per episode: -7.14
Num timesteps: 3882000
Best mean reward: -6.55 - Last mean reward per episode: -6.80
Num timesteps: 3885000
Best mean reward: -6.55 - Last mean reward per episode: -7.24
Num timesteps: 3888000
Best mean reward: -6.55 - Last mean reward per episode: -6.94
Num timesteps: 3891000
Best mean reward: -6.55 - Last mean reward per episode: -7.11
Num timesteps: 3894000
Best mean reward: -6.55 - Last mean reward per episode: -6.88
Num timesteps: 3897000
Best mean reward: -6.55 - Last mean reward per episode: -7.10
Num timesteps: 3900000
Best mean reward: -6.55 - Last mean reward per episode: -7.08
Num timesteps: 3903000
Best mean reward: -6.55 - Last mean reward per episode: -7.40
Num timesteps: 3906000
Best mean reward: -6.55 - Last mean reward per episode: -7.09
Num timesteps: 3909000
Best mean reward: -6.55 - Last mean reward per episode: -6.77
Num timesteps: 3912000
Best mean reward: -6.55 - Last mean reward per episode: -6.83
Num timesteps: 3915000
Best mean reward: -6.55 - Last mean reward per episode: -7.13
Num timesteps: 3918000
Best mean reward: -6.55 - Last mean reward per episode: -6.87
Num timesteps: 3921000
Best mean reward: -6.55 - Last mean reward per episode: -7.01
Num timesteps: 3924000
Best mean reward: -6.55 - Last mean reward per episode: -7.49
Num timesteps: 3927000
Best mean reward: -6.55 - Last mean reward per episode: -7.28
Num timesteps: 3930000
Best mean reward: -6.55 - Last mean reward per episode: -7.13
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 50          |
|    ep_rew_mean          | -7.07       |
| time/                   |             |
|    fps                  | 447         |
|    iterations           | 40          |
|    time_elapsed         | 8787        |
|    total_timesteps      | 3932160     |
| train/                  |             |
|    approx_kl            | 0.005059632 |
|    clip_fraction        | 0.405       |
|    clip_range           | 0.075       |
|    entropy_loss         | -2.2        |
|    explained_variance   | 0.622       |
|    learning_rate        | 0.00134     |
|    loss                 | 0.244       |
|    n_updates            | 351         |
|    policy_gradient_loss | -0.000861   |
|    std                  | 0.504       |
|    value_loss           | 0.361       |
-----------------------------------------
Num timesteps: 3933000
Best mean reward: -6.55 - Last mean reward per episode: -7.06
Num timesteps: 3936000
Best mean reward: -6.55 - Last mean reward per episode: -6.28

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_214212_numTimesteps_3936000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 3939000
Best mean reward: -6.28 - Last mean reward per episode: -6.57
Num timesteps: 3942000
Best mean reward: -6.28 - Last mean reward per episode: -6.65
Num timesteps: 3945000
Best mean reward: -6.28 - Last mean reward per episode: -6.61
Num timesteps: 3948000
Best mean reward: -6.28 - Last mean reward per episode: -6.70
Num timesteps: 3951000
Best mean reward: -6.28 - Last mean reward per episode: -6.07

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_214235_numTimesteps_3951000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 3954000
Best mean reward: -6.07 - Last mean reward per episode: -6.12
Num timesteps: 3957000
Best mean reward: -6.07 - Last mean reward per episode: -6.47
Num timesteps: 3960000
Best mean reward: -6.07 - Last mean reward per episode: -6.60
Num timesteps: 3963000
Best mean reward: -6.07 - Last mean reward per episode: -7.23
Num timesteps: 3966000
Best mean reward: -6.07 - Last mean reward per episode: -6.79
Num timesteps: 3969000
Best mean reward: -6.07 - Last mean reward per episode: -6.69
Num timesteps: 3972000
Best mean reward: -6.07 - Last mean reward per episode: -6.47
Num timesteps: 3975000
Best mean reward: -6.07 - Last mean reward per episode: -6.18
Num timesteps: 3978000
Best mean reward: -6.07 - Last mean reward per episode: -6.54
Num timesteps: 3981000
Best mean reward: -6.07 - Last mean reward per episode: -6.50
Num timesteps: 3984000
Best mean reward: -6.07 - Last mean reward per episode: -6.33
Num timesteps: 3987000
Best mean reward: -6.07 - Last mean reward per episode: -6.44
Num timesteps: 3990000
Best mean reward: -6.07 - Last mean reward per episode: -6.35
Num timesteps: 3993000
Best mean reward: -6.07 - Last mean reward per episode: -6.41
Num timesteps: 3996000
Best mean reward: -6.07 - Last mean reward per episode: -6.82
Num timesteps: 3999000
Best mean reward: -6.07 - Last mean reward per episode: -6.35
Num timesteps: 4002000
Best mean reward: -6.07 - Last mean reward per episode: -6.41
Num timesteps: 4005000
Best mean reward: -6.07 - Last mean reward per episode: -6.19
Num timesteps: 4008000
Best mean reward: -6.07 - Last mean reward per episode: -6.64
Num timesteps: 4011000
Best mean reward: -6.07 - Last mean reward per episode: -6.85
Num timesteps: 4014000
Best mean reward: -6.07 - Last mean reward per episode: -6.51
Num timesteps: 4017000
Best mean reward: -6.07 - Last mean reward per episode: -6.31
Num timesteps: 4020000
Best mean reward: -6.07 - Last mean reward per episode: -6.64
Num timesteps: 4023000
Best mean reward: -6.07 - Last mean reward per episode: -6.74
Num timesteps: 4026000
Best mean reward: -6.07 - Last mean reward per episode: -6.27
Num timesteps: 4029000
Best mean reward: -6.07 - Last mean reward per episode: -6.47
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 50          |
|    ep_rew_mean          | -6.46       |
| time/                   |             |
|    fps                  | 448         |
|    iterations           | 41          |
|    time_elapsed         | 8986        |
|    total_timesteps      | 4030464     |
| train/                  |             |
|    approx_kl            | 0.005864702 |
|    clip_fraction        | 0.426       |
|    clip_range           | 0.075       |
|    entropy_loss         | -2.12       |
|    explained_variance   | 0.602       |
|    learning_rate        | 0.00134     |
|    loss                 | 0.229       |
|    n_updates            | 360         |
|    policy_gradient_loss | 0.000658    |
|    std                  | 0.492       |
|    value_loss           | 0.331       |
-----------------------------------------
Num timesteps: 4032000
Best mean reward: -6.07 - Last mean reward per episode: -6.40
Num timesteps: 4035000
Best mean reward: -6.07 - Last mean reward per episode: -6.44
Num timesteps: 4038000
Best mean reward: -6.07 - Last mean reward per episode: -6.28
Num timesteps: 4041000
Best mean reward: -6.07 - Last mean reward per episode: -5.90

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_214541_numTimesteps_4041000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 4044000
Best mean reward: -5.90 - Last mean reward per episode: -5.55

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_214548_numTimesteps_4044000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 4047000
Best mean reward: -5.55 - Last mean reward per episode: -6.04
Num timesteps: 4050000
Best mean reward: -5.55 - Last mean reward per episode: -6.07
Num timesteps: 4053000
Best mean reward: -5.55 - Last mean reward per episode: -6.11
Num timesteps: 4056000
Best mean reward: -5.55 - Last mean reward per episode: -6.13
Num timesteps: 4059000
Best mean reward: -5.55 - Last mean reward per episode: -5.42

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_214614_numTimesteps_4059000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 4062000
Best mean reward: -5.42 - Last mean reward per episode: -5.74
Num timesteps: 4065000
Best mean reward: -5.42 - Last mean reward per episode: -6.28
Num timesteps: 4068000
Best mean reward: -5.42 - Last mean reward per episode: -5.85
Num timesteps: 4071000
Best mean reward: -5.42 - Last mean reward per episode: -5.84
Num timesteps: 4074000
Best mean reward: -5.42 - Last mean reward per episode: -5.78
Num timesteps: 4077000
Best mean reward: -5.42 - Last mean reward per episode: -6.39
Num timesteps: 4080000
Best mean reward: -5.42 - Last mean reward per episode: -6.09
Num timesteps: 4083000
Best mean reward: -5.42 - Last mean reward per episode: -5.74
Num timesteps: 4086000
Best mean reward: -5.42 - Last mean reward per episode: -5.82
Num timesteps: 4089000
Best mean reward: -5.42 - Last mean reward per episode: -5.82
Num timesteps: 4092000
Best mean reward: -5.42 - Last mean reward per episode: -5.87
Num timesteps: 4095000
Best mean reward: -5.42 - Last mean reward per episode: -5.71
Num timesteps: 4098000
Best mean reward: -5.42 - Last mean reward per episode: -6.40
Num timesteps: 4101000
Best mean reward: -5.42 - Last mean reward per episode: -6.44
Num timesteps: 4104000
Best mean reward: -5.42 - Last mean reward per episode: -6.34
Num timesteps: 4107000
Best mean reward: -5.42 - Last mean reward per episode: -6.05
Num timesteps: 4110000
Best mean reward: -5.42 - Last mean reward per episode: -5.91
Num timesteps: 4113000
Best mean reward: -5.42 - Last mean reward per episode: -6.07
Num timesteps: 4116000
Best mean reward: -5.42 - Last mean reward per episode: -6.29
Num timesteps: 4119000
Best mean reward: -5.42 - Last mean reward per episode: -5.88
Num timesteps: 4122000
Best mean reward: -5.42 - Last mean reward per episode: -5.67
Num timesteps: 4125000
Best mean reward: -5.42 - Last mean reward per episode: -5.80
Num timesteps: 4128000
Best mean reward: -5.42 - Last mean reward per episode: -6.09
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 50           |
|    ep_rew_mean          | -6.03        |
| time/                   |              |
|    fps                  | 449          |
|    iterations           | 42           |
|    time_elapsed         | 9188         |
|    total_timesteps      | 4128768      |
| train/                  |              |
|    approx_kl            | 0.0062605976 |
|    clip_fraction        | 0.414        |
|    clip_range           | 0.075        |
|    entropy_loss         | -2.05        |
|    explained_variance   | 0.595        |
|    learning_rate        | 0.00134      |
|    loss                 | 0.216        |
|    n_updates            | 369          |
|    policy_gradient_loss | 0.000992     |
|    std                  | 0.479        |
|    value_loss           | 0.304        |
------------------------------------------
Num timesteps: 4131000
Best mean reward: -5.42 - Last mean reward per episode: -5.83
Num timesteps: 4134000
Best mean reward: -5.42 - Last mean reward per episode: -5.57
Num timesteps: 4137000
Best mean reward: -5.42 - Last mean reward per episode: -5.40

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_214858_numTimesteps_4137000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 4140000
Best mean reward: -5.40 - Last mean reward per episode: -5.21

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_214903_numTimesteps_4140000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 4143000
Best mean reward: -5.21 - Last mean reward per episode: -5.37
Num timesteps: 4146000
Best mean reward: -5.21 - Last mean reward per episode: -5.91
Num timesteps: 4149000
Best mean reward: -5.21 - Last mean reward per episode: -5.91
Num timesteps: 4152000
Best mean reward: -5.21 - Last mean reward per episode: -5.36
Num timesteps: 4155000
Best mean reward: -5.21 - Last mean reward per episode: -6.06
Num timesteps: 4158000
Best mean reward: -5.21 - Last mean reward per episode: -5.80
Num timesteps: 4161000
Best mean reward: -5.21 - Last mean reward per episode: -5.32
Num timesteps: 4164000
Best mean reward: -5.21 - Last mean reward per episode: -5.11

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_214941_numTimesteps_4164000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 4167000
Best mean reward: -5.11 - Last mean reward per episode: -5.30
Num timesteps: 4170000
Best mean reward: -5.11 - Last mean reward per episode: -5.38
Num timesteps: 4173000
Best mean reward: -5.11 - Last mean reward per episode: -5.06

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_214956_numTimesteps_4173000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 4176000
Best mean reward: -5.06 - Last mean reward per episode: -4.88

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_215001_numTimesteps_4176000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 4179000
Best mean reward: -4.88 - Last mean reward per episode: -5.34
Num timesteps: 4182000
Best mean reward: -4.88 - Last mean reward per episode: -5.83
Num timesteps: 4185000
Best mean reward: -4.88 - Last mean reward per episode: -5.68
Num timesteps: 4188000
Best mean reward: -4.88 - Last mean reward per episode: -5.48
Num timesteps: 4191000
Best mean reward: -4.88 - Last mean reward per episode: -5.39
Num timesteps: 4194000
Best mean reward: -4.88 - Last mean reward per episode: -5.52
Num timesteps: 4197000
Best mean reward: -4.88 - Last mean reward per episode: -5.61
Num timesteps: 4200000
Best mean reward: -4.88 - Last mean reward per episode: -5.23
Num timesteps: 4203000
Best mean reward: -4.88 - Last mean reward per episode: -5.07
Num timesteps: 4206000
Best mean reward: -4.88 - Last mean reward per episode: -5.54
Num timesteps: 4209000
Best mean reward: -4.88 - Last mean reward per episode: -5.56
Num timesteps: 4212000
Best mean reward: -4.88 - Last mean reward per episode: -5.60
Num timesteps: 4215000
Best mean reward: -4.88 - Last mean reward per episode: -6.02
Num timesteps: 4218000
Best mean reward: -4.88 - Last mean reward per episode: -5.67
Num timesteps: 4221000
Best mean reward: -4.88 - Last mean reward per episode: -5.50
Num timesteps: 4224000
Best mean reward: -4.88 - Last mean reward per episode: -5.42
Num timesteps: 4227000
Best mean reward: -4.88 - Last mean reward per episode: -5.63
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 50          |
|    ep_rew_mean          | -5.63       |
| time/                   |             |
|    fps                  | 451         |
|    iterations           | 43          |
|    time_elapsed         | 9369        |
|    total_timesteps      | 4227072     |
| train/                  |             |
|    approx_kl            | 0.006222083 |
|    clip_fraction        | 0.423       |
|    clip_range           | 0.075       |
|    entropy_loss         | -1.97       |
|    explained_variance   | 0.58        |
|    learning_rate        | 0.00134     |
|    loss                 | 0.183       |
|    n_updates            | 378         |
|    policy_gradient_loss | 0.00144     |
|    std                  | 0.467       |
|    value_loss           | 0.265       |
-----------------------------------------
Num timesteps: 4230000
Best mean reward: -4.88 - Last mean reward per episode: -5.45
Num timesteps: 4233000
Best mean reward: -4.88 - Last mean reward per episode: -4.98
Num timesteps: 4236000
Best mean reward: -4.88 - Last mean reward per episode: -4.76

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_215203_numTimesteps_4236000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 4239000
Best mean reward: -4.76 - Last mean reward per episode: -4.99
Num timesteps: 4242000
Best mean reward: -4.76 - Last mean reward per episode: -4.81
Num timesteps: 4245000
Best mean reward: -4.76 - Last mean reward per episode: -4.85
Num timesteps: 4248000
Best mean reward: -4.76 - Last mean reward per episode: -5.10
Num timesteps: 4251000
Best mean reward: -4.76 - Last mean reward per episode: -5.38
Num timesteps: 4254000
Best mean reward: -4.76 - Last mean reward per episode: -4.66

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_215238_numTimesteps_4254000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 4257000
Best mean reward: -4.66 - Last mean reward per episode: -5.23
Num timesteps: 4260000
Best mean reward: -4.66 - Last mean reward per episode: -5.32
Num timesteps: 4263000
Best mean reward: -4.66 - Last mean reward per episode: -5.20
Num timesteps: 4266000
Best mean reward: -4.66 - Last mean reward per episode: -4.79
Num timesteps: 4269000
Best mean reward: -4.66 - Last mean reward per episode: -5.21
Num timesteps: 4272000
Best mean reward: -4.66 - Last mean reward per episode: -5.02
Num timesteps: 4275000
Best mean reward: -4.66 - Last mean reward per episode: -5.28
Num timesteps: 4278000
Best mean reward: -4.66 - Last mean reward per episode: -5.14
Num timesteps: 4281000
Best mean reward: -4.66 - Last mean reward per episode: -5.03
Num timesteps: 4284000
Best mean reward: -4.66 - Last mean reward per episode: -5.00
Num timesteps: 4287000
Best mean reward: -4.66 - Last mean reward per episode: -5.38
Num timesteps: 4290000
Best mean reward: -4.66 - Last mean reward per episode: -5.39
Num timesteps: 4293000
Best mean reward: -4.66 - Last mean reward per episode: -5.54
Num timesteps: 4296000
Best mean reward: -4.66 - Last mean reward per episode: -5.36
Num timesteps: 4299000
Best mean reward: -4.66 - Last mean reward per episode: -5.13
Num timesteps: 4302000
Best mean reward: -4.66 - Last mean reward per episode: -4.94
Num timesteps: 4305000
Best mean reward: -4.66 - Last mean reward per episode: -5.13
Num timesteps: 4308000
Best mean reward: -4.66 - Last mean reward per episode: -4.80
Num timesteps: 4311000
Best mean reward: -4.66 - Last mean reward per episode: -4.60

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_215428_numTimesteps_4311000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 4314000
Best mean reward: -4.60 - Last mean reward per episode: -4.93
Num timesteps: 4317000
Best mean reward: -4.60 - Last mean reward per episode: -4.96
Num timesteps: 4320000
Best mean reward: -4.60 - Last mean reward per episode: -5.34
Num timesteps: 4323000
Best mean reward: -4.60 - Last mean reward per episode: -5.67
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 50           |
|    ep_rew_mean          | -5.46        |
| time/                   |              |
|    fps                  | 451          |
|    iterations           | 44           |
|    time_elapsed         | 9578         |
|    total_timesteps      | 4325376      |
| train/                  |              |
|    approx_kl            | 0.0068768524 |
|    clip_fraction        | 0.428        |
|    clip_range           | 0.075        |
|    entropy_loss         | -1.89        |
|    explained_variance   | 0.573        |
|    learning_rate        | 0.00134      |
|    loss                 | 0.179        |
|    n_updates            | 387          |
|    policy_gradient_loss | 0.0018       |
|    std                  | 0.455        |
|    value_loss           | 0.246        |
------------------------------------------
Num timesteps: 4326000
Best mean reward: -4.60 - Last mean reward per episode: -5.06
Num timesteps: 4329000
Best mean reward: -4.60 - Last mean reward per episode: -4.90
Num timesteps: 4332000
Best mean reward: -4.60 - Last mean reward per episode: -4.74
Num timesteps: 4335000
Best mean reward: -4.60 - Last mean reward per episode: -4.86
Num timesteps: 4338000
Best mean reward: -4.60 - Last mean reward per episode: -5.12
Num timesteps: 4341000
Best mean reward: -4.60 - Last mean reward per episode: -4.61
Num timesteps: 4344000
Best mean reward: -4.60 - Last mean reward per episode: -4.54

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_215550_numTimesteps_4344000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 4347000
Best mean reward: -4.54 - Last mean reward per episode: -4.36

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_215555_numTimesteps_4347000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 4350000
Best mean reward: -4.36 - Last mean reward per episode: -4.80
Num timesteps: 4353000
Best mean reward: -4.36 - Last mean reward per episode: -4.79
Num timesteps: 4356000
Best mean reward: -4.36 - Last mean reward per episode: -4.71
Num timesteps: 4359000
Best mean reward: -4.36 - Last mean reward per episode: -5.01
Num timesteps: 4362000
Best mean reward: -4.36 - Last mean reward per episode: -4.68
Num timesteps: 4365000
Best mean reward: -4.36 - Last mean reward per episode: -4.67
Num timesteps: 4368000
Best mean reward: -4.36 - Last mean reward per episode: -4.77
Num timesteps: 4371000
Best mean reward: -4.36 - Last mean reward per episode: -5.01
Num timesteps: 4374000
Best mean reward: -4.36 - Last mean reward per episode: -4.82
Num timesteps: 4377000
Best mean reward: -4.36 - Last mean reward per episode: -4.57
Num timesteps: 4380000
Best mean reward: -4.36 - Last mean reward per episode: -4.83
Num timesteps: 4383000
Best mean reward: -4.36 - Last mean reward per episode: -4.94
Num timesteps: 4386000
Best mean reward: -4.36 - Last mean reward per episode: -4.93
Num timesteps: 4389000
Best mean reward: -4.36 - Last mean reward per episode: -4.98
Num timesteps: 4392000
Best mean reward: -4.36 - Last mean reward per episode: -4.77
Num timesteps: 4395000
Best mean reward: -4.36 - Last mean reward per episode: -4.89
Num timesteps: 4398000
Best mean reward: -4.36 - Last mean reward per episode: -5.22
Num timesteps: 4401000
Best mean reward: -4.36 - Last mean reward per episode: -4.85
Num timesteps: 4404000
Best mean reward: -4.36 - Last mean reward per episode: -5.30
Num timesteps: 4407000
Best mean reward: -4.36 - Last mean reward per episode: -5.23
Num timesteps: 4410000
Best mean reward: -4.36 - Last mean reward per episode: -4.84
Num timesteps: 4413000
Best mean reward: -4.36 - Last mean reward per episode: -4.92
Num timesteps: 4416000
Best mean reward: -4.36 - Last mean reward per episode: -5.22
Num timesteps: 4419000
Best mean reward: -4.36 - Last mean reward per episode: -5.12
Num timesteps: 4422000
Best mean reward: -4.36 - Last mean reward per episode: -5.16
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 50          |
|    ep_rew_mean          | -4.93       |
| time/                   |             |
|    fps                  | 452         |
|    iterations           | 45          |
|    time_elapsed         | 9771        |
|    total_timesteps      | 4423680     |
| train/                  |             |
|    approx_kl            | 0.005971665 |
|    clip_fraction        | 0.441       |
|    clip_range           | 0.075       |
|    entropy_loss         | -1.81       |
|    explained_variance   | 0.569       |
|    learning_rate        | 0.00134     |
|    loss                 | 0.192       |
|    n_updates            | 396         |
|    policy_gradient_loss | 0.00272     |
|    std                  | 0.445       |
|    value_loss           | 0.226       |
-----------------------------------------
Num timesteps: 4425000
Best mean reward: -4.36 - Last mean reward per episode: -4.60
Num timesteps: 4428000
Best mean reward: -4.36 - Last mean reward per episode: -4.80
Num timesteps: 4431000
Best mean reward: -4.36 - Last mean reward per episode: -4.86
Num timesteps: 4434000
Best mean reward: -4.36 - Last mean reward per episode: -4.80
Num timesteps: 4437000
Best mean reward: -4.36 - Last mean reward per episode: -5.02
Num timesteps: 4440000
Best mean reward: -4.36 - Last mean reward per episode: -4.79
Num timesteps: 4443000
Best mean reward: -4.36 - Last mean reward per episode: -4.39
Num timesteps: 4446000
Best mean reward: -4.36 - Last mean reward per episode: -4.42
Num timesteps: 4449000
Best mean reward: -4.36 - Last mean reward per episode: -4.68
Num timesteps: 4452000
Best mean reward: -4.36 - Last mean reward per episode: -4.65
Num timesteps: 4455000
Best mean reward: -4.36 - Last mean reward per episode: -4.69
Num timesteps: 4458000
Best mean reward: -4.36 - Last mean reward per episode: -4.28

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_215932_numTimesteps_4458000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 4461000
Best mean reward: -4.28 - Last mean reward per episode: -4.54
Num timesteps: 4464000
Best mean reward: -4.28 - Last mean reward per episode: -4.53
Num timesteps: 4467000
Best mean reward: -4.28 - Last mean reward per episode: -4.10

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_215949_numTimesteps_4467000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 4470000
Best mean reward: -4.10 - Last mean reward per episode: -4.15
Num timesteps: 4473000
Best mean reward: -4.10 - Last mean reward per episode: -4.49
Num timesteps: 4476000
Best mean reward: -4.10 - Last mean reward per episode: -4.34
Num timesteps: 4479000
Best mean reward: -4.10 - Last mean reward per episode: -4.21
Num timesteps: 4482000
Best mean reward: -4.10 - Last mean reward per episode: -4.67
Num timesteps: 4485000
Best mean reward: -4.10 - Last mean reward per episode: -4.48
Num timesteps: 4488000
Best mean reward: -4.10 - Last mean reward per episode: -4.33
Num timesteps: 4491000
Best mean reward: -4.10 - Last mean reward per episode: -4.61
Num timesteps: 4494000
Best mean reward: -4.10 - Last mean reward per episode: -4.87
Num timesteps: 4497000
Best mean reward: -4.10 - Last mean reward per episode: -4.63
Num timesteps: 4500000
Best mean reward: -4.10 - Last mean reward per episode: -4.54
Num timesteps: 4503000
Best mean reward: -4.10 - Last mean reward per episode: -4.71
Num timesteps: 4506000
Best mean reward: -4.10 - Last mean reward per episode: -4.71
Num timesteps: 4509000
Best mean reward: -4.10 - Last mean reward per episode: -4.46
Num timesteps: 4512000
Best mean reward: -4.10 - Last mean reward per episode: -4.54
Num timesteps: 4515000
Best mean reward: -4.10 - Last mean reward per episode: -4.28
Num timesteps: 4518000
Best mean reward: -4.10 - Last mean reward per episode: -4.55
Num timesteps: 4521000
Best mean reward: -4.10 - Last mean reward per episode: -4.82
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 50          |
|    ep_rew_mean          | -4.82       |
| time/                   |             |
|    fps                  | 453         |
|    iterations           | 46          |
|    time_elapsed         | 9976        |
|    total_timesteps      | 4521984     |
| train/                  |             |
|    approx_kl            | 0.006264268 |
|    clip_fraction        | 0.44        |
|    clip_range           | 0.075       |
|    entropy_loss         | -1.74       |
|    explained_variance   | 0.562       |
|    learning_rate        | 0.00134     |
|    loss                 | 0.144       |
|    n_updates            | 405         |
|    policy_gradient_loss | 0.00314     |
|    std                  | 0.435       |
|    value_loss           | 0.213       |
-----------------------------------------
Num timesteps: 4524000
Best mean reward: -4.10 - Last mean reward per episode: -4.29
Num timesteps: 4527000
Best mean reward: -4.10 - Last mean reward per episode: -3.92

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_220201_numTimesteps_4527000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 4530000
Best mean reward: -3.92 - Last mean reward per episode: -3.95
Num timesteps: 4533000
Best mean reward: -3.92 - Last mean reward per episode: -3.81

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_220210_numTimesteps_4533000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 4536000
Best mean reward: -3.81 - Last mean reward per episode: -4.21
Num timesteps: 4539000
Best mean reward: -3.81 - Last mean reward per episode: -4.46
Num timesteps: 4542000
Best mean reward: -3.81 - Last mean reward per episode: -4.24
Num timesteps: 4545000
Best mean reward: -3.81 - Last mean reward per episode: -4.34
Num timesteps: 4548000
Best mean reward: -3.81 - Last mean reward per episode: -4.57
Num timesteps: 4551000
Best mean reward: -3.81 - Last mean reward per episode: -4.22
Num timesteps: 4554000
Best mean reward: -3.81 - Last mean reward per episode: -4.42
Num timesteps: 4557000
Best mean reward: -3.81 - Last mean reward per episode: -4.34
Num timesteps: 4560000
Best mean reward: -3.81 - Last mean reward per episode: -3.96
Num timesteps: 4563000
Best mean reward: -3.81 - Last mean reward per episode: -4.25
Num timesteps: 4566000
Best mean reward: -3.81 - Last mean reward per episode: -4.27
Num timesteps: 4569000
Best mean reward: -3.81 - Last mean reward per episode: -4.42
Num timesteps: 4572000
Best mean reward: -3.81 - Last mean reward per episode: -4.22
Num timesteps: 4575000
Best mean reward: -3.81 - Last mean reward per episode: -4.12
Num timesteps: 4578000
Best mean reward: -3.81 - Last mean reward per episode: -4.06
Num timesteps: 4581000
Best mean reward: -3.81 - Last mean reward per episode: -3.77

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_220334_numTimesteps_4581000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 4584000
Best mean reward: -3.77 - Last mean reward per episode: -4.33
Num timesteps: 4587000
Best mean reward: -3.77 - Last mean reward per episode: -4.19
Num timesteps: 4590000
Best mean reward: -3.77 - Last mean reward per episode: -4.03
Num timesteps: 4593000
Best mean reward: -3.77 - Last mean reward per episode: -4.11
Num timesteps: 4596000
Best mean reward: -3.77 - Last mean reward per episode: -4.46
Num timesteps: 4599000
Best mean reward: -3.77 - Last mean reward per episode: -4.41
Num timesteps: 4602000
Best mean reward: -3.77 - Last mean reward per episode: -4.58
Num timesteps: 4605000
Best mean reward: -3.77 - Last mean reward per episode: -3.90
Num timesteps: 4608000
Best mean reward: -3.77 - Last mean reward per episode: -3.94
Num timesteps: 4611000
Best mean reward: -3.77 - Last mean reward per episode: -3.82
Num timesteps: 4614000
Best mean reward: -3.77 - Last mean reward per episode: -4.12
Num timesteps: 4617000
Best mean reward: -3.77 - Last mean reward per episode: -3.96
Num timesteps: 4620000
Best mean reward: -3.77 - Last mean reward per episode: -3.87
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 50           |
|    ep_rew_mean          | -3.92        |
| time/                   |              |
|    fps                  | 454          |
|    iterations           | 47           |
|    time_elapsed         | 10168        |
|    total_timesteps      | 4620288      |
| train/                  |              |
|    approx_kl            | 0.0061938143 |
|    clip_fraction        | 0.443        |
|    clip_range           | 0.075        |
|    entropy_loss         | -1.68        |
|    explained_variance   | 0.565        |
|    learning_rate        | 0.00134      |
|    loss                 | 0.131        |
|    n_updates            | 414          |
|    policy_gradient_loss | 0.00326      |
|    std                  | 0.425        |
|    value_loss           | 0.191        |
------------------------------------------
Num timesteps: 4623000
Best mean reward: -3.77 - Last mean reward per episode: -4.08
Num timesteps: 4626000
Best mean reward: -3.77 - Last mean reward per episode: -3.87
Num timesteps: 4629000
Best mean reward: -3.77 - Last mean reward per episode: -3.78
Num timesteps: 4632000
Best mean reward: -3.77 - Last mean reward per episode: -3.95
Num timesteps: 4635000
Best mean reward: -3.77 - Last mean reward per episode: -3.74

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_220532_numTimesteps_4635000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 4638000
Best mean reward: -3.74 - Last mean reward per episode: -3.98
Num timesteps: 4641000
Best mean reward: -3.74 - Last mean reward per episode: -4.16
Num timesteps: 4644000
Best mean reward: -3.74 - Last mean reward per episode: -4.22
Num timesteps: 4647000
Best mean reward: -3.74 - Last mean reward per episode: -3.86
Num timesteps: 4650000
Best mean reward: -3.74 - Last mean reward per episode: -3.81
Num timesteps: 4653000
Best mean reward: -3.74 - Last mean reward per episode: -3.97
Num timesteps: 4656000
Best mean reward: -3.74 - Last mean reward per episode: -3.75
Num timesteps: 4659000
Best mean reward: -3.74 - Last mean reward per episode: -4.07
Num timesteps: 4662000
Best mean reward: -3.74 - Last mean reward per episode: -4.32
Num timesteps: 4665000
Best mean reward: -3.74 - Last mean reward per episode: -4.09
Num timesteps: 4668000
Best mean reward: -3.74 - Last mean reward per episode: -4.05
Num timesteps: 4671000
Best mean reward: -3.74 - Last mean reward per episode: -4.08
Num timesteps: 4674000
Best mean reward: -3.74 - Last mean reward per episode: -3.99
Num timesteps: 4677000
Best mean reward: -3.74 - Last mean reward per episode: -3.96
Num timesteps: 4680000
Best mean reward: -3.74 - Last mean reward per episode: -3.97
Num timesteps: 4683000
Best mean reward: -3.74 - Last mean reward per episode: -3.70

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_220702_numTimesteps_4683000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 4686000
Best mean reward: -3.70 - Last mean reward per episode: -3.71
Num timesteps: 4689000
Best mean reward: -3.70 - Last mean reward per episode: -4.28
Num timesteps: 4692000
Best mean reward: -3.70 - Last mean reward per episode: -3.96
Num timesteps: 4695000
Best mean reward: -3.70 - Last mean reward per episode: -3.62

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_220726_numTimesteps_4695000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 4698000
Best mean reward: -3.62 - Last mean reward per episode: -3.70
Num timesteps: 4701000
Best mean reward: -3.62 - Last mean reward per episode: -3.85
Num timesteps: 4704000
Best mean reward: -3.62 - Last mean reward per episode: -4.05
Num timesteps: 4707000
Best mean reward: -3.62 - Last mean reward per episode: -4.27
Num timesteps: 4710000
Best mean reward: -3.62 - Last mean reward per episode: -4.44
Num timesteps: 4713000
Best mean reward: -3.62 - Last mean reward per episode: -4.26
Num timesteps: 4716000
Best mean reward: -3.62 - Last mean reward per episode: -3.97
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 50           |
|    ep_rew_mean          | -4.22        |
| time/                   |              |
|    fps                  | 454          |
|    iterations           | 48           |
|    time_elapsed         | 10376        |
|    total_timesteps      | 4718592      |
| train/                  |              |
|    approx_kl            | 0.0067348466 |
|    clip_fraction        | 0.444        |
|    clip_range           | 0.075        |
|    entropy_loss         | -1.61        |
|    explained_variance   | 0.56         |
|    learning_rate        | 0.00134      |
|    loss                 | 0.141        |
|    n_updates            | 423          |
|    policy_gradient_loss | 0.00341      |
|    std                  | 0.416        |
|    value_loss           | 0.165        |
------------------------------------------
Num timesteps: 4719000
Best mean reward: -3.62 - Last mean reward per episode: -4.19
Num timesteps: 4722000
Best mean reward: -3.62 - Last mean reward per episode: -4.15
Num timesteps: 4725000
Best mean reward: -3.62 - Last mean reward per episode: -4.14
Num timesteps: 4728000
Best mean reward: -3.62 - Last mean reward per episode: -3.81
Num timesteps: 4731000
Best mean reward: -3.62 - Last mean reward per episode: -3.79
Num timesteps: 4734000
Best mean reward: -3.62 - Last mean reward per episode: -3.76
Num timesteps: 4737000
Best mean reward: -3.62 - Last mean reward per episode: -3.72
Num timesteps: 4740000
Best mean reward: -3.62 - Last mean reward per episode: -3.69
Num timesteps: 4743000
Best mean reward: -3.62 - Last mean reward per episode: -3.83
Num timesteps: 4746000
Best mean reward: -3.62 - Last mean reward per episode: -3.79
Num timesteps: 4749000
Best mean reward: -3.62 - Last mean reward per episode: -3.63
Num timesteps: 4752000
Best mean reward: -3.62 - Last mean reward per episode: -3.64
Num timesteps: 4755000
Best mean reward: -3.62 - Last mean reward per episode: -3.55

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_220938_numTimesteps_4755000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 4758000
Best mean reward: -3.55 - Last mean reward per episode: -3.70
Num timesteps: 4761000
Best mean reward: -3.55 - Last mean reward per episode: -3.50

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_220949_numTimesteps_4761000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 4764000
Best mean reward: -3.50 - Last mean reward per episode: -3.64
Num timesteps: 4767000
Best mean reward: -3.50 - Last mean reward per episode: -3.85
Num timesteps: 4770000
Best mean reward: -3.50 - Last mean reward per episode: -3.64
Num timesteps: 4773000
Best mean reward: -3.50 - Last mean reward per episode: -3.74
Num timesteps: 4776000
Best mean reward: -3.50 - Last mean reward per episode: -3.68
Num timesteps: 4779000
Best mean reward: -3.50 - Last mean reward per episode: -3.66
Num timesteps: 4782000
Best mean reward: -3.50 - Last mean reward per episode: -3.36

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_221026_numTimesteps_4782000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 4785000
Best mean reward: -3.36 - Last mean reward per episode: -3.74
Num timesteps: 4788000
Best mean reward: -3.36 - Last mean reward per episode: -3.77
Num timesteps: 4791000
Best mean reward: -3.36 - Last mean reward per episode: -3.69
Num timesteps: 4794000
Best mean reward: -3.36 - Last mean reward per episode: -3.87
Num timesteps: 4797000
Best mean reward: -3.36 - Last mean reward per episode: -3.76
Num timesteps: 4800000
Best mean reward: -3.36 - Last mean reward per episode: -3.72
Num timesteps: 4803000
Best mean reward: -3.36 - Last mean reward per episode: -3.87
Num timesteps: 4806000
Best mean reward: -3.36 - Last mean reward per episode: -3.57
Num timesteps: 4809000
Best mean reward: -3.36 - Last mean reward per episode: -3.61
Num timesteps: 4812000
Best mean reward: -3.36 - Last mean reward per episode: -3.80
Num timesteps: 4815000
Best mean reward: -3.36 - Last mean reward per episode: -3.84
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 50           |
|    ep_rew_mean          | -3.76        |
| time/                   |              |
|    fps                  | 455          |
|    iterations           | 49           |
|    time_elapsed         | 10566        |
|    total_timesteps      | 4816896      |
| train/                  |              |
|    approx_kl            | 0.0072469735 |
|    clip_fraction        | 0.452        |
|    clip_range           | 0.075        |
|    entropy_loss         | -1.54        |
|    explained_variance   | 0.548        |
|    learning_rate        | 0.00134      |
|    loss                 | 0.128        |
|    n_updates            | 432          |
|    policy_gradient_loss | 0.00399      |
|    std                  | 0.407        |
|    value_loss           | 0.162        |
------------------------------------------
Num timesteps: 4818000
Best mean reward: -3.36 - Last mean reward per episode: -3.77
Num timesteps: 4821000
Best mean reward: -3.36 - Last mean reward per episode: -3.61
Num timesteps: 4824000
Best mean reward: -3.36 - Last mean reward per episode: -3.56
Num timesteps: 4827000
Best mean reward: -3.36 - Last mean reward per episode: -3.55
Num timesteps: 4830000
Best mean reward: -3.36 - Last mean reward per episode: -3.21

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_221205_numTimesteps_4830000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 4833000
Best mean reward: -3.21 - Last mean reward per episode: -3.32
Num timesteps: 4836000
Best mean reward: -3.21 - Last mean reward per episode: -3.44
Num timesteps: 4839000
Best mean reward: -3.21 - Last mean reward per episode: -3.46
Num timesteps: 4842000
Best mean reward: -3.21 - Last mean reward per episode: -3.71
Num timesteps: 4845000
Best mean reward: -3.21 - Last mean reward per episode: -3.64
Num timesteps: 4848000
Best mean reward: -3.21 - Last mean reward per episode: -3.74
Num timesteps: 4851000
Best mean reward: -3.21 - Last mean reward per episode: -3.64
Num timesteps: 4854000
Best mean reward: -3.21 - Last mean reward per episode: -3.65
Num timesteps: 4857000
Best mean reward: -3.21 - Last mean reward per episode: -3.76
Num timesteps: 4860000
Best mean reward: -3.21 - Last mean reward per episode: -3.87
Num timesteps: 4863000
Best mean reward: -3.21 - Last mean reward per episode: -3.75
Num timesteps: 4866000
Best mean reward: -3.21 - Last mean reward per episode: -3.55
Num timesteps: 4869000
Best mean reward: -3.21 - Last mean reward per episode: -3.50
Num timesteps: 4872000
Best mean reward: -3.21 - Last mean reward per episode: -3.27
Num timesteps: 4875000
Best mean reward: -3.21 - Last mean reward per episode: -3.17

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_221321_numTimesteps_4875000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 4878000
Best mean reward: -3.17 - Last mean reward per episode: -3.58
Num timesteps: 4881000
Best mean reward: -3.17 - Last mean reward per episode: -3.73
Num timesteps: 4884000
Best mean reward: -3.17 - Last mean reward per episode: -3.48
Num timesteps: 4887000
Best mean reward: -3.17 - Last mean reward per episode: -3.26
Num timesteps: 4890000
Best mean reward: -3.17 - Last mean reward per episode: -3.26
Num timesteps: 4893000
Best mean reward: -3.17 - Last mean reward per episode: -3.29
Num timesteps: 4896000
Best mean reward: -3.17 - Last mean reward per episode: -3.36
Num timesteps: 4899000
Best mean reward: -3.17 - Last mean reward per episode: -3.50
Num timesteps: 4902000
Best mean reward: -3.17 - Last mean reward per episode: -3.22
Num timesteps: 4905000
Best mean reward: -3.17 - Last mean reward per episode: -3.60
Num timesteps: 4908000
Best mean reward: -3.17 - Last mean reward per episode: -3.53
Num timesteps: 4911000
Best mean reward: -3.17 - Last mean reward per episode: -3.40
Num timesteps: 4914000
Best mean reward: -3.17 - Last mean reward per episode: -3.43
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 50          |
|    ep_rew_mean          | -3.38       |
| time/                   |             |
|    fps                  | 456         |
|    iterations           | 50          |
|    time_elapsed         | 10758       |
|    total_timesteps      | 4915200     |
| train/                  |             |
|    approx_kl            | 0.009364768 |
|    clip_fraction        | 0.469       |
|    clip_range           | 0.075       |
|    entropy_loss         | -1.47       |
|    explained_variance   | 0.56        |
|    learning_rate        | 0.00134     |
|    loss                 | 0.103       |
|    n_updates            | 441         |
|    policy_gradient_loss | 0.00507     |
|    std                  | 0.397       |
|    value_loss           | 0.136       |
-----------------------------------------
Num timesteps: 4917000
Best mean reward: -3.17 - Last mean reward per episode: -3.30
Num timesteps: 4920000
Best mean reward: -3.17 - Last mean reward per episode: -3.24
Num timesteps: 4923000
Best mean reward: -3.17 - Last mean reward per episode: -3.05

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_221503_numTimesteps_4923000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 4926000
Best mean reward: -3.05 - Last mean reward per episode: -2.97

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_221508_numTimesteps_4926000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 4929000
Best mean reward: -2.97 - Last mean reward per episode: -3.29
Num timesteps: 4932000
Best mean reward: -2.97 - Last mean reward per episode: -3.48
Num timesteps: 4935000
Best mean reward: -2.97 - Last mean reward per episode: -3.48
Num timesteps: 4938000
Best mean reward: -2.97 - Last mean reward per episode: -3.58
Num timesteps: 4941000
Best mean reward: -2.97 - Last mean reward per episode: -3.20
Num timesteps: 4944000
Best mean reward: -2.97 - Last mean reward per episode: -3.19
Num timesteps: 4947000
Best mean reward: -2.97 - Last mean reward per episode: -3.02
Num timesteps: 4950000
Best mean reward: -2.97 - Last mean reward per episode: -3.20
Num timesteps: 4953000
Best mean reward: -2.97 - Last mean reward per episode: -3.02
Num timesteps: 4956000
Best mean reward: -2.97 - Last mean reward per episode: -3.45
Num timesteps: 4959000
Best mean reward: -2.97 - Last mean reward per episode: -3.55
Num timesteps: 4962000
Best mean reward: -2.97 - Last mean reward per episode: -3.62
Num timesteps: 4965000
Best mean reward: -2.97 - Last mean reward per episode: -3.27
Num timesteps: 4968000
Best mean reward: -2.97 - Last mean reward per episode: -3.15
Num timesteps: 4971000
Best mean reward: -2.97 - Last mean reward per episode: -3.13
Num timesteps: 4974000
Best mean reward: -2.97 - Last mean reward per episode: -3.06
Num timesteps: 4977000
Best mean reward: -2.97 - Last mean reward per episode: -3.23
Num timesteps: 4980000
Best mean reward: -2.97 - Last mean reward per episode: -3.05
Num timesteps: 4983000
Best mean reward: -2.97 - Last mean reward per episode: -2.99
Num timesteps: 4986000
Best mean reward: -2.97 - Last mean reward per episode: -3.20
Num timesteps: 4989000
Best mean reward: -2.97 - Last mean reward per episode: -3.15
Num timesteps: 4992000
Best mean reward: -2.97 - Last mean reward per episode: -3.51
Num timesteps: 4995000
Best mean reward: -2.97 - Last mean reward per episode: -3.42
Num timesteps: 4998000
Best mean reward: -2.97 - Last mean reward per episode: -3.33
Num timesteps: 5001000
Best mean reward: -2.97 - Last mean reward per episode: -3.07
Num timesteps: 5004000
Best mean reward: -2.97 - Last mean reward per episode: -2.76

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_221719_numTimesteps_5004000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 5007000
Best mean reward: -2.76 - Last mean reward per episode: -2.92
Num timesteps: 5010000
Best mean reward: -2.76 - Last mean reward per episode: -3.32
Num timesteps: 5013000
Best mean reward: -2.76 - Last mean reward per episode: -3.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 50        |
|    ep_rew_mean          | -3.32     |
| time/                   |           |
|    fps                  | 458       |
|    iterations           | 51        |
|    time_elapsed         | 10943     |
|    total_timesteps      | 5013504   |
| train/                  |           |
|    approx_kl            | 0.0078046 |
|    clip_fraction        | 0.473     |
|    clip_range           | 0.075     |
|    entropy_loss         | -1.4      |
|    explained_variance   | 0.559     |
|    learning_rate        | 0.00134   |
|    loss                 | 0.0956    |
|    n_updates            | 450       |
|    policy_gradient_loss | 0.00529   |
|    std                  | 0.389     |
|    value_loss           | 0.119     |
---------------------------------------
Num timesteps: 5016000
Best mean reward: -2.76 - Last mean reward per episode: -3.09
Num timesteps: 5019000
Best mean reward: -2.76 - Last mean reward per episode: -2.95
Num timesteps: 5022000
Best mean reward: -2.76 - Last mean reward per episode: -3.17
Num timesteps: 5025000
Best mean reward: -2.76 - Last mean reward per episode: -3.02
Num timesteps: 5028000
Best mean reward: -2.76 - Last mean reward per episode: -3.03
Num timesteps: 5031000
Best mean reward: -2.76 - Last mean reward per episode: -3.22
Num timesteps: 5034000
Best mean reward: -2.76 - Last mean reward per episode: -2.98
Num timesteps: 5037000
Best mean reward: -2.76 - Last mean reward per episode: -3.22
Num timesteps: 5040000
Best mean reward: -2.76 - Last mean reward per episode: -3.20
Num timesteps: 5043000
Best mean reward: -2.76 - Last mean reward per episode: -2.80
Num timesteps: 5046000
Best mean reward: -2.76 - Last mean reward per episode: -2.80
Num timesteps: 5049000
Best mean reward: -2.76 - Last mean reward per episode: -2.99
Num timesteps: 5052000
Best mean reward: -2.76 - Last mean reward per episode: -3.11
Num timesteps: 5055000
Best mean reward: -2.76 - Last mean reward per episode: -3.02
Num timesteps: 5058000
Best mean reward: -2.76 - Last mean reward per episode: -2.98
Num timesteps: 5061000
Best mean reward: -2.76 - Last mean reward per episode: -3.33
Num timesteps: 5064000
Best mean reward: -2.76 - Last mean reward per episode: -3.23
Num timesteps: 5067000
Best mean reward: -2.76 - Last mean reward per episode: -2.95
Num timesteps: 5070000
Best mean reward: -2.76 - Last mean reward per episode: -3.06
Num timesteps: 5073000
Best mean reward: -2.76 - Last mean reward per episode: -3.15
Num timesteps: 5076000
Best mean reward: -2.76 - Last mean reward per episode: -3.07
Num timesteps: 5079000
Best mean reward: -2.76 - Last mean reward per episode: -2.89
Num timesteps: 5082000
Best mean reward: -2.76 - Last mean reward per episode: -2.71

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_221956_numTimesteps_5082000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 5085000
Best mean reward: -2.71 - Last mean reward per episode: -2.78
Num timesteps: 5088000
Best mean reward: -2.71 - Last mean reward per episode: -2.92
Num timesteps: 5091000
Best mean reward: -2.71 - Last mean reward per episode: -3.15
Num timesteps: 5094000
Best mean reward: -2.71 - Last mean reward per episode: -3.30
Num timesteps: 5097000
Best mean reward: -2.71 - Last mean reward per episode: -2.97
Num timesteps: 5100000
Best mean reward: -2.71 - Last mean reward per episode: -3.15
Num timesteps: 5103000
Best mean reward: -2.71 - Last mean reward per episode: -3.09
Num timesteps: 5106000
Best mean reward: -2.71 - Last mean reward per episode: -2.99
Num timesteps: 5109000
Best mean reward: -2.71 - Last mean reward per episode: -2.79
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 50          |
|    ep_rew_mean          | -2.88       |
| time/                   |             |
|    fps                  | 459         |
|    iterations           | 52          |
|    time_elapsed         | 11128       |
|    total_timesteps      | 5111808     |
| train/                  |             |
|    approx_kl            | 0.009229604 |
|    clip_fraction        | 0.475       |
|    clip_range           | 0.075       |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.595       |
|    learning_rate        | 0.00134     |
|    loss                 | 0.0747      |
|    n_updates            | 459         |
|    policy_gradient_loss | 0.00558     |
|    std                  | 0.38        |
|    value_loss           | 0.0977      |
-----------------------------------------
Num timesteps: 5112000
Best mean reward: -2.71 - Last mean reward per episode: -2.99
Num timesteps: 5115000
Best mean reward: -2.71 - Last mean reward per episode: -2.83
Num timesteps: 5118000
Best mean reward: -2.71 - Last mean reward per episode: -2.90
Num timesteps: 5121000
Best mean reward: -2.71 - Last mean reward per episode: -2.88
Num timesteps: 5124000
Best mean reward: -2.71 - Last mean reward per episode: -2.95
Num timesteps: 5127000
Best mean reward: -2.71 - Last mean reward per episode: -2.71
Num timesteps: 5130000
Best mean reward: -2.71 - Last mean reward per episode: -2.85
Num timesteps: 5133000
Best mean reward: -2.71 - Last mean reward per episode: -3.06
Num timesteps: 5136000
Best mean reward: -2.71 - Last mean reward per episode: -3.13
Num timesteps: 5139000
Best mean reward: -2.71 - Last mean reward per episode: -3.10
Num timesteps: 5142000
Best mean reward: -2.71 - Last mean reward per episode: -2.74
Num timesteps: 5145000
Best mean reward: -2.71 - Last mean reward per episode: -2.96
Num timesteps: 5148000
Best mean reward: -2.71 - Last mean reward per episode: -2.76
Num timesteps: 5151000
Best mean reward: -2.71 - Last mean reward per episode: -2.81
Num timesteps: 5154000
Best mean reward: -2.71 - Last mean reward per episode: -3.10
Num timesteps: 5157000
Best mean reward: -2.71 - Last mean reward per episode: -3.11
Num timesteps: 5160000
Best mean reward: -2.71 - Last mean reward per episode: -2.90
Num timesteps: 5163000
Best mean reward: -2.71 - Last mean reward per episode: -2.84
Num timesteps: 5166000
Best mean reward: -2.71 - Last mean reward per episode: -2.91
Num timesteps: 5169000
Best mean reward: -2.71 - Last mean reward per episode: -2.84
Num timesteps: 5172000
Best mean reward: -2.71 - Last mean reward per episode: -3.06
Num timesteps: 5175000
Best mean reward: -2.71 - Last mean reward per episode: -2.83
Num timesteps: 5178000
Best mean reward: -2.71 - Last mean reward per episode: -2.87
Num timesteps: 5181000
Best mean reward: -2.71 - Last mean reward per episode: -2.77
Num timesteps: 5184000
Best mean reward: -2.71 - Last mean reward per episode: -2.74
Num timesteps: 5187000
Best mean reward: -2.71 - Last mean reward per episode: -2.76
Num timesteps: 5190000
Best mean reward: -2.71 - Last mean reward per episode: -2.79
Num timesteps: 5193000
Best mean reward: -2.71 - Last mean reward per episode: -2.73
Num timesteps: 5196000
Best mean reward: -2.71 - Last mean reward per episode: -2.90
Num timesteps: 5199000
Best mean reward: -2.71 - Last mean reward per episode: -3.13
Num timesteps: 5202000
Best mean reward: -2.71 - Last mean reward per episode: -3.05
Num timesteps: 5205000
Best mean reward: -2.71 - Last mean reward per episode: -3.00
Num timesteps: 5208000
Best mean reward: -2.71 - Last mean reward per episode: -2.96
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 50          |
|    ep_rew_mean          | -3.01       |
| time/                   |             |
|    fps                  | 460         |
|    iterations           | 53          |
|    time_elapsed         | 11318       |
|    total_timesteps      | 5210112     |
| train/                  |             |
|    approx_kl            | 0.009561452 |
|    clip_fraction        | 0.498       |
|    clip_range           | 0.075       |
|    entropy_loss         | -1.26       |
|    explained_variance   | 0.59        |
|    learning_rate        | 0.00134     |
|    loss                 | 0.0789      |
|    n_updates            | 468         |
|    policy_gradient_loss | 0.00635     |
|    std                  | 0.372       |
|    value_loss           | 0.0914      |
-----------------------------------------
Num timesteps: 5211000
Best mean reward: -2.71 - Last mean reward per episode: -2.98
Num timesteps: 5214000
Best mean reward: -2.71 - Last mean reward per episode: -3.01
Num timesteps: 5217000
Best mean reward: -2.71 - Last mean reward per episode: -2.76
Num timesteps: 5220000
Best mean reward: -2.71 - Last mean reward per episode: -2.71
Num timesteps: 5223000
Best mean reward: -2.71 - Last mean reward per episode: -2.67

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_222432_numTimesteps_5223000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 5226000
Best mean reward: -2.67 - Last mean reward per episode: -2.54

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_222437_numTimesteps_5226000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 5229000
Best mean reward: -2.54 - Last mean reward per episode: -2.74
Num timesteps: 5232000
Best mean reward: -2.54 - Last mean reward per episode: -2.67
Num timesteps: 5235000
Best mean reward: -2.54 - Last mean reward per episode: -2.71
Num timesteps: 5238000
Best mean reward: -2.54 - Last mean reward per episode: -2.88
Num timesteps: 5241000
Best mean reward: -2.54 - Last mean reward per episode: -3.08
Num timesteps: 5244000
Best mean reward: -2.54 - Last mean reward per episode: -2.89
Num timesteps: 5247000
Best mean reward: -2.54 - Last mean reward per episode: -2.70
Num timesteps: 5250000
Best mean reward: -2.54 - Last mean reward per episode: -2.85
Num timesteps: 5253000
Best mean reward: -2.54 - Last mean reward per episode: -2.77
Num timesteps: 5256000
Best mean reward: -2.54 - Last mean reward per episode: -2.66
Num timesteps: 5259000
Best mean reward: -2.54 - Last mean reward per episode: -2.39

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_222532_numTimesteps_5259000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 5262000
Best mean reward: -2.39 - Last mean reward per episode: -2.46
Num timesteps: 5265000
Best mean reward: -2.39 - Last mean reward per episode: -2.60
Num timesteps: 5268000
Best mean reward: -2.39 - Last mean reward per episode: -2.84
Num timesteps: 5271000
Best mean reward: -2.39 - Last mean reward per episode: -2.83
Num timesteps: 5274000
Best mean reward: -2.39 - Last mean reward per episode: -2.82
Num timesteps: 5277000
Best mean reward: -2.39 - Last mean reward per episode: -2.75
Num timesteps: 5280000
Best mean reward: -2.39 - Last mean reward per episode: -2.68
Num timesteps: 5283000
Best mean reward: -2.39 - Last mean reward per episode: -2.93
Num timesteps: 5286000
Best mean reward: -2.39 - Last mean reward per episode: -2.84
Num timesteps: 5289000
Best mean reward: -2.39 - Last mean reward per episode: -2.50
Num timesteps: 5292000
Best mean reward: -2.39 - Last mean reward per episode: -2.51
Num timesteps: 5295000
Best mean reward: -2.39 - Last mean reward per episode: -2.46
Num timesteps: 5298000
Best mean reward: -2.39 - Last mean reward per episode: -2.47
Num timesteps: 5301000
Best mean reward: -2.39 - Last mean reward per episode: -2.71
Num timesteps: 5304000
Best mean reward: -2.39 - Last mean reward per episode: -2.81
Num timesteps: 5307000
Best mean reward: -2.39 - Last mean reward per episode: -2.86
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 50          |
|    ep_rew_mean          | -2.89       |
| time/                   |             |
|    fps                  | 460         |
|    iterations           | 54          |
|    time_elapsed         | 11522       |
|    total_timesteps      | 5308416     |
| train/                  |             |
|    approx_kl            | 0.010264213 |
|    clip_fraction        | 0.508       |
|    clip_range           | 0.075       |
|    entropy_loss         | -1.2        |
|    explained_variance   | 0.619       |
|    learning_rate        | 0.00134     |
|    loss                 | 0.0463      |
|    n_updates            | 477         |
|    policy_gradient_loss | 0.00692     |
|    std                  | 0.363       |
|    value_loss           | 0.0797      |
-----------------------------------------
Num timesteps: 5310000
Best mean reward: -2.39 - Last mean reward per episode: -2.76
Num timesteps: 5313000
Best mean reward: -2.39 - Last mean reward per episode: -2.61
Num timesteps: 5316000
Best mean reward: -2.39 - Last mean reward per episode: -2.40
Num timesteps: 5319000
Best mean reward: -2.39 - Last mean reward per episode: -2.29

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_222758_numTimesteps_5319000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 5322000
Best mean reward: -2.29 - Last mean reward per episode: -2.42
Num timesteps: 5325000
Best mean reward: -2.29 - Last mean reward per episode: -2.48
Num timesteps: 5328000
Best mean reward: -2.29 - Last mean reward per episode: -2.54
Num timesteps: 5331000
Best mean reward: -2.29 - Last mean reward per episode: -2.54
Num timesteps: 5334000
Best mean reward: -2.29 - Last mean reward per episode: -2.48
Num timesteps: 5337000
Best mean reward: -2.29 - Last mean reward per episode: -2.41
Num timesteps: 5340000
Best mean reward: -2.29 - Last mean reward per episode: -2.63
Num timesteps: 5343000
Best mean reward: -2.29 - Last mean reward per episode: -2.71
Num timesteps: 5346000
Best mean reward: -2.29 - Last mean reward per episode: -2.89
Num timesteps: 5349000
Best mean reward: -2.29 - Last mean reward per episode: -2.75
Num timesteps: 5352000
Best mean reward: -2.29 - Last mean reward per episode: -2.77
Num timesteps: 5355000
Best mean reward: -2.29 - Last mean reward per episode: -2.61
Num timesteps: 5358000
Best mean reward: -2.29 - Last mean reward per episode: -2.60
Num timesteps: 5361000
Best mean reward: -2.29 - Last mean reward per episode: -2.53
Num timesteps: 5364000
Best mean reward: -2.29 - Last mean reward per episode: -2.55
Num timesteps: 5367000
Best mean reward: -2.29 - Last mean reward per episode: -2.66
Num timesteps: 5370000
Best mean reward: -2.29 - Last mean reward per episode: -2.52
Num timesteps: 5373000
Best mean reward: -2.29 - Last mean reward per episode: -2.48
Num timesteps: 5376000
Best mean reward: -2.29 - Last mean reward per episode: -2.54
Num timesteps: 5379000
Best mean reward: -2.29 - Last mean reward per episode: -2.70
Num timesteps: 5382000
Best mean reward: -2.29 - Last mean reward per episode: -2.41
Num timesteps: 5385000
Best mean reward: -2.29 - Last mean reward per episode: -2.61
Num timesteps: 5388000
Best mean reward: -2.29 - Last mean reward per episode: -2.52
Num timesteps: 5391000
Best mean reward: -2.29 - Last mean reward per episode: -2.57
Num timesteps: 5394000
Best mean reward: -2.29 - Last mean reward per episode: -2.64
Num timesteps: 5397000
Best mean reward: -2.29 - Last mean reward per episode: -2.68
Num timesteps: 5400000
Best mean reward: -2.29 - Last mean reward per episode: -2.55
Num timesteps: 5403000
Best mean reward: -2.29 - Last mean reward per episode: -2.40
Num timesteps: 5406000
Best mean reward: -2.29 - Last mean reward per episode: -2.74
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 50          |
|    ep_rew_mean          | -2.64       |
| time/                   |             |
|    fps                  | 461         |
|    iterations           | 55          |
|    time_elapsed         | 11723       |
|    total_timesteps      | 5406720     |
| train/                  |             |
|    approx_kl            | 0.009655253 |
|    clip_fraction        | 0.517       |
|    clip_range           | 0.075       |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.626       |
|    learning_rate        | 0.00134     |
|    loss                 | 0.038       |
|    n_updates            | 486         |
|    policy_gradient_loss | 0.00754     |
|    std                  | 0.355       |
|    value_loss           | 0.0693      |
-----------------------------------------
Num timesteps: 5409000
Best mean reward: -2.29 - Last mean reward per episode: -2.35
Num timesteps: 5412000
Best mean reward: -2.29 - Last mean reward per episode: -2.36
Num timesteps: 5415000
Best mean reward: -2.29 - Last mean reward per episode: -2.40
Num timesteps: 5418000
Best mean reward: -2.29 - Last mean reward per episode: -2.48
Num timesteps: 5421000
Best mean reward: -2.29 - Last mean reward per episode: -2.49
Num timesteps: 5424000
Best mean reward: -2.29 - Last mean reward per episode: -2.53
Num timesteps: 5427000
Best mean reward: -2.29 - Last mean reward per episode: -2.56
Num timesteps: 5430000
Best mean reward: -2.29 - Last mean reward per episode: -2.39
Num timesteps: 5433000
Best mean reward: -2.29 - Last mean reward per episode: -2.42
Num timesteps: 5436000
Best mean reward: -2.29 - Last mean reward per episode: -2.40
Num timesteps: 5439000
Best mean reward: -2.29 - Last mean reward per episode: -2.51
Num timesteps: 5442000
Best mean reward: -2.29 - Last mean reward per episode: -2.58
Num timesteps: 5445000
Best mean reward: -2.29 - Last mean reward per episode: -2.51
Num timesteps: 5448000
Best mean reward: -2.29 - Last mean reward per episode: -2.40
Num timesteps: 5451000
Best mean reward: -2.29 - Last mean reward per episode: -2.36
Num timesteps: 5454000
Best mean reward: -2.29 - Last mean reward per episode: -2.37
Num timesteps: 5457000
Best mean reward: -2.29 - Last mean reward per episode: -2.50
Num timesteps: 5460000
Best mean reward: -2.29 - Last mean reward per episode: -2.37
Num timesteps: 5463000
Best mean reward: -2.29 - Last mean reward per episode: -2.39
Num timesteps: 5466000
Best mean reward: -2.29 - Last mean reward per episode: -2.54
Num timesteps: 5469000
Best mean reward: -2.29 - Last mean reward per episode: -2.53
Num timesteps: 5472000
Best mean reward: -2.29 - Last mean reward per episode: -2.16

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_223247_numTimesteps_5472000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 5475000
Best mean reward: -2.16 - Last mean reward per episode: -2.41
Num timesteps: 5478000
Best mean reward: -2.16 - Last mean reward per episode: -2.72
Num timesteps: 5481000
Best mean reward: -2.16 - Last mean reward per episode: -2.72
Num timesteps: 5484000
Best mean reward: -2.16 - Last mean reward per episode: -2.77
Num timesteps: 5487000
Best mean reward: -2.16 - Last mean reward per episode: -2.44
Num timesteps: 5490000
Best mean reward: -2.16 - Last mean reward per episode: -2.36
Num timesteps: 5493000
Best mean reward: -2.16 - Last mean reward per episode: -2.48
Num timesteps: 5496000
Best mean reward: -2.16 - Last mean reward per episode: -2.59
Num timesteps: 5499000
Best mean reward: -2.16 - Last mean reward per episode: -2.51
Num timesteps: 5502000
Best mean reward: -2.16 - Last mean reward per episode: -2.57
Num timesteps: 5505000
Best mean reward: -2.16 - Last mean reward per episode: -2.41
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 50         |
|    ep_rew_mean          | -2.41      |
| time/                   |            |
|    fps                  | 462        |
|    iterations           | 56         |
|    time_elapsed         | 11911      |
|    total_timesteps      | 5505024    |
| train/                  |            |
|    approx_kl            | 0.01444756 |
|    clip_fraction        | 0.543      |
|    clip_range           | 0.075      |
|    entropy_loss         | -1.06      |
|    explained_variance   | 0.646      |
|    learning_rate        | 0.00134    |
|    loss                 | 0.0483     |
|    n_updates            | 495        |
|    policy_gradient_loss | 0.00891    |
|    std                  | 0.348      |
|    value_loss           | 0.0583     |
----------------------------------------
Num timesteps: 5508000
Best mean reward: -2.16 - Last mean reward per episode: -2.13

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_223415_numTimesteps_5508000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 5511000
Best mean reward: -2.13 - Last mean reward per episode: -2.26
Num timesteps: 5514000
Best mean reward: -2.13 - Last mean reward per episode: -2.37
Num timesteps: 5517000
Best mean reward: -2.13 - Last mean reward per episode: -2.49
Num timesteps: 5520000
Best mean reward: -2.13 - Last mean reward per episode: -2.57
Num timesteps: 5523000
Best mean reward: -2.13 - Last mean reward per episode: -2.29
Num timesteps: 5526000
Best mean reward: -2.13 - Last mean reward per episode: -2.32
Num timesteps: 5529000
Best mean reward: -2.13 - Last mean reward per episode: -2.53
Num timesteps: 5532000
Best mean reward: -2.13 - Last mean reward per episode: -2.46
Num timesteps: 5535000
Best mean reward: -2.13 - Last mean reward per episode: -2.36
Num timesteps: 5538000
Best mean reward: -2.13 - Last mean reward per episode: -2.39
Num timesteps: 5541000
Best mean reward: -2.13 - Last mean reward per episode: -2.34
Num timesteps: 5544000
Best mean reward: -2.13 - Last mean reward per episode: -2.21
Num timesteps: 5547000
Best mean reward: -2.13 - Last mean reward per episode: -2.23
Num timesteps: 5550000
Best mean reward: -2.13 - Last mean reward per episode: -2.29
Num timesteps: 5553000
Best mean reward: -2.13 - Last mean reward per episode: -2.18
Num timesteps: 5556000
Best mean reward: -2.13 - Last mean reward per episode: -2.47
Num timesteps: 5559000
Best mean reward: -2.13 - Last mean reward per episode: -2.70
Num timesteps: 5562000
Best mean reward: -2.13 - Last mean reward per episode: -2.62
Num timesteps: 5565000
Best mean reward: -2.13 - Last mean reward per episode: -2.47
Num timesteps: 5568000
Best mean reward: -2.13 - Last mean reward per episode: -2.43
Num timesteps: 5571000
Best mean reward: -2.13 - Last mean reward per episode: -2.40
Num timesteps: 5574000
Best mean reward: -2.13 - Last mean reward per episode: -2.21
Num timesteps: 5577000
Best mean reward: -2.13 - Last mean reward per episode: -2.20
Num timesteps: 5580000
Best mean reward: -2.13 - Last mean reward per episode: -2.14
Num timesteps: 5583000
Best mean reward: -2.13 - Last mean reward per episode: -2.32
Num timesteps: 5586000
Best mean reward: -2.13 - Last mean reward per episode: -2.25
Num timesteps: 5589000
Best mean reward: -2.13 - Last mean reward per episode: -2.04

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_223636_numTimesteps_5589000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 5592000
Best mean reward: -2.04 - Last mean reward per episode: -2.25
Num timesteps: 5595000
Best mean reward: -2.04 - Last mean reward per episode: -2.35
Num timesteps: 5598000
Best mean reward: -2.04 - Last mean reward per episode: -2.28
Num timesteps: 5601000
Best mean reward: -2.04 - Last mean reward per episode: -2.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 50          |
|    ep_rew_mean          | -2.28       |
| time/                   |             |
|    fps                  | 462         |
|    iterations           | 57          |
|    time_elapsed         | 12106       |
|    total_timesteps      | 5603328     |
| train/                  |             |
|    approx_kl            | 0.012037173 |
|    clip_fraction        | 0.549       |
|    clip_range           | 0.075       |
|    entropy_loss         | -0.994      |
|    explained_variance   | 0.67        |
|    learning_rate        | 0.00134     |
|    loss                 | 0.0511      |
|    n_updates            | 504         |
|    policy_gradient_loss | 0.00902     |
|    std                  | 0.34        |
|    value_loss           | 0.0514      |
-----------------------------------------
Num timesteps: 5604000
Best mean reward: -2.04 - Last mean reward per episode: -2.21
Num timesteps: 5607000
Best mean reward: -2.04 - Last mean reward per episode: -2.28
Num timesteps: 5610000
Best mean reward: -2.04 - Last mean reward per episode: -2.43
Num timesteps: 5613000
Best mean reward: -2.04 - Last mean reward per episode: -2.47
Num timesteps: 5616000
Best mean reward: -2.04 - Last mean reward per episode: -2.56
Num timesteps: 5619000
Best mean reward: -2.04 - Last mean reward per episode: -2.40
Num timesteps: 5622000
Best mean reward: -2.04 - Last mean reward per episode: -2.35
Num timesteps: 5625000
Best mean reward: -2.04 - Last mean reward per episode: -2.26
Num timesteps: 5628000
Best mean reward: -2.04 - Last mean reward per episode: -2.17
Num timesteps: 5631000
Best mean reward: -2.04 - Last mean reward per episode: -2.34
Num timesteps: 5634000
Best mean reward: -2.04 - Last mean reward per episode: -2.41
Num timesteps: 5637000
Best mean reward: -2.04 - Last mean reward per episode: -2.25
Num timesteps: 5640000
Best mean reward: -2.04 - Last mean reward per episode: -2.16
Num timesteps: 5643000
Best mean reward: -2.04 - Last mean reward per episode: -2.06
Num timesteps: 5646000
Best mean reward: -2.04 - Last mean reward per episode: -2.18
Num timesteps: 5649000
Best mean reward: -2.04 - Last mean reward per episode: -2.20
Num timesteps: 5652000
Best mean reward: -2.04 - Last mean reward per episode: -2.20
Num timesteps: 5655000
Best mean reward: -2.04 - Last mean reward per episode: -2.10
Num timesteps: 5658000
Best mean reward: -2.04 - Last mean reward per episode: -2.18
Num timesteps: 5661000
Best mean reward: -2.04 - Last mean reward per episode: -2.12
Num timesteps: 5664000
Best mean reward: -2.04 - Last mean reward per episode: -2.14
Num timesteps: 5667000
Best mean reward: -2.04 - Last mean reward per episode: -2.39
Num timesteps: 5670000
Best mean reward: -2.04 - Last mean reward per episode: -2.22
Num timesteps: 5673000
Best mean reward: -2.04 - Last mean reward per episode: -2.21
Num timesteps: 5676000
Best mean reward: -2.04 - Last mean reward per episode: -2.25
Num timesteps: 5679000
Best mean reward: -2.04 - Last mean reward per episode: -2.23
Num timesteps: 5682000
Best mean reward: -2.04 - Last mean reward per episode: -2.22
Num timesteps: 5685000
Best mean reward: -2.04 - Last mean reward per episode: -2.26
Num timesteps: 5688000
Best mean reward: -2.04 - Last mean reward per episode: -2.28
Num timesteps: 5691000
Best mean reward: -2.04 - Last mean reward per episode: -2.19
Num timesteps: 5694000
Best mean reward: -2.04 - Last mean reward per episode: -2.22
Num timesteps: 5697000
Best mean reward: -2.04 - Last mean reward per episode: -2.37
Num timesteps: 5700000
Best mean reward: -2.04 - Last mean reward per episode: -2.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 50          |
|    ep_rew_mean          | -2.37       |
| time/                   |             |
|    fps                  | 463         |
|    iterations           | 58          |
|    time_elapsed         | 12308       |
|    total_timesteps      | 5701632     |
| train/                  |             |
|    approx_kl            | 0.013529708 |
|    clip_fraction        | 0.56        |
|    clip_range           | 0.075       |
|    entropy_loss         | -0.926      |
|    explained_variance   | 0.693       |
|    learning_rate        | 0.00134     |
|    loss                 | 0.0411      |
|    n_updates            | 513         |
|    policy_gradient_loss | 0.00982     |
|    std                  | 0.331       |
|    value_loss           | 0.0433      |
-----------------------------------------
Num timesteps: 5703000
Best mean reward: -2.04 - Last mean reward per episode: -2.31
Num timesteps: 5706000
Best mean reward: -2.04 - Last mean reward per episode: -2.09
Num timesteps: 5709000
Best mean reward: -2.04 - Last mean reward per episode: -2.14
Num timesteps: 5712000
Best mean reward: -2.04 - Last mean reward per episode: -2.21
Num timesteps: 5715000
Best mean reward: -2.04 - Last mean reward per episode: -2.04
Num timesteps: 5718000
Best mean reward: -2.04 - Last mean reward per episode: -2.16
Num timesteps: 5721000
Best mean reward: -2.04 - Last mean reward per episode: -2.30
Num timesteps: 5724000
Best mean reward: -2.04 - Last mean reward per episode: -2.40
Num timesteps: 5727000
Best mean reward: -2.04 - Last mean reward per episode: -2.24
Num timesteps: 5730000
Best mean reward: -2.04 - Last mean reward per episode: -2.14
Num timesteps: 5733000
Best mean reward: -2.04 - Last mean reward per episode: -2.19
Num timesteps: 5736000
Best mean reward: -2.04 - Last mean reward per episode: -2.04
Num timesteps: 5739000
Best mean reward: -2.04 - Last mean reward per episode: -2.12
Num timesteps: 5742000
Best mean reward: -2.04 - Last mean reward per episode: -2.29
Num timesteps: 5745000
Best mean reward: -2.04 - Last mean reward per episode: -2.21
Num timesteps: 5748000
Best mean reward: -2.04 - Last mean reward per episode: -2.07
Num timesteps: 5751000
Best mean reward: -2.04 - Last mean reward per episode: -2.19
Num timesteps: 5754000
Best mean reward: -2.04 - Last mean reward per episode: -2.17
Num timesteps: 5757000
Best mean reward: -2.04 - Last mean reward per episode: -2.08
Num timesteps: 5760000
Best mean reward: -2.04 - Last mean reward per episode: -2.03

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_224224_numTimesteps_5760000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 5763000
Best mean reward: -2.03 - Last mean reward per episode: -2.26
Num timesteps: 5766000
Best mean reward: -2.03 - Last mean reward per episode: -2.19
Num timesteps: 5769000
Best mean reward: -2.03 - Last mean reward per episode: -1.98

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_224241_numTimesteps_5769000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 5772000
Best mean reward: -1.98 - Last mean reward per episode: -2.14
Num timesteps: 5775000
Best mean reward: -1.98 - Last mean reward per episode: -2.13
Num timesteps: 5778000
Best mean reward: -1.98 - Last mean reward per episode: -2.08
Num timesteps: 5781000
Best mean reward: -1.98 - Last mean reward per episode: -1.96

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_224302_numTimesteps_5781000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 5784000
Best mean reward: -1.96 - Last mean reward per episode: -2.04
Num timesteps: 5787000
Best mean reward: -1.96 - Last mean reward per episode: -2.19
Num timesteps: 5790000
Best mean reward: -1.96 - Last mean reward per episode: -2.23
Num timesteps: 5793000
Best mean reward: -1.96 - Last mean reward per episode: -2.32
Num timesteps: 5796000
Best mean reward: -1.96 - Last mean reward per episode: -2.10
Num timesteps: 5799000
Best mean reward: -1.96 - Last mean reward per episode: -2.08
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 50          |
|    ep_rew_mean          | -2.02       |
| time/                   |             |
|    fps                  | 463         |
|    iterations           | 59          |
|    time_elapsed         | 12500       |
|    total_timesteps      | 5799936     |
| train/                  |             |
|    approx_kl            | 0.011953359 |
|    clip_fraction        | 0.579       |
|    clip_range           | 0.075       |
|    entropy_loss         | -0.845      |
|    explained_variance   | 0.737       |
|    learning_rate        | 0.00134     |
|    loss                 | 0.0332      |
|    n_updates            | 522         |
|    policy_gradient_loss | 0.00993     |
|    std                  | 0.324       |
|    value_loss           | 0.0358      |
-----------------------------------------
Num timesteps: 5802000
Best mean reward: -1.96 - Last mean reward per episode: -2.18
Num timesteps: 5805000
Best mean reward: -1.96 - Last mean reward per episode: -2.22
Num timesteps: 5808000
Best mean reward: -1.96 - Last mean reward per episode: -2.22
Num timesteps: 5811000
Best mean reward: -1.96 - Last mean reward per episode: -2.21
Num timesteps: 5814000
Best mean reward: -1.96 - Last mean reward per episode: -2.16
Num timesteps: 5817000
Best mean reward: -1.96 - Last mean reward per episode: -2.11
Num timesteps: 5820000
Best mean reward: -1.96 - Last mean reward per episode: -2.14
Num timesteps: 5823000
Best mean reward: -1.96 - Last mean reward per episode: -2.22
Num timesteps: 5826000
Best mean reward: -1.96 - Last mean reward per episode: -2.26
Num timesteps: 5829000
Best mean reward: -1.96 - Last mean reward per episode: -2.32
Num timesteps: 5832000
Best mean reward: -1.96 - Last mean reward per episode: -2.25
Num timesteps: 5835000
Best mean reward: -1.96 - Last mean reward per episode: -2.21
Num timesteps: 5838000
Best mean reward: -1.96 - Last mean reward per episode: -2.06
Num timesteps: 5841000
Best mean reward: -1.96 - Last mean reward per episode: -2.05
Num timesteps: 5844000
Best mean reward: -1.96 - Last mean reward per episode: -2.13
Num timesteps: 5847000
Best mean reward: -1.96 - Last mean reward per episode: -2.19
Num timesteps: 5850000
Best mean reward: -1.96 - Last mean reward per episode: -2.25
Num timesteps: 5853000
Best mean reward: -1.96 - Last mean reward per episode: -2.16
Num timesteps: 5856000
Best mean reward: -1.96 - Last mean reward per episode: -2.07
Num timesteps: 5859000
Best mean reward: -1.96 - Last mean reward per episode: -2.16
Num timesteps: 5862000
Best mean reward: -1.96 - Last mean reward per episode: -2.14
Num timesteps: 5865000
Best mean reward: -1.96 - Last mean reward per episode: -2.23
Num timesteps: 5868000
Best mean reward: -1.96 - Last mean reward per episode: -2.13
Num timesteps: 5871000
Best mean reward: -1.96 - Last mean reward per episode: -2.09
Num timesteps: 5874000
Best mean reward: -1.96 - Last mean reward per episode: -2.11
Num timesteps: 5877000
Best mean reward: -1.96 - Last mean reward per episode: -2.02
Num timesteps: 5880000
Best mean reward: -1.96 - Last mean reward per episode: -2.19
Num timesteps: 5883000
Best mean reward: -1.96 - Last mean reward per episode: -2.19
Num timesteps: 5886000
Best mean reward: -1.96 - Last mean reward per episode: -1.91

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_224646_numTimesteps_5886000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 5889000
Best mean reward: -1.91 - Last mean reward per episode: -1.94
Num timesteps: 5892000
Best mean reward: -1.91 - Last mean reward per episode: -2.08
Num timesteps: 5895000
Best mean reward: -1.91 - Last mean reward per episode: -1.94
Num timesteps: 5898000
Best mean reward: -1.91 - Last mean reward per episode: -2.02
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 50          |
|    ep_rew_mean          | -2.05       |
| time/                   |             |
|    fps                  | 463         |
|    iterations           | 60          |
|    time_elapsed         | 12714       |
|    total_timesteps      | 5898240     |
| train/                  |             |
|    approx_kl            | 0.018130116 |
|    clip_fraction        | 0.608       |
|    clip_range           | 0.075       |
|    entropy_loss         | -0.773      |
|    explained_variance   | 0.77        |
|    learning_rate        | 0.00134     |
|    loss                 | 0.0323      |
|    n_updates            | 531         |
|    policy_gradient_loss | 0.0117      |
|    std                  | 0.317       |
|    value_loss           | 0.0294      |
-----------------------------------------
Num timesteps: 5901000
Best mean reward: -1.91 - Last mean reward per episode: -2.16
Num timesteps: 5904000
Best mean reward: -1.91 - Last mean reward per episode: -2.07
Num timesteps: 5907000
Best mean reward: -1.91 - Last mean reward per episode: -2.03
Num timesteps: 5910000
Best mean reward: -1.91 - Last mean reward per episode: -2.11
Num timesteps: 5913000
Best mean reward: -1.91 - Last mean reward per episode: -2.12
Num timesteps: 5916000
Best mean reward: -1.91 - Last mean reward per episode: -2.16
Num timesteps: 5919000
Best mean reward: -1.91 - Last mean reward per episode: -2.09
Num timesteps: 5922000
Best mean reward: -1.91 - Last mean reward per episode: -2.20
Num timesteps: 5925000
Best mean reward: -1.91 - Last mean reward per episode: -2.19
Num timesteps: 5928000
Best mean reward: -1.91 - Last mean reward per episode: -2.22
Num timesteps: 5931000
Best mean reward: -1.91 - Last mean reward per episode: -1.94
Num timesteps: 5934000
Best mean reward: -1.91 - Last mean reward per episode: -1.88

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_224836_numTimesteps_5934000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 5937000
Best mean reward: -1.88 - Last mean reward per episode: -1.99
Num timesteps: 5940000
Best mean reward: -1.88 - Last mean reward per episode: -2.08
Num timesteps: 5943000
Best mean reward: -1.88 - Last mean reward per episode: -2.12
Num timesteps: 5946000
Best mean reward: -1.88 - Last mean reward per episode: -2.04
Num timesteps: 5949000
Best mean reward: -1.88 - Last mean reward per episode: -2.14
Num timesteps: 5952000
Best mean reward: -1.88 - Last mean reward per episode: -2.05
Num timesteps: 5955000
Best mean reward: -1.88 - Last mean reward per episode: -2.10
Num timesteps: 5958000
Best mean reward: -1.88 - Last mean reward per episode: -2.05
Num timesteps: 5961000
Best mean reward: -1.88 - Last mean reward per episode: -1.98
Num timesteps: 5964000
Best mean reward: -1.88 - Last mean reward per episode: -2.09
Num timesteps: 5967000
Best mean reward: -1.88 - Last mean reward per episode: -1.93
Num timesteps: 5970000
Best mean reward: -1.88 - Last mean reward per episode: -1.93
Num timesteps: 5973000
Best mean reward: -1.88 - Last mean reward per episode: -2.12
Num timesteps: 5976000
Best mean reward: -1.88 - Last mean reward per episode: -2.12
Num timesteps: 5979000
Best mean reward: -1.88 - Last mean reward per episode: -2.00
Num timesteps: 5982000
Best mean reward: -1.88 - Last mean reward per episode: -1.90
Num timesteps: 5985000
Best mean reward: -1.88 - Last mean reward per episode: -1.89
Num timesteps: 5988000
Best mean reward: -1.88 - Last mean reward per episode: -2.06
Num timesteps: 5991000
Best mean reward: -1.88 - Last mean reward per episode: -1.99
Num timesteps: 5994000
Best mean reward: -1.88 - Last mean reward per episode: -1.97
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 50          |
|    ep_rew_mean          | -1.97       |
| time/                   |             |
|    fps                  | 464         |
|    iterations           | 61          |
|    time_elapsed         | 12913       |
|    total_timesteps      | 5996544     |
| train/                  |             |
|    approx_kl            | 0.021195233 |
|    clip_fraction        | 0.641       |
|    clip_range           | 0.075       |
|    entropy_loss         | -0.703      |
|    explained_variance   | 0.782       |
|    learning_rate        | 0.00134     |
|    loss                 | 0.0293      |
|    n_updates            | 540         |
|    policy_gradient_loss | 0.0135      |
|    std                  | 0.31        |
|    value_loss           | 0.0264      |
-----------------------------------------
Num timesteps: 5997000
Best mean reward: -1.88 - Last mean reward per episode: -1.95
Num timesteps: 6000000
Best mean reward: -1.88 - Last mean reward per episode: -1.89
Num timesteps: 6003000
Best mean reward: -1.88 - Last mean reward per episode: -1.97
Num timesteps: 6006000
Best mean reward: -1.88 - Last mean reward per episode: -1.92
Num timesteps: 6009000
Best mean reward: -1.88 - Last mean reward per episode: -1.87

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_225115_numTimesteps_6009000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 6012000
Best mean reward: -1.87 - Last mean reward per episode: -2.16
Num timesteps: 6015000
Best mean reward: -1.87 - Last mean reward per episode: -2.15
Num timesteps: 6018000
Best mean reward: -1.87 - Last mean reward per episode: -2.05
Num timesteps: 6021000
Best mean reward: -1.87 - Last mean reward per episode: -2.12
Num timesteps: 6024000
Best mean reward: -1.87 - Last mean reward per episode: -2.11
Num timesteps: 6027000
Best mean reward: -1.87 - Last mean reward per episode: -2.03
Num timesteps: 6030000
Best mean reward: -1.87 - Last mean reward per episode: -1.94
Num timesteps: 6033000
Best mean reward: -1.87 - Last mean reward per episode: -2.05
Num timesteps: 6036000
Best mean reward: -1.87 - Last mean reward per episode: -2.07
Num timesteps: 6039000
Best mean reward: -1.87 - Last mean reward per episode: -2.13
Num timesteps: 6042000
Best mean reward: -1.87 - Last mean reward per episode: -2.02
Num timesteps: 6045000
Best mean reward: -1.87 - Last mean reward per episode: -1.92
Num timesteps: 6048000
Best mean reward: -1.87 - Last mean reward per episode: -1.85

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_225232_numTimesteps_6048000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 6051000
Best mean reward: -1.85 - Last mean reward per episode: -1.89
Num timesteps: 6054000
Best mean reward: -1.85 - Last mean reward per episode: -1.74

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/model_20220702_225243_numTimesteps_6054000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-07-02/PPOReach/callback/best_model
Num timesteps: 6057000
Best mean reward: -1.74 - Last mean reward per episode: -1.85
Num timesteps: 6060000
Best mean reward: -1.74 - Last mean reward per episode: -2.17
Num timesteps: 6063000
Best mean reward: -1.74 - Last mean reward per episode: -1.94
Num timesteps: 6066000
Best mean reward: -1.74 - Last mean reward per episode: -2.00
Num timesteps: 6069000
Best mean reward: -1.74 - Last mean reward per episode: -2.06
Num timesteps: 6072000
Best mean reward: -1.74 - Last mean reward per episode: -2.08
Num timesteps: 6075000
Best mean reward: -1.74 - Last mean reward per episode: -2.04
Num timesteps: 6078000
Best mean reward: -1.74 - Last mean reward per episode: -2.02
Num timesteps: 6081000
Best mean reward: -1.74 - Last mean reward per episode: -1.92
Num timesteps: 6084000
Best mean reward: -1.74 - Last mean reward per episode: -1.82
Num timesteps: 6087000
Best mean reward: -1.74 - Last mean reward per episode: -1.91
Num timesteps: 6090000
Best mean reward: -1.74 - Last mean reward per episode: -2.09
Num timesteps: 6093000
Best mean reward: -1.74 - Last mean reward per episode: -2.03
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 50          |
|    ep_rew_mean          | -1.9        |
| time/                   |             |
|    fps                  | 464         |
|    iterations           | 62          |
|    time_elapsed         | 13111       |
|    to