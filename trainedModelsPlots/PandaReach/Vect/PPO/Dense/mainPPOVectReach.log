Script started on 2022-06-30 18:26:44-04:00 [TERM="xterm-256color" TTY="/dev/pts/10" COLUMNS="203" LINES="53"]
bash: devel/setup.bash: No such file or directory
bash: /home/hjkwon/catkin_ws/src/moveit/devel/setup.bash: No such file or directory
]0;hjkwon@HJK-AI-Robotics: ~/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30[01;32mhjkwon@HJK-AI-Robotics[00m:[01;34m~/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30[00m$ cd ..
]0;hjkwon@HJK-AI-Robotics: ~/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining[01;32mhjkwon@HJK-AI-Robotics[00m:[01;34m~/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining[00m$ conda activate rl_env
(rl_env) ]0;hjkwon@HJK-AI-Robotics: ~/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining[01;32mhjkwon@HJK-AI-Robotics[00m:[01;34m~/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining[00m$ pt[Kython3 mainPPOVect2.py 
pybullet build time: Dec  1 2021 18:34:28
_XSERVTransSocketUNIXCreateListener: ...SocketCreateListener() failed
_XSERVTransMakeAllCOTSServerListeners: server already running
(EE) 
Fatal server error:
(EE) Cannot establish any listening sockets - Make sure an X server isn't already running(EE) 
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413

 
 kwargs:  {'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 1, 'gamma': 0.9093373345474445, 'gae_lambda': 0.9290620915181558, 'learning_rate': 0.003576327533197799, 'ent_coef': 1e-06, 'max_grad_norm': 0.5, 'vf_coef': 0.75, 'clip_range': 0.075, 'batch_size': 4096, 'n_steps': 16384, 'n_epochs': 7, 'policy_kwargs': {'net_arch': [{'pi': [128, 128, 128, 128, 128], 'vf': [128, 128, 128, 128, 128]}]}}
n_envs:  4
NOTE: Testing the model which performed well for grasping with the reach problem


Using cpu device
Num timesteps: 4000
Best mean reward: -inf - Last mean reward per episode: -12.22

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_182709_numTimesteps_4000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 8000
Best mean reward: -12.22 - Last mean reward per episode: -12.45
Num timesteps: 12000
Best mean reward: -12.22 - Last mean reward per episode: -13.44
Num timesteps: 16000
Best mean reward: -12.22 - Last mean reward per episode: -12.45
^Cstarting video recorder: 
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
startThreads creating 1 threads.
starting thread 0
started thread 0 
argc=45
argv[0] = --unused
argv[1] = --background_color_red=0.8745098039215686
argv[2] = 
argv[3] = 
argv[4] = 
argv[5] = 
argv[6] = 
argv[7] = 
argv[8] = 
argv[9] = 
argv[10] = 
argv[11] = 
argv[12] = 
argv[13] = 
argv[14] = 
argv[15] = 
argv[16] = 
argv[17] = 
argv[18] = 
argv[19] = 
argv[20] = 
argv[21] = 
argv[22] = --background_color_green=0.21176470588235294
argv[23] = 
argv[24] = 
argv[25] = 
argv[26] = 
argv[27] = 
argv[28] = 
argv[29] = 
argv[30] = 
argv[31] = 
argv[32] = 
argv[33] = 
argv[34] = 
argv[35] = 
argv[36] = 
argv[37] = 
argv[38] = 
argv[39] = 
argv[40] = 
argv[41] = 
argv[42] = 
argv[43] = --background_color_blue=0.17647058823529413
argv[44] = --start_demo_name=Physics Server
ExampleBrowserThreadFunc started
X11 functions dynamically loaded using dlopen/dlsym OK!
X11 functions dynamically loaded using dlopen/dlsym OK!
Creating context
Created GL 3.3 context
Direct GLX rendering context obtained
Making context current
GL_VENDOR=NVIDIA Corporation
GL_RENDERER=NVIDIA GeForce RTX 3070/PCIe/SSE2
GL_VERSION=3.3.0 NVIDIA 470.82.00
GL_SHADING_LANGUAGE_VERSION=3.30 NVIDIA via Cg compiler
pthread_getconcurrency()=0
Version = 3.3.0 NVIDIA 470.82.00
Vendor = NVIDIA Corporation
Renderer = NVIDIA GeForce RTX 3070/PCIe/SSE2
b3Printf: Selected demo: Physics Server
startThreads creating 1 threads.
starting thread 0
started thread 0 
MotionThreadFunc thread started
ven = NVIDIA Corporation
ven = NVIDIA Corporation
^CTraceback (most recent call last):
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/stable_baselines3/common/policies.py", line 338, in predict
    actions = self._predict(observation, deterministic=deterministic)
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/stable_baselines3/common/policies.py", line 630, in _predict
    return self.get_distribution(observation).get_actions(deterministic=deterministic)
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/stable_baselines3/common/policies.py", line 658, in get_distribution
    latent_pi = self.mlp_extractor.forward_actor(features)
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/stable_baselines3/common/torch_layers.py", line 231, in forward_actor
    return self.policy_net(self.shared_net(features))
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1111, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/container.py", line 139, in forward
    input = module(input)
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1111, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/mainPPOVect2.py", line 116, in <module>
    record_video_single(model=loaded_model, env_id=ENV_ID, video_dir=vidDir, render=RENDER, \
  File "/home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/../SB3/recordVideo.py", line 33, in record_video_single
    action, _ = model.predict(obs)
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/stable_baselines3/common/base_class.py", line 562, in predict
    return self.policy.predict(observation, state, episode_start, deterministic)
  File "/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/stable_baselines3/common/policies.py", line 338, in predict
    actions = self._predict(observation, deterministic=deterministic)
KeyboardInterrupt
numActiveThreads = 0
stopping threads
Thread with taskId 0 exiting
Thread TERMINATED
destroy semaphore
semaphore destroyed
destroy main semaphore
main semaphore destroyed
finished
numActiveThreads = 0
btShutDownExampleBrowser stopping threads
Thread with taskId 0 exiting
Thread TERMINATED
destroy semaphore
semaphore destroyed
destroy main semaphore
main semaphore destroyed
^C
(rl_env) ]0;hjkwon@HJK-AI-Robotics: ~/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining[01;32mhjkwon@HJK-AI-Robotics[00m:[01;34m~/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining[00m$ python3 mainPPOVect2.py 
pybullet build time: Dec  1 2021 18:34:28
_XSERVTransSocketUNIXCreateListener: ...SocketCreateListener() failed
_XSERVTransMakeAllCOTSServerListeners: server already running
(EE) 
Fatal server error:
(EE) Cannot establish any listening sockets - Make sure an X server isn't already running(EE) 
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413

 
 kwargs:  {'policy': 'MlpPolicy', 'device': 'cpu', 'verbose': 1, 'gamma': 0.9093373345474445, 'gae_lambda': 0.9290620915181558, 'learning_rate': 0.003576327533197799, 'ent_coef': 1e-06, 'max_grad_norm': 0.5, 'vf_coef': 0.75, 'clip_range': 0.075, 'batch_size': 4096, 'n_steps': 16384, 'n_epochs': 7, 'policy_kwargs': {'net_arch': [{'pi': [128, 128, 128, 128, 128], 'vf': [128, 128, 128, 128, 128]}]}}
n_envs:  4
NOTE: Testing the model which performed well for grasping with the reach problem


Using cpu device
Num timesteps: 4000
Best mean reward: -inf - Last mean reward per episode: -11.77

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_182800_numTimesteps_4000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Num timesteps: 8000
Best mean reward: -11.77 - Last mean reward per episode: -11.80
Num timesteps: 12000
Best mean reward: -11.77 - Last mean reward per episode: -12.85
Num timesteps: 16000
Best mean reward: -11.77 - Last mean reward per episode: -12.64
Num timesteps: 20000
Best mean reward: -11.77 - Last mean reward per episode: -12.26
Num timesteps: 24000
Best mean reward: -11.77 - Last mean reward per episode: -12.71
Num timesteps: 28000
Best mean reward: -11.77 - Last mean reward per episode: -11.61

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_182854_numTimesteps_28000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 32000
Best mean reward: -11.61 - Last mean reward per episode: -12.43
Num timesteps: 36000
Best mean reward: -11.61 - Last mean reward per episode: -12.32
Num timesteps: 40000
Best mean reward: -11.61 - Last mean reward per episode: -11.94
Num timesteps: 44000
Best mean reward: -11.61 - Last mean reward per episode: -12.02
Num timesteps: 48000
Best mean reward: -11.61 - Last mean reward per episode: -12.02
Num timesteps: 52000
Best mean reward: -11.61 - Last mean reward per episode: -11.93
Num timesteps: 56000
Best mean reward: -11.61 - Last mean reward per episode: -12.40
Num timesteps: 60000
Best mean reward: -11.61 - Last mean reward per episode: -12.80
Num timesteps: 64000
Best mean reward: -11.61 - Last mean reward per episode: -12.37
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | -12.5    |
| time/              |          |
|    fps             | 425      |
|    iterations      | 1        |
|    time_elapsed    | 153      |
|    total_timesteps | 65536    |
---------------------------------
Num timesteps: 68000
Best mean reward: -11.61 - Last mean reward per episode: -11.85
Num timesteps: 72000
Best mean reward: -11.61 - Last mean reward per episode: -11.33

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_183052_numTimesteps_72000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 76000
Best mean reward: -11.33 - Last mean reward per episode: -11.06

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_183101_numTimesteps_76000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 80000
Best mean reward: -11.06 - Last mean reward per episode: -11.47
Num timesteps: 84000
Best mean reward: -11.06 - Last mean reward per episode: -11.61
Num timesteps: 88000
Best mean reward: -11.06 - Last mean reward per episode: -10.83

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_183127_numTimesteps_88000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 92000
Best mean reward: -10.83 - Last mean reward per episode: -11.48
Num timesteps: 96000
Best mean reward: -10.83 - Last mean reward per episode: -11.46
Num timesteps: 100000
Best mean reward: -10.83 - Last mean reward per episode: -10.78

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_183154_numTimesteps_100000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 104000
Best mean reward: -10.78 - Last mean reward per episode: -11.81
Num timesteps: 108000
Best mean reward: -10.78 - Last mean reward per episode: -11.36
Num timesteps: 112000
Best mean reward: -10.78 - Last mean reward per episode: -11.81
Num timesteps: 116000
Best mean reward: -10.78 - Last mean reward per episode: -11.82
Num timesteps: 120000
Best mean reward: -10.78 - Last mean reward per episode: -11.85
Num timesteps: 124000
Best mean reward: -10.78 - Last mean reward per episode: -11.23
Num timesteps: 128000
Best mean reward: -10.78 - Last mean reward per episode: -11.84
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 50           |
|    ep_rew_mean          | -11.9        |
| time/                   |              |
|    fps                  | 427          |
|    iterations           | 2            |
|    time_elapsed         | 306          |
|    total_timesteps      | 131072       |
| train/                  |              |
|    approx_kl            | 0.0013680321 |
|    clip_fraction        | 0.133        |
|    clip_range           | 0.075        |
|    entropy_loss         | -4.26        |
|    explained_variance   | -0.00968     |
|    learning_rate        | 0.00358      |
|    loss                 | 0.166        |
|    n_updates            | 7            |
|    policy_gradient_loss | -0.00404     |
|    std                  | 1            |
|    value_loss           | 0.363        |
------------------------------------------
Num timesteps: 132000
Best mean reward: -10.78 - Last mean reward per episode: -11.10
Num timesteps: 136000
Best mean reward: -10.78 - Last mean reward per episode: -9.51

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_183319_numTimesteps_136000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 140000
Best mean reward: -9.51 - Last mean reward per episode: -10.33
Num timesteps: 144000
Best mean reward: -9.51 - Last mean reward per episode: -10.53
Num timesteps: 148000
Best mean reward: -9.51 - Last mean reward per episode: -10.39
Num timesteps: 152000
Best mean reward: -9.51 - Last mean reward per episode: -10.83
Num timesteps: 156000
Best mean reward: -9.51 - Last mean reward per episode: -10.52
Num timesteps: 160000
Best mean reward: -9.51 - Last mean reward per episode: -10.81
Num timesteps: 164000
Best mean reward: -9.51 - Last mean reward per episode: -10.71
Num timesteps: 168000
Best mean reward: -9.51 - Last mean reward per episode: -10.73
Num timesteps: 172000
Best mean reward: -9.51 - Last mean reward per episode: -10.50
Num timesteps: 176000
Best mean reward: -9.51 - Last mean reward per episode: -10.71
Num timesteps: 180000
Best mean reward: -9.51 - Last mean reward per episode: -10.98
Num timesteps: 184000
Best mean reward: -9.51 - Last mean reward per episode: -10.54
Num timesteps: 188000
Best mean reward: -9.51 - Last mean reward per episode: -10.63
Num timesteps: 192000
Best mean reward: -9.51 - Last mean reward per episode: -10.47
Num timesteps: 196000
Best mean reward: -9.51 - Last mean reward per episode: -10.45
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 50           |
|    ep_rew_mean          | -10.3        |
| time/                   |              |
|    fps                  | 435          |
|    iterations           | 3            |
|    time_elapsed         | 451          |
|    total_timesteps      | 196608       |
| train/                  |              |
|    approx_kl            | 0.0026638233 |
|    clip_fraction        | 0.283        |
|    clip_range           | 0.075        |
|    entropy_loss         | -4.25        |
|    explained_variance   | 0.662        |
|    learning_rate        | 0.00358      |
|    loss                 | 0.0737       |
|    n_updates            | 14           |
|    policy_gradient_loss | -0.011       |
|    std                  | 0.999        |
|    value_loss           | 0.164        |
------------------------------------------
Num timesteps: 200000
Best mean reward: -9.51 - Last mean reward per episode: -9.72
Num timesteps: 204000
Best mean reward: -9.51 - Last mean reward per episode: -9.12

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_183551_numTimesteps_204000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 208000
Best mean reward: -9.12 - Last mean reward per episode: -9.31
Num timesteps: 212000
Best mean reward: -9.12 - Last mean reward per episode: -9.57
Num timesteps: 216000
Best mean reward: -9.12 - Last mean reward per episode: -9.22
Num timesteps: 220000
Best mean reward: -9.12 - Last mean reward per episode: -9.34
Num timesteps: 224000
Best mean reward: -9.12 - Last mean reward per episode: -8.97

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_183638_numTimesteps_224000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 228000
Best mean reward: -8.97 - Last mean reward per episode: -9.44
Num timesteps: 232000
Best mean reward: -8.97 - Last mean reward per episode: -9.40
Num timesteps: 236000
Best mean reward: -8.97 - Last mean reward per episode: -9.47
Num timesteps: 240000
Best mean reward: -8.97 - Last mean reward per episode: -9.75
Num timesteps: 244000
Best mean reward: -8.97 - Last mean reward per episode: -9.70
Num timesteps: 248000
Best mean reward: -8.97 - Last mean reward per episode: -10.29
Num timesteps: 252000
Best mean reward: -8.97 - Last mean reward per episode: -9.78
Num timesteps: 256000
Best mean reward: -8.97 - Last mean reward per episode: -9.41
Num timesteps: 260000
Best mean reward: -8.97 - Last mean reward per episode: -9.35
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 50          |
|    ep_rew_mean          | -9.63       |
| time/                   |             |
|    fps                  | 434         |
|    iterations           | 4           |
|    time_elapsed         | 603         |
|    total_timesteps      | 262144      |
| train/                  |             |
|    approx_kl            | 0.003624891 |
|    clip_fraction        | 0.376       |
|    clip_range           | 0.075       |
|    entropy_loss         | -4.24       |
|    explained_variance   | 0.815       |
|    learning_rate        | 0.00358     |
|    loss                 | 0.0517      |
|    n_updates            | 21          |
|    policy_gradient_loss | -0.0164     |
|    std                  | 0.992       |
|    value_loss           | 0.106       |
-----------------------------------------
Num timesteps: 264000
Best mean reward: -8.97 - Last mean reward per episode: -9.33
Num timesteps: 268000
Best mean reward: -8.97 - Last mean reward per episode: -8.78

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_183819_numTimesteps_268000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 272000
Best mean reward: -8.78 - Last mean reward per episode: -8.85
Num timesteps: 276000
Best mean reward: -8.78 - Last mean reward per episode: -8.71

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_183837_numTimesteps_276000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 280000
Best mean reward: -8.71 - Last mean reward per episode: -8.80
Num timesteps: 284000
Best mean reward: -8.71 - Last mean reward per episode: -8.81
Num timesteps: 288000
Best mean reward: -8.71 - Last mean reward per episode: -8.45

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_183908_numTimesteps_288000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 292000
Best mean reward: -8.45 - Last mean reward per episode: -8.38

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_183918_numTimesteps_292000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 296000
Best mean reward: -8.38 - Last mean reward per episode: -8.57
Num timesteps: 300000
Best mean reward: -8.38 - Last mean reward per episode: -8.77
Num timesteps: 304000
Best mean reward: -8.38 - Last mean reward per episode: -8.59
Num timesteps: 308000
Best mean reward: -8.38 - Last mean reward per episode: -8.42
Num timesteps: 312000
Best mean reward: -8.38 - Last mean reward per episode: -8.42
Num timesteps: 316000
Best mean reward: -8.38 - Last mean reward per episode: -8.21

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_184021_numTimesteps_316000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 320000
Best mean reward: -8.21 - Last mean reward per episode: -7.70

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_184032_numTimesteps_320000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 324000
Best mean reward: -7.70 - Last mean reward per episode: -9.08
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 50           |
|    ep_rew_mean          | -8.37        |
| time/                   |              |
|    fps                  | 421          |
|    iterations           | 5            |
|    time_elapsed         | 777          |
|    total_timesteps      | 327680       |
| train/                  |              |
|    approx_kl            | 0.0041107307 |
|    clip_fraction        | 0.408        |
|    clip_range           | 0.075        |
|    entropy_loss         | -4.21        |
|    explained_variance   | 0.847        |
|    learning_rate        | 0.00358      |
|    loss                 | 0.044        |
|    n_updates            | 28           |
|    policy_gradient_loss | -0.0184      |
|    std                  | 0.985        |
|    value_loss           | 0.0832       |
------------------------------------------
Num timesteps: 328000
Best mean reward: -7.70 - Last mean reward per episode: -8.26
Num timesteps: 332000
Best mean reward: -7.70 - Last mean reward per episode: -7.83
Num timesteps: 336000
Best mean reward: -7.70 - Last mean reward per episode: -8.02
Num timesteps: 340000
Best mean reward: -7.70 - Last mean reward per episode: -8.23
Num timesteps: 344000
Best mean reward: -7.70 - Last mean reward per episode: -7.45

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_184130_numTimesteps_344000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 348000
Best mean reward: -7.45 - Last mean reward per episode: -6.93

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_184139_numTimesteps_348000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 352000
Best mean reward: -6.93 - Last mean reward per episode: -7.37
Num timesteps: 356000
Best mean reward: -6.93 - Last mean reward per episode: -7.98
Num timesteps: 360000
Best mean reward: -6.93 - Last mean reward per episode: -7.57
Num timesteps: 364000
Best mean reward: -6.93 - Last mean reward per episode: -8.03
Num timesteps: 368000
Best mean reward: -6.93 - Last mean reward per episode: -8.03
Num timesteps: 372000
Best mean reward: -6.93 - Last mean reward per episode: -8.28
Num timesteps: 376000
Best mean reward: -6.93 - Last mean reward per episode: -8.27
Num timesteps: 380000
Best mean reward: -6.93 - Last mean reward per episode: -7.82
Num timesteps: 384000
Best mean reward: -6.93 - Last mean reward per episode: -7.81
Num timesteps: 388000
Best mean reward: -6.93 - Last mean reward per episode: -8.11
Num timesteps: 392000
Best mean reward: -6.93 - Last mean reward per episode: -7.81
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 50          |
|    ep_rew_mean          | -7.85       |
| time/                   |             |
|    fps                  | 422         |
|    iterations           | 6           |
|    time_elapsed         | 930         |
|    total_timesteps      | 393216      |
| train/                  |             |
|    approx_kl            | 0.004900084 |
|    clip_fraction        | 0.428       |
|    clip_range           | 0.075       |
|    entropy_loss         | -4.18       |
|    explained_variance   | 0.857       |
|    learning_rate        | 0.00358     |
|    loss                 | 0.0361      |
|    n_updates            | 35          |
|    policy_gradient_loss | -0.0178     |
|    std                  | 0.973       |
|    value_loss           | 0.0732      |
-----------------------------------------
Num timesteps: 396000
Best mean reward: -6.93 - Last mean reward per episode: -7.72
Num timesteps: 400000
Best mean reward: -6.93 - Last mean reward per episode: -6.78

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_184345_numTimesteps_400000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 404000
Best mean reward: -6.78 - Last mean reward per episode: -7.14
Num timesteps: 408000
Best mean reward: -6.78 - Last mean reward per episode: -7.01
Num timesteps: 412000
Best mean reward: -6.78 - Last mean reward per episode: -7.50
Num timesteps: 416000
Best mean reward: -6.78 - Last mean reward per episode: -7.75
Num timesteps: 420000
Best mean reward: -6.78 - Last mean reward per episode: -7.08
Num timesteps: 424000
Best mean reward: -6.78 - Last mean reward per episode: -7.29
Num timesteps: 428000
Best mean reward: -6.78 - Last mean reward per episode: -6.81
Num timesteps: 432000
Best mean reward: -6.78 - Last mean reward per episode: -6.98
Num timesteps: 436000
Best mean reward: -6.78 - Last mean reward per episode: -6.76

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_184501_numTimesteps_436000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 440000
Best mean reward: -6.76 - Last mean reward per episode: -6.84
Num timesteps: 444000
Best mean reward: -6.76 - Last mean reward per episode: -7.29
Num timesteps: 448000
Best mean reward: -6.76 - Last mean reward per episode: -7.32
Num timesteps: 452000
Best mean reward: -6.76 - Last mean reward per episode: -7.07
Num timesteps: 456000
Best mean reward: -6.76 - Last mean reward per episode: -7.01
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 50           |
|    ep_rew_mean          | -7.21        |
| time/                   |              |
|    fps                  | 424          |
|    iterations           | 7            |
|    time_elapsed         | 1080         |
|    total_timesteps      | 458752       |
| train/                  |              |
|    approx_kl            | 0.0045618857 |
|    clip_fraction        | 0.411        |
|    clip_range           | 0.075        |
|    entropy_loss         | -4.14        |
|    explained_variance   | 0.873        |
|    learning_rate        | 0.00358      |
|    loss                 | 0.0249       |
|    n_updates            | 42           |
|    policy_gradient_loss | -0.0183      |
|    std                  | 0.964        |
|    value_loss           | 0.0578       |
------------------------------------------
Num timesteps: 460000
Best mean reward: -6.76 - Last mean reward per episode: -7.23
Num timesteps: 464000
Best mean reward: -6.76 - Last mean reward per episode: -6.70

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_184611_numTimesteps_464000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 468000
Best mean reward: -6.70 - Last mean reward per episode: -6.44

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_184619_numTimesteps_468000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 472000
Best mean reward: -6.44 - Last mean reward per episode: -6.79
Num timesteps: 476000
Best mean reward: -6.44 - Last mean reward per episode: -6.55
Num timesteps: 480000
Best mean reward: -6.44 - Last mean reward per episode: -6.63
Num timesteps: 484000
Best mean reward: -6.44 - Last mean reward per episode: -6.56
Num timesteps: 488000
Best mean reward: -6.44 - Last mean reward per episode: -6.96
Num timesteps: 492000
Best mean reward: -6.44 - Last mean reward per episode: -6.53
Num timesteps: 496000
Best mean reward: -6.44 - Last mean reward per episode: -6.65
Num timesteps: 500000
Best mean reward: -6.44 - Last mean reward per episode: -6.52
Num timesteps: 504000
Best mean reward: -6.44 - Last mean reward per episode: -6.35

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_184739_numTimesteps_504000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 508000
Best mean reward: -6.35 - Last mean reward per episode: -6.73
Num timesteps: 512000
Best mean reward: -6.35 - Last mean reward per episode: -6.81
Num timesteps: 516000
Best mean reward: -6.35 - Last mean reward per episode: -6.86
Num timesteps: 520000
Best mean reward: -6.35 - Last mean reward per episode: -6.65
Num timesteps: 524000
Best mean reward: -6.35 - Last mean reward per episode: -6.42
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 50           |
|    ep_rew_mean          | -6.38        |
| time/                   |              |
|    fps                  | 425          |
|    iterations           | 8            |
|    time_elapsed         | 1233         |
|    total_timesteps      | 524288       |
| train/                  |              |
|    approx_kl            | 0.0042564077 |
|    clip_fraction        | 0.419        |
|    clip_range           | 0.075        |
|    entropy_loss         | -4.11        |
|    explained_variance   | 0.879        |
|    learning_rate        | 0.00358      |
|    loss                 | 0.0164       |
|    n_updates            | 49           |
|    policy_gradient_loss | -0.0168      |
|    std                  | 0.951        |
|    value_loss           | 0.05         |
------------------------------------------
Num timesteps: 528000
Best mean reward: -6.35 - Last mean reward per episode: -5.84

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_184843_numTimesteps_528000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 532000
Best mean reward: -5.84 - Last mean reward per episode: -6.09
Num timesteps: 536000
Best mean reward: -5.84 - Last mean reward per episode: -6.20
Num timesteps: 540000
Best mean reward: -5.84 - Last mean reward per episode: -6.24
Num timesteps: 544000
Best mean reward: -5.84 - Last mean reward per episode: -6.12
Num timesteps: 548000
Best mean reward: -5.84 - Last mean reward per episode: -5.95
Num timesteps: 552000
Best mean reward: -5.84 - Last mean reward per episode: -5.85
Num timesteps: 556000
Best mean reward: -5.84 - Last mean reward per episode: -5.74

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_184938_numTimesteps_556000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 560000
Best mean reward: -5.74 - Last mean reward per episode: -5.82
Num timesteps: 564000
Best mean reward: -5.74 - Last mean reward per episode: -5.89
Num timesteps: 568000
Best mean reward: -5.74 - Last mean reward per episode: -6.05
Num timesteps: 572000
Best mean reward: -5.74 - Last mean reward per episode: -6.33
Num timesteps: 576000
Best mean reward: -5.74 - Last mean reward per episode: -5.97
Num timesteps: 580000
Best mean reward: -5.74 - Last mean reward per episode: -6.00
Num timesteps: 584000
Best mean reward: -5.74 - Last mean reward per episode: -6.10
Num timesteps: 588000
Best mean reward: -5.74 - Last mean reward per episode: -6.21
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 50           |
|    ep_rew_mean          | -6.15        |
| time/                   |              |
|    fps                  | 428          |
|    iterations           | 9            |
|    time_elapsed         | 1377         |
|    total_timesteps      | 589824       |
| train/                  |              |
|    approx_kl            | 0.0051008197 |
|    clip_fraction        | 0.44         |
|    clip_range           | 0.075        |
|    entropy_loss         | -4.06        |
|    explained_variance   | 0.876        |
|    learning_rate        | 0.00358      |
|    loss                 | 0.019        |
|    n_updates            | 56           |
|    policy_gradient_loss | -0.0155      |
|    std                  | 0.938        |
|    value_loss           | 0.0484       |
------------------------------------------
Num timesteps: 592000
Best mean reward: -5.74 - Last mean reward per episode: -5.67

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_185104_numTimesteps_592000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 596000
Best mean reward: -5.67 - Last mean reward per episode: -5.57

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_185114_numTimesteps_596000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 600000
Best mean reward: -5.57 - Last mean reward per episode: -5.41

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_185124_numTimesteps_600000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 604000
Best mean reward: -5.41 - Last mean reward per episode: -5.38

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_185133_numTimesteps_604000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 608000
Best mean reward: -5.38 - Last mean reward per episode: -5.15

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_185143_numTimesteps_608000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 612000
Best mean reward: -5.15 - Last mean reward per episode: -5.35
Num timesteps: 616000
Best mean reward: -5.15 - Last mean reward per episode: -5.48
Num timesteps: 620000
Best mean reward: -5.15 - Last mean reward per episode: -5.47
Num timesteps: 624000
Best mean reward: -5.15 - Last mean reward per episode: -5.50
Num timesteps: 628000
Best mean reward: -5.15 - Last mean reward per episode: -5.58
Num timesteps: 632000
Best mean reward: -5.15 - Last mean reward per episode: -5.41
Num timesteps: 636000
Best mean reward: -5.15 - Last mean reward per episode: -5.39
Num timesteps: 640000
Best mean reward: -5.15 - Last mean reward per episode: -5.44
Num timesteps: 644000
Best mean reward: -5.15 - Last mean reward per episode: -5.38
Num timesteps: 648000
Best mean reward: -5.15 - Last mean reward per episode: -5.50
Num timesteps: 652000
Best mean reward: -5.15 - Last mean reward per episode: -5.73
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 50         |
|    ep_rew_mean          | -5.54      |
| time/                   |            |
|    fps                  | 427        |
|    iterations           | 10         |
|    time_elapsed         | 1531       |
|    total_timesteps      | 655360     |
| train/                  |            |
|    approx_kl            | 0.00458786 |
|    clip_fraction        | 0.425      |
|    clip_range           | 0.075      |
|    entropy_loss         | -4.02      |
|    explained_variance   | 0.865      |
|    learning_rate        | 0.00358    |
|    loss                 | 0.0168     |
|    n_updates            | 63         |
|    policy_gradient_loss | -0.0152    |
|    std                  | 0.925      |
|    value_loss           | 0.0459     |
----------------------------------------
Num timesteps: 656000
Best mean reward: -5.15 - Last mean reward per episode: -5.40
Num timesteps: 660000
Best mean reward: -5.15 - Last mean reward per episode: -4.84

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_185342_numTimesteps_660000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 664000
Best mean reward: -4.84 - Last mean reward per episode: -4.73

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_185348_numTimesteps_664000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 668000
Best mean reward: -4.73 - Last mean reward per episode: -5.30
Num timesteps: 672000
Best mean reward: -4.73 - Last mean reward per episode: -5.11
Num timesteps: 676000
Best mean reward: -4.73 - Last mean reward per episode: -4.93
Num timesteps: 680000
Best mean reward: -4.73 - Last mean reward per episode: -4.77
Num timesteps: 684000
Best mean reward: -4.73 - Last mean reward per episode: -4.81
Num timesteps: 688000
Best mean reward: -4.73 - Last mean reward per episode: -4.93
Num timesteps: 692000
Best mean reward: -4.73 - Last mean reward per episode: -4.79
Num timesteps: 696000
Best mean reward: -4.73 - Last mean reward per episode: -4.59

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_185452_numTimesteps_696000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 700000
Best mean reward: -4.59 - Last mean reward per episode: -4.84
Num timesteps: 704000
Best mean reward: -4.59 - Last mean reward per episode: -5.17
Num timesteps: 708000
Best mean reward: -4.59 - Last mean reward per episode: -4.98
Num timesteps: 712000
Best mean reward: -4.59 - Last mean reward per episode: -4.86
Num timesteps: 716000
Best mean reward: -4.59 - Last mean reward per episode: -4.97
Num timesteps: 720000
Best mean reward: -4.59 - Last mean reward per episode: -5.17
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 50          |
|    ep_rew_mean          | -5.09       |
| time/                   |             |
|    fps                  | 431         |
|    iterations           | 11          |
|    time_elapsed         | 1671        |
|    total_timesteps      | 720896      |
| train/                  |             |
|    approx_kl            | 0.005176425 |
|    clip_fraction        | 0.432       |
|    clip_range           | 0.075       |
|    entropy_loss         | -3.97       |
|    explained_variance   | 0.833       |
|    learning_rate        | 0.00358     |
|    loss                 | 0.0175      |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0145     |
|    std                  | 0.911       |
|    value_loss           | 0.0434      |
-----------------------------------------
Num timesteps: 724000
Best mean reward: -4.59 - Last mean reward per episode: -4.40

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_185601_numTimesteps_724000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 728000
Best mean reward: -4.40 - Last mean reward per episode: -4.45
Num timesteps: 732000
Best mean reward: -4.40 - Last mean reward per episode: -4.44
Num timesteps: 736000
Best mean reward: -4.40 - Last mean reward per episode: -4.67
Num timesteps: 740000
Best mean reward: -4.40 - Last mean reward per episode: -4.39

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_185634_numTimesteps_740000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 744000
Best mean reward: -4.39 - Last mean reward per episode: -4.50
Num timesteps: 748000
Best mean reward: -4.39 - Last mean reward per episode: -4.27

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_185651_numTimesteps_748000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 752000
Best mean reward: -4.27 - Last mean reward per episode: -4.56
Num timesteps: 756000
Best mean reward: -4.27 - Last mean reward per episode: -4.47
Num timesteps: 760000
Best mean reward: -4.27 - Last mean reward per episode: -4.32
Num timesteps: 764000
Best mean reward: -4.27 - Last mean reward per episode: -4.30
Num timesteps: 768000
Best mean reward: -4.27 - Last mean reward per episode: -4.46
Num timesteps: 772000
Best mean reward: -4.27 - Last mean reward per episode: -4.36
Num timesteps: 776000
Best mean reward: -4.27 - Last mean reward per episode: -4.38
Num timesteps: 780000
Best mean reward: -4.27 - Last mean reward per episode: -4.47
Num timesteps: 784000
Best mean reward: -4.27 - Last mean reward per episode: -4.37
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 50          |
|    ep_rew_mean          | -4.54       |
| time/                   |             |
|    fps                  | 434         |
|    iterations           | 12          |
|    time_elapsed         | 1811        |
|    total_timesteps      | 786432      |
| train/                  |             |
|    approx_kl            | 0.004676169 |
|    clip_fraction        | 0.422       |
|    clip_range           | 0.075       |
|    entropy_loss         | -3.93       |
|    explained_variance   | 0.807       |
|    learning_rate        | 0.00358     |
|    loss                 | 0.0129      |
|    n_updates            | 77          |
|    policy_gradient_loss | -0.0134     |
|    std                  | 0.895       |
|    value_loss           | 0.0373      |
-----------------------------------------
Num timesteps: 788000
Best mean reward: -4.27 - Last mean reward per episode: -4.48
Num timesteps: 792000
Best mean reward: -4.27 - Last mean reward per episode: -4.24

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_185821_numTimesteps_792000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 796000
Best mean reward: -4.24 - Last mean reward per episode: -4.12

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_185828_numTimesteps_796000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 800000
Best mean reward: -4.12 - Last mean reward per episode: -4.02

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_185835_numTimesteps_800000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 804000
Best mean reward: -4.02 - Last mean reward per episode: -3.97

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_185842_numTimesteps_804000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 808000
Best mean reward: -3.97 - Last mean reward per episode: -3.79

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_185848_numTimesteps_808000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 812000
Best mean reward: -3.79 - Last mean reward per episode: -4.09
Num timesteps: 816000
Best mean reward: -3.79 - Last mean reward per episode: -4.21
Num timesteps: 820000
Best mean reward: -3.79 - Last mean reward per episode: -4.25
Num timesteps: 824000
Best mean reward: -3.79 - Last mean reward per episode: -3.97
Num timesteps: 828000
Best mean reward: -3.79 - Last mean reward per episode: -4.09
Num timesteps: 832000
Best mean reward: -3.79 - Last mean reward per episode: -3.78

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_185927_numTimesteps_832000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 836000
Best mean reward: -3.78 - Last mean reward per episode: -4.00
Num timesteps: 840000
Best mean reward: -3.78 - Last mean reward per episode: -3.93
Num timesteps: 844000
Best mean reward: -3.78 - Last mean reward per episode: -4.17
Num timesteps: 848000
Best mean reward: -3.78 - Last mean reward per episode: -4.30
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 50          |
|    ep_rew_mean          | -4.02       |
| time/                   |             |
|    fps                  | 442         |
|    iterations           | 13          |
|    time_elapsed         | 1926        |
|    total_timesteps      | 851968      |
| train/                  |             |
|    approx_kl            | 0.005160415 |
|    clip_fraction        | 0.42        |
|    clip_range           | 0.075       |
|    entropy_loss         | -3.87       |
|    explained_variance   | 0.782       |
|    learning_rate        | 0.00358     |
|    loss                 | 0.00893     |
|    n_updates            | 84          |
|    policy_gradient_loss | -0.013      |
|    std                  | 0.88        |
|    value_loss           | 0.0315      |
-----------------------------------------
Num timesteps: 852000
Best mean reward: -3.78 - Last mean reward per episode: -4.02
Num timesteps: 856000
Best mean reward: -3.78 - Last mean reward per episode: -3.66

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_190013_numTimesteps_856000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 860000
Best mean reward: -3.66 - Last mean reward per episode: -3.79
Num timesteps: 864000
Best mean reward: -3.66 - Last mean reward per episode: -3.71
Num timesteps: 868000
Best mean reward: -3.66 - Last mean reward per episode: -3.84
Num timesteps: 872000
Best mean reward: -3.66 - Last mean reward per episode: -3.73
Num timesteps: 876000
Best mean reward: -3.66 - Last mean reward per episode: -3.66

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_190051_numTimesteps_876000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 880000
Best mean reward: -3.66 - Last mean reward per episode: -3.65

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_190059_numTimesteps_880000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 884000
Best mean reward: -3.65 - Last mean reward per episode: -3.66
Num timesteps: 888000
Best mean reward: -3.65 - Last mean reward per episode: -3.88
Num timesteps: 892000
Best mean reward: -3.65 - Last mean reward per episode: -3.84
Num timesteps: 896000
Best mean reward: -3.65 - Last mean reward per episode: -3.84
Num timesteps: 900000
Best mean reward: -3.65 - Last mean reward per episode: -3.76
Num timesteps: 904000
Best mean reward: -3.65 - Last mean reward per episode: -3.69
Num timesteps: 908000
Best mean reward: -3.65 - Last mean reward per episode: -3.90
Num timesteps: 912000
Best mean reward: -3.65 - Last mean reward per episode: -3.77
Num timesteps: 916000
Best mean reward: -3.65 - Last mean reward per episode: -3.78
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 50           |
|    ep_rew_mean          | -3.82        |
| time/                   |              |
|    fps                  | 445          |
|    iterations           | 14           |
|    time_elapsed         | 2058         |
|    total_timesteps      | 917504       |
| train/                  |              |
|    approx_kl            | 0.0051335683 |
|    clip_fraction        | 0.426        |
|    clip_range           | 0.075        |
|    entropy_loss         | -3.82        |
|    explained_variance   | 0.752        |
|    learning_rate        | 0.00358      |
|    loss                 | 0.00394      |
|    n_updates            | 91           |
|    policy_gradient_loss | -0.0125      |
|    std                  | 0.863        |
|    value_loss           | 0.0252       |
------------------------------------------
Num timesteps: 920000
Best mean reward: -3.65 - Last mean reward per episode: -3.66
Num timesteps: 924000
Best mean reward: -3.65 - Last mean reward per episode: -3.58

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_190232_numTimesteps_924000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 928000
Best mean reward: -3.58 - Last mean reward per episode: -3.60
Num timesteps: 932000
Best mean reward: -3.58 - Last mean reward per episode: -3.59
Num timesteps: 936000
Best mean reward: -3.58 - Last mean reward per episode: -3.43

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_190251_numTimesteps_936000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 940000
Best mean reward: -3.43 - Last mean reward per episode: -3.57
Num timesteps: 944000
Best mean reward: -3.43 - Last mean reward per episode: -3.63
Num timesteps: 948000
Best mean reward: -3.43 - Last mean reward per episode: -3.73
Num timesteps: 952000
Best mean reward: -3.43 - Last mean reward per episode: -3.66
Num timesteps: 956000
Best mean reward: -3.43 - Last mean reward per episode: -3.61
Num timesteps: 960000
Best mean reward: -3.43 - Last mean reward per episode: -3.42

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_190337_numTimesteps_960000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 964000
Best mean reward: -3.42 - Last mean reward per episode: -3.52
Num timesteps: 968000
Best mean reward: -3.42 - Last mean reward per episode: -3.71
Num timesteps: 972000
Best mean reward: -3.42 - Last mean reward per episode: -3.63
Num timesteps: 976000
Best mean reward: -3.42 - Last mean reward per episode: -3.70
Num timesteps: 980000
Best mean reward: -3.42 - Last mean reward per episode: -3.56
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 50           |
|    ep_rew_mean          | -3.66        |
| time/                   |              |
|    fps                  | 448          |
|    iterations           | 15           |
|    time_elapsed         | 2190         |
|    total_timesteps      | 983040       |
| train/                  |              |
|    approx_kl            | 0.0050267004 |
|    clip_fraction        | 0.433        |
|    clip_range           | 0.075        |
|    entropy_loss         | -3.75        |
|    explained_variance   | 0.7          |
|    learning_rate        | 0.00358      |
|    loss                 | 0.00125      |
|    n_updates            | 98           |
|    policy_gradient_loss | -0.0121      |
|    std                  | 0.844        |
|    value_loss           | 0.0202       |
------------------------------------------
Num timesteps: 984000
Best mean reward: -3.42 - Last mean reward per episode: -3.60
Num timesteps: 988000
Best mean reward: -3.42 - Last mean reward per episode: -3.33

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_190438_numTimesteps_988000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 992000
Best mean reward: -3.33 - Last mean reward per episode: -3.16

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_190444_numTimesteps_992000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 996000
Best mean reward: -3.16 - Last mean reward per episode: -3.28
Num timesteps: 1000000
Best mean reward: -3.16 - Last mean reward per episode: -3.49
Num timesteps: 1004000
Best mean reward: -3.16 - Last mean reward per episode: -3.40
Num timesteps: 1008000
Best mean reward: -3.16 - Last mean reward per episode: -3.32
Num timesteps: 1012000
Best mean reward: -3.16 - Last mean reward per episode: -3.37
Num timesteps: 1016000
Best mean reward: -3.16 - Last mean reward per episode: -3.46
Num timesteps: 1020000
Best mean reward: -3.16 - Last mean reward per episode: -3.39
Num timesteps: 1024000
Best mean reward: -3.16 - Last mean reward per episode: -3.40
Num timesteps: 1028000
Best mean reward: -3.16 - Last mean reward per episode: -3.47
Num timesteps: 1032000
Best mean reward: -3.16 - Last mean reward per episode: -3.31
Num timesteps: 1036000
Best mean reward: -3.16 - Last mean reward per episode: -3.33
Num timesteps: 1040000
Best mean reward: -3.16 - Last mean reward per episode: -3.37
Num timesteps: 1044000
Best mean reward: -3.16 - Last mean reward per episode: -3.36
Num timesteps: 1048000
Best mean reward: -3.16 - Last mean reward per episode: -3.35
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 50          |
|    ep_rew_mean          | -3.35       |
| time/                   |             |
|    fps                  | 452         |
|    iterations           | 16          |
|    time_elapsed         | 2319        |
|    total_timesteps      | 1048576     |
| train/                  |             |
|    approx_kl            | 0.005248243 |
|    clip_fraction        | 0.444       |
|    clip_range           | 0.075       |
|    entropy_loss         | -3.69       |
|    explained_variance   | 0.703       |
|    learning_rate        | 0.00358     |
|    loss                 | 0.00158     |
|    n_updates            | 105         |
|    policy_gradient_loss | -0.0109     |
|    std                  | 0.828       |
|    value_loss           | 0.0166      |
-----------------------------------------
Num timesteps: 1052000
Best mean reward: -3.16 - Last mean reward per episode: -3.19
Num timesteps: 1056000
Best mean reward: -3.16 - Last mean reward per episode: -3.18
Num timesteps: 1060000
Best mean reward: -3.16 - Last mean reward per episode: -3.22
Num timesteps: 1064000
Best mean reward: -3.16 - Last mean reward per episode: -3.22
Num timesteps: 1068000
Best mean reward: -3.16 - Last mean reward per episode: -3.20
Num timesteps: 1072000
Best mean reward: -3.16 - Last mean reward per episode: -3.17
Num timesteps: 1076000
Best mean reward: -3.16 - Last mean reward per episode: -3.28
Num timesteps: 1080000
Best mean reward: -3.16 - Last mean reward per episode: -3.21
Num timesteps: 1084000
Best mean reward: -3.16 - Last mean reward per episode: -3.06

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_190736_numTimesteps_1084000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 1088000
Best mean reward: -3.06 - Last mean reward per episode: -3.10
Num timesteps: 1092000
Best mean reward: -3.06 - Last mean reward per episode: -3.11
Num timesteps: 1096000
Best mean reward: -3.06 - Last mean reward per episode: -3.13
Num timesteps: 1100000
Best mean reward: -3.06 - Last mean reward per episode: -3.18
Num timesteps: 1104000
Best mean reward: -3.06 - Last mean reward per episode: -3.22
Num timesteps: 1108000
Best mean reward: -3.06 - Last mean reward per episode: -3.10
Num timesteps: 1112000
Best mean reward: -3.06 - Last mean reward per episode: -3.23
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 50           |
|    ep_rew_mean          | -3.19        |
| time/                   |              |
|    fps                  | 458          |
|    iterations           | 17           |
|    time_elapsed         | 2431         |
|    total_timesteps      | 1114112      |
| train/                  |              |
|    approx_kl            | 0.0053749364 |
|    clip_fraction        | 0.459        |
|    clip_range           | 0.075        |
|    entropy_loss         | -3.63        |
|    explained_variance   | 0.678        |
|    learning_rate        | 0.00358      |
|    loss                 | -0.00406     |
|    n_updates            | 112          |
|    policy_gradient_loss | -0.00925     |
|    std                  | 0.811        |
|    value_loss           | 0.0128       |
------------------------------------------
Num timesteps: 1116000
Best mean reward: -3.06 - Last mean reward per episode: -3.07
Num timesteps: 1120000
Best mean reward: -3.06 - Last mean reward per episode: -2.99

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_190840_numTimesteps_1120000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 1124000
Best mean reward: -2.99 - Last mean reward per episode: -3.06
Num timesteps: 1128000
Best mean reward: -2.99 - Last mean reward per episode: -2.95

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_190852_numTimesteps_1128000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 1132000
Best mean reward: -2.95 - Last mean reward per episode: -3.00
Num timesteps: 1136000
Best mean reward: -2.95 - Last mean reward per episode: -3.05
Num timesteps: 1140000
Best mean reward: -2.95 - Last mean reward per episode: -2.88

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_190911_numTimesteps_1140000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 1144000
Best mean reward: -2.88 - Last mean reward per episode: -2.94
Num timesteps: 1148000
Best mean reward: -2.88 - Last mean reward per episode: -3.05
Num timesteps: 1152000
Best mean reward: -2.88 - Last mean reward per episode: -3.06
Num timesteps: 1156000
Best mean reward: -2.88 - Last mean reward per episode: -3.18
Num timesteps: 1160000
Best mean reward: -2.88 - Last mean reward per episode: -3.03
Num timesteps: 1164000
Best mean reward: -2.88 - Last mean reward per episode: -2.96
Num timesteps: 1168000
Best mean reward: -2.88 - Last mean reward per episode: -2.90
Num timesteps: 1172000
Best mean reward: -2.88 - Last mean reward per episode: -2.90
Num timesteps: 1176000
Best mean reward: -2.88 - Last mean reward per episode: -3.01
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 50         |
|    ep_rew_mean          | -3.02      |
| time/                   |            |
|    fps                  | 464        |
|    iterations           | 18         |
|    time_elapsed         | 2541       |
|    total_timesteps      | 1179648    |
| train/                  |            |
|    approx_kl            | 0.00654398 |
|    clip_fraction        | 0.453      |
|    clip_range           | 0.075      |
|    entropy_loss         | -3.57      |
|    explained_variance   | 0.682      |
|    learning_rate        | 0.00358    |
|    loss                 | -0.00316   |
|    n_updates            | 119        |
|    policy_gradient_loss | -0.00984   |
|    std                  | 0.793      |
|    value_loss           | 0.00955    |
----------------------------------------
Num timesteps: 1180000
Best mean reward: -2.88 - Last mean reward per episode: -3.01
Num timesteps: 1184000
Best mean reward: -2.88 - Last mean reward per episode: -2.91
Num timesteps: 1188000
Best mean reward: -2.88 - Last mean reward per episode: -2.81

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_191037_numTimesteps_1188000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 1192000
Best mean reward: -2.81 - Last mean reward per episode: -2.88
Num timesteps: 1196000
Best mean reward: -2.81 - Last mean reward per episode: -2.85
Num timesteps: 1200000
Best mean reward: -2.81 - Last mean reward per episode: -2.86
Num timesteps: 1204000
Best mean reward: -2.81 - Last mean reward per episode: -2.96
Num timesteps: 1208000
Best mean reward: -2.81 - Last mean reward per episode: -2.89
Num timesteps: 1212000
Best mean reward: -2.81 - Last mean reward per episode: -2.83
Num timesteps: 1216000
Best mean reward: -2.81 - Last mean reward per episode: -2.90
Num timesteps: 1220000
Best mean reward: -2.81 - Last mean reward per episode: -2.89
Num timesteps: 1224000
Best mean reward: -2.81 - Last mean reward per episode: -2.81

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_191136_numTimesteps_1224000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 1228000
Best mean reward: -2.81 - Last mean reward per episode: -2.77

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_191143_numTimesteps_1228000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 1232000
Best mean reward: -2.77 - Last mean reward per episode: -2.89
Num timesteps: 1236000
Best mean reward: -2.77 - Last mean reward per episode: -2.84
Num timesteps: 1240000
Best mean reward: -2.77 - Last mean reward per episode: -2.79
Num timesteps: 1244000
Best mean reward: -2.77 - Last mean reward per episode: -2.84
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 50           |
|    ep_rew_mean          | -2.84        |
| time/                   |              |
|    fps                  | 467          |
|    iterations           | 19           |
|    time_elapsed         | 2665         |
|    total_timesteps      | 1245184      |
| train/                  |              |
|    approx_kl            | 0.0064632753 |
|    clip_fraction        | 0.463        |
|    clip_range           | 0.075        |
|    entropy_loss         | -3.5         |
|    explained_variance   | 0.687        |
|    learning_rate        | 0.00358      |
|    loss                 | -0.00588     |
|    n_updates            | 126          |
|    policy_gradient_loss | -0.00807     |
|    std                  | 0.777        |
|    value_loss           | 0.00748      |
------------------------------------------
Num timesteps: 1248000
Best mean reward: -2.77 - Last mean reward per episode: -2.81
Num timesteps: 1252000
Best mean reward: -2.77 - Last mean reward per episode: -2.72

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_191239_numTimesteps_1252000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 1256000
Best mean reward: -2.72 - Last mean reward per episode: -2.68

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_191247_numTimesteps_1256000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 1260000
Best mean reward: -2.68 - Last mean reward per episode: -2.73
Num timesteps: 1264000
Best mean reward: -2.68 - Last mean reward per episode: -2.76
Num timesteps: 1268000
Best mean reward: -2.68 - Last mean reward per episode: -2.72
Num timesteps: 1272000
Best mean reward: -2.68 - Last mean reward per episode: -2.73
Num timesteps: 1276000
Best mean reward: -2.68 - Last mean reward per episode: -2.69
Num timesteps: 1280000
Best mean reward: -2.68 - Last mean reward per episode: -2.71
Num timesteps: 1284000
Best mean reward: -2.68 - Last mean reward per episode: -2.77
Num timesteps: 1288000
Best mean reward: -2.68 - Last mean reward per episode: -2.73
Num timesteps: 1292000
Best mean reward: -2.68 - Last mean reward per episode: -2.66

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_191349_numTimesteps_1292000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 1296000
Best mean reward: -2.66 - Last mean reward per episode: -2.75
Num timesteps: 1300000
Best mean reward: -2.66 - Last mean reward per episode: -2.78
Num timesteps: 1304000
Best mean reward: -2.66 - Last mean reward per episode: -2.79
Num timesteps: 1308000
Best mean reward: -2.66 - Last mean reward per episode: -2.75
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 50          |
|    ep_rew_mean          | -2.77       |
| time/                   |             |
|    fps                  | 468         |
|    iterations           | 20          |
|    time_elapsed         | 2797        |
|    total_timesteps      | 1310720     |
| train/                  |             |
|    approx_kl            | 0.005293828 |
|    clip_fraction        | 0.429       |
|    clip_range           | 0.075       |
|    entropy_loss         | -3.44       |
|    explained_variance   | 0.668       |
|    learning_rate        | 0.00358     |
|    loss                 | -0.00541    |
|    n_updates            | 133         |
|    policy_gradient_loss | -0.0083     |
|    std                  | 0.761       |
|    value_loss           | 0.00669     |
-----------------------------------------
Num timesteps: 1312000
Best mean reward: -2.66 - Last mean reward per episode: -2.74
Num timesteps: 1316000
Best mean reward: -2.66 - Last mean reward per episode: -2.66
Num timesteps: 1320000
Best mean reward: -2.66 - Last mean reward per episode: -2.69
Num timesteps: 1324000
Best mean reward: -2.66 - Last mean reward per episode: -2.60

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_191507_numTimesteps_1324000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 1328000
Best mean reward: -2.60 - Last mean reward per episode: -2.50

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_191516_numTimesteps_1328000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 1332000
Best mean reward: -2.50 - Last mean reward per episode: -2.62
Num timesteps: 1336000
Best mean reward: -2.50 - Last mean reward per episode: -2.59
Num timesteps: 1340000
Best mean reward: -2.50 - Last mean reward per episode: -2.55
Num timesteps: 1344000
Best mean reward: -2.50 - Last mean reward per episode: -2.63
Num timesteps: 1348000
Best mean reward: -2.50 - Last mean reward per episode: -2.61
Num timesteps: 1352000
Best mean reward: -2.50 - Last mean reward per episode: -2.66
Num timesteps: 1356000
Best mean reward: -2.50 - Last mean reward per episode: -2.65
Num timesteps: 1360000
Best mean reward: -2.50 - Last mean reward per episode: -2.66
Num timesteps: 1364000
Best mean reward: -2.50 - Last mean reward per episode: -2.66
Num timesteps: 1368000
Best mean reward: -2.50 - Last mean reward per episode: -2.70
Num timesteps: 1372000
Best mean reward: -2.50 - Last mean reward per episode: -2.58
Num timesteps: 1376000
Best mean reward: -2.50 - Last mean reward per episode: -2.64
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 50           |
|    ep_rew_mean          | -2.65        |
| time/                   |              |
|    fps                  | 467          |
|    iterations           | 21           |
|    time_elapsed         | 2942         |
|    total_timesteps      | 1376256      |
| train/                  |              |
|    approx_kl            | 0.0068149697 |
|    clip_fraction        | 0.466        |
|    clip_range           | 0.075        |
|    entropy_loss         | -3.37        |
|    explained_variance   | 0.673        |
|    learning_rate        | 0.00358      |
|    loss                 | -0.00637     |
|    n_updates            | 140          |
|    policy_gradient_loss | -0.00652     |
|    std                  | 0.745        |
|    value_loss           | 0.00558      |
------------------------------------------
Num timesteps: 1380000
Best mean reward: -2.50 - Last mean reward per episode: -2.59
Num timesteps: 1384000
Best mean reward: -2.50 - Last mean reward per episode: -2.55
Num timesteps: 1388000
Best mean reward: -2.50 - Last mean reward per episode: -2.57
Num timesteps: 1392000
Best mean reward: -2.50 - Last mean reward per episode: -2.45

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_191728_numTimesteps_1392000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 1396000
Best mean reward: -2.45 - Last mean reward per episode: -2.50
Num timesteps: 1400000
Best mean reward: -2.45 - Last mean reward per episode: -2.48
Num timesteps: 1404000
Best mean reward: -2.45 - Last mean reward per episode: -2.56
Num timesteps: 1408000
Best mean reward: -2.45 - Last mean reward per episode: -2.60
Num timesteps: 1412000
Best mean reward: -2.45 - Last mean reward per episode: -2.51
Num timesteps: 1416000
Best mean reward: -2.45 - Last mean reward per episode: -2.52
Num timesteps: 1420000
Best mean reward: -2.45 - Last mean reward per episode: -2.53
Num timesteps: 1424000
Best mean reward: -2.45 - Last mean reward per episode: -2.50
Num timesteps: 1428000
Best mean reward: -2.45 - Last mean reward per episode: -2.54
Num timesteps: 1432000
Best mean reward: -2.45 - Last mean reward per episode: -2.56
Num timesteps: 1436000
Best mean reward: -2.45 - Last mean reward per episode: -2.53
Num timesteps: 1440000
Best mean reward: -2.45 - Last mean reward per episode: -2.50
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 50           |
|    ep_rew_mean          | -2.49        |
| time/                   |              |
|    fps                  | 470          |
|    iterations           | 22           |
|    time_elapsed         | 3062         |
|    total_timesteps      | 1441792      |
| train/                  |              |
|    approx_kl            | 0.0064589935 |
|    clip_fraction        | 0.427        |
|    clip_range           | 0.075        |
|    entropy_loss         | -3.31        |
|    explained_variance   | 0.664        |
|    learning_rate        | 0.00358      |
|    loss                 | -0.00351     |
|    n_updates            | 147          |
|    policy_gradient_loss | -0.00809     |
|    std                  | 0.728        |
|    value_loss           | 0.00467      |
------------------------------------------
Num timesteps: 1444000
Best mean reward: -2.45 - Last mean reward per episode: -2.50
Num timesteps: 1448000
Best mean reward: -2.45 - Last mean reward per episode: -2.43

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_191916_numTimesteps_1448000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 1452000
Best mean reward: -2.43 - Last mean reward per episode: -2.42

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_191923_numTimesteps_1452000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 1456000
Best mean reward: -2.42 - Last mean reward per episode: -2.39

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_191931_numTimesteps_1456000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 1460000
Best mean reward: -2.39 - Last mean reward per episode: -2.41
Num timesteps: 1464000
Best mean reward: -2.39 - Last mean reward per episode: -2.42
Num timesteps: 1468000
Best mean reward: -2.39 - Last mean reward per episode: -2.40
Num timesteps: 1472000
Best mean reward: -2.39 - Last mean reward per episode: -2.45
Num timesteps: 1476000
Best mean reward: -2.39 - Last mean reward per episode: -2.39
Num timesteps: 1480000
Best mean reward: -2.39 - Last mean reward per episode: -2.45
Num timesteps: 1484000
Best mean reward: -2.39 - Last mean reward per episode: -2.43
Num timesteps: 1488000
Best mean reward: -2.39 - Last mean reward per episode: -2.37

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_192033_numTimesteps_1488000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 1492000
Best mean reward: -2.37 - Last mean reward per episode: -2.40
Num timesteps: 1496000
Best mean reward: -2.37 - Last mean reward per episode: -2.51
Num timesteps: 1500000
Best mean reward: -2.37 - Last mean reward per episode: -2.49
Num timesteps: 1504000
Best mean reward: -2.37 - Last mean reward per episode: -2.42
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 50          |
|    ep_rew_mean          | -2.46       |
| time/                   |             |
|    fps                  | 471         |
|    iterations           | 23          |
|    time_elapsed         | 3196        |
|    total_timesteps      | 1507328     |
| train/                  |             |
|    approx_kl            | 0.012605489 |
|    clip_fraction        | 0.515       |
|    clip_range           | 0.075       |
|    entropy_loss         | -3.23       |
|    explained_variance   | 0.651       |
|    learning_rate        | 0.00358     |
|    loss                 | -0.00129    |
|    n_updates            | 154         |
|    policy_gradient_loss | -0.00137    |
|    std                  | 0.708       |
|    value_loss           | 0.00437     |
-----------------------------------------
Num timesteps: 1508000
Best mean reward: -2.37 - Last mean reward per episode: -2.42
Num timesteps: 1512000
Best mean reward: -2.37 - Last mean reward per episode: -2.31

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_192122_numTimesteps_1512000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 1516000
Best mean reward: -2.31 - Last mean reward per episode: -2.33
Num timesteps: 1520000
Best mean reward: -2.31 - Last mean reward per episode: -2.33
Num timesteps: 1524000
Best mean reward: -2.31 - Last mean reward per episode: -2.34
Num timesteps: 1528000
Best mean reward: -2.31 - Last mean reward per episode: -2.41
Num timesteps: 1532000
Best mean reward: -2.31 - Last mean reward per episode: -2.35
Num timesteps: 1536000
Best mean reward: -2.31 - Last mean reward per episode: -2.33
Num timesteps: 1540000
Best mean reward: -2.31 - Last mean reward per episode: -2.32
Num timesteps: 1544000
Best mean reward: -2.31 - Last mean reward per episode: -2.32
Num timesteps: 1548000
Best mean reward: -2.31 - Last mean reward per episode: -2.31
Num timesteps: 1552000
Best mean reward: -2.31 - Last mean reward per episode: -2.35
Num timesteps: 1556000
Best mean reward: -2.31 - Last mean reward per episode: -2.37
Num timesteps: 1560000
Best mean reward: -2.31 - Last mean reward per episode: -2.35
Num timesteps: 1564000
Best mean reward: -2.31 - Last mean reward per episode: -2.36
Num timesteps: 1568000
Best mean reward: -2.31 - Last mean reward per episode: -2.31
Num timesteps: 1572000
Best mean reward: -2.31 - Last mean reward per episode: -2.39
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 50          |
|    ep_rew_mean          | -2.39       |
| time/                   |             |
|    fps                  | 475         |
|    iterations           | 24          |
|    time_elapsed         | 3309        |
|    total_timesteps      | 1572864     |
| train/                  |             |
|    approx_kl            | 0.008584466 |
|    clip_fraction        | 0.458       |
|    clip_range           | 0.075       |
|    entropy_loss         | -3.15       |
|    explained_variance   | 0.68        |
|    learning_rate        | 0.00358     |
|    loss                 | -0.00617    |
|    n_updates            | 161         |
|    policy_gradient_loss | -0.00581    |
|    std                  | 0.691       |
|    value_loss           | 0.00322     |
-----------------------------------------
Num timesteps: 1576000
Best mean reward: -2.31 - Last mean reward per episode: -2.29

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_192320_numTimesteps_1576000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 1580000
Best mean reward: -2.29 - Last mean reward per episode: -2.25

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_192327_numTimesteps_1580000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 1584000
Best mean reward: -2.25 - Last mean reward per episode: -2.27
Num timesteps: 1588000
Best mean reward: -2.25 - Last mean reward per episode: -2.31
Num timesteps: 1592000
Best mean reward: -2.25 - Last mean reward per episode: -2.32
Num timesteps: 1596000
Best mean reward: -2.25 - Last mean reward per episode: -2.25
Num timesteps: 1600000
Best mean reward: -2.25 - Last mean reward per episode: -2.22

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_192406_numTimesteps_1600000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 1604000
Best mean reward: -2.22 - Last mean reward per episode: -2.22

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_192412_numTimesteps_1604000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 1608000
Best mean reward: -2.22 - Last mean reward per episode: -2.30
Num timesteps: 1612000
Best mean reward: -2.22 - Last mean reward per episode: -2.29
Num timesteps: 1616000
Best mean reward: -2.22 - Last mean reward per episode: -2.28
Num timesteps: 1620000
Best mean reward: -2.22 - Last mean reward per episode: -2.32
Num timesteps: 1624000
Best mean reward: -2.22 - Last mean reward per episode: -2.32
Num timesteps: 1628000
Best mean reward: -2.22 - Last mean reward per episode: -2.25
Num timesteps: 1632000
Best mean reward: -2.22 - Last mean reward per episode: -2.27
Num timesteps: 1636000
Best mean reward: -2.22 - Last mean reward per episode: -2.26
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 50          |
|    ep_rew_mean          | -2.25       |
| time/                   |             |
|    fps                  | 476         |
|    iterations           | 25          |
|    time_elapsed         | 3435        |
|    total_timesteps      | 1638400     |
| train/                  |             |
|    approx_kl            | 0.005886357 |
|    clip_fraction        | 0.507       |
|    clip_range           | 0.075       |
|    entropy_loss         | -3.07       |
|    explained_variance   | 0.649       |
|    learning_rate        | 0.00358     |
|    loss                 | 0.00207     |
|    n_updates            | 168         |
|    policy_gradient_loss | -0.000795   |
|    std                  | 0.673       |
|    value_loss           | 0.0031      |
-----------------------------------------
Num timesteps: 1640000
Best mean reward: -2.22 - Last mean reward per episode: -2.29
Num timesteps: 1644000
Best mean reward: -2.22 - Last mean reward per episode: -2.26
Num timesteps: 1648000
Best mean reward: -2.22 - Last mean reward per episode: -2.27
Num timesteps: 1652000
Best mean reward: -2.22 - Last mean reward per episode: -2.28
Num timesteps: 1656000
Best mean reward: -2.22 - Last mean reward per episode: -2.25
Num timesteps: 1660000
Best mean reward: -2.22 - Last mean reward per episode: -2.27
Num timesteps: 1664000
Best mean reward: -2.22 - Last mean reward per episode: -2.24
Num timesteps: 1668000
Best mean reward: -2.22 - Last mean reward per episode: -2.21

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_192608_numTimesteps_1668000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 1672000
Best mean reward: -2.21 - Last mean reward per episode: -2.24
Num timesteps: 1676000
Best mean reward: -2.21 - Last mean reward per episode: -2.25
Num timesteps: 1680000
Best mean reward: -2.21 - Last mean reward per episode: -2.26
Num timesteps: 1684000
Best mean reward: -2.21 - Last mean reward per episode: -2.29
Num timesteps: 1688000
Best mean reward: -2.21 - Last mean reward per episode: -2.20

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_192649_numTimesteps_1688000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 1692000
Best mean reward: -2.20 - Last mean reward per episode: -2.22
Num timesteps: 1696000
Best mean reward: -2.20 - Last mean reward per episode: -2.25
Num timesteps: 1700000
Best mean reward: -2.20 - Last mean reward per episode: -2.21
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 50         |
|    ep_rew_mean          | -2.18      |
| time/                   |            |
|    fps                  | 476        |
|    iterations           | 26         |
|    time_elapsed         | 3573       |
|    total_timesteps      | 1703936    |
| train/                  |            |
|    approx_kl            | 0.00592003 |
|    clip_fraction        | 0.524      |
|    clip_range           | 0.075      |
|    entropy_loss         | -2.99      |
|    explained_variance   | 0.665      |
|    learning_rate        | 0.00358    |
|    loss                 | -0.00576   |
|    n_updates            | 175        |
|    policy_gradient_loss | 0.000719   |
|    std                  | 0.656      |
|    value_loss           | 0.00269    |
----------------------------------------
Num timesteps: 1704000
Best mean reward: -2.20 - Last mean reward per episode: -2.20

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_192734_numTimesteps_1704000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 1708000
Best mean reward: -2.20 - Last mean reward per episode: -2.14

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_192739_numTimesteps_1708000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 1712000
Best mean reward: -2.14 - Last mean reward per episode: -2.13

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_192747_numTimesteps_1712000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 1716000
Best mean reward: -2.13 - Last mean reward per episode: -2.18
Num timesteps: 1720000
Best mean reward: -2.13 - Last mean reward per episode: -2.11

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_192759_numTimesteps_1720000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 1724000
Best mean reward: -2.11 - Last mean reward per episode: -2.17
Num timesteps: 1728000
Best mean reward: -2.11 - Last mean reward per episode: -2.17
Num timesteps: 1732000
Best mean reward: -2.11 - Last mean reward per episode: -2.11
Num timesteps: 1736000
Best mean reward: -2.11 - Last mean reward per episode: -2.19
Num timesteps: 1740000
Best mean reward: -2.11 - Last mean reward per episode: -2.23
Num timesteps: 1744000
Best mean reward: -2.11 - Last mean reward per episode: -2.18
Num timesteps: 1748000
Best mean reward: -2.11 - Last mean reward per episode: -2.19
Num timesteps: 1752000
Best mean reward: -2.11 - Last mean reward per episode: -2.17
Num timesteps: 1756000
Best mean reward: -2.11 - Last mean reward per episode: -2.21
Num timesteps: 1760000
Best mean reward: -2.11 - Last mean reward per episode: -2.19
Num timesteps: 1764000
Best mean reward: -2.11 - Last mean reward per episode: -2.20
Num timesteps: 1768000
Best mean reward: -2.11 - Last mean reward per episode: -2.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 50          |
|    ep_rew_mean          | -2.21       |
| time/                   |             |
|    fps                  | 478         |
|    iterations           | 27          |
|    time_elapsed         | 3699        |
|    total_timesteps      | 1769472     |
| train/                  |             |
|    approx_kl            | 0.016589254 |
|    clip_fraction        | 0.548       |
|    clip_range           | 0.075       |
|    entropy_loss         | -2.91       |
|    explained_variance   | 0.648       |
|    learning_rate        | 0.00358     |
|    loss                 | 0.0221      |
|    n_updates            | 182         |
|    policy_gradient_loss | 0.00408     |
|    std                  | 0.636       |
|    value_loss           | 0.00255     |
-----------------------------------------
Num timesteps: 1772000
Best mean reward: -2.11 - Last mean reward per episode: -2.15
Num timesteps: 1776000
Best mean reward: -2.11 - Last mean reward per episode: -2.06

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_192951_numTimesteps_1776000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 1780000
Best mean reward: -2.06 - Last mean reward per episode: -2.06

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_193000_numTimesteps_1780000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 1784000
Best mean reward: -2.06 - Last mean reward per episode: -2.11
Num timesteps: 1788000
Best mean reward: -2.06 - Last mean reward per episode: -2.08
Num timesteps: 1792000
Best mean reward: -2.06 - Last mean reward per episode: -2.08
Num timesteps: 1796000
Best mean reward: -2.06 - Last mean reward per episode: -2.11
Num timesteps: 1800000
Best mean reward: -2.06 - Last mean reward per episode: -2.08
Num timesteps: 1804000
Best mean reward: -2.06 - Last mean reward per episode: -2.11
Num timesteps: 1808000
Best mean reward: -2.06 - Last mean reward per episode: -2.07
Num timesteps: 1812000
Best mean reward: -2.06 - Last mean reward per episode: -2.06

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_193051_numTimesteps_1812000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 1816000
Best mean reward: -2.06 - Last mean reward per episode: -2.08
Num timesteps: 1820000
Best mean reward: -2.06 - Last mean reward per episode: -2.12
Num timesteps: 1824000
Best mean reward: -2.06 - Last mean reward per episode: -2.11
Num timesteps: 1828000
Best mean reward: -2.06 - Last mean reward per episode: -2.10
Num timesteps: 1832000
Best mean reward: -2.06 - Last mean reward per episode: -2.12
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 50          |
|    ep_rew_mean          | -2.1        |
| time/                   |             |
|    fps                  | 480         |
|    iterations           | 28          |
|    time_elapsed         | 3816        |
|    total_timesteps      | 1835008     |
| train/                  |             |
|    approx_kl            | 0.009122162 |
|    clip_fraction        | 0.515       |
|    clip_range           | 0.075       |
|    entropy_loss         | -2.83       |
|    explained_variance   | 0.656       |
|    learning_rate        | 0.00358     |
|    loss                 | 0.00592     |
|    n_updates            | 189         |
|    policy_gradient_loss | -0.000458   |
|    std                  | 0.62        |
|    value_loss           | 0.00226     |
-----------------------------------------
Num timesteps: 1836000
Best mean reward: -2.06 - Last mean reward per episode: -2.08
Num timesteps: 1840000
Best mean reward: -2.06 - Last mean reward per episode: -2.07
Num timesteps: 1844000
Best mean reward: -2.06 - Last mean reward per episode: -2.04

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_193149_numTimesteps_1844000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 1848000
Best mean reward: -2.04 - Last mean reward per episode: -2.04
Num timesteps: 1852000
Best mean reward: -2.04 - Last mean reward per episode: -1.98

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_193202_numTimesteps_1852000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 1856000
Best mean reward: -1.98 - Last mean reward per episode: -1.99
Num timesteps: 1860000
Best mean reward: -1.98 - Last mean reward per episode: -2.06
Num timesteps: 1864000
Best mean reward: -1.98 - Last mean reward per episode: -2.04
Num timesteps: 1868000
Best mean reward: -1.98 - Last mean reward per episode: -2.01
Num timesteps: 1872000
Best mean reward: -1.98 - Last mean reward per episode: -2.02
Num timesteps: 1876000
Best mean reward: -1.98 - Last mean reward per episode: -1.98
Num timesteps: 1880000
Best mean reward: -1.98 - Last mean reward per episode: -2.04
Num timesteps: 1884000
Best mean reward: -1.98 - Last mean reward per episode: -2.04
Num timesteps: 1888000
Best mean reward: -1.98 - Last mean reward per episode: -2.06
Num timesteps: 1892000
Best mean reward: -1.98 - Last mean reward per episode: -2.06
Num timesteps: 1896000
Best mean reward: -1.98 - Last mean reward per episode: -2.01
Num timesteps: 1900000
Best mean reward: -1.98 - Last mean reward per episode: -2.05
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 50          |
|    ep_rew_mean          | -2.05       |
| time/                   |             |
|    fps                  | 482         |
|    iterations           | 29          |
|    time_elapsed         | 3938        |
|    total_timesteps      | 1900544     |
| train/                  |             |
|    approx_kl            | 0.006872924 |
|    clip_fraction        | 0.478       |
|    clip_range           | 0.075       |
|    entropy_loss         | -2.75       |
|    explained_variance   | 0.67        |
|    learning_rate        | 0.00358     |
|    loss                 | -0.00395    |
|    n_updates            | 196         |
|    policy_gradient_loss | -0.00225    |
|    std                  | 0.604       |
|    value_loss           | 0.00198     |
-----------------------------------------
Num timesteps: 1904000
Best mean reward: -1.98 - Last mean reward per episode: -2.01
Num timesteps: 1908000
Best mean reward: -1.98 - Last mean reward per episode: -1.95

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_193355_numTimesteps_1908000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 1912000
Best mean reward: -1.95 - Last mean reward per episode: -2.00
Num timesteps: 1916000
Best mean reward: -1.95 - Last mean reward per episode: -1.98
Num timesteps: 1920000
Best mean reward: -1.95 - Last mean reward per episode: -1.95

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_193421_numTimesteps_1920000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 1924000
Best mean reward: -1.95 - Last mean reward per episode: -2.00
Num timesteps: 1928000
Best mean reward: -1.95 - Last mean reward per episode: -1.98
Num timesteps: 1932000
Best mean reward: -1.95 - Last mean reward per episode: -1.94

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_193442_numTimesteps_1932000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 1936000
Best mean reward: -1.94 - Last mean reward per episode: -2.01
Num timesteps: 1940000
Best mean reward: -1.94 - Last mean reward per episode: -1.98
Num timesteps: 1944000
Best mean reward: -1.94 - Last mean reward per episode: -1.96
Num timesteps: 1948000
Best mean reward: -1.94 - Last mean reward per episode: -1.98
Num timesteps: 1952000
Best mean reward: -1.94 - Last mean reward per episode: -1.99
Num timesteps: 1956000
Best mean reward: -1.94 - Last mean reward per episode: -1.94
Num timesteps: 1960000
Best mean reward: -1.94 - Last mean reward per episode: -1.99
Num timesteps: 1964000
Best mean reward: -1.94 - Last mean reward per episode: -1.94
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 50           |
|    ep_rew_mean          | -1.95        |
| time/                   |              |
|    fps                  | 482          |
|    iterations           | 30           |
|    time_elapsed         | 4078         |
|    total_timesteps      | 1966080      |
| train/                  |              |
|    approx_kl            | 0.0095763635 |
|    clip_fraction        | 0.535        |
|    clip_range           | 0.075        |
|    entropy_loss         | -2.66        |
|    explained_variance   | 0.662        |
|    learning_rate        | 0.00358      |
|    loss                 | -0.00465     |
|    n_updates            | 203          |
|    policy_gradient_loss | 0.00301      |
|    std                  | 0.587        |
|    value_loss           | 0.00184      |
------------------------------------------
Num timesteps: 1968000
Best mean reward: -1.94 - Last mean reward per episode: -1.95
Num timesteps: 1972000
Best mean reward: -1.94 - Last mean reward per episode: -1.93

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_193611_numTimesteps_1972000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 1976000
Best mean reward: -1.93 - Last mean reward per episode: -1.90

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_193617_numTimesteps_1976000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 1980000
Best mean reward: -1.90 - Last mean reward per episode: -1.94
Num timesteps: 1984000
Best mean reward: -1.90 - Last mean reward per episode: -1.92
Num timesteps: 1988000
Best mean reward: -1.90 - Last mean reward per episode: -1.95
Num timesteps: 1992000
Best mean reward: -1.90 - Last mean reward per episode: -1.95
Num timesteps: 1996000
Best mean reward: -1.90 - Last mean reward per episode: -1.90

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_193653_numTimesteps_1996000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 2000000
Best mean reward: -1.90 - Last mean reward per episode: -1.93
Num timesteps: 2004000
Best mean reward: -1.90 - Last mean reward per episode: -1.92
Num timesteps: 2008000
Best mean reward: -1.90 - Last mean reward per episode: -1.94
Num timesteps: 2012000
Best mean reward: -1.90 - Last mean reward per episode: -1.94
Num timesteps: 2016000
Best mean reward: -1.90 - Last mean reward per episode: -1.94
Num timesteps: 2020000
Best mean reward: -1.90 - Last mean reward per episode: -1.88

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_193733_numTimesteps_2020000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 2024000
Best mean reward: -1.88 - Last mean reward per episode: -1.91
Num timesteps: 2028000
Best mean reward: -1.88 - Last mean reward per episode: -1.93
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 50          |
|    ep_rew_mean          | -1.91       |
| time/                   |             |
|    fps                  | 483         |
|    iterations           | 31          |
|    time_elapsed         | 4202        |
|    total_timesteps      | 2031616     |
| train/                  |             |
|    approx_kl            | 0.010170439 |
|    clip_fraction        | 0.545       |
|    clip_range           | 0.075       |
|    entropy_loss         | -2.57       |
|    explained_variance   | 0.67        |
|    learning_rate        | 0.00358     |
|    loss                 | 0.00188     |
|    n_updates            | 210         |
|    policy_gradient_loss | 0.00333     |
|    std                  | 0.571       |
|    value_loss           | 0.00175     |
-----------------------------------------
Num timesteps: 2032000
Best mean reward: -1.88 - Last mean reward per episode: -1.89
Num timesteps: 2036000
Best mean reward: -1.88 - Last mean reward per episode: -1.87

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_193814_numTimesteps_2036000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 2040000
Best mean reward: -1.87 - Last mean reward per episode: -1.89
Num timesteps: 2044000
Best mean reward: -1.87 - Last mean reward per episode: -1.88
Num timesteps: 2048000
Best mean reward: -1.87 - Last mean reward per episode: -1.88
Num timesteps: 2052000
Best mean reward: -1.87 - Last mean reward per episode: -1.88
Num timesteps: 2056000
Best mean reward: -1.87 - Last mean reward per episode: -1.87
Num timesteps: 2060000
Best mean reward: -1.87 - Last mean reward per episode: -1.87

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_193907_numTimesteps_2060000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 2064000
Best mean reward: -1.87 - Last mean reward per episode: -1.86

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_193915_numTimesteps_2064000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 2068000
Best mean reward: -1.86 - Last mean reward per episode: -1.89
Num timesteps: 2072000
Best mean reward: -1.86 - Last mean reward per episode: -1.89
Num timesteps: 2076000
Best mean reward: -1.86 - Last mean reward per episode: -1.86
Num timesteps: 2080000
Best mean reward: -1.86 - Last mean reward per episode: -1.87
Num timesteps: 2084000
Best mean reward: -1.86 - Last mean reward per episode: -1.86

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_193945_numTimesteps_2084000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 2088000
Best mean reward: -1.86 - Last mean reward per episode: -1.85

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_193951_numTimesteps_2088000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 2092000
Best mean reward: -1.85 - Last mean reward per episode: -1.90
Num timesteps: 2096000
Best mean reward: -1.85 - Last mean reward per episode: -1.87
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 50          |
|    ep_rew_mean          | -1.85       |
| time/                   |             |
|    fps                  | 483         |
|    iterations           | 32          |
|    time_elapsed         | 4333        |
|    total_timesteps      | 2097152     |
| train/                  |             |
|    approx_kl            | 0.010480758 |
|    clip_fraction        | 0.604       |
|    clip_range           | 0.075       |
|    entropy_loss         | -2.48       |
|    explained_variance   | 0.651       |
|    learning_rate        | 0.00358     |
|    loss                 | 0.00533     |
|    n_updates            | 217         |
|    policy_gradient_loss | 0.0124      |
|    std                  | 0.554       |
|    value_loss           | 0.00171     |
-----------------------------------------
Num timesteps: 2100000
Best mean reward: -1.85 - Last mean reward per episode: -1.84

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_194021_numTimesteps_2100000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 2104000
Best mean reward: -1.84 - Last mean reward per episode: -1.84
Num timesteps: 2108000
Best mean reward: -1.84 - Last mean reward per episode: -1.82

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_194039_numTimesteps_2108000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 2112000
Best mean reward: -1.82 - Last mean reward per episode: -1.86
Num timesteps: 2116000
Best mean reward: -1.82 - Last mean reward per episode: -1.87
Num timesteps: 2120000
Best mean reward: -1.82 - Last mean reward per episode: -1.84
Num timesteps: 2124000
Best mean reward: -1.82 - Last mean reward per episode: -1.83
Num timesteps: 2128000
Best mean reward: -1.82 - Last mean reward per episode: -1.84
Num timesteps: 2132000
Best mean reward: -1.82 - Last mean reward per episode: -1.85
Num timesteps: 2136000
Best mean reward: -1.82 - Last mean reward per episode: -1.84
Num timesteps: 2140000
Best mean reward: -1.82 - Last mean reward per episode: -1.79

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_194143_numTimesteps_2140000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 2144000
Best mean reward: -1.79 - Last mean reward per episode: -1.83
Num timesteps: 2148000
Best mean reward: -1.79 - Last mean reward per episode: -1.82
Num timesteps: 2152000
Best mean reward: -1.79 - Last mean reward per episode: -1.82
Num timesteps: 2156000
Best mean reward: -1.79 - Last mean reward per episode: -1.81
Num timesteps: 2160000
Best mean reward: -1.79 - Last mean reward per episode: -1.82
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 50          |
|    ep_rew_mean          | -1.83       |
| time/                   |             |
|    fps                  | 483         |
|    iterations           | 33          |
|    time_elapsed         | 4472        |
|    total_timesteps      | 2162688     |
| train/                  |             |
|    approx_kl            | 0.006765664 |
|    clip_fraction        | 0.488       |
|    clip_range           | 0.075       |
|    entropy_loss         | -2.39       |
|    explained_variance   | 0.635       |
|    learning_rate        | 0.00358     |
|    loss                 | -0.00556    |
|    n_updates            | 224         |
|    policy_gradient_loss | -0.000335   |
|    std                  | 0.538       |
|    value_loss           | 0.00154     |
-----------------------------------------
Num timesteps: 2164000
Best mean reward: -1.79 - Last mean reward per episode: -1.81
Num timesteps: 2168000
Best mean reward: -1.79 - Last mean reward per episode: -1.78

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_194242_numTimesteps_2168000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 2172000
Best mean reward: -1.78 - Last mean reward per episode: -1.81
Num timesteps: 2176000
Best mean reward: -1.78 - Last mean reward per episode: -1.82
Num timesteps: 2180000
Best mean reward: -1.78 - Last mean reward per episode: -1.77

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_194302_numTimesteps_2180000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 2184000
Best mean reward: -1.77 - Last mean reward per episode: -1.80
Num timesteps: 2188000
Best mean reward: -1.77 - Last mean reward per episode: -1.80
Num timesteps: 2192000
Best mean reward: -1.77 - Last mean reward per episode: -1.81
Num timesteps: 2196000
Best mean reward: -1.77 - Last mean reward per episode: -1.76

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_194330_numTimesteps_2196000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 2200000
Best mean reward: -1.76 - Last mean reward per episode: -1.78
Num timesteps: 2204000
Best mean reward: -1.76 - Last mean reward per episode: -1.79
Num timesteps: 2208000
Best mean reward: -1.76 - Last mean reward per episode: -1.80
Num timesteps: 2212000
Best mean reward: -1.76 - Last mean reward per episode: -1.78
Num timesteps: 2216000
Best mean reward: -1.76 - Last mean reward per episode: -1.78
Num timesteps: 2220000
Best mean reward: -1.76 - Last mean reward per episode: -1.77
Num timesteps: 2224000
Best mean reward: -1.76 - Last mean reward per episode: -1.80
Num timesteps: 2228000
Best mean reward: -1.76 - Last mean reward per episode: -1.80
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 50           |
|    ep_rew_mean          | -1.8         |
| time/                   |              |
|    fps                  | 484          |
|    iterations           | 34           |
|    time_elapsed         | 4599         |
|    total_timesteps      | 2228224      |
| train/                  |              |
|    approx_kl            | 0.0091728205 |
|    clip_fraction        | 0.593        |
|    clip_range           | 0.075        |
|    entropy_loss         | -2.3         |
|    explained_variance   | 0.643        |
|    learning_rate        | 0.00358      |
|    loss                 | 0.000302     |
|    n_updates            | 231          |
|    policy_gradient_loss | 0.00937      |
|    std                  | 0.522        |
|    value_loss           | 0.00135      |
------------------------------------------
Num timesteps: 2232000
Best mean reward: -1.76 - Last mean reward per episode: -1.76

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_194451_numTimesteps_2232000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 2236000
Best mean reward: -1.76 - Last mean reward per episode: -1.75

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_194500_numTimesteps_2236000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 2240000
Best mean reward: -1.75 - Last mean reward per episode: -1.73

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_194510_numTimesteps_2240000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 2244000
Best mean reward: -1.73 - Last mean reward per episode: -1.72

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_194516_numTimesteps_2244000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 2248000
Best mean reward: -1.72 - Last mean reward per episode: -1.75
Num timesteps: 2252000
Best mean reward: -1.72 - Last mean reward per episode: -1.78
Num timesteps: 2256000
Best mean reward: -1.72 - Last mean reward per episode: -1.76
Num timesteps: 2260000
Best mean reward: -1.72 - Last mean reward per episode: -1.74
Num timesteps: 2264000
Best mean reward: -1.72 - Last mean reward per episode: -1.77
Num timesteps: 2268000
Best mean reward: -1.72 - Last mean reward per episode: -1.75
Num timesteps: 2272000
Best mean reward: -1.72 - Last mean reward per episode: -1.74
Num timesteps: 2276000
Best mean reward: -1.72 - Last mean reward per episode: -1.74
Num timesteps: 2280000
Best mean reward: -1.72 - Last mean reward per episode: -1.71

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_194623_numTimesteps_2280000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 2284000
Best mean reward: -1.71 - Last mean reward per episode: -1.75
Num timesteps: 2288000
Best mean reward: -1.71 - Last mean reward per episode: -1.74
Num timesteps: 2292000
Best mean reward: -1.71 - Last mean reward per episode: -1.75
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 50          |
|    ep_rew_mean          | -1.74       |
| time/                   |             |
|    fps                  | 484         |
|    iterations           | 35          |
|    time_elapsed         | 4736        |
|    total_timesteps      | 2293760     |
| train/                  |             |
|    approx_kl            | 0.016023695 |
|    clip_fraction        | 0.613       |
|    clip_range           | 0.075       |
|    entropy_loss         | -2.21       |
|    explained_variance   | 0.66        |
|    learning_rate        | 0.00358     |
|    loss                 | -0.00411    |
|    n_updates            | 238         |
|    policy_gradient_loss | 0.015       |
|    std                  | 0.506       |
|    value_loss           | 0.0013      |
-----------------------------------------
Num timesteps: 2296000
Best mean reward: -1.71 - Last mean reward per episode: -1.71
Num timesteps: 2300000
Best mean reward: -1.71 - Last mean reward per episode: -1.71

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_194709_numTimesteps_2300000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 2304000
Best mean reward: -1.71 - Last mean reward per episode: -1.66

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_194718_numTimesteps_2304000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 2308000
Best mean reward: -1.66 - Last mean reward per episode: -1.69
Num timesteps: 2312000
Best mean reward: -1.66 - Last mean reward per episode: -1.71
Num timesteps: 2316000
Best mean reward: -1.66 - Last mean reward per episode: -1.69
Num timesteps: 2320000
Best mean reward: -1.66 - Last mean reward per episode: -1.70
Num timesteps: 2324000
Best mean reward: -1.66 - Last mean reward per episode: -1.70
Num timesteps: 2328000
Best mean reward: -1.66 - Last mean reward per episode: -1.74
Num timesteps: 2332000
Best mean reward: -1.66 - Last mean reward per episode: -1.74
Num timesteps: 2336000
Best mean reward: -1.66 - Last mean reward per episode: -1.72
Num timesteps: 2340000
Best mean reward: -1.66 - Last mean reward per episode: -1.72
Num timesteps: 2344000
Best mean reward: -1.66 - Last mean reward per episode: -1.71
Num timesteps: 2348000
Best mean reward: -1.66 - Last mean reward per episode: -1.72
Num timesteps: 2352000
Best mean reward: -1.66 - Last mean reward per episode: -1.74
Num timesteps: 2356000
Best mean reward: -1.66 - Last mean reward per episode: -1.71
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 50          |
|    ep_rew_mean          | -1.72       |
| time/                   |             |
|    fps                  | 484         |
|    iterations           | 36          |
|    time_elapsed         | 4866        |
|    total_timesteps      | 2359296     |
| train/                  |             |
|    approx_kl            | 0.009391166 |
|    clip_fraction        | 0.592       |
|    clip_range           | 0.075       |
|    entropy_loss         | -2.12       |
|    explained_variance   | 0.667       |
|    learning_rate        | 0.00358     |
|    loss                 | -0.00342    |
|    n_updates            | 245         |
|    policy_gradient_loss | 0.0114      |
|    std                  | 0.492       |
|    value_loss           | 0.00117     |
-----------------------------------------
Num timesteps: 2360000
Best mean reward: -1.66 - Last mean reward per episode: -1.70
Num timesteps: 2364000
Best mean reward: -1.66 - Last mean reward per episode: -1.67
Num timesteps: 2368000
Best mean reward: -1.66 - Last mean reward per episode: -1.66
Num timesteps: 2372000
Best mean reward: -1.66 - Last mean reward per episode: -1.65

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_194930_numTimesteps_2372000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 2376000
Best mean reward: -1.65 - Last mean reward per episode: -1.69
Num timesteps: 2380000
Best mean reward: -1.65 - Last mean reward per episode: -1.67
Num timesteps: 2384000
Best mean reward: -1.65 - Last mean reward per episode: -1.66
Num timesteps: 2388000
Best mean reward: -1.65 - Last mean reward per episode: -1.69
Num timesteps: 2392000
Best mean reward: -1.65 - Last mean reward per episode: -1.68
Num timesteps: 2396000
Best mean reward: -1.65 - Last mean reward per episode: -1.66
Num timesteps: 2400000
Best mean reward: -1.65 - Last mean reward per episode: -1.64

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_195025_numTimesteps_2400000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 2404000
Best mean reward: -1.64 - Last mean reward per episode: -1.67
Num timesteps: 2408000
Best mean reward: -1.64 - Last mean reward per episode: -1.67
Num timesteps: 2412000
Best mean reward: -1.64 - Last mean reward per episode: -1.67
Num timesteps: 2416000
Best mean reward: -1.64 - Last mean reward per episode: -1.66
Num timesteps: 2420000
Best mean reward: -1.64 - Last mean reward per episode: -1.64

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_195058_numTimesteps_2420000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 2424000
Best mean reward: -1.64 - Last mean reward per episode: -1.66
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 50          |
|    ep_rew_mean          | -1.67       |
| time/                   |             |
|    fps                  | 485         |
|    iterations           | 37          |
|    time_elapsed         | 4995        |
|    total_timesteps      | 2424832     |
| train/                  |             |
|    approx_kl            | 0.015033252 |
|    clip_fraction        | 0.594       |
|    clip_range           | 0.075       |
|    entropy_loss         | -2.03       |
|    explained_variance   | 0.662       |
|    learning_rate        | 0.00358     |
|    loss                 | 0.00609     |
|    n_updates            | 252         |
|    policy_gradient_loss | 0.00957     |
|    std                  | 0.477       |
|    value_loss           | 0.00114     |
-----------------------------------------
Num timesteps: 2428000
Best mean reward: -1.64 - Last mean reward per episode: -1.64

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_195126_numTimesteps_2428000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 2432000
Best mean reward: -1.64 - Last mean reward per episode: -1.62

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_195133_numTimesteps_2432000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 2436000
Best mean reward: -1.62 - Last mean reward per episode: -1.61

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_195141_numTimesteps_2436000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 2440000
Best mean reward: -1.61 - Last mean reward per episode: -1.61

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_195147_numTimesteps_2440000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 2444000
Best mean reward: -1.61 - Last mean reward per episode: -1.60

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_195153_numTimesteps_2444000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 2448000
Best mean reward: -1.60 - Last mean reward per episode: -1.61
Num timesteps: 2452000
Best mean reward: -1.60 - Last mean reward per episode: -1.61
Num timesteps: 2456000
Best mean reward: -1.60 - Last mean reward per episode: -1.62
Num timesteps: 2460000
Best mean reward: -1.60 - Last mean reward per episode: -1.63
Num timesteps: 2464000
Best mean reward: -1.60 - Last mean reward per episode: -1.63
Num timesteps: 2468000
Best mean reward: -1.60 - Last mean reward per episode: -1.61
Num timesteps: 2472000
Best mean reward: -1.60 - Last mean reward per episode: -1.60
Num timesteps: 2476000
Best mean reward: -1.60 - Last mean reward per episode: -1.59

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_195250_numTimesteps_2476000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 2480000
Best mean reward: -1.59 - Last mean reward per episode: -1.60
Num timesteps: 2484000
Best mean reward: -1.59 - Last mean reward per episode: -1.62
Num timesteps: 2488000
Best mean reward: -1.59 - Last mean reward per episode: -1.65
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 50          |
|    ep_rew_mean          | -1.62       |
| time/                   |             |
|    fps                  | 485         |
|    iterations           | 38          |
|    time_elapsed         | 5126        |
|    total_timesteps      | 2490368     |
| train/                  |             |
|    approx_kl            | 0.015717216 |
|    clip_fraction        | 0.668       |
|    clip_range           | 0.075       |
|    entropy_loss         | -1.92       |
|    explained_variance   | 0.637       |
|    learning_rate        | 0.00358     |
|    loss                 | 0.00456     |
|    n_updates            | 259         |
|    policy_gradient_loss | 0.0236      |
|    std                  | 0.46        |
|    value_loss           | 0.00112     |
-----------------------------------------
Num timesteps: 2492000
Best mean reward: -1.59 - Last mean reward per episode: -1.57

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_195333_numTimesteps_2492000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 2496000
Best mean reward: -1.57 - Last mean reward per episode: -1.60
Num timesteps: 2500000
Best mean reward: -1.57 - Last mean reward per episode: -1.59
Num timesteps: 2504000
Best mean reward: -1.57 - Last mean reward per episode: -1.57

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_195400_numTimesteps_2504000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 2508000
Best mean reward: -1.57 - Last mean reward per episode: -1.59
Num timesteps: 2512000
Best mean reward: -1.57 - Last mean reward per episode: -1.61
Num timesteps: 2516000
Best mean reward: -1.57 - Last mean reward per episode: -1.57

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_195424_numTimesteps_2516000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 2520000
Best mean reward: -1.57 - Last mean reward per episode: -1.56

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_195430_numTimesteps_2520000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 2524000
Best mean reward: -1.56 - Last mean reward per episode: -1.57
Num timesteps: 2528000
Best mean reward: -1.56 - Last mean reward per episode: -1.56
Num timesteps: 2532000
Best mean reward: -1.56 - Last mean reward per episode: -1.58
Num timesteps: 2536000
Best mean reward: -1.56 - Last mean reward per episode: -1.60
Num timesteps: 2540000
Best mean reward: -1.56 - Last mean reward per episode: -1.58
Num timesteps: 2544000
Best mean reward: -1.56 - Last mean reward per episode: -1.59
Num timesteps: 2548000
Best mean reward: -1.56 - Last mean reward per episode: -1.59
Num timesteps: 2552000
Best mean reward: -1.56 - Last mean reward per episode: -1.58
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 50         |
|    ep_rew_mean          | -1.59      |
| time/                   |            |
|    fps                  | 485        |
|    iterations           | 39         |
|    time_elapsed         | 5264       |
|    total_timesteps      | 2555904    |
| train/                  |            |
|    approx_kl            | 0.01786221 |
|    clip_fraction        | 0.592      |
|    clip_range           | 0.075      |
|    entropy_loss         | -1.83      |
|    explained_variance   | 0.644      |
|    learning_rate        | 0.00358    |
|    loss                 | 0.0134     |
|    n_updates            | 266        |
|    policy_gradient_loss | 0.0106     |
|    std                  | 0.445      |
|    value_loss           | 0.0011     |
----------------------------------------
Num timesteps: 2556000
Best mean reward: -1.56 - Last mean reward per episode: -1.59
Num timesteps: 2560000
Best mean reward: -1.56 - Last mean reward per episode: -1.56

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_195557_numTimesteps_2560000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 2564000
Best mean reward: -1.56 - Last mean reward per episode: -1.54

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_195604_numTimesteps_2564000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 2568000
Best mean reward: -1.54 - Last mean reward per episode: -1.55
Num timesteps: 2572000
Best mean reward: -1.54 - Last mean reward per episode: -1.53

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_195616_numTimesteps_2572000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 2576000
Best mean reward: -1.53 - Last mean reward per episode: -1.54
Num timesteps: 2580000
Best mean reward: -1.53 - Last mean reward per episode: -1.52

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_195628_numTimesteps_2580000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 2584000
Best mean reward: -1.52 - Last mean reward per episode: -1.53
Num timesteps: 2588000
Best mean reward: -1.52 - Last mean reward per episode: -1.55
Num timesteps: 2592000
Best mean reward: -1.52 - Last mean reward per episode: -1.54
Num timesteps: 2596000
Best mean reward: -1.52 - Last mean reward per episode: -1.54
Num timesteps: 2600000
Best mean reward: -1.52 - Last mean reward per episode: -1.54
Num timesteps: 2604000
Best mean reward: -1.52 - Last mean reward per episode: -1.53
Num timesteps: 2608000
Best mean reward: -1.52 - Last mean reward per episode: -1.55
Num timesteps: 2612000
Best mean reward: -1.52 - Last mean reward per episode: -1.52
Num timesteps: 2616000
Best mean reward: -1.52 - Last mean reward per episode: -1.58
Num timesteps: 2620000
Best mean reward: -1.52 - Last mean reward per episode: -1.59
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 50          |
|    ep_rew_mean          | -1.57       |
| time/                   |             |
|    fps                  | 486         |
|    iterations           | 40          |
|    time_elapsed         | 5389        |
|    total_timesteps      | 2621440     |
| train/                  |             |
|    approx_kl            | 0.011659426 |
|    clip_fraction        | 0.599       |
|    clip_range           | 0.075       |
|    entropy_loss         | -1.72       |
|    explained_variance   | 0.676       |
|    learning_rate        | 0.00358     |
|    loss                 | 0.00583     |
|    n_updates            | 273         |
|    policy_gradient_loss | 0.00848     |
|    std                  | 0.43        |
|    value_loss           | 0.000963    |
-----------------------------------------
Num timesteps: 2624000
Best mean reward: -1.52 - Last mean reward per episode: -1.58
Num timesteps: 2628000
Best mean reward: -1.52 - Last mean reward per episode: -1.51

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_195805_numTimesteps_2628000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 2632000
Best mean reward: -1.51 - Last mean reward per episode: -1.53
Num timesteps: 2636000
Best mean reward: -1.51 - Last mean reward per episode: -1.53
Num timesteps: 2640000
Best mean reward: -1.51 - Last mean reward per episode: -1.52
Num timesteps: 2644000
Best mean reward: -1.51 - Last mean reward per episode: -1.52
Num timesteps: 2648000
Best mean reward: -1.51 - Last mean reward per episode: -1.49

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_195851_numTimesteps_2648000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 2652000
Best mean reward: -1.49 - Last mean reward per episode: -1.51
Num timesteps: 2656000
Best mean reward: -1.49 - Last mean reward per episode: -1.50
Num timesteps: 2660000
Best mean reward: -1.49 - Last mean reward per episode: -1.53
Num timesteps: 2664000
Best mean reward: -1.49 - Last mean reward per episode: -1.51
Num timesteps: 2668000
Best mean reward: -1.49 - Last mean reward per episode: -1.52
Num timesteps: 2672000
Best mean reward: -1.49 - Last mean reward per episode: -1.54
Num timesteps: 2676000
Best mean reward: -1.49 - Last mean reward per episode: -1.52
Num timesteps: 2680000
Best mean reward: -1.49 - Last mean reward per episode: -1.53
Num timesteps: 2684000
Best mean reward: -1.49 - Last mean reward per episode: -1.52
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 50          |
|    ep_rew_mean          | -1.53       |
| time/                   |             |
|    fps                  | 486         |
|    iterations           | 41          |
|    time_elapsed         | 5524        |
|    total_timesteps      | 2686976     |
| train/                  |             |
|    approx_kl            | 0.017216418 |
|    clip_fraction        | 0.59        |
|    clip_range           | 0.075       |
|    entropy_loss         | -1.62       |
|    explained_variance   | 0.674       |
|    learning_rate        | 0.00358     |
|    loss                 | 0.00721     |
|    n_updates            | 280         |
|    policy_gradient_loss | 0.0084      |
|    std                  | 0.416       |
|    value_loss           | 0.000905    |
-----------------------------------------
Num timesteps: 2688000
Best mean reward: -1.49 - Last mean reward per episode: -1.50
Num timesteps: 2692000
Best mean reward: -1.49 - Last mean reward per episode: -1.47

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_200015_numTimesteps_2692000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 2696000
Best mean reward: -1.47 - Last mean reward per episode: -1.47
Num timesteps: 2700000
Best mean reward: -1.47 - Last mean reward per episode: -1.49
Num timesteps: 2704000
Best mean reward: -1.47 - Last mean reward per episode: -1.50
Num timesteps: 2708000
Best mean reward: -1.47 - Last mean reward per episode: -1.50
Num timesteps: 2712000
Best mean reward: -1.47 - Last mean reward per episode: -1.52
Num timesteps: 2716000
Best mean reward: -1.47 - Last mean reward per episode: -1.50
Num timesteps: 2720000
Best mean reward: -1.47 - Last mean reward per episode: -1.47
Num timesteps: 2724000
Best mean reward: -1.47 - Last mean reward per episode: -1.49
Num timesteps: 2728000
Best mean reward: -1.47 - Last mean reward per episode: -1.49
Num timesteps: 2732000
Best mean reward: -1.47 - Last mean reward per episode: -1.50
Num timesteps: 2736000
Best mean reward: -1.47 - Last mean reward per episode: -1.46

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_200136_numTimesteps_2736000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 2740000
Best mean reward: -1.46 - Last mean reward per episode: -1.51
Num timesteps: 2744000
Best mean reward: -1.46 - Last mean reward per episode: -1.46

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_200152_numTimesteps_2744000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 2748000
Best mean reward: -1.46 - Last mean reward per episode: -1.48
Num timesteps: 2752000
Best mean reward: -1.46 - Last mean reward per episode: -1.49
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 50          |
|    ep_rew_mean          | -1.49       |
| time/                   |             |
|    fps                  | 486         |
|    iterations           | 42          |
|    time_elapsed         | 5660        |
|    total_timesteps      | 2752512     |
| train/                  |             |
|    approx_kl            | 0.016565869 |
|    clip_fraction        | 0.621       |
|    clip_range           | 0.075       |
|    entropy_loss         | -1.51       |
|    explained_variance   | 0.681       |
|    learning_rate        | 0.00358     |
|    loss                 | 0.0106      |
|    n_updates            | 287         |
|    policy_gradient_loss | 0.0122      |
|    std                  | 0.402       |
|    value_loss           | 0.00084     |
-----------------------------------------
Num timesteps: 2756000
Best mean reward: -1.46 - Last mean reward per episode: -1.49
Num timesteps: 2760000
Best mean reward: -1.46 - Last mean reward per episode: -1.46
Num timesteps: 2764000
Best mean reward: -1.46 - Last mean reward per episode: -1.47
Num timesteps: 2768000
Best mean reward: -1.46 - Last mean reward per episode: -1.45

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_200255_numTimesteps_2768000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 2772000
Best mean reward: -1.45 - Last mean reward per episode: -1.48
Num timesteps: 2776000
Best mean reward: -1.45 - Last mean reward per episode: -1.47
Num timesteps: 2780000
Best mean reward: -1.45 - Last mean reward per episode: -1.47
Num timesteps: 2784000
Best mean reward: -1.45 - Last mean reward per episode: -1.47
Num timesteps: 2788000
Best mean reward: -1.45 - Last mean reward per episode: -1.45

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_200330_numTimesteps_2788000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 2792000
Best mean reward: -1.45 - Last mean reward per episode: -1.46
Num timesteps: 2796000
Best mean reward: -1.45 - Last mean reward per episode: -1.47
Num timesteps: 2800000
Best mean reward: -1.45 - Last mean reward per episode: -1.49
Num timesteps: 2804000
Best mean reward: -1.45 - Last mean reward per episode: -1.47
Num timesteps: 2808000
Best mean reward: -1.45 - Last mean reward per episode: -1.44

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_200402_numTimesteps_2808000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 2812000
Best mean reward: -1.44 - Last mean reward per episode: -1.46
Num timesteps: 2816000
Best mean reward: -1.44 - Last mean reward per episode: -1.48
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 50          |
|    ep_rew_mean          | -1.47       |
| time/                   |             |
|    fps                  | 486         |
|    iterations           | 43          |
|    time_elapsed         | 5787        |
|    total_timesteps      | 2818048     |
| train/                  |             |
|    approx_kl            | 0.017515734 |
|    clip_fraction        | 0.661       |
|    clip_range           | 0.075       |
|    entropy_loss         | -1.41       |
|    explained_variance   | 0.686       |
|    learning_rate        | 0.00358     |
|    loss                 | 0.0216      |
|    n_updates            | 294         |
|    policy_gradient_loss | 0.0168      |
|    std                  | 0.389       |
|    value_loss           | 0.00077     |
-----------------------------------------
Num timesteps: 2820000
Best mean reward: -1.44 - Last mean reward per episode: -1.46
Num timesteps: 2824000
Best mean reward: -1.44 - Last mean reward per episode: -1.44

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_200437_numTimesteps_2824000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 2828000
Best mean reward: -1.44 - Last mean reward per episode: -1.43

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_200445_numTimesteps_2828000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 2832000
Best mean reward: -1.43 - Last mean reward per episode: -1.41

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_200452_numTimesteps_2832000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 2836000
Best mean reward: -1.41 - Last mean reward per episode: -1.45
Num timesteps: 2840000
Best mean reward: -1.41 - Last mean reward per episode: -1.44
Num timesteps: 2844000
Best mean reward: -1.41 - Last mean reward per episode: -1.45
Num timesteps: 2848000
Best mean reward: -1.41 - Last mean reward per episode: -1.44
Num timesteps: 2852000
Best mean reward: -1.41 - Last mean reward per episode: -1.43
Num timesteps: 2856000
Best mean reward: -1.41 - Last mean reward per episode: -1.41

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_200532_numTimesteps_2856000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 2860000
Best mean reward: -1.41 - Last mean reward per episode: -1.42
Num timesteps: 2864000
Best mean reward: -1.41 - Last mean reward per episode: -1.43
Num timesteps: 2868000
Best mean reward: -1.41 - Last mean reward per episode: -1.44
Num timesteps: 2872000
Best mean reward: -1.41 - Last mean reward per episode: -1.45
Num timesteps: 2876000
Best mean reward: -1.41 - Last mean reward per episode: -1.45
Num timesteps: 2880000
Best mean reward: -1.41 - Last mean reward per episode: -1.46
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 50          |
|    ep_rew_mean          | -1.43       |
| time/                   |             |
|    fps                  | 487         |
|    iterations           | 44          |
|    time_elapsed         | 5919        |
|    total_timesteps      | 2883584     |
| train/                  |             |
|    approx_kl            | 0.014106024 |
|    clip_fraction        | 0.652       |
|    clip_range           | 0.075       |
|    entropy_loss         | -1.31       |
|    explained_variance   | 0.657       |
|    learning_rate        | 0.00358     |
|    loss                 | -4.42e-06   |
|    n_updates            | 301         |
|    policy_gradient_loss | 0.0168      |
|    std                  | 0.377       |
|    value_loss           | 0.000769    |
-----------------------------------------
Num timesteps: 2884000
Best mean reward: -1.41 - Last mean reward per episode: -1.43
Num timesteps: 2888000
Best mean reward: -1.41 - Last mean reward per episode: -1.35

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_200649_numTimesteps_2888000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 2892000
Best mean reward: -1.35 - Last mean reward per episode: -1.35
Num timesteps: 2896000
Best mean reward: -1.35 - Last mean reward per episode: -1.38
Num timesteps: 2900000
Best mean reward: -1.35 - Last mean reward per episode: -1.38
Num timesteps: 2904000
Best mean reward: -1.35 - Last mean reward per episode: -1.36
Num timesteps: 2908000
Best mean reward: -1.35 - Last mean reward per episode: -1.37
Num timesteps: 2912000
Best mean reward: -1.35 - Last mean reward per episode: -1.38
Num timesteps: 2916000
Best mean reward: -1.35 - Last mean reward per episode: -1.37
Num timesteps: 2920000
Best mean reward: -1.35 - Last mean reward per episode: -1.37
Num timesteps: 2924000
Best mean reward: -1.35 - Last mean reward per episode: -1.37
Num timesteps: 2928000
Best mean reward: -1.35 - Last mean reward per episode: -1.36
Num timesteps: 2932000
Best mean reward: -1.35 - Last mean reward per episode: -1.37
Num timesteps: 2936000
Best mean reward: -1.35 - Last mean reward per episode: -1.37
Num timesteps: 2940000
Best mean reward: -1.35 - Last mean reward per episode: -1.35

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_200816_numTimesteps_2940000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 2944000
Best mean reward: -1.35 - Last mean reward per episode: -1.36
Num timesteps: 2948000
Best mean reward: -1.35 - Last mean reward per episode: -1.37
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 50          |
|    ep_rew_mean          | -1.36       |
| time/                   |             |
|    fps                  | 488         |
|    iterations           | 45          |
|    time_elapsed         | 6038        |
|    total_timesteps      | 2949120     |
| train/                  |             |
|    approx_kl            | 0.026300086 |
|    clip_fraction        | 0.667       |
|    clip_range           | 0.075       |
|    entropy_loss         | -1.21       |
|    explained_variance   | 0.694       |
|    learning_rate        | 0.00358     |
|    loss                 | 0.0184      |
|    n_updates            | 308         |
|    policy_gradient_loss | 0.0193      |
|    std                  | 0.363       |
|    value_loss           | 0.000689    |
-----------------------------------------
Num timesteps: 2952000
Best mean reward: -1.35 - Last mean reward per episode: -1.32

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_200844_numTimesteps_2952000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 2956000
Best mean reward: -1.32 - Last mean reward per episode: -1.30

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_200851_numTimesteps_2956000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 2960000
Best mean reward: -1.30 - Last mean reward per episode: -1.32
Num timesteps: 2964000
Best mean reward: -1.30 - Last mean reward per episode: -1.33
Num timesteps: 2968000
Best mean reward: -1.30 - Last mean reward per episode: -1.33
Num timesteps: 2972000
Best mean reward: -1.30 - Last mean reward per episode: -1.33
Num timesteps: 2976000
Best mean reward: -1.30 - Last mean reward per episode: -1.34
Num timesteps: 2980000
Best mean reward: -1.30 - Last mean reward per episode: -1.33
Num timesteps: 2984000
Best mean reward: -1.30 - Last mean reward per episode: -1.33
Num timesteps: 2988000
Best mean reward: -1.30 - Last mean reward per episode: -1.33
Num timesteps: 2992000
Best mean reward: -1.30 - Last mean reward per episode: -1.31
Num timesteps: 2996000
Best mean reward: -1.30 - Last mean reward per episode: -1.33
Num timesteps: 3000000
Best mean reward: -1.30 - Last mean reward per episode: -1.33
Num timesteps: 3004000
Best mean reward: -1.30 - Last mean reward per episode: -1.31
Num timesteps: 3008000
Best mean reward: -1.30 - Last mean reward per episode: -1.32
Num timesteps: 3012000
Best mean reward: -1.30 - Last mean reward per episode: -1.32
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 50         |
|    ep_rew_mean          | -1.32      |
| time/                   |            |
|    fps                  | 489        |
|    iterations           | 46         |
|    time_elapsed         | 6164       |
|    total_timesteps      | 3014656    |
| train/                  |            |
|    approx_kl            | 0.03389908 |
|    clip_fraction        | 0.658      |
|    clip_range           | 0.075      |
|    entropy_loss         | -1.09      |
|    explained_variance   | 0.691      |
|    learning_rate        | 0.00358    |
|    loss                 | 0.0273     |
|    n_updates            | 315        |
|    policy_gradient_loss | 0.0158     |
|    std                  | 0.348      |
|    value_loss           | 0.000692   |
----------------------------------------
Num timesteps: 3016000
Best mean reward: -1.30 - Last mean reward per episode: -1.33
Num timesteps: 3020000
Best mean reward: -1.30 - Last mean reward per episode: -1.30
Num timesteps: 3024000
Best mean reward: -1.30 - Last mean reward per episode: -1.29

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_201101_numTimesteps_3024000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 3028000
Best mean reward: -1.29 - Last mean reward per episode: -1.31
Num timesteps: 3032000
Best mean reward: -1.29 - Last mean reward per episode: -1.29
Num timesteps: 3036000
Best mean reward: -1.29 - Last mean reward per episode: -1.28

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_201121_numTimesteps_3036000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 3040000
Best mean reward: -1.28 - Last mean reward per episode: -1.28

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_201127_numTimesteps_3040000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 3044000
Best mean reward: -1.28 - Last mean reward per episode: -1.27

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_201134_numTimesteps_3044000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 3048000
Best mean reward: -1.27 - Last mean reward per episode: -1.28
Num timesteps: 3052000
Best mean reward: -1.27 - Last mean reward per episode: -1.29
Num timesteps: 3056000
Best mean reward: -1.27 - Last mean reward per episode: -1.30
Num timesteps: 3060000
Best mean reward: -1.27 - Last mean reward per episode: -1.28
Num timesteps: 3064000
Best mean reward: -1.27 - Last mean reward per episode: -1.28
Num timesteps: 3068000
Best mean reward: -1.27 - Last mean reward per episode: -1.29
Num timesteps: 3072000
Best mean reward: -1.27 - Last mean reward per episode: -1.30
Num timesteps: 3076000
Best mean reward: -1.27 - Last mean reward per episode: -1.28
Num timesteps: 3080000
Best mean reward: -1.27 - Last mean reward per episode: -1.30
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 50          |
|    ep_rew_mean          | -1.3        |
| time/                   |             |
|    fps                  | 489         |
|    iterations           | 47          |
|    time_elapsed         | 6296        |
|    total_timesteps      | 3080192     |
| train/                  |             |
|    approx_kl            | 0.039380148 |
|    clip_fraction        | 0.701       |
|    clip_range           | 0.075       |
|    entropy_loss         | -0.963      |
|    explained_variance   | 0.695       |
|    learning_rate        | 0.00358     |
|    loss                 | 0.0529      |
|    n_updates            | 322         |
|    policy_gradient_loss | 0.0261      |
|    std                  | 0.332       |
|    value_loss           | 0.0006      |
-----------------------------------------
Num timesteps: 3084000
Best mean reward: -1.27 - Last mean reward per episode: -1.25

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_201306_numTimesteps_3084000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 3088000
Best mean reward: -1.25 - Last mean reward per episode: -1.24

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_201314_numTimesteps_3088000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 3092000
Best mean reward: -1.24 - Last mean reward per episode: -1.23

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_201321_numTimesteps_3092000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 3096000
Best mean reward: -1.23 - Last mean reward per episode: -1.23

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_201328_numTimesteps_3096000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 3100000
Best mean reward: -1.23 - Last mean reward per episode: -1.23
Num timesteps: 3104000
Best mean reward: -1.23 - Last mean reward per episode: -1.26
Num timesteps: 3108000
Best mean reward: -1.23 - Last mean reward per episode: -1.23
Num timesteps: 3112000
Best mean reward: -1.23 - Last mean reward per episode: -1.23
Num timesteps: 3116000
Best mean reward: -1.23 - Last mean reward per episode: -1.22

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_201402_numTimesteps_3116000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 3120000
Best mean reward: -1.22 - Last mean reward per episode: -1.25
Num timesteps: 3124000
Best mean reward: -1.22 - Last mean reward per episode: -1.22

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_201416_numTimesteps_3124000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 3128000
Best mean reward: -1.22 - Last mean reward per episode: -1.24
Num timesteps: 3132000
Best mean reward: -1.22 - Last mean reward per episode: -1.24
Num timesteps: 3136000
Best mean reward: -1.22 - Last mean reward per episode: -1.24
Num timesteps: 3140000
Best mean reward: -1.22 - Last mean reward per episode: -1.24
Num timesteps: 3144000
Best mean reward: -1.22 - Last mean reward per episode: -1.24
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 50         |
|    ep_rew_mean          | -1.23      |
| time/                   |            |
|    fps                  | 489        |
|    iterations           | 48         |
|    time_elapsed         | 6425       |
|    total_timesteps      | 3145728    |
| train/                  |            |
|    approx_kl            | 0.07125339 |
|    clip_fraction        | 0.694      |
|    clip_range           | 0.075      |
|    entropy_loss         | -0.826     |
|    explained_variance   | 0.718      |
|    learning_rate        | 0.00358    |
|    loss                 | 0.0544     |
|    n_updates            | 329        |
|    policy_gradient_loss | 0.0247     |
|    std                  | 0.318      |
|    value_loss           | 0.000547   |
----------------------------------------
Num timesteps: 3148000
Best mean reward: -1.22 - Last mean reward per episode: -1.21

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_201512_numTimesteps_3148000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 3152000
Best mean reward: -1.21 - Last mean reward per episode: -1.16

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_201520_numTimesteps_3152000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 3156000
Best mean reward: -1.16 - Last mean reward per episode: -1.18
Num timesteps: 3160000
Best mean reward: -1.16 - Last mean reward per episode: -1.19
Num timesteps: 3164000
Best mean reward: -1.16 - Last mean reward per episode: -1.18
Num timesteps: 3168000
Best mean reward: -1.16 - Last mean reward per episode: -1.17
Num timesteps: 3172000
Best mean reward: -1.16 - Last mean reward per episode: -1.19
Num timesteps: 3176000
Best mean reward: -1.16 - Last mean reward per episode: -1.17
Num timesteps: 3180000
Best mean reward: -1.16 - Last mean reward per episode: -1.21
Num timesteps: 3184000
Best mean reward: -1.16 - Last mean reward per episode: -1.18
Num timesteps: 3188000
Best mean reward: -1.16 - Last mean reward per episode: -1.18
Num timesteps: 3192000
Best mean reward: -1.16 - Last mean reward per episode: -1.18
Num timesteps: 3196000
Best mean reward: -1.16 - Last mean reward per episode: -1.18
Num timesteps: 3200000
Best mean reward: -1.16 - Last mean reward per episode: -1.18
Num timesteps: 3204000
Best mean reward: -1.16 - Last mean reward per episode: -1.19
Num timesteps: 3208000
Best mean reward: -1.16 - Last mean reward per episode: -1.17
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 50          |
|    ep_rew_mean          | -1.18       |
| time/                   |             |
|    fps                  | 489         |
|    iterations           | 49          |
|    time_elapsed         | 6557        |
|    total_timesteps      | 3211264     |
| train/                  |             |
|    approx_kl            | 0.017041499 |
|    clip_fraction        | 0.689       |
|    clip_range           | 0.075       |
|    entropy_loss         | -0.69       |
|    explained_variance   | 0.705       |
|    learning_rate        | 0.00358     |
|    loss                 | -0.00243    |
|    n_updates            | 336         |
|    policy_gradient_loss | 0.021       |
|    std                  | 0.306       |
|    value_loss           | 0.00055     |
-----------------------------------------
Num timesteps: 3212000
Best mean reward: -1.16 - Last mean reward per episode: -1.19
Num timesteps: 3216000
Best mean reward: -1.16 - Last mean reward per episode: -1.16

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_201727_numTimesteps_3216000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 3220000
Best mean reward: -1.16 - Last mean reward per episode: -1.13

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_201735_numTimesteps_3220000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 3224000
Best mean reward: -1.13 - Last mean reward per episode: -1.12

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_201741_numTimesteps_3224000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 3228000
Best mean reward: -1.12 - Last mean reward per episode: -1.11

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_201751_numTimesteps_3228000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 3232000
Best mean reward: -1.11 - Last mean reward per episode: -1.13
Num timesteps: 3236000
Best mean reward: -1.11 - Last mean reward per episode: -1.13
Num timesteps: 3240000
Best mean reward: -1.11 - Last mean reward per episode: -1.15
Num timesteps: 3244000
Best mean reward: -1.11 - Last mean reward per episode: -1.16
Num timesteps: 3248000
Best mean reward: -1.11 - Last mean reward per episode: -1.14
Num timesteps: 3252000
Best mean reward: -1.11 - Last mean reward per episode: -1.17
Num timesteps: 3256000
Best mean reward: -1.11 - Last mean reward per episode: -1.14
Num timesteps: 3260000
Best mean reward: -1.11 - Last mean reward per episode: -1.14
Num timesteps: 3264000
Best mean reward: -1.11 - Last mean reward per episode: -1.13
Num timesteps: 3268000
Best mean reward: -1.11 - Last mean reward per episode: -1.14
Num timesteps: 3272000
Best mean reward: -1.11 - Last mean reward per episode: -1.14
Num timesteps: 3276000
Best mean reward: -1.11 - Last mean reward per episode: -1.15
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 50          |
|    ep_rew_mean          | -1.14       |
| time/                   |             |
|    fps                  | 489         |
|    iterations           | 50          |
|    time_elapsed         | 6688        |
|    total_timesteps      | 3276800     |
| train/                  |             |
|    approx_kl            | 0.019225474 |
|    clip_fraction        | 0.698       |
|    clip_range           | 0.075       |
|    entropy_loss         | -0.558      |
|    explained_variance   | 0.676       |
|    learning_rate        | 0.00358     |
|    loss                 | 0.0127      |
|    n_updates            | 343         |
|    policy_gradient_loss | 0.0192      |
|    std                  | 0.294       |
|    value_loss           | 0.000611    |
-----------------------------------------
Num timesteps: 3280000
Best mean reward: -1.11 - Last mean reward per episode: -1.13
Num timesteps: 3284000
Best mean reward: -1.11 - Last mean reward per episode: -1.12
Num timesteps: 3288000
Best mean reward: -1.11 - Last mean reward per episode: -1.07

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_201949_numTimesteps_3288000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 3292000
Best mean reward: -1.07 - Last mean reward per episode: -1.09
Num timesteps: 3296000
Best mean reward: -1.07 - Last mean reward per episode: -1.10
Num timesteps: 3300000
Best mean reward: -1.07 - Last mean reward per episode: -1.09
Num timesteps: 3304000
Best mean reward: -1.07 - Last mean reward per episode: -1.10
Num timesteps: 3308000
Best mean reward: -1.07 - Last mean reward per episode: -1.09
Num timesteps: 3312000
Best mean reward: -1.07 - Last mean reward per episode: -1.08
Num timesteps: 3316000
Best mean reward: -1.07 - Last mean reward per episode: -1.10
Num timesteps: 3320000
Best mean reward: -1.07 - Last mean reward per episode: -1.11
Num timesteps: 3324000
Best mean reward: -1.07 - Last mean reward per episode: -1.10
Num timesteps: 3328000
Best mean reward: -1.07 - Last mean reward per episode: -1.11
Num timesteps: 3332000
Best mean reward: -1.07 - Last mean reward per episode: -1.10
Num timesteps: 3336000
Best mean reward: -1.07 - Last mean reward per episode: -1.09
Num timesteps: 3340000
Best mean reward: -1.07 - Last mean reward per episode: -1.09
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 50          |
|    ep_rew_mean          | -1.09       |
| time/                   |             |
|    fps                  | 490         |
|    iterations           | 51          |
|    time_elapsed         | 6820        |
|    total_timesteps      | 3342336     |
| train/                  |             |
|    approx_kl            | 0.025545474 |
|    clip_fraction        | 0.721       |
|    clip_range           | 0.075       |
|    entropy_loss         | -0.43       |
|    explained_variance   | 0.695       |
|    learning_rate        | 0.00358     |
|    loss                 | -0.00322    |
|    n_updates            | 350         |
|    policy_gradient_loss | 0.0253      |
|    std                  | 0.281       |
|    value_loss           | 0.000642    |
-----------------------------------------
Num timesteps: 3344000
Best mean reward: -1.07 - Last mean reward per episode: -1.08
Num timesteps: 3348000
Best mean reward: -1.07 - Last mean reward per episode: -1.06

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_202151_numTimesteps_3348000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 3352000
Best mean reward: -1.06 - Last mean reward per episode: -1.06

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_202159_numTimesteps_3352000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 3356000
Best mean reward: -1.06 - Last mean reward per episode: -1.05

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_202206_numTimesteps_3356000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 3360000
Best mean reward: -1.05 - Last mean reward per episode: -1.06
Num timesteps: 3364000
Best mean reward: -1.05 - Last mean reward per episode: -1.07
Num timesteps: 3368000
Best mean reward: -1.05 - Last mean reward per episode: -1.07
Num timesteps: 3372000
Best mean reward: -1.05 - Last mean reward per episode: -1.06
Num timesteps: 3376000
Best mean reward: -1.05 - Last mean reward per episode: -1.04

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_202245_numTimesteps_3376000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 3380000
Best mean reward: -1.04 - Last mean reward per episode: -1.04

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_202255_numTimesteps_3380000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 3384000
Best mean reward: -1.04 - Last mean reward per episode: -1.07
Num timesteps: 3388000
Best mean reward: -1.04 - Last mean reward per episode: -1.06
Num timesteps: 3392000
Best mean reward: -1.04 - Last mean reward per episode: -1.05
Num timesteps: 3396000
Best mean reward: -1.04 - Last mean reward per episode: -1.08
Num timesteps: 3400000
Best mean reward: -1.04 - Last mean reward per episode: -1.07
Num timesteps: 3404000
Best mean reward: -1.04 - Last mean reward per episode: -1.06
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 50          |
|    ep_rew_mean          | -1.06       |
| time/                   |             |
|    fps                  | 488         |
|    iterations           | 52          |
|    time_elapsed         | 6971        |
|    total_timesteps      | 3407872     |
| train/                  |             |
|    approx_kl            | 0.030026706 |
|    clip_fraction        | 0.772       |
|    clip_range           | 0.075       |
|    entropy_loss         | -0.273      |
|    explained_variance   | 0.697       |
|    learning_rate        | 0.00358     |
|    loss                 | 0.0211      |
|    n_updates            | 357         |
|    policy_gradient_loss | 0.0427      |
|    std                  | 0.269       |
|    value_loss           | 0.000476    |
-----------------------------------------
Num timesteps: 3408000
Best mean reward: -1.04 - Last mean reward per episode: -1.06
Num timesteps: 3412000
Best mean reward: -1.04 - Last mean reward per episode: -1.01

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_202424_numTimesteps_3412000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 3416000
Best mean reward: -1.01 - Last mean reward per episode: -1.03
Num timesteps: 3420000
Best mean reward: -1.01 - Last mean reward per episode: -1.02
Num timesteps: 3424000
Best mean reward: -1.01 - Last mean reward per episode: -0.99

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_202447_numTimesteps_3424000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 3428000
Best mean reward: -0.99 - Last mean reward per episode: -1.00
Num timesteps: 3432000
Best mean reward: -0.99 - Last mean reward per episode: -1.02
Num timesteps: 3436000
Best mean reward: -0.99 - Last mean reward per episode: -1.00
Num timesteps: 3440000
Best mean reward: -0.99 - Last mean reward per episode: -1.01
Num timesteps: 3444000
Best mean reward: -0.99 - Last mean reward per episode: -1.04
Num timesteps: 3448000
Best mean reward: -0.99 - Last mean reward per episode: -1.02
Num timesteps: 3452000
Best mean reward: -0.99 - Last mean reward per episode: -1.01
Num timesteps: 3456000
Best mean reward: -0.99 - Last mean reward per episode: -1.01
Num timesteps: 3460000
Best mean reward: -0.99 - Last mean reward per episode: -1.03
Num timesteps: 3464000
Best mean reward: -0.99 - Last mean reward per episode: -1.02
Num timesteps: 3468000
Best mean reward: -0.99 - Last mean reward per episode: -1.03
Num timesteps: 3472000
Best mean reward: -0.99 - Last mean reward per episode: -1.02
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 50         |
|    ep_rew_mean          | -1.02      |
| time/                   |            |
|    fps                  | 488        |
|    iterations           | 53         |
|    time_elapsed         | 7113       |
|    total_timesteps      | 3473408    |
| train/                  |            |
|    approx_kl            | 0.04117762 |
|    clip_fraction        | 0.742      |
|    clip_range           | 0.075      |
|    entropy_loss         | -0.157     |
|    explained_variance   | 0.719      |
|    learning_rate        | 0.00358    |
|    loss                 | 0.00916    |
|    n_updates            | 364        |
|    policy_gradient_loss | 0.0304     |
|    std                  | 0.256      |
|    value_loss           | 0.000434   |
----------------------------------------
Num timesteps: 3476000
Best mean reward: -0.99 - Last mean reward per episode: -1.00
Num timesteps: 3480000
Best mean reward: -0.99 - Last mean reward per episode: -0.97

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_202646_numTimesteps_3480000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 3484000
Best mean reward: -0.97 - Last mean reward per episode: -0.99
Num timesteps: 3488000
Best mean reward: -0.97 - Last mean reward per episode: -0.98
Num timesteps: 3492000
Best mean reward: -0.97 - Last mean reward per episode: -0.98
Num timesteps: 3496000
Best mean reward: -0.97 - Last mean reward per episode: -0.99
Num timesteps: 3500000
Best mean reward: -0.97 - Last mean reward per episode: -0.96

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_202727_numTimesteps_3500000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 3504000
Best mean reward: -0.96 - Last mean reward per episode: -0.95

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_202736_numTimesteps_3504000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 3508000
Best mean reward: -0.95 - Last mean reward per episode: -0.96
Num timesteps: 3512000
Best mean reward: -0.95 - Last mean reward per episode: -0.97
Num timesteps: 3516000
Best mean reward: -0.95 - Last mean reward per episode: -0.99
Num timesteps: 3520000
Best mean reward: -0.95 - Last mean reward per episode: -0.97
Num timesteps: 3524000
Best mean reward: -0.95 - Last mean reward per episode: -1.00
Num timesteps: 3528000
Best mean reward: -0.95 - Last mean reward per episode: -0.99
Num timesteps: 3532000
Best mean reward: -0.95 - Last mean reward per episode: -0.98
Num timesteps: 3536000
Best mean reward: -0.95 - Last mean reward per episode: -0.97
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 50          |
|    ep_rew_mean          | -0.973      |
| time/                   |             |
|    fps                  | 488         |
|    iterations           | 54          |
|    time_elapsed         | 7251        |
|    total_timesteps      | 3538944     |
| train/                  |             |
|    approx_kl            | 0.034725375 |
|    clip_fraction        | 0.749       |
|    clip_range           | 0.075       |
|    entropy_loss         | -0.00385    |
|    explained_variance   | 0.744       |
|    learning_rate        | 0.00358     |
|    loss                 | 0.028       |
|    n_updates            | 371         |
|    policy_gradient_loss | 0.0315      |
|    std                  | 0.243       |
|    value_loss           | 0.000388    |
-----------------------------------------
Num timesteps: 3540000
Best mean reward: -0.95 - Last mean reward per episode: -0.97
Num timesteps: 3544000
Best mean reward: -0.95 - Last mean reward per episode: -0.93

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_202904_numTimesteps_3544000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 3548000
Best mean reward: -0.93 - Last mean reward per episode: -0.91

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_202911_numTimesteps_3548000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 3552000
Best mean reward: -0.91 - Last mean reward per episode: -0.94
Num timesteps: 3556000
Best mean reward: -0.91 - Last mean reward per episode: -0.93
Num timesteps: 3560000
Best mean reward: -0.91 - Last mean reward per episode: -0.93
Num timesteps: 3564000
Best mean reward: -0.91 - Last mean reward per episode: -0.92
Num timesteps: 3568000
Best mean reward: -0.91 - Last mean reward per episode: -0.93
Num timesteps: 3572000
Best mean reward: -0.91 - Last mean reward per episode: -0.92
Num timesteps: 3576000
Best mean reward: -0.91 - Last mean reward per episode: -0.93
Num timesteps: 3580000
Best mean reward: -0.91 - Last mean reward per episode: -0.92
Num timesteps: 3584000
Best mean reward: -0.91 - Last mean reward per episode: -0.93
Num timesteps: 3588000
Best mean reward: -0.91 - Last mean reward per episode: -0.94
Num timesteps: 3592000
Best mean reward: -0.91 - Last mean reward per episode: -0.94
Num timesteps: 3596000
Best mean reward: -0.91 - Last mean reward per episode: -0.92
Num timesteps: 3600000
Best mean reward: -0.91 - Last mean reward per episode: -0.93
Num timesteps: 3604000
Best mean reward: -0.91 - Last mean reward per episode: -0.94
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 50         |
|    ep_rew_mean          | -0.939     |
| time/                   |            |
|    fps                  | 487        |
|    iterations           | 55         |
|    time_elapsed         | 7390       |
|    total_timesteps      | 3604480    |
| train/                  |            |
|    approx_kl            | 0.12330837 |
|    clip_fraction        | 0.791      |
|    clip_range           | 0.075      |
|    entropy_loss         | 0.172      |
|    explained_variance   | 0.759      |
|    learning_rate        | 0.00358    |
|    loss                 | 0.0328     |
|    n_updates            | 378        |
|    policy_gradient_loss | 0.053      |
|    std                  | 0.23       |
|    value_loss           | 0.000545   |
----------------------------------------
Num timesteps: 3608000
Best mean reward: -0.91 - Last mean reward per episode: -0.91
Num timesteps: 3612000
Best mean reward: -0.91 - Last mean reward per episode: -0.91

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_203126_numTimesteps_3612000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 3616000
Best mean reward: -0.91 - Last mean reward per episode: -0.87

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_203134_numTimesteps_3616000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 3620000
Best mean reward: -0.87 - Last mean reward per episode: -0.90
Num timesteps: 3624000
Best mean reward: -0.87 - Last mean reward per episode: -0.91
Num timesteps: 3628000
Best mean reward: -0.87 - Last mean reward per episode: -0.90
Num timesteps: 3632000
Best mean reward: -0.87 - Last mean reward per episode: -0.90
Num timesteps: 3636000
Best mean reward: -0.87 - Last mean reward per episode: -0.90
Num timesteps: 3640000
Best mean reward: -0.87 - Last mean reward per episode: -0.91
Num timesteps: 3644000
Best mean reward: -0.87 - Last mean reward per episode: -0.90
Num timesteps: 3648000
Best mean reward: -0.87 - Last mean reward per episode: -0.90
Num timesteps: 3652000
Best mean reward: -0.87 - Last mean reward per episode: -0.91
Num timesteps: 3656000
Best mean reward: -0.87 - Last mean reward per episode: -0.90
Num timesteps: 3660000
Best mean reward: -0.87 - Last mean reward per episode: -0.89
Num timesteps: 3664000
Best mean reward: -0.87 - Last mean reward per episode: -0.90
Num timesteps: 3668000
Best mean reward: -0.87 - Last mean reward per episode: -0.91
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 50          |
|    ep_rew_mean          | -0.9        |
| time/                   |             |
|    fps                  | 487         |
|    iterations           | 56          |
|    time_elapsed         | 7529        |
|    total_timesteps      | 3670016     |
| train/                  |             |
|    approx_kl            | 0.035478473 |
|    clip_fraction        | 0.737       |
|    clip_range           | 0.075       |
|    entropy_loss         | 0.314       |
|    explained_variance   | 0.784       |
|    learning_rate        | 0.00358     |
|    loss                 | 0.0266      |
|    n_updates            | 385         |
|    policy_gradient_loss | 0.0249      |
|    std                  | 0.218       |
|    value_loss           | 0.000299    |
-----------------------------------------
Num timesteps: 3672000
Best mean reward: -0.87 - Last mean reward per episode: -0.88
Num timesteps: 3676000
Best mean reward: -0.87 - Last mean reward per episode: -0.87
Num timesteps: 3680000
Best mean reward: -0.87 - Last mean reward per episode: -0.85

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_203348_numTimesteps_3680000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 3684000
Best mean reward: -0.85 - Last mean reward per episode: -0.85
Num timesteps: 3688000
Best mean reward: -0.85 - Last mean reward per episode: -0.83

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_203401_numTimesteps_3688000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 3692000
Best mean reward: -0.83 - Last mean reward per episode: -0.82

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_203407_numTimesteps_3692000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 3696000
Best mean reward: -0.82 - Last mean reward per episode: -0.87
Num timesteps: 3700000
Best mean reward: -0.82 - Last mean reward per episode: -0.84
Num timesteps: 3704000
Best mean reward: -0.82 - Last mean reward per episode: -0.87
Num timesteps: 3708000
Best mean reward: -0.82 - Last mean reward per episode: -0.87
Num timesteps: 3712000
Best mean reward: -0.82 - Last mean reward per episode: -0.87
Num timesteps: 3716000
Best mean reward: -0.82 - Last mean reward per episode: -0.83
Num timesteps: 3720000
Best mean reward: -0.82 - Last mean reward per episode: -0.86
Num timesteps: 3724000
Best mean reward: -0.82 - Last mean reward per episode: -0.86
Num timesteps: 3728000
Best mean reward: -0.82 - Last mean reward per episode: -0.87
Num timesteps: 3732000
Best mean reward: -0.82 - Last mean reward per episode: -0.87
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 50         |
|    ep_rew_mean          | -0.857     |
| time/                   |            |
|    fps                  | 488        |
|    iterations           | 57         |
|    time_elapsed         | 7651       |
|    total_timesteps      | 3735552    |
| train/                  |            |
|    approx_kl            | 0.06799955 |
|    clip_fraction        | 0.749      |
|    clip_range           | 0.075      |
|    entropy_loss         | 0.467      |
|    explained_variance   | 0.81       |
|    learning_rate        | 0.00358    |
|    loss                 | 0.0314     |
|    n_updates            | 392        |
|    policy_gradient_loss | 0.0315     |
|    std                  | 0.207      |
|    value_loss           | 0.000257   |
----------------------------------------
Num timesteps: 3736000
Best mean reward: -0.82 - Last mean reward per episode: -0.85
Num timesteps: 3740000
Best mean reward: -0.82 - Last mean reward per episode: -0.83
Num timesteps: 3744000
Best mean reward: -0.82 - Last mean reward per episode: -0.82

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_203546_numTimesteps_3744000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 3748000
Best mean reward: -0.82 - Last mean reward per episode: -0.81

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_203553_numTimesteps_3748000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 3752000
Best mean reward: -0.81 - Last mean reward per episode: -0.82
Num timesteps: 3756000
Best mean reward: -0.81 - Last mean reward per episode: -0.81

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_203606_numTimesteps_3756000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 3760000
Best mean reward: -0.81 - Last mean reward per episode: -0.81

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_203613_numTimesteps_3760000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 3764000
Best mean reward: -0.81 - Last mean reward per episode: -0.82
Num timesteps: 3768000
Best mean reward: -0.81 - Last mean reward per episode: -0.80

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_203628_numTimesteps_3768000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 3772000
Best mean reward: -0.80 - Last mean reward per episode: -0.80
Num timesteps: 3776000
Best mean reward: -0.80 - Last mean reward per episode: -0.81
Num timesteps: 3780000
Best mean reward: -0.80 - Last mean reward per episode: -0.82
Num timesteps: 3784000
Best mean reward: -0.80 - Last mean reward per episode: -0.81
Num timesteps: 3788000
Best mean reward: -0.80 - Last mean reward per episode: -0.81
Num timesteps: 3792000
Best mean reward: -0.80 - Last mean reward per episode: -0.83
Num timesteps: 3796000
Best mean reward: -0.80 - Last mean reward per episode: -0.84
Num timesteps: 3800000
Best mean reward: -0.80 - Last mean reward per episode: -0.82
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 50          |
|    ep_rew_mean          | -0.807      |
| time/                   |             |
|    fps                  | 488         |
|    iterations           | 58          |
|    time_elapsed         | 7779        |
|    total_timesteps      | 3801088     |
| train/                  |             |
|    approx_kl            | 0.034492373 |
|    clip_fraction        | 0.755       |
|    clip_range           | 0.075       |
|    entropy_loss         | 0.625       |
|    explained_variance   | 0.823       |
|    learning_rate        | 0.00358     |
|    loss                 | 0.0248      |
|    n_updates            | 399         |
|    policy_gradient_loss | 0.0315      |
|    std                  | 0.197       |
|    value_loss           | 0.000233    |
-----------------------------------------
Num timesteps: 3804000
Best mean reward: -0.80 - Last mean reward per episode: -0.81
Num timesteps: 3808000
Best mean reward: -0.80 - Last mean reward per episode: -0.79

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_203754_numTimesteps_3808000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 3812000
Best mean reward: -0.79 - Last mean reward per episode: -0.80
Num timesteps: 3816000
Best mean reward: -0.79 - Last mean reward per episode: -0.80
Num timesteps: 3820000
Best mean reward: -0.79 - Last mean reward per episode: -0.80
Num timesteps: 3824000
Best mean reward: -0.79 - Last mean reward per episode: -0.80
Num timesteps: 3828000
Best mean reward: -0.79 - Last mean reward per episode: -0.81
Num timesteps: 3832000
Best mean reward: -0.79 - Last mean reward per episode: -0.79

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_203837_numTimesteps_3832000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 3836000
Best mean reward: -0.79 - Last mean reward per episode: -0.79

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_203844_numTimesteps_3836000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 3840000
Best mean reward: -0.79 - Last mean reward per episode: -0.80
Num timesteps: 3844000
Best mean reward: -0.79 - Last mean reward per episode: -0.79

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_203900_numTimesteps_3844000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 3848000
Best mean reward: -0.79 - Last mean reward per episode: -0.79
Num timesteps: 3852000
Best mean reward: -0.79 - Last mean reward per episode: -0.82
Num timesteps: 3856000
Best mean reward: -0.79 - Last mean reward per episode: -0.80
Num timesteps: 3860000
Best mean reward: -0.79 - Last mean reward per episode: -0.82
Num timesteps: 3864000
Best mean reward: -0.79 - Last mean reward per episode: -0.80
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 50          |
|    ep_rew_mean          | -0.779      |
| time/                   |             |
|    fps                  | 488         |
|    iterations           | 59          |
|    time_elapsed         | 7917        |
|    total_timesteps      | 3866624     |
| train/                  |             |
|    approx_kl            | 0.050313905 |
|    clip_fraction        | 0.783       |
|    clip_range           | 0.075       |
|    entropy_loss         | 0.806       |
|    explained_variance   | 0.843       |
|    learning_rate        | 0.00358     |
|    loss                 | 0.0326      |
|    n_updates            | 406         |
|    policy_gradient_loss | 0.0392      |
|    std                  | 0.187       |
|    value_loss           | 0.000227    |
-----------------------------------------
Num timesteps: 3868000
Best mean reward: -0.79 - Last mean reward per episode: -0.78

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_204001_numTimesteps_3868000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 3872000
Best mean reward: -0.78 - Last mean reward per episode: -0.79
Num timesteps: 3876000
Best mean reward: -0.78 - Last mean reward per episode: -0.79
Num timesteps: 3880000
Best mean reward: -0.78 - Last mean reward per episode: -0.80
Num timesteps: 3884000
Best mean reward: -0.78 - Last mean reward per episode: -0.79
Num timesteps: 3888000
Best mean reward: -0.78 - Last mean reward per episode: -0.79
Num timesteps: 3892000
Best mean reward: -0.78 - Last mean reward per episode: -0.79
Num timesteps: 3896000
Best mean reward: -0.78 - Last mean reward per episode: -0.79
Num timesteps: 3900000
Best mean reward: -0.78 - Last mean reward per episode: -0.79
Num timesteps: 3904000
Best mean reward: -0.78 - Last mean reward per episode: -0.79
Num timesteps: 3908000
Best mean reward: -0.78 - Last mean reward per episode: -0.78
Num timesteps: 3912000
Best mean reward: -0.78 - Last mean reward per episode: -0.79
Num timesteps: 3916000
Best mean reward: -0.78 - Last mean reward per episode: -0.79
Num timesteps: 3920000
Best mean reward: -0.78 - Last mean reward per episode: -0.79
Num timesteps: 3924000
Best mean reward: -0.78 - Last mean reward per episode: -0.79
Num timesteps: 3928000
Best mean reward: -0.78 - Last mean reward per episode: -0.79
Num timesteps: 3932000
Best mean reward: -0.78 - Last mean reward per episode: -0.80
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 50         |
|    ep_rew_mean          | -0.797     |
| time/                   |            |
|    fps                  | 487        |
|    iterations           | 60         |
|    time_elapsed         | 8061       |
|    total_timesteps      | 3932160    |
| train/                  |            |
|    approx_kl            | 0.07969197 |
|    clip_fraction        | 0.782      |
|    clip_range           | 0.075      |
|    entropy_loss         | 0.949      |
|    explained_variance   | 0.839      |
|    learning_rate        | 0.00358    |
|    loss                 | 0.0813     |
|    n_updates            | 413        |
|    policy_gradient_loss | 0.0452     |
|    std                  | 0.177      |
|    value_loss           | 0.000224   |
----------------------------------------
Num timesteps: 3936000
Best mean reward: -0.78 - Last mean reward per episode: -0.74

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_204234_numTimesteps_3936000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 3940000
Best mean reward: -0.74 - Last mean reward per episode: -0.74
Num timesteps: 3944000
Best mean reward: -0.74 - Last mean reward per episode: -0.71

 Most recently saved model:  /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/model_20220630_204251_numTimesteps_3944000 

/home/hjkwon/anaconda3/envs/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Saving new best model to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/callback/best_model
Num timesteps: 3948000
Best mean reward: -0.71 - Last mean reward per episode: -0.71
Num timesteps: 3952000
Best mean reward: -0.71 - Last mean reward per episode: -0.74
Num timesteps: 3956000
Best mean reward: -0.71 - Last mean reward per episode: -0.73
Num timesteps: 3960000
Best mean reward: -0.71 - Last mean reward per episode: -0.72
Num timesteps: 3964000
Best mean reward: -0.71 - Last mean reward per episode: -0.72
Num timesteps: 3968000
Best mean reward: -0.71 - Last mean reward per episode: -0.74
Num timesteps: 3972000
Best mean reward: -0.71 - Last mean reward per episode: -0.74
Num timesteps: 3976000
Best mean reward: -0.71 - Last mean reward per episode: -0.75
Num timesteps: 3980000
Best mean reward: -0.71 - Last mean reward per episode: -0.75
Num timesteps: 3984000
Best mean reward: -0.71 - Last mean reward per episode: -0.74
Num timesteps: 3988000
Best mean reward: -0.71 - Last mean reward per episode: -0.75
Num timesteps: 3992000
Best mean reward: -0.71 - Last mean reward per episode: -0.76
Num timesteps: 3996000
Best mean reward: -0.71 - Last mean reward per episode: -0.74
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 50         |
|    ep_rew_mean          | -0.73      |
| time/                   |            |
|    fps                  | 486        |
|    iterations           | 61         |
|    time_elapsed         | 8212       |
|    total_timesteps      | 3997696    |
| train/                  |            |
|    approx_kl            | 0.07306552 |
|    clip_fraction        | 0.795      |
|    clip_range           | 0.075      |
|    entropy_loss         | 1.11       |
|    explained_variance   | 0.856      |
|    learning_rate        | 0.00358    |
|    loss                 | 0.0487     |
|    n_updates            | 420        |
|    policy_gradient_loss | 0.0441     |
|    std                  | 0.167      |
|    value_loss           | 0.000179   |
----------------------------------------
Num timesteps: 4000000
Best mean reward: -0.71 - Last mean reward per episode: -0.78
Num timesteps: 4004000
Best mean reward: -0.71 - Last mean reward per episode: -0.76
Num timesteps: 4008000
Best mean reward: -0.71 - Last mean reward per episode: -0.76
Num timesteps: 4012000
Best mean reward: -0.71 - Last mean reward per episode: -0.75
Num timesteps: 4016000
Best mean reward: -0.71 - Last mean reward per episode: -0.81
Num timesteps: 4020000
Best mean reward: -0.71 - Last mean reward per episode: -0.77
Num timesteps: 4024000
Best mean reward: -0.71 - Last mean reward per episode: -0.78
Num timesteps: 4028000
Best mean reward: -0.71 - Last mean reward per episode: -0.77
Num timesteps: 4032000
Best mean reward: -0.71 - Last mean reward per episode: -0.78
Num timesteps: 4036000
Best mean reward: -0.71 - Last mean reward per episode: -0.79
Num timesteps: 4040000
Best mean reward: -0.71 - Last mean reward per episode: -0.80
Num timesteps: 4044000
Best mean reward: -0.71 - Last mean reward per episode: -0.78
Num timesteps: 4048000
Best mean reward: -0.71 - Last mean reward per episode: -0.76
Num timesteps: 4052000
Best mean reward: -0.71 - Last mean reward per episode: -0.78
Num timesteps: 4056000
Best mean reward: -0.71 - Last mean reward per episode: -0.76
Num timesteps: 4060000
Best mean reward: -0.71 - Last mean reward per episode: -0.74
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 50        |
|    ep_rew_mean          | -0.766    |
| time/                   |           |
|    fps                  | 486       |
|    iterations           | 62        |
|    time_elapsed         | 8351      |
|    total_timesteps      | 4063232   |
| train/                  |           |
|    approx_kl            | 1.0874116 |
|    clip_fraction        | 0.861     |
|    clip_range           | 0.075     |
|    entropy_loss         | 1.34      |
|    explained_variance   | 0.877     |
|    learning_rate        | 0.00358   |
|    loss                 | 0.0932    |
|    n_updates            | 427       |
|    policy_gradient_loss | 0.109     |
|    std                  | 0.154     |
|    value_loss           | 0.000169  |
---------------------------------------
starting video recorder: 
argv[0]=--background_color_red=0.8745098039215686
argv[1]=
argv[2]=
argv[3]=
argv[4]=
argv[5]=
argv[6]=
argv[7]=
argv[8]=
argv[9]=
argv[10]=
argv[11]=
argv[12]=
argv[13]=
argv[14]=
argv[15]=
argv[16]=
argv[17]=
argv[18]=
argv[19]=
argv[20]=
argv[21]=--background_color_green=0.21176470588235294
argv[22]=
argv[23]=
argv[24]=
argv[25]=
argv[26]=
argv[27]=
argv[28]=
argv[29]=
argv[30]=
argv[31]=
argv[32]=
argv[33]=
argv[34]=
argv[35]=
argv[36]=
argv[37]=
argv[38]=
argv[39]=
argv[40]=
argv[41]=
argv[42]=--background_color_blue=0.17647058823529413
startThreads creating 1 threads.
starting thread 0
started thread 0 
argc=45
argv[0] = --unused
argv[1] = --background_color_red=0.8745098039215686
argv[2] = 
argv[3] = 
argv[4] = 
argv[5] = 
argv[6] = 
argv[7] = 
argv[8] = 
argv[9] = 
argv[10] = 
argv[11] = 
argv[12] = 
argv[13] = 
argv[14] = 
argv[15] = 
argv[16] = 
argv[17] = 
argv[18] = 
argv[19] = 
argv[20] = 
argv[21] = 
argv[22] = --background_color_green=0.21176470588235294
argv[23] = 
argv[24] = 
argv[25] = 
argv[26] = 
argv[27] = 
argv[28] = 
argv[29] = 
argv[30] = 
argv[31] = 
argv[32] = 
argv[33] = 
argv[34] = 
argv[35] = 
argv[36] = 
argv[37] = 
argv[38] = 
argv[39] = 
argv[40] = 
argv[41] = 
argv[42] = 
argv[43] = --background_color_blue=0.17647058823529413
argv[44] = --start_demo_name=Physics Server
ExampleBrowserThreadFunc started
X11 functions dynamically loaded using dlopen/dlsym OK!
X11 functions dynamically loaded using dlopen/dlsym OK!
Creating context
Created GL 3.3 context
Direct GLX rendering context obtained
Making context current
GL_VENDOR=NVIDIA Corporation
GL_RENDERER=NVIDIA GeForce RTX 3070/PCIe/SSE2
GL_VERSION=3.3.0 NVIDIA 470.82.00
GL_SHADING_LANGUAGE_VERSION=3.30 NVIDIA via Cg compiler
pthread_getconcurrency()=0
Version = 3.3.0 NVIDIA 470.82.00
Vendor = NVIDIA Corporation
Renderer = NVIDIA GeForce RTX 3070/PCIe/SSE2
b3Printf: Selected demo: Physics Server
startThreads creating 1 threads.
starting thread 0
started thread 0 
MotionThreadFunc thread started
ven = NVIDIA Corporation
ven = NVIDIA Corporation
Saving video to /home/hjkwon/Documents/Panda-Robot-RL-Control-with-RGBD-Sensor/vectTraining/2022-06-30/PPOReach/videos/PPO-PandaGrasp-step-0-to-step-400.mp4
numActiveThreads = 0
stopping threads
Thread with taskId 0 exiting
Thread TERMINATED
destroy semaphore
semaphore destroyed
destroy main semaphore
main semaphore destroyed
finished
numActiveThreads = 0
btShutDownExampleBrowser sto